{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"P\u00e1gina Inicial","text":"<p>Apresentamos o Ultralytics YOLOv8, a mais recente vers\u00e3o do aclamado modelo de detec\u00e7\u00e3o de objetos em tempo real e segmenta\u00e7\u00e3o de imagens. O YOLOv8 \u00e9 baseado nos mais recentes avan\u00e7os do aprendizado profundo e vis\u00e3o computacional, oferecendo um desempenho sem paralelo em termos de velocidade e precis\u00e3o. Seu design simplificado o torna adequado para v\u00e1rias aplica\u00e7\u00f5es e facilmente adapt\u00e1vel a diferentes plataformas de hardware, desde dispositivos de borda at\u00e9 APIs na nuvem.</p> <p>Explore os Documentos do YOLOv8, um recurso abrangente projetado para ajud\u00e1-lo a entender e utilizar suas caracter\u00edsticas e capacidades. Seja voc\u00ea um praticante experiente de aprendizado de m\u00e1quina ou novo no campo, este hub tem como objetivo maximizar o potencial do YOLOv8 em seus projetos</p> <p>Nota</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o em v\u00e1rios idiomas est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando arduamente para aprimor\u00e1-la. Agradecemos sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"#por-onde-comecar","title":"Por Onde Come\u00e7ar","text":"<ul> <li>Instalar <code>ultralytics</code> com pip e come\u00e7ar a funcionar em minutos \u00a0  Come\u00e7ar</li> <li>Prever novas imagens e v\u00eddeos com o YOLOv8 \u00a0  Prever em Imagens</li> <li>Treinar um novo modelo YOLOv8 em seu pr\u00f3prio conjunto de dados personalizado \u00a0  Treinar um Modelo</li> <li>Explorar tarefas do YOLOv8 como segmentar, classificar, estimar pose e rastrear \u00a0  Explorar Tarefas</li> </ul> <p> Assistir: Como Treinar um Modelo YOLOv8 em Seu Conjunto de Dados Personalizado no Google Colab. </p>"},{"location":"#yolo-uma-breve-historia","title":"YOLO: Uma Breve Hist\u00f3ria","text":"<p>YOLO (You Only Look Once), um popular modelo de detec\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o de imagens, foi desenvolvido por Joseph Redmon e Ali Farhadi na Universidade de Washington. Lan\u00e7ado em 2015, o YOLO rapidamente ganhou popularidade por sua alta velocidade e precis\u00e3o.</p> <ul> <li>YOLOv2, lan\u00e7ado em 2016, aprimorou o modelo original incorporando normaliza\u00e7\u00e3o em lote, caixas \u00e2ncora e aglomerados dimensionais.</li> <li>YOLOv3, lan\u00e7ado em 2018, melhorou ainda mais o desempenho do modelo usando uma rede dorsal mais eficiente, m\u00faltiplas \u00e2ncoras e pooling piramidal espacial.</li> <li>YOLOv4 foi lan\u00e7ado em 2020, introduzindo inova\u00e7\u00f5es como a amplia\u00e7\u00e3o de dados Mosaic, uma nova cabe\u00e7a de detec\u00e7\u00e3o sem \u00e2ncoras e uma nova fun\u00e7\u00e3o de perda.</li> <li>YOLOv5 melhorou ainda mais o desempenho do modelo e adicionou novos recursos, como otimiza\u00e7\u00e3o de hiperpar\u00e2metros, rastreamento integrado de experimentos e exporta\u00e7\u00e3o autom\u00e1tica para formatos de exporta\u00e7\u00e3o populares.</li> <li>YOLOv6 foi disponibilizado em c\u00f3digo aberto por Meituan em 2022 e est\u00e1 em uso em muitos dos rob\u00f4s aut\u00f4nomos de entrega da empresa.</li> <li>YOLOv7 adicionou tarefas adicionais, como estimativa de pose no conjunto de dados de keypoints COCO.</li> <li>YOLOv8, a mais recente vers\u00e3o do YOLO pela Ultralytics. Como um modelo de \u00faltima gera\u00e7\u00e3o, o YOLOv8 baseia-se no sucesso das vers\u00f5es anteriores, introduzindo novos recursos e melhorias para desempenho, flexibilidade e efici\u00eancia aprimorados. O YOLOv8 suporta uma gama completa de tarefas de IA de vis\u00e3o, incluindo detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, estimativa de pose, rastreamento e classifica\u00e7\u00e3o. Essa versatilidade permite que os usu\u00e1rios aproveitem as capacidades do YOLOv8 em diversas aplica\u00e7\u00f5es e dom\u00ednios.</li> </ul>"},{"location":"#licencas-yolo-como-o-yolo-da-ultralytics-e-licenciado","title":"Licen\u00e7as YOLO: Como o YOLO da Ultralytics \u00e9 licenciado?","text":"<p>A Ultralytics oferece duas op\u00e7\u00f5es de licen\u00e7a para acomodar casos de uso diversos:</p> <ul> <li>Licen\u00e7a AGPL-3.0: Essa licen\u00e7a de c\u00f3digo aberto aprovada pela OSI \u00e9 ideal para estudantes e entusiastas, promovendo colabora\u00e7\u00e3o aberta e compartilhamento de conhecimento. Veja o arquivo LICENSE para mais detalhes.</li> <li>Licen\u00e7a Empresarial: Projetada para uso comercial, esta licen\u00e7a permite a integra\u00e7\u00e3o perfeita do software Ultralytics e modelos de IA em bens e servi\u00e7os comerciais, contornando os requisitos de c\u00f3digo aberto da AGPL-3.0. Se o seu cen\u00e1rio envolver a incorpora\u00e7\u00e3o de nossas solu\u00e7\u00f5es em uma oferta comercial, entre em contato atrav\u00e9s do Licenciamento da Ultralytics.</li> </ul> <p>Nossa estrat\u00e9gia de licenciamento \u00e9 projetada para garantir que qualquer melhoria em nossos projetos de c\u00f3digo aberto retorne \u00e0 comunidade. Mantemos os princ\u00edpios de c\u00f3digo aberto pr\u00f3ximos ao nosso cora\u00e7\u00e3o \u2764\ufe0f, e nossa miss\u00e3o \u00e9 garantir que nossas contribui\u00e7\u00f5es possam ser utilizadas e expandidas de formas que beneficiem todos.</p>"},{"location":"quickstart/","title":"In\u00edcio R\u00e1pido","text":""},{"location":"quickstart/#instalacao-do-ultralytics","title":"Instala\u00e7\u00e3o do Ultralytics","text":"<p>O Ultralytics oferece diversos m\u00e9todos de instala\u00e7\u00e3o, incluindo pip, conda e Docker. Instale o YOLOv8 atrav\u00e9s do pacote <code>ultralytics</code> pip para a vers\u00e3o est\u00e1vel mais recente ou clonando o reposit\u00f3rio GitHub do Ultralytics para obter a vers\u00e3o mais atualizada. O Docker pode ser usado para executar o pacote em um cont\u00eainer isolado, evitando a instala\u00e7\u00e3o local.</p> <p>Instalar</p> Pip install (recomendado)Conda installGit clone <p>Instale o pacote <code>ultralytics</code> usando pip, ou atualize uma instala\u00e7\u00e3o existente executando <code>pip install -U ultralytics</code>. Visite o \u00cdndice de Pacotes Python (PyPI) para mais detalhes sobre o pacote <code>ultralytics</code>: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Instalar o pacote ultralytics do PyPI\npip install ultralytics\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode instalar o pacote <code>ultralytics</code> diretamente do reposit\u00f3rio GitHub. Isso pode ser \u00fatil se voc\u00ea desejar a vers\u00e3o de desenvolvimento mais recente. Certifique-se de ter a ferramenta de linha de comando Git instalada no seu sistema. O comando <code>@main</code> instala a branch <code>main</code> e pode ser modificado para outra branch, ou seja, <code>@my-branch</code>, ou removido completamente para padr\u00e3o na branch <code>main</code>.</p> <pre><code># Instalar o pacote ultralytics do GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda \u00e9 um gerenciador de pacotes alternativo ao pip que tamb\u00e9m pode ser usado para instala\u00e7\u00e3o. Visite Anaconda para mais detalhes em https://anaconda.org/conda-forge/ultralytics. O reposit\u00f3rio de feedstock do Ultralytics para atualizar o pacote conda est\u00e1 em https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Instalar o pacote ultralytics usando conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Nota</p> <p>Se voc\u00ea est\u00e1 instalando em um ambiente CUDA a pr\u00e1tica recomendada \u00e9 instalar <code>ultralytics</code>, <code>pytorch</code> e <code>pytorch-cuda</code> no mesmo comando para permitir que o gerenciador de pacotes conda resolva quaisquer conflitos, ou instalar <code>pytorch-cuda</code> por \u00faltimo para permitir que ele substitua o pacote espec\u00edfico para CPU <code>pytorch</code>, se necess\u00e1rio. <pre><code># Instalar todos os pacotes juntos usando conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clone o reposit\u00f3rio <code>ultralytics</code> se voc\u00ea est\u00e1 interessado em contribuir para o desenvolvimento ou deseja experimentar com o c\u00f3digo-fonte mais recente. Ap\u00f3s clonar, navegue at\u00e9 o diret\u00f3rio e instale o pacote em modo edit\u00e1vel <code>-e</code> usando pip. <pre><code># Clonar o reposit\u00f3rio ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navegar para o diret\u00f3rio clonado\ncd ultralytics\n\n# Instalar o pacote em modo edit\u00e1vel para desenvolvimento\npip install -e .\n</code></pre></p> <p>Veja o arquivo requirements.txt do <code>ultralytics</code> para uma lista de depend\u00eancias. Note que todos os exemplos acima instalam todas as depend\u00eancias necess\u00e1rias.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Dica</p> <p>Os requisitos do PyTorch variam pelo sistema operacional e pelos requisitos de CUDA, ent\u00e3o \u00e9 recomendado instalar o PyTorch primeiro seguindo as instru\u00e7\u00f5es em https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#imagem-docker-conda","title":"Imagem Docker Conda","text":"<p>As imagens Docker Conda do Ultralytics tamb\u00e9m est\u00e3o dispon\u00edveis em DockerHub. Estas imagens s\u00e3o baseadas em Miniconda3 e s\u00e3o um modo simples de come\u00e7ar a usar <code>ultralytics</code> em um ambiente Conda.</p> <pre><code># Definir o nome da imagem como uma vari\u00e1vel\nt=ultralytics/ultralytics:latest-conda\n\n# Puxar a imagem mais recente do ultralytics do Docker Hub\nsudo docker pull $t\n\n# Executar a imagem ultralytics em um cont\u00eainer com suporte a GPU\nsudo docker run -it --ipc=host --gpus all $t  # todas as GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # especificar GPUs\n</code></pre>"},{"location":"quickstart/#use-o-ultralytics-com-cli","title":"Use o Ultralytics com CLI","text":"<p>A interface de linha de comando (CLI) do Ultralytics permite comandos simples de uma \u00fanica linha sem a necessidade de um ambiente Python. O CLI n\u00e3o requer personaliza\u00e7\u00e3o ou c\u00f3digo Python. Voc\u00ea pode simplesmente rodar todas as tarefas do terminal com o comando <code>yolo</code>. Confira o Guia CLI para aprender mais sobre o uso do YOLOv8 pela linha de comando.</p> <p>Exemplo</p> SintaxeTrainPredictValExportSpecial <p>Os comandos <code>yolo</code> do Ultralytics usam a seguinte sintaxe: <pre><code>yolo TAREFA MODO ARGUMENTOS\n\nOnde   TAREFA (opcional) \u00e9 um entre [detect, segment, classify]\n        MODO (obrigat\u00f3rio) \u00e9 um entre [train, val, predict, export, track]\n        ARGUMENTOS (opcional) s\u00e3o qualquer n\u00famero de pares personalizados 'arg=valor' como 'imgsz=320' que substituem os padr\u00f5es.\n</code></pre> Veja todos os ARGUMENTOS no guia completo de Configura\u00e7\u00e3o ou com <code>yolo cfg</code></p> <p>Treinar um modelo de detec\u00e7\u00e3o por 10 \u00e9pocas com uma taxa de aprendizado inicial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Prever um v\u00eddeo do YouTube usando um modelo de segmenta\u00e7\u00e3o pr\u00e9-treinado com tamanho de imagem 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validar um modelo de detec\u00e7\u00e3o pr\u00e9-treinado com tamanho de lote 1 e tamanho de imagem 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportar um modelo de classifica\u00e7\u00e3o YOLOv8n para formato ONNX com tamanho de imagem 224 por 128 (nenhuma TAREFA necess\u00e1ria) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Executar comandos especiais para ver vers\u00e3o, visualizar configura\u00e7\u00f5es, rodar verifica\u00e7\u00f5es e mais: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Aviso</p> <p>Argumentos devem ser passados como pares <code>arg=valor</code>, separados por um sinal de igual <code>=</code> e delimitados por espa\u00e7os <code></code> entre pares. N\u00e3o use prefixos de argumentos <code>--</code> ou v\u00edrgulas <code>,</code> entre os argumentos.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Guia CLI</p>"},{"location":"quickstart/#use-o-ultralytics-com-python","title":"Use o Ultralytics com Python","text":"<p>A interface Python do YOLOv8 permite uma integra\u00e7\u00e3o tranquila em seus projetos Python, tornando f\u00e1cil carregar, executar e processar a sa\u00edda do modelo. Projetada com simplicidade e facilidade de uso em mente, a interface Python permite que os usu\u00e1rios implementem rapidamente detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e classifica\u00e7\u00e3o em seus projetos. Isto torna a interface Python do YOLOv8 uma ferramenta inestim\u00e1vel para qualquer pessoa buscando incorporar essas funcionalidades em seus projetos Python.</p> <p>Por exemplo, os usu\u00e1rios podem carregar um modelo, trein\u00e1-lo, avaliar o seu desempenho em um conjunto de valida\u00e7\u00e3o e at\u00e9 export\u00e1-lo para o formato ONNX com apenas algumas linhas de c\u00f3digo. Confira o Guia Python para aprender mais sobre o uso do YOLOv8 dentro dos seus projetos Python.</p> <p>Exemplo</p> <pre><code>from ultralytics import YOLO\n\n# Criar um novo modelo YOLO do zero\nmodel = YOLO('yolov8n.yaml')\n\n# Carregar um modelo YOLO pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.pt')\n\n# Treinar o modelo usando o conjunto de dados 'coco128.yaml' por 3 \u00e9pocas\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Avaliar o desempenho do modelo no conjunto de valida\u00e7\u00e3o\nresults = model.val()\n\n# Realizar detec\u00e7\u00e3o de objetos em uma imagem usando o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportar o modelo para formato ONNX\nsuccess = model.export(format='onnx')\n</code></pre> <p>Guia Python</p>"},{"location":"datasets/","title":"Vis\u00e3o Geral de Conjuntos de Dados","text":"<p>A Ultralytics oferece suporte para diversos conjuntos de dados para facilitar tarefas de vis\u00e3o computacional, como detec\u00e7\u00e3o, segmenta\u00e7\u00e3o de inst\u00e2ncia, estimativa de pose, classifica\u00e7\u00e3o e rastreamento de m\u00faltiplos objetos. Abaixo est\u00e1 uma lista dos principais conjuntos de dados da Ultralytics, seguidos por um resumo de cada tarefa de vis\u00e3o computacional e os respectivos conjuntos de dados.</p> <p>Nota</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o multil\u00edngue est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando arduamente para melhor\u00e1-la. Obrigado pela sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"datasets/#conjuntos-de-dados-de-deteccao","title":"Conjuntos de Dados de Detec\u00e7\u00e3o","text":"<p>A t\u00e9cnica de detec\u00e7\u00e3o de objetos com caixas delimitadoras envolve detectar e localizar objetos em uma imagem desenhando uma caixa delimitadora ao redor de cada objeto.</p> <ul> <li>Argoverse: Um conjunto de dados contendo dados de rastreamento 3D e previs\u00e3o de movimento de ambientes urbanos com anota\u00e7\u00f5es detalhadas.</li> <li>COCO: Um conjunto de dados em grande escala projetado para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e legendagem com mais de 200 mil imagens etiquetadas.</li> <li>COCO8: Cont\u00e9m as primeiras 4 imagens do COCO train e COCO val, adequado para testes r\u00e1pidos.</li> <li>Global Wheat 2020: Um conjunto de dados de imagens de espiga de trigo coletadas ao redor do mundo para tarefas de detec\u00e7\u00e3o e localiza\u00e7\u00e3o de objetos.</li> <li>Objects365: Um conjunto de dados de alta qualidade de grande escala para detec\u00e7\u00e3o de objetos com 365 categorias e mais de 600 mil imagens anotadas.</li> <li>OpenImagesV7: Um conjunto de dados abrangente do Google com 1,7 milh\u00e3o de imagens de treino e 42 mil imagens de valida\u00e7\u00e3o.</li> <li>SKU-110K: Um conjunto de dados apresentando detec\u00e7\u00e3o de objetos densos em ambientes de varejo com mais de 11 mil imagens e 1,7 milh\u00e3o de caixas delimitadoras.</li> <li>VisDrone: Um conjunto de dados que cont\u00e9m informa\u00e7\u00e3o de detec\u00e7\u00e3o de objetos e rastreamento de m\u00faltiplos objetos a partir de imagens capturadas por drones com mais de 10 mil imagens e sequ\u00eancias de v\u00eddeo.</li> <li>VOC: O conjunto de dados Visual Object Classes (VOC) Pascal para detec\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o com 20 classes de objetos e mais de 11 mil imagens.</li> <li>xView: Um conjunto de dados para detec\u00e7\u00e3o de objetos em imagens a\u00e9reas com 60 categorias de objetos e mais de 1 milh\u00e3o de objetos anotados.</li> </ul>"},{"location":"datasets/#conjuntos-de-dados-de-segmentacao-de-instancia","title":"Conjuntos de Dados de Segmenta\u00e7\u00e3o de Inst\u00e2ncia","text":"<p>A segmenta\u00e7\u00e3o de inst\u00e2ncia \u00e9 uma t\u00e9cnica de vis\u00e3o computacional que identifica e localiza objetos em uma imagem ao n\u00edvel de pixel.</p> <ul> <li>COCO: Um conjunto de dados em grande escala projetado para detec\u00e7\u00e3o de objetos, tarefas de segmenta\u00e7\u00e3o e legendagem com mais de 200 mil imagens etiquetadas.</li> <li>COCO8-seg: Um conjunto de dados menor para tarefas de segmenta\u00e7\u00e3o de inst\u00e2ncias, contendo um subconjunto de 8 imagens COCO com anota\u00e7\u00f5es de segmenta\u00e7\u00e3o.</li> </ul>"},{"location":"datasets/#estimativa-de-pose","title":"Estimativa de Pose","text":"<p>A estimativa de pose \u00e9 uma t\u00e9cnica usada para determinar a pose do objeto em rela\u00e7\u00e3o \u00e0 c\u00e2mera ou ao sistema de coordenadas do mundo.</p> <ul> <li>COCO: Um conjunto de dados em grande escala com anota\u00e7\u00f5es de pose humana projetado para tarefas de estimativa de pose.</li> <li>COCO8-pose: Um conjunto de dados menor para tarefas de estimativa de pose, contendo um subconjunto de 8 imagens COCO com anota\u00e7\u00f5es de pose humana.</li> <li>Tiger-pose: Um conjunto de dados compacto consistindo de 263 imagens focadas em tigres, anotadas com 12 pontos-chave por tigre para tarefas de estimativa de pose.</li> </ul>"},{"location":"datasets/#classificacao","title":"Classifica\u00e7\u00e3o","text":"<p>Classifica\u00e7\u00e3o de imagens \u00e9 uma tarefa de vis\u00e3o computacional que envolve categorizar uma imagem em uma ou mais classes ou categorias predefinidas com base em seu conte\u00fado visual.</p> <ul> <li>Caltech 101: Um conjunto de dados contendo imagens de 101 categorias de objetos para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>Caltech 256: Uma vers\u00e3o estendida do Caltech 101 com 256 categorias de objetos e imagens mais desafiadoras.</li> <li>CIFAR-10: Um conjunto de dados de 60 mil imagens coloridas de 32x32 em 10 classes, com 6 mil imagens por classe.</li> <li>CIFAR-100: Uma vers\u00e3o estendida do CIFAR-10 com 100 categorias de objetos e 600 imagens por classe.</li> <li>Fashion-MNIST: Um conjunto de dados consistindo de 70 mil imagens em escala de cinza de 10 categorias de moda para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>ImageNet: Um conjunto de dados em grande escala para detec\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de imagens com mais de 14 milh\u00f5es de imagens e 20 mil categorias.</li> <li>ImageNet-10: Um subconjunto menor do ImageNet com 10 categorias para experimenta\u00e7\u00e3o e teste mais r\u00e1pidos.</li> <li>Imagenette: Um subconjunto menor do ImageNet que cont\u00e9m 10 classes facilmente distingu\u00edveis para treinamento e teste mais r\u00e1pidos.</li> <li>Imagewoof: Um subconjunto do ImageNet mais desafiador contendo 10 categorias de ra\u00e7as de c\u00e3es para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>MNIST: Um conjunto de dados de 70 mil imagens em escala de cinza de d\u00edgitos manuscritos para tarefas de classifica\u00e7\u00e3o de imagens.</li> </ul>"},{"location":"datasets/#caixas-delimitadoras-orientadas-obb","title":"Caixas Delimitadoras Orientadas (OBB)","text":"<p>As Caixas Delimitadoras Orientadas (OBB) \u00e9 um m\u00e9todo em vis\u00e3o computacional para detectar objetos angulados em imagens usando caixas delimitadoras rotacionadas, muitas vezes aplicado em imagens a\u00e9reas e de sat\u00e9lite.</p> <ul> <li>DOTAv2: Um popular conjunto de dados de imagens a\u00e9reas OBB com 1,7 milh\u00e3o de inst\u00e2ncias e 11.268 imagens.</li> </ul>"},{"location":"datasets/#rastreamento-de-multiplos-objetos","title":"Rastreamento de M\u00faltiplos Objetos","text":"<p>O rastreamento de m\u00faltiplos objetos \u00e9 uma t\u00e9cnica de vis\u00e3o computacional que envolve detectar e rastrear v\u00e1rios objetos ao longo do tempo em uma sequ\u00eancia de v\u00eddeo.</p> <ul> <li>Argoverse: Um conjunto de dados contendo dados de rastreamento 3D e previs\u00e3o de movimento de ambientes urbanos com anota\u00e7\u00f5es ricas para tarefas de rastreamento de m\u00faltiplos objetos.</li> <li>VisDrone: Um conjunto de dados que cont\u00e9m informa\u00e7\u00e3o de detec\u00e7\u00e3o de objetos e rastreamento de m\u00faltiplos objetos a partir de imagens capturadas por drones com mais de 10 mil imagens e sequ\u00eancias de v\u00eddeo.</li> </ul>"},{"location":"datasets/#contribuir-com-novos-conjuntos-de-dados","title":"Contribuir com Novos Conjuntos de Dados","text":"<p>Contribuir com um novo conjunto de dados envolve v\u00e1rias etapas para garantir que ele se alinhe bem com a infraestrutura existente. Abaixo est\u00e3o as etapas necess\u00e1rias:</p>"},{"location":"datasets/#etapas-para-contribuir-com-um-novo-conjunto-de-dados","title":"Etapas para Contribuir com um Novo Conjunto de Dados","text":"<ol> <li> <p>Coletar Imagens: Re\u00fana as imagens que pertencem ao conjunto de dados. Estas podem ser coletadas de v\u00e1rias fontes, como bancos de dados p\u00fablicos ou sua pr\u00f3pria cole\u00e7\u00e3o.</p> </li> <li> <p>Anotar Imagens: Anote essas imagens com caixas delimitadoras, segmentos ou pontos-chave, dependendo da tarefa.</p> </li> <li> <p>Exportar Anota\u00e7\u00f5es: Converta essas anota\u00e7\u00f5es no formato de arquivo *.txt YOLO que a Ultralytics suporta.</p> </li> <li> <p>Organizar Conjunto de Dados: Organize seu conjunto de dados na estrutura de pastas correta. Voc\u00ea deve ter diret\u00f3rios de topo <code>train/</code> e <code>val/</code>, e dentro de cada um, um subdiret\u00f3rio <code>images/</code> e <code>labels/</code>.</p> <pre><code>conjunto_de_dados/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Criar um Arquivo <code>data.yaml</code>: No diret\u00f3rio raiz do seu conjunto de dados, crie um arquivo <code>data.yaml</code> que descreva o conjunto de dados, as classes e outras informa\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>Otimizar Imagens (Opcional): Se voc\u00ea quiser reduzir o tamanho do conjunto de dados para um processamento mais eficiente, pode otimizar as imagens usando o c\u00f3digo abaixo. Isso n\u00e3o \u00e9 obrigat\u00f3rio, mas recomendado para tamanhos menores de conjunto de dados e velocidades de download mais r\u00e1pidas.</p> </li> <li> <p>Compactar Conjunto de Dados: Compacte toda a pasta do conjunto de dados em um arquivo zip.</p> </li> <li> <p>Documentar e PR: Crie uma p\u00e1gina de documenta\u00e7\u00e3o descrevendo seu conjunto de dados e como ele se encaixa no framework existente. Depois disso, submeta um Pull Request (PR). Consulte Diretrizes de Contribui\u00e7\u00e3o da Ultralytics para mais detalhes sobre como submeter um PR.</p> </li> </ol>"},{"location":"datasets/#exemplo-de-codigo-para-otimizar-e-compactar-um-conjunto-de-dados","title":"Exemplo de C\u00f3digo para Otimizar e Compactar um Conjunto de Dados","text":"<p>Otimizar e Compactar um Conjunto de Dados</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definir diret\u00f3rio do conjunto de dados\npath = Path('caminho/para/conjunto_de_dados')\n\n# Otimizar imagens no conjunto de dados (opcional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Compactar conjunto de dados em 'caminho/para/conjunto_de_dados.zip'\nzip_directory(path)\n</code></pre> <p>Seguindo esses passos, voc\u00ea poder\u00e1 contribuir com um novo conjunto de dados que se integra bem com a estrutura existente da Ultralytics.</p>"},{"location":"models/","title":"Modelos Suportados pela Ultralytics","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o de modelos da Ultralytics! Oferecemos suporte para uma ampla variedade de modelos, cada um adaptado para tarefas espec\u00edficas como detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o de inst\u00e2ncias, classifica\u00e7\u00e3o de imagens, estimativa de pose, e rastreamento de m\u00faltiplos objetos. Se voc\u00ea tem interesse em contribuir com sua arquitetura de modelo para a Ultralytics, confira nosso Guia de Contribui\u00e7\u00e3o.</p> <p>Nota</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o em v\u00e1rios idiomas est\u00e1 atualmente em constru\u00e7\u00e3o, e estamos trabalhando arduamente para melhor\u00e1-la. Agradecemos sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"models/#modelos-em-destaque","title":"Modelos em Destaque","text":"<p>Aqui est\u00e3o alguns dos principais modelos suportados:</p> <ol> <li>YOLOv3: A terceira itera\u00e7\u00e3o da fam\u00edlia de modelos YOLO, originalmente por Joseph Redmon, conhecida por suas capacidades eficientes de detec\u00e7\u00e3o de objetos em tempo real.</li> <li>YOLOv4: Uma atualiza\u00e7\u00e3o nativa para o darknet do YOLOv3, lan\u00e7ada por Alexey Bochkovskiy em 2020.</li> <li>YOLOv5: Uma vers\u00e3o aprimorada da arquitetura YOLO pela Ultralytics, oferecendo melhor desempenho e compensa\u00e7\u00f5es de velocidade em compara\u00e7\u00e3o com as vers\u00f5es anteriores.</li> <li>YOLOv6: Lan\u00e7ado pela Meituan em 2022, e em uso em muitos dos rob\u00f4s aut\u00f4nomos de entregas da empresa.</li> <li>YOLOv7: Modelos YOLO atualizados lan\u00e7ados em 2022 pelos autores do YOLOv4.</li> <li>YOLOv8 NOVO \ud83d\ude80: A vers\u00e3o mais recente da fam\u00edlia YOLO, apresentando capacidades aprimoradas, como segmenta\u00e7\u00e3o de inst\u00e2ncias, estimativa de pose/pontos-chave e classifica\u00e7\u00e3o.</li> <li>Segment Anything Model (SAM): Modelo Segment Anything (SAM) da Meta.</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM para aplica\u00e7\u00f5es m\u00f3veis, pela Universidade Kyung Hee.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM pelo Grupo de An\u00e1lise de Imagem e V\u00eddeo, Instituto de Automa\u00e7\u00e3o, Academia Chinesa de Ci\u00eancias.</li> <li>YOLO-NAS: Modelos de Pesquisa de Arquitetura Neural YOLO (NAS).</li> <li>Realtime Detection Transformers (RT-DETR): Modelos de Transformador de Detec\u00e7\u00e3o em Tempo Real (RT-DETR) do PaddlePaddle da Baidu.</li> </ol> <p> Assista: Execute modelos YOLO da Ultralytics em apenas algumas linhas de c\u00f3digo. </p>"},{"location":"models/#introducao-exemplos-de-uso","title":"Introdu\u00e7\u00e3o: Exemplos de Uso","text":"<p>Este exemplo oferece exemplos simples de treinamento e infer\u00eancia com YOLO. Para uma documenta\u00e7\u00e3o completa sobre estes e outros modos, veja as p\u00e1ginas de documenta\u00e7\u00e3o de Previs\u00e3o, Treinamento, Valida\u00e7\u00e3o e Exporta\u00e7\u00e3o.</p> <p>Note que o exemplo abaixo \u00e9 para modelos YOLOv8 Detect para detec\u00e7\u00e3o de objetos. Para tarefas suportadas adicionais, veja as documenta\u00e7\u00f5es de Segmenta\u00e7\u00e3o, Classifica\u00e7\u00e3o e Pose.</p> <p>Exemplo</p> PythonCLI <p>Modelos <code>*.pt</code> pr\u00e9-treinados em PyTorch, bem como arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados para as classes <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> e <code>RTDETR()</code> para criar uma inst\u00e2ncia de modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO\nmodelo = YOLO('yolov8n.pt')\n\n# Exibir informa\u00e7\u00f5es do modelo (opcional)\nmodelo.info()\n\n# Treinar o modelo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresultados = modelo.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Executar infer\u00eancia com o modelo YOLOv8n na imagem 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Comandos CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO e trein\u00e1-lo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO e executar infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribuindo-com-novos-modelos","title":"Contribuindo com Novos Modelos","text":"<p>Interessado em contribuir com seu modelo para a Ultralytics? \u00d3timo! Estamos sempre abertos a expandir nosso portf\u00f3lio de modelos.</p> <ol> <li> <p>Fork do Reposit\u00f3rio: Comece fazendo um fork do reposit\u00f3rio no GitHub da Ultralytics.</p> </li> <li> <p>Clone Seu Fork: Clone seu fork para a sua m\u00e1quina local e crie uma nova branch para trabalhar.</p> </li> <li> <p>Implemente Seu Modelo: Adicione seu modelo seguindo as normas e diretrizes de codifica\u00e7\u00e3o fornecidas no nosso Guia de Contribui\u00e7\u00e3o.</p> </li> <li> <p>Teste Cuidadosamente: Assegure-se de testar seu modelo rigorosamente, tanto isoladamente quanto como parte do pipeline.</p> </li> <li> <p>Crie um Pull Request: Uma vez que estiver satisfeito com seu modelo, crie um pull request para o reposit\u00f3rio principal para revis\u00e3o.</p> </li> <li> <p>Revis\u00e3o de C\u00f3digo &amp; Mesclagem: Ap\u00f3s a revis\u00e3o, se seu modelo atender aos nossos crit\u00e9rios, ele ser\u00e1 integrado ao reposit\u00f3rio principal.</p> </li> </ol> <p>Para etapas detalhadas, consulte nosso Guia de Contribui\u00e7\u00e3o.</p>"},{"location":"models/fast-sam/","title":"Fast Segment Anything Model (FastSAM)","text":"<p>O Fast Segment Anything Model (FastSAM) \u00e9 uma solu\u00e7\u00e3o inovadora baseada em CNN em tempo real para a tarefa de Segmentar Qualquer Coisa. Essa tarefa foi projetada para segmentar qualquer objeto dentro de uma imagem com base em v\u00e1rias poss\u00edveis instru\u00e7\u00f5es de intera\u00e7\u00e3o do usu\u00e1rio. O FastSAM reduz significativamente as demandas computacionais, mantendo um desempenho competitivo, tornando-o uma escolha pr\u00e1tica para uma variedade de tarefas de vis\u00e3o.</p> <p></p>"},{"location":"models/fast-sam/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O FastSAM \u00e9 projetado para abordar as limita\u00e7\u00f5es do Segment Anything Model (SAM), um modelo Transformer pesado com requisitos substanciais de recursos computacionais. O FastSAM divide a tarefa de segmentar qualquer coisa em duas etapas sequenciais: segmenta\u00e7\u00e3o de todas as inst\u00e2ncias e sele\u00e7\u00e3o guiada por instru\u00e7\u00f5es. A primeira etapa usa o YOLOv8-seg para produzir as m\u00e1scaras de segmenta\u00e7\u00e3o de todas as inst\u00e2ncias na imagem. Na segunda etapa, ele gera a regi\u00e3o de interesse correspondente \u00e0 instru\u00e7\u00e3o.</p>"},{"location":"models/fast-sam/#recursos-principais","title":"Recursos Principais","text":"<ol> <li> <p>Solu\u00e7\u00e3o em Tempo Real: Aproveitando a efici\u00eancia computacional das CNNs, o FastSAM fornece uma solu\u00e7\u00e3o em tempo real para a tarefa de segmentar qualquer coisa, tornando-o valioso para aplica\u00e7\u00f5es industriais que exigem resultados r\u00e1pidos.</p> </li> <li> <p>Efici\u00eancia e Desempenho: O FastSAM oferece uma redu\u00e7\u00e3o significativa nas demandas computacionais e de recursos sem comprometer a qualidade do desempenho. Ele alcan\u00e7a um desempenho compar\u00e1vel ao SAM, mas com recursos computacionais drasticamente reduzidos, permitindo aplica\u00e7\u00f5es em tempo real.</p> </li> <li> <p>Segmenta\u00e7\u00e3o Guiada por Instru\u00e7\u00f5es: O FastSAM pode segmentar qualquer objeto dentro de uma imagem com base em v\u00e1rias poss\u00edveis instru\u00e7\u00f5es de intera\u00e7\u00e3o do usu\u00e1rio, proporcionando flexibilidade e adaptabilidade em diferentes cen\u00e1rios.</p> </li> <li> <p>Baseado em YOLOv8-seg: O FastSAM \u00e9 baseado no YOLOv8-seg, um detector de objetos equipado com um ramo de segmenta\u00e7\u00e3o de inst\u00e2ncias. Isso permite que ele produza efetivamente as m\u00e1scaras de segmenta\u00e7\u00e3o de todas as inst\u00e2ncias em uma imagem.</p> </li> <li> <p>Resultados Competitivos em Bancos de Dados de Refer\u00eancia: Na tarefa de proposta de objetos no MS COCO, o FastSAM alcan\u00e7a pontua\u00e7\u00f5es altas em uma velocidade significativamente mais r\u00e1pida do que o SAM em uma \u00fanica NVIDIA RTX 3090, demonstrando sua efici\u00eancia e capacidade.</p> </li> <li> <p>Aplica\u00e7\u00f5es Pr\u00e1ticas: A abordagem proposta fornece uma nova solu\u00e7\u00e3o pr\u00e1tica para um grande n\u00famero de tarefas de vis\u00e3o em alta velocidade, dezenas ou centenas de vezes mais r\u00e1pido do que os m\u00e9todos atuais.</p> </li> <li> <p>Viabilidade de Compress\u00e3o do Modelo: O FastSAM demonstra a viabilidade de um caminho que pode reduzir significativamente o esfor\u00e7o computacional, introduzindo uma prioridade artificial \u00e0 estrutura, abrindo assim novas possibilidades para arquiteturas de modelos grandes para tarefas gerais de vis\u00e3o.</p> </li> </ol>"},{"location":"models/fast-sam/#modelos-disponiveis-tarefas-suportadas-e-modos-de-operacao","title":"Modelos Dispon\u00edveis, Tarefas Suportadas e Modos de Opera\u00e7\u00e3o","text":"<p>Esta tabela apresenta os modelos dispon\u00edveis com seus pesos pr\u00e9-treinados espec\u00edficos, as tarefas que eles suportam e sua compatibilidade com diferentes modos de opera\u00e7\u00e3o, como Infer\u00eancia, Valida\u00e7\u00e3o, Treinamento e Exporta\u00e7\u00e3o, indicados por emojis \u2705 para modos suportados e emojis \u274c para modos n\u00e3o suportados.</p> Tipo de Modelo Pesos Pr\u00e9-treinados Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o FastSAM-s <code>FastSAM-s.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u274c \u274c \u2705 FastSAM-x <code>FastSAM-x.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u274c \u274c \u2705"},{"location":"models/fast-sam/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Os modelos FastSAM s\u00e3o f\u00e1ceis de integrar em suas aplica\u00e7\u00f5es Python. A Ultralytics fornece uma API Python amig\u00e1vel ao usu\u00e1rio e comandos de linha de comando (CLI) para facilitar o desenvolvimento.</p>"},{"location":"models/fast-sam/#uso-de-predicao","title":"Uso de Predi\u00e7\u00e3o","text":"<p>Para realizar detec\u00e7\u00e3o de objetos em uma imagem, use o m\u00e9todo <code>predict</code> conforme mostrado abaixo:</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import FastSAM\nfrom ultralytics.models.fastsam import FastSAMPrompt\n\n# Definir uma fonte de infer\u00eancia\nsource = 'caminho/para/onibus.jpg'\n\n# Criar um modelo FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # ou FastSAM-x.pt\n\n# Executar infer\u00eancia em uma imagem\neverything_results = model(source, device='cpu', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n\n# Preparar um objeto de Processo de Instru\u00e7\u00f5es\nprompt_process = FastSAMPrompt(source, everything_results, device='cpu')\n\n# Instru\u00e7\u00e3o: tudo\nann = prompt_process.everything_prompt()\n\n# Forma padr\u00e3o (bbox) [0,0,0,0] -&gt; [x1,y1,x2,y2]\nann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n\n# Instru\u00e7\u00e3o: texto\nann = prompt_process.text_prompt(text='uma foto de um cachorro')\n\n# Instru\u00e7\u00e3o: ponto\n# pontos padr\u00e3o [[0,0]] [[x1,y1],[x2,y2]]\n# ponto_label padr\u00e3o [0] [1,0] 0:fundo, 1:frente\nann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\nprompt_process.plot(annotations=ann, output='./')\n</code></pre> <pre><code># Carregar um modelo FastSAM e segmentar tudo com ele\nyolo segment predict model=FastSAM-s.pt source=caminho/para/onibus.jpg imgsz=640\n</code></pre> <p>Este trecho de c\u00f3digo demonstra a simplicidade de carregar um modelo pr\u00e9-treinado e executar uma predi\u00e7\u00e3o em uma imagem.</p>"},{"location":"models/fast-sam/#uso-de-validacao","title":"Uso de Valida\u00e7\u00e3o","text":"<p>A valida\u00e7\u00e3o do modelo em um conjunto de dados pode ser feita da seguinte forma:</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import FastSAM\n\n# Criar um modelo FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # ou FastSAM-x.pt\n\n# Validar o modelo\nresults = model.val(data='coco8-seg.yaml')\n</code></pre> <pre><code># Carregar um modelo FastSAM e valid\u00e1-lo no conjunto de dados de exemplo COCO8 com tamanho de imagem 640\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n</code></pre> <p>Observe que o FastSAM suporta apenas detec\u00e7\u00e3o e segmenta\u00e7\u00e3o de uma \u00fanica classe de objeto. Isso significa que ele reconhecer\u00e1 e segmentar\u00e1 todos os objetos como a mesma classe. Portanto, ao preparar o conjunto de dados, voc\u00ea precisar\u00e1 converter todos os IDs de categoria de objeto para 0.</p>"},{"location":"models/fast-sam/#uso-oficial-do-fastsam","title":"Uso Oficial do FastSAM","text":"<p>O FastSAM tamb\u00e9m est\u00e1 dispon\u00edvel diretamente no reposit\u00f3rio https://github.com/CASIA-IVA-Lab/FastSAM. Aqui est\u00e1 uma vis\u00e3o geral breve das etapas t\u00edpicas que voc\u00ea pode seguir para usar o FastSAM:</p>"},{"location":"models/fast-sam/#instalacao","title":"Instala\u00e7\u00e3o","text":"<ol> <li> <p>Clone o reposit\u00f3rio do FastSAM:    <pre><code>git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n</code></pre></p> </li> <li> <p>Crie e ative um ambiente Conda com Python 3.9:    <pre><code>conda create -n FastSAM python=3.9\nconda activate FastSAM\n</code></pre></p> </li> <li> <p>Navegue at\u00e9 o reposit\u00f3rio clonado e instale os pacotes necess\u00e1rios:    <pre><code>cd FastSAM\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Instale o modelo CLIP:    <pre><code>pip install git+https://github.com/openai/CLIP.git\n</code></pre></p> </li> </ol>"},{"location":"models/fast-sam/#exemplo-de-uso","title":"Exemplo de Uso","text":"<ol> <li> <p>Baixe um checkpoint do modelo.</p> </li> <li> <p>Use o FastSAM para infer\u00eancia. Exemplos de comandos:</p> <ul> <li> <p>Segmentar tudo em uma imagem:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n</code></pre></p> </li> <li> <p>Segmentar objetos espec\u00edficos usando uma instru\u00e7\u00e3o de texto:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"o cachorro amarelo\"\n</code></pre></p> </li> <li> <p>Segmentar objetos dentro de uma caixa delimitadora (fornecer coordenadas da caixa no formato xywh):   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n</code></pre></p> </li> <li> <p>Segmentar objetos pr\u00f3ximos a pontos espec\u00edficos:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n</code></pre></p> </li> </ul> </li> </ol> <p>Al\u00e9m disso, voc\u00ea pode experimentar o FastSAM atrav\u00e9s de um demo no Colab ou no demo web do HuggingFace para ter uma experi\u00eancia visual.</p>"},{"location":"models/fast-sam/#citacoes-e-reconhecimentos","title":"Cita\u00e7\u00f5es e Reconhecimentos","text":"<p>Gostar\u00edamos de reconhecer os autores do FastSAM por suas contribui\u00e7\u00f5es significativas no campo da segmenta\u00e7\u00e3o de inst\u00e2ncias em tempo real:</p> BibTeX <pre><code>@misc{zhao2023fast,\n      title={Fast Segment Anything},\n      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\n      year={2023},\n      eprint={2306.12156},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>O artigo original do FastSAM pode ser encontrado no arXiv. Os autores disponibilizaram seu trabalho publicamente, e o c\u00f3digo pode ser acessado no GitHub. Agradecemos seus esfor\u00e7os em avan\u00e7ar o campo e tornar seu trabalho acess\u00edvel \u00e0 comunidade em geral.</p>"},{"location":"models/mobile-sam/","title":"MobileSAM (Mobile Segment Anything Model)","text":""},{"location":"models/mobile-sam/#segmentacao-movel-de-qualquer-coisa-mobilesam","title":"Segmenta\u00e7\u00e3o M\u00f3vel de Qualquer Coisa (MobileSAM)","text":"<p>O artigo do MobileSAM agora est\u00e1 dispon\u00edvel no arXiv.</p> <p>Uma demonstra\u00e7\u00e3o do MobileSAM executando em uma CPU pode ser acessada neste link de demonstra\u00e7\u00e3o. O desempenho em um Mac i5 CPU leva aproximadamente 3 segundos. Na demonstra\u00e7\u00e3o do Hugging Face, a interface e CPUs de menor desempenho contribuem para uma resposta mais lenta, mas ela continua funcionando efetivamente.</p> <p>O MobileSAM \u00e9 implementado em v\u00e1rios projetos, incluindo Grounding-SAM, AnyLabeling e Segment Anything in 3D.</p> <p>O MobileSAM \u00e9 treinado em uma \u00fanica GPU com um conjunto de dados de 100 mil imagens (1% das imagens originais) em menos de um dia. O c\u00f3digo para esse treinamento ser\u00e1 disponibilizado no futuro.</p>"},{"location":"models/mobile-sam/#modelos-disponiveis-tarefas-suportadas-e-modos-de-operacao","title":"Modelos Dispon\u00edveis, Tarefas Suportadas e Modos de Opera\u00e7\u00e3o","text":"<p>Esta tabela apresenta os modelos dispon\u00edveis com seus pesos pr\u00e9-treinados espec\u00edficos, as tarefas que eles suportam e sua compatibilidade com diferentes modos de opera\u00e7\u00e3o, como Inference, Validation, Training e Export, indicados pelos emojis \u2705 para os modos suportados e \u274c para os modos n\u00e3o suportados.</p> Tipo de Modelo Pesos Pr\u00e9-treinados Tarefas Suportadas Inference Validation Training Export MobileSAM <code>mobile_sam.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u274c \u274c \u2705"},{"location":"models/mobile-sam/#adaptacao-de-sam-para-mobilesam","title":"Adapta\u00e7\u00e3o de SAM para MobileSAM","text":"<p>Como o MobileSAM mant\u00e9m o mesmo pipeline do SAM original, incorporamos o pr\u00e9-processamento original, p\u00f3s-processamento e todas as outras interfaces. Consequentemente, aqueles que est\u00e3o atualmente usando o SAM original podem fazer a transi\u00e7\u00e3o para o MobileSAM com um esfor\u00e7o m\u00ednimo.</p> <p>O MobileSAM tem um desempenho compar\u00e1vel ao SAM original e mant\u00e9m o mesmo pipeline, exceto por uma mudan\u00e7a no codificador de imagens. Especificamente, substitu\u00edmos o codificador de imagens ViT-H original (632M) por um ViT menor (5M). Em uma \u00fanica GPU, o MobileSAM opera em cerca de 12 ms por imagem: 8 ms no codificador de imagens e 4 ms no decodificador de m\u00e1scaras.</p> <p>A tabela a seguir fornece uma compara\u00e7\u00e3o dos codificadores de imagens baseados em ViT:</p> Codificador de Imagens SAM Original MobileSAM Par\u00e2metros 611M 5M Velocidade 452ms 8ms <p>Tanto o SAM original quanto o MobileSAM utilizam o mesmo decodificador de m\u00e1scaras baseado em prompt:</p> Decodificador de M\u00e1scaras SAM Original MobileSAM Par\u00e2metros 3,876M 3,876M Velocidade 4ms 4ms <p>Aqui est\u00e1 a compara\u00e7\u00e3o de todo o pipeline:</p> Pipeline Completo (Enc+Dec) SAM Original MobileSAM Par\u00e2metros 615M 9,66M Velocidade 456ms 12ms <p>O desempenho do MobileSAM e do SAM original \u00e9 demonstrado usando tanto um ponto quanto uma caixa como prompts.</p> <p></p> <p></p> <p>Com seu desempenho superior, o MobileSAM \u00e9 aproximadamente 5 vezes menor e 7 vezes mais r\u00e1pido que o FastSAM atual. Mais detalhes est\u00e3o dispon\u00edveis na p\u00e1gina do projeto MobileSAM.</p>"},{"location":"models/mobile-sam/#testando-o-mobilesam-no-ultralytics","title":"Testando o MobileSAM no Ultralytics","text":"<p>Assim como o SAM original, oferecemos um m\u00e9todo de teste simples no Ultralytics, incluindo modos para prompts de Ponto e Caixa.</p>"},{"location":"models/mobile-sam/#download-do-modelo","title":"Download do Modelo","text":"<p>Voc\u00ea pode baixar o modelo aqui.</p>"},{"location":"models/mobile-sam/#prompt-de-ponto","title":"Prompt de Ponto","text":"<p>Exemplo</p> Python <pre><code>from ultralytics import SAM\n\n# Carregar o modelo\nmodel = SAM('mobile_sam.pt')\n\n# Prever um segmento com base em um prompt de ponto\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre>"},{"location":"models/mobile-sam/#prompt-de-caixa","title":"Prompt de Caixa","text":"<p>Exemplo</p> Python <pre><code>from ultralytics import SAM\n\n# Carregar o modelo\nmodel = SAM('mobile_sam.pt')\n\n# Prever um segmento com base em um prompt de caixa\nmodel.predict('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n</code></pre> <p>Implementamos <code>MobileSAM</code> e <code>SAM</code> usando a mesma API. Para obter mais informa\u00e7\u00f5es sobre o uso, consulte a p\u00e1gina do SAM.</p>"},{"location":"models/mobile-sam/#citacoes-e-agradecimentos","title":"Cita\u00e7\u00f5es e Agradecimentos","text":"<p>Se voc\u00ea achar o MobileSAM \u00fatil em sua pesquisa ou trabalho de desenvolvimento, considere citar nosso artigo:</p> BibTeX <p>```bibtex @article{mobile_sam,   title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},   author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},   journal={arXiv preprint arXiv:2306.14289},   year={2023} }</p>"},{"location":"models/rtdetr/","title":"RT-DETR da Baidu: Um Detector de Objetos em Tempo Real Baseado em Vision Transformers","text":""},{"location":"models/rtdetr/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O Real-Time Detection Transformer (RT-DETR), desenvolvido pela Baidu, \u00e9 um detector de objetos de \u00faltima gera\u00e7\u00e3o que proporciona desempenho em tempo real mantendo alta precis\u00e3o. Ele utiliza a pot\u00eancia dos Vision Transformers (ViT) para processar eficientemente recursos multiescala, separando a intera\u00e7\u00e3o intra-escala e a fus\u00e3o entre escalas. O RT-DETR \u00e9 altamente adapt\u00e1vel, com suporte para ajuste flex\u00edvel da velocidade de infer\u00eancia usando diferentes camadas de decodificador sem a necessidade de retratamento. O modelo se destaca em backends acelerados como o CUDA com o TensorRT, superando muitos outros detectores de objetos em tempo real.</p> <p> Vis\u00e3o geral do RT-DETR da Baidu. O diagrama da arquitetura do modelo RT-DETR mostra as \u00faltimas tr\u00eas etapas da espinha dorsal {S3, S4, S5} como entrada para o codificador. O codificador h\u00edbrido eficiente transforma recursos multiescala em uma sequ\u00eancia de recursos de imagem por meio da intera\u00e7\u00e3o de recursos intra-escala (AIFI) e do m\u00f3dulo de fus\u00e3o de recursos entre escalas (CCFM). A sele\u00e7\u00e3o de consulta, consciente da IoU, \u00e9 utilizada para selecionar um n\u00famero fixo de recursos de imagem para servir como consultas de objeto iniciais para o decodificador. Por fim, o decodificador com cabe\u00e7otes de previs\u00e3o auxiliares otimiza iterativamente as consultas de objeto para gerar caixas e pontua\u00e7\u00f5es de confian\u00e7a (fonte).</p>"},{"location":"models/rtdetr/#caracteristicas-principais","title":"Caracter\u00edsticas Principais","text":"<ul> <li>Codificador H\u00edbrido Eficiente: O RT-DETR da Baidu utiliza um codificador h\u00edbrido eficiente para processar recursos multiescala por meio da separa\u00e7\u00e3o da intera\u00e7\u00e3o intra-escala e da fus\u00e3o entre escalas. Esse design exclusivo baseado em Vision Transformers reduz os custos computacionais e permite a detec\u00e7\u00e3o de objetos em tempo real.</li> <li>Sele\u00e7\u00e3o de Consulta Consciente de IoU: O RT-DETR da Baidu melhora a inicializa\u00e7\u00e3o das consultas de objeto ao utilizar sele\u00e7\u00e3o de consulta consciente de IoU. Isso permite que o modelo foque nos objetos mais relevantes na cena, aprimorando a precis\u00e3o da detec\u00e7\u00e3o.</li> <li>Velocidade de Infer\u00eancia Adapt\u00e1vel: O RT-DETR da Baidu suporta ajustes flex\u00edveis da velocidade de infer\u00eancia ao utilizar diferentes camadas de decodificador sem a necessidade de retratamento. Essa adaptabilidade facilita a aplica\u00e7\u00e3o pr\u00e1tica em diversos cen\u00e1rios de detec\u00e7\u00e3o de objetos em tempo real.</li> </ul>"},{"location":"models/rtdetr/#modelos-pre-treinados","title":"Modelos Pr\u00e9-Treinados","text":"<p>A API Python do Ultralytics fornece modelos pr\u00e9-treinados do RT-DETR do PaddlePaddle com diferentes escalas:</p> <ul> <li>RT-DETR-L: 53,0% de AP em COCO val2017, 114 FPS em GPU T4</li> <li>RT-DETR-X: 54,8% de AP em COCO val2017, 74 FPS em GPU T4</li> </ul>"},{"location":"models/rtdetr/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Este exemplo fornece exemplos simples de treinamento e infer\u00eancia com o RT-DETRR. Para obter documenta\u00e7\u00e3o completa sobre esses e outros modos, consulte as p\u00e1ginas de documenta\u00e7\u00e3o Predict, Train, Val e Export.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import RTDETR\n\n# Carregue um modelo RT-DETR-l pr\u00e9-treinado no COCO\nmodel = RTDETR('rtdetr-l.pt')\n\n# Exiba informa\u00e7\u00f5es do modelo (opcional)\nmodel.info()\n\n# Treine o modelo com o conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Execute a infer\u00eancia com o modelo RT-DETR-l na imagem 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <pre><code># Carregue um modelo RT-DETR-l pr\u00e9-treinado no COCO e treine-o com o conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carregue um modelo RT-DETR-l pr\u00e9-treinado no COCO e execute a infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/rtdetr/#tarefas-e-modos-suportados","title":"Tarefas e Modos Suportados","text":"<p>Esta tabela apresenta os tipos de modelo, os pesos pr\u00e9-treinados espec\u00edficos, as tarefas suportadas por cada modelo e os v\u00e1rios modos (Train, Val, Predict, Export) que s\u00e3o suportados, indicados por emojis \u2705.</p> Tipo de Modelo Pesos Pr\u00e9-treinados Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o RT-DETR Grande <code>rtdetr-l.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 RT-DETR Extra-Grande <code>rtdetr-x.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705"},{"location":"models/rtdetr/#citacoes-e-reconhecimentos","title":"Cita\u00e7\u00f5es e Reconhecimentos","text":"<p>Se voc\u00ea utilizar o RT-DETR da Baidu em seu trabalho de pesquisa ou desenvolvimento, por favor cite o artigo original:</p> BibTeX <pre><code>@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Wenyu Lv and Shangliang Xu and Yian Zhao and Guanzhong Wang and Jinman Wei and Cheng Cui and Yuning Du and Qingqing Dang and Yi Liu},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Gostar\u00edamos de agradecer \u00e0 Baidu e \u00e0 equipe do PaddlePaddle por criar e manter esse recurso valioso para a comunidade de vis\u00e3o computacional. Sua contribui\u00e7\u00e3o para o campo com o desenvolvimento do detector de objetos em tempo real baseado em Vision Transformers, RT-DETR, \u00e9 muito apreciada.</p> <p>keywords: RT-DETR, Transformer, ViT, Vision Transformers, RT-DETR da Baidu, PaddlePaddle, modelos pr\u00e9-treinados PaddlePaddle RT-DETR, uso do RT-DETR da Baidu, API Python do Ultralytics</p>"},{"location":"models/sam/","title":"Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa (SAM)","text":"<p>Bem-vindo \u00e0 fronteira da segmenta\u00e7\u00e3o de imagem com o Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa, ou SAM. Este modelo revolucion\u00e1rio mudou o jogo ao introduzir a segmenta\u00e7\u00e3o de imagem baseada em prompts com desempenho em tempo real, estabelecendo novos padr\u00f5es no campo.</p>"},{"location":"models/sam/#introducao-ao-sam-o-modelo-de-segmentacao-de-qualquer-coisa","title":"Introdu\u00e7\u00e3o ao SAM: O Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa","text":"<p>O Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa, ou SAM, \u00e9 um modelo de segmenta\u00e7\u00e3o de imagem de ponta que permite a segmenta\u00e7\u00e3o baseada em prompts, proporcionando uma versatilidade incompar\u00e1vel em tarefas de an\u00e1lise de imagem. O SAM \u00e9 o cerne da iniciativa Segment Anything, um projeto inovador que introduz um modelo, tarefa e conjunto de dados novos para a segmenta\u00e7\u00e3o de imagem.</p> <p>O design avan\u00e7ado do SAM permite que ele se adapte a novas distribui\u00e7\u00f5es de imagem e tarefas sem conhecimento pr\u00e9vio, um recurso conhecido como transfer\u00eancia zero. Treinado no abrangente conjunto de dados SA-1B, que cont\u00e9m mais de 1 bilh\u00e3o de m\u00e1scaras espalhadas por 11 milh\u00f5es de imagens cuidadosamente selecionadas, o SAM tem demonstrado um impressionante desempenho de transfer\u00eancia zero, superando os resultados totalmente supervisionados anteriores em muitos casos.</p> <p> Imagens de exemplo com m\u00e1scaras sobrepostas do nosso conjunto de dados rec\u00e9m-introduzido, SA-1B. O SA-1B cont\u00e9m 11 milh\u00f5es de imagens diversas, de alta resolu\u00e7\u00e3o, licenciadas e com prote\u00e7\u00e3o de privacidade, e 1,1 bilh\u00e3o de m\u00e1scaras de segmenta\u00e7\u00e3o de alta qualidade. Essas m\u00e1scaras foram anotadas totalmente automaticamente pelo SAM, e, como verificado por classifica\u00e7\u00f5es humanas e in\u00fameros experimentos, s\u00e3o de alta qualidade e diversidade. As imagens s\u00e3o agrupadas pelo n\u00famero de m\u00e1scaras por imagem para visualiza\u00e7\u00e3o (em m\u00e9dia, h\u00e1 \u223c100 m\u00e1scaras por imagem).</p>"},{"location":"models/sam/#recursos-principais-do-modelo-de-segmentacao-de-qualquer-coisa-sam","title":"Recursos Principais do Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa (SAM)","text":"<ul> <li>Tarefa de Segmenta\u00e7\u00e3o Baseada em Prompts: O SAM foi projetado com uma tarefa de segmenta\u00e7\u00e3o baseada em prompts em mente, permitindo que ele gere m\u00e1scaras de segmenta\u00e7\u00e3o v\u00e1lidas a partir de qualquer prompt fornecido, como dicas espaciais ou textuais que identifiquem um objeto.</li> <li>Arquitetura Avan\u00e7ada: O Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa utiliza um poderoso codificador de imagens, um codificador de prompts e um decodificador de m\u00e1scaras leve. Essa arquitetura \u00fanica possibilita o uso flex\u00edvel de prompts, c\u00e1lculo de m\u00e1scaras em tempo real e consci\u00eancia de ambiguidade em tarefas de segmenta\u00e7\u00e3o.</li> <li>O Conjunto de Dados SA-1B: Introduzido pelo projeto Segment Anything, o conjunto de dados SA-1B apresenta mais de 1 bilh\u00e3o de m\u00e1scaras em 11 milh\u00f5es de imagens. Como o maior conjunto de dados de segmenta\u00e7\u00e3o at\u00e9 o momento, ele fornece ao SAM uma fonte diversificada e em grande escala de dados de treinamento.</li> <li>Desempenho de Transfer\u00eancia Zero: O SAM apresenta um desempenho de transfer\u00eancia zero excepcional em diversas tarefas de segmenta\u00e7\u00e3o, tornando-se uma ferramenta pronta para uso em aplica\u00e7\u00f5es diversas com necessidade m\u00ednima de engenharia de prompts.</li> </ul> <p>Para obter uma vis\u00e3o mais aprofundada do Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa e do conjunto de dados SA-1B, visite o site do Segment Anything e consulte o artigo de pesquisa Segment Anything.</p>"},{"location":"models/sam/#modelos-disponiveis-tarefas-suportadas-e-modos-de-operacao","title":"Modelos Dispon\u00edveis, Tarefas Suportadas e Modos de Opera\u00e7\u00e3o","text":"<p>Esta tabela apresenta os modelos dispon\u00edveis com seus pesos pr\u00e9-treinados espec\u00edficos, as tarefas suportadas por eles e sua compatibilidade com diferentes modos de opera\u00e7\u00e3o, como Infer\u00eancia, Valida\u00e7\u00e3o, Treinamento e Exporta\u00e7\u00e3o, indicados pelos emojis \u2705 para modos suportados e \u274c para modos n\u00e3o suportados.</p> Tipo de Modelo Pesos Pr\u00e9-Treinados Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o SAM base <code>sam_b.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u274c \u274c \u2705 SAM large <code>sam_l.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u274c \u274c \u2705"},{"location":"models/sam/#como-usar-o-sam-versatilidade-e-poder-na-segmentacao-de-imagens","title":"Como Usar o SAM: Versatilidade e Poder na Segmenta\u00e7\u00e3o de Imagens","text":"<p>O Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa pode ser utilizado para uma variedade de tarefas secund\u00e1rias que v\u00e3o al\u00e9m dos dados de treinamento. Isso inclui detec\u00e7\u00e3o de bordas, gera\u00e7\u00e3o de propostas de objeto, segmenta\u00e7\u00e3o de inst\u00e2ncias e predi\u00e7\u00e3o preliminar de texto para m\u00e1scara. Com a engenharia de prompts, o SAM pode se adaptar rapidamente a novas tarefas e distribui\u00e7\u00f5es de dados de maneira inovadora, estabelecendo-se como uma ferramenta vers\u00e1til e poderosa para todas as suas necessidades de segmenta\u00e7\u00e3o de imagem.</p>"},{"location":"models/sam/#exemplo-de-predicao-do-sam","title":"Exemplo de predi\u00e7\u00e3o do SAM","text":"<p>Segmentar com prompts</p> <p>Segmenta a imagem com prompts fornecidos.</p> Python <pre><code>from ultralytics import SAM\n\n# Carregar o modelo\nmodelo = SAM('sam_b.pt')\n\n# Exibir informa\u00e7\u00f5es do modelo (opcional)\nmodelo.info()\n\n# Executar infer\u00eancia com prompt de bboxes\nmodelo('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n\n# Executar infer\u00eancia com prompt de pontos\nmodelo('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre> <p>Segmentar tudo</p> <p>Segmenta toda a imagem.</p> PythonCLI <pre><code>from ultralytics import SAM\n\n# Carregar o modelo\nmodelo = SAM('sam_b.pt')\n\n# Exibir informa\u00e7\u00f5es do modelo (opcional)\nmodelo.info()\n\n# Executar infer\u00eancia\nmodelo('caminho/para/imagem.jpg')\n</code></pre> <pre><code># Executar infer\u00eancia com um modelo SAM\nyolo predict model=sam_b.pt source=caminho/para/imagem.jpg\n</code></pre> <ul> <li>A l\u00f3gica aqui \u00e9 segmentar toda a imagem se nenhum prompt (bboxes/pontos/m\u00e1scaras) for especificado.</li> </ul> <p>Exemplo do SAMPredictor</p> <p>Desta forma, voc\u00ea pode definir a imagem uma vez e executar infer\u00eancia de prompts v\u00e1rias vezes sem executar o codificador de imagem v\u00e1rias vezes.</p> Infer\u00eancia com prompt <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Criar o SAMPredictor\nsubstitui\u00e7\u00f5es = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(substitui\u00e7\u00f5es=substitui\u00e7\u00f5es)\n\n# Definir imagem\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # definir com arquivo de imagem\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # definir com np.ndarray\nresults = predictor(bboxes=[439, 437, 524, 709])\nresults = predictor(points=[900, 370], labels=[1])\n\n# Redefinir imagem\npredictor.reset_image()\n</code></pre> <p>Segmentar tudo com argumentos adicionais.</p> Segmentar tudo <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Criar o SAMPredictor\nsubstitui\u00e7\u00f5es = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(substitui\u00e7\u00f5es=substitui\u00e7\u00f5es)\n\n# Segmentar com argumentos adicionais\nresults = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n</code></pre> <ul> <li>Mais argumentos adicionais para <code>Segmentar tudo</code> consulte a Refer\u00eancia do <code>Predictor/generate</code>.</li> </ul>"},{"location":"models/sam/#comparacao-sam-vs-yolov8","title":"Compara\u00e7\u00e3o SAM vs. YOLOv8","text":"<p>Aqui, comparamos o menor modelo SAM-b da Meta com o menor modelo de segmenta\u00e7\u00e3o da Ultralytics, YOLOv8n-seg:</p> Modelo Tamanho Par\u00e2metros Velocidade (CPU) SAM-b da Meta 358 MB 94,7 M 51096 ms/im MobileSAM 40,7 MB 10,1 M 46122 ms/im FastSAM-s com YOLOv8 como base 23,7 MB 11,8 M 115 ms/im YOLOv8n-seg da Ultralytics 6,7 MB (53,4 vezes menor) 3,4 M (27,9 vezes a menos) 59 ms/im (866 vezes mais r\u00e1pido) <p>Essa compara\u00e7\u00e3o mostra as diferen\u00e7as de ordem de magnitude nos tamanhos e velocidades dos modelos. Enquanto o SAM apresenta capacidades exclusivas para segmenta\u00e7\u00e3o autom\u00e1tica, ele n\u00e3o \u00e9 um concorrente direto dos modelos de segmenta\u00e7\u00e3o YOLOv8, que s\u00e3o menores, mais r\u00e1pidos e mais eficientes.</p> <p>Os testes foram executados em um MacBook Apple M2 de 2023 com 16GB de RAM. Para reproduzir este teste:</p> <p>Exemplo</p> Python <pre><code>from ultralytics import FastSAM, SAM, YOLO\n\n# Perfil do SAM-b\nmodelo = SAM('sam_b.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil do MobileSAM\nmodelo = SAM('mobile_sam.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil do FastSAM-s\nmodelo = FastSAM('FastSAM-s.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil do YOLOv8n-seg\nmodelo = YOLO('yolov8n-seg.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n</code></pre>"},{"location":"models/sam/#autoanotacao-um-caminho-rapido-para-conjuntos-de-dados-de-segmentacao","title":"Autoanota\u00e7\u00e3o: Um Caminho R\u00e1pido para Conjuntos de Dados de Segmenta\u00e7\u00e3o","text":"<p>A autoanota\u00e7\u00e3o \u00e9 um recurso-chave do SAM que permite aos usu\u00e1rios gerar um conjunto de dados de segmenta\u00e7\u00e3o usando um modelo de detec\u00e7\u00e3o pr\u00e9-treinado. Esse recurso permite a anota\u00e7\u00e3o r\u00e1pida e precisa de um grande n\u00famero de imagens, contornando a necessidade de anota\u00e7\u00e3o manual demorada.</p>"},{"location":"models/sam/#gere-seu-conjunto-de-dados-de-segmentacao-usando-um-modelo-de-deteccao","title":"Gere seu Conjunto de Dados de Segmenta\u00e7\u00e3o Usando um Modelo de Detec\u00e7\u00e3o","text":"<p>Para fazer a autoanota\u00e7\u00e3o do seu conjunto de dados com o framework Ultralytics, use a fun\u00e7\u00e3o <code>auto_annotate</code> conforme mostrado abaixo:</p> <p>Exemplo</p> Python <pre><code>from ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"caminho/para/imagens\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argumento Tipo Descri\u00e7\u00e3o Padr\u00e3o data str Caminho para uma pasta que cont\u00e9m as imagens a serem anotadas. det_model str, opcional Modelo de detec\u00e7\u00e3o YOLO pr\u00e9-treinado. O padr\u00e3o \u00e9 'yolov8x.pt'. 'yolov8x.pt' sam_model str, opcional Modelo de segmenta\u00e7\u00e3o SAM pr\u00e9-treinado. O padr\u00e3o \u00e9 'sam_b.pt'. 'sam_b.pt' device str, opcional Dispositivo no qual executar os modelos. O padr\u00e3o \u00e9 uma string vazia (CPU ou GPU, se dispon\u00edvel). output_dir str, None, opcional Diret\u00f3rio para salvar os resultados anotados. O padr\u00e3o \u00e9 uma pasta 'labels' no mesmo diret\u00f3rio de 'data'. None <p>A fun\u00e7\u00e3o <code>auto_annotate</code> recebe o caminho para suas imagens, com argumentos opcionais para especificar os modelos de detec\u00e7\u00e3o pr\u00e9-treinados e de segmenta\u00e7\u00e3o SAM, o dispositivo onde executar os modelos e o diret\u00f3rio de sa\u00edda para salvar os resultados anotados.</p> <p>A autoanota\u00e7\u00e3o com modelos pr\u00e9-treinados pode reduzir drasticamente o tempo e o esfor\u00e7o necess\u00e1rios para criar conjuntos de dados de segmenta\u00e7\u00e3o de alta qualidade. Esse recurso \u00e9 especialmente ben\u00e9fico para pesquisadores e desenvolvedores que lidam com grandes cole\u00e7\u00f5es de imagens, pois permite que eles se concentrem no desenvolvimento e na avalia\u00e7\u00e3o do modelo, em vez de na anota\u00e7\u00e3o manual.</p>"},{"location":"models/sam/#citacoes-e-reconhecimentos","title":"Cita\u00e7\u00f5es e Reconhecimentos","text":"<p>Se voc\u00ea encontrar o SAM \u00fatil em seu trabalho de pesquisa ou desenvolvimento, considere citar nosso artigo:</p> BibTeX <pre><code>@misc{kirillov2023segment,\n      title={Segment Anything},\n      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross Girshick},\n      year={2023},\n      eprint={2304.02643},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Gostar\u00edamos de expressar nossa gratid\u00e3o \u00e0 Meta AI por criar e manter esse recurso valioso para a comunidade de vis\u00e3o computacional.</p> <p>keywords: Segment Anything, Modelo de Segmenta\u00e7\u00e3o de Qualquer Coisa, SAM, SAM da Meta, segmenta\u00e7\u00e3o de imagem, segmenta\u00e7\u00e3o baseada em prompts, desempenho de transfer\u00eancia zero, conjunto de dados SA-1B, arquitetura avan\u00e7ada, autoanota\u00e7\u00e3o, Ultralytics, modelos pr\u00e9-treinados, SAM base, SAM large, segmenta\u00e7\u00e3o de inst\u00e2ncias, vis\u00e3o computacional, IA, intelig\u00eancia artificial, aprendizado de m\u00e1quina, anota\u00e7\u00e3o de dados, m\u00e1scaras de segmenta\u00e7\u00e3o, modelo de detec\u00e7\u00e3o, modelo de detec\u00e7\u00e3o YOLO, bibtex, Meta AI.</p>"},{"location":"models/yolo-nas/","title":"YOLO-NAS","text":""},{"location":"models/yolo-nas/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Desenvolvido pela Deci AI, o YOLO-NAS \u00e9 um modelo de detec\u00e7\u00e3o de objetos inovador. \u00c9 o produto da tecnologia avan\u00e7ada de Busca de Arquitetura Neural, meticulosamente projetado para superar as limita\u00e7\u00f5es dos modelos YOLO anteriores. Com melhorias significativas no suporte \u00e0 quantiza\u00e7\u00e3o e compromisso entre precis\u00e3o e lat\u00eancia, o YOLO-NAS representa um grande avan\u00e7o na detec\u00e7\u00e3o de objetos.</p> <p> Vis\u00e3o geral do YOLO-NAS. O YOLO-NAS utiliza blocos que suportam quantiza\u00e7\u00e3o e quantiza\u00e7\u00e3o seletiva para obter um desempenho ideal. O modelo, quando convertido para sua vers\u00e3o quantizada INT8, apresenta uma queda m\u00ednima na precis\u00e3o, uma melhoria significativa em rela\u00e7\u00e3o a outros modelos. Esses avan\u00e7os culminam em uma arquitetura superior com capacidades de detec\u00e7\u00e3o de objetos sem precedentes e desempenho excepcional.</p>"},{"location":"models/yolo-nas/#principais-caracteristicas","title":"Principais Caracter\u00edsticas","text":"<ul> <li>Bloco B\u00e1sico Amig\u00e1vel para Quantiza\u00e7\u00e3o: O YOLO-NAS introduz um novo bloco b\u00e1sico que \u00e9 amigo da quantiza\u00e7\u00e3o, abordando uma das limita\u00e7\u00f5es significativas dos modelos YOLO anteriores.</li> <li>Treinamento e Quantiza\u00e7\u00e3o Sofisticados: O YOLO-NAS utiliza esquemas avan\u00e7ados de treinamento e quantiza\u00e7\u00e3o p\u00f3s-treinamento para melhorar o desempenho.</li> <li>Otimiza\u00e7\u00e3o AutoNAC e Pr\u00e9-Treinamento: O YOLO-NAS utiliza a otimiza\u00e7\u00e3o AutoNAC e \u00e9 pr\u00e9-treinado em conjuntos de dados proeminentes, como COCO, Objects365 e Roboflow 100. Esse pr\u00e9-treinamento torna o modelo extremamente adequado para tarefas de detec\u00e7\u00e3o de objetos em ambientes de produ\u00e7\u00e3o.</li> </ul>"},{"location":"models/yolo-nas/#modelos-pre-treinados","title":"Modelos Pr\u00e9-Treinados","text":"<p>Experimente o poder da detec\u00e7\u00e3o de objetos de \u00faltima gera\u00e7\u00e3o com os modelos pr\u00e9-treinados do YOLO-NAS fornecidos pela Ultralytics. Esses modelos foram projetados para oferecer um desempenho excelente em termos de velocidade e precis\u00e3o. Escolha entre v\u00e1rias op\u00e7\u00f5es adaptadas \u00e0s suas necessidades espec\u00edficas:</p> Modelo mAP Lat\u00eancia (ms) YOLO-NAS S 47.5 3.21 YOLO-NAS M 51.55 5.85 YOLO-NAS L 52.22 7.87 YOLO-NAS S INT-8 47.03 2.36 YOLO-NAS M INT-8 51.0 3.78 YOLO-NAS L INT-8 52.1 4.78 <p>Cada variante do modelo foi projetada para oferecer um equil\u00edbrio entre Precis\u00e3o M\u00e9dia M\u00e9dia (mAP) e lat\u00eancia, ajudando voc\u00ea a otimizar suas tarefas de detec\u00e7\u00e3o de objetos em termos de desempenho e velocidade.</p>"},{"location":"models/yolo-nas/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>A Ultralytics tornou os modelos YOLO-NAS f\u00e1ceis de serem integrados em suas aplica\u00e7\u00f5es Python por meio de nosso pacote <code>ultralytics</code>. O pacote fornece uma API Python de f\u00e1cil utiliza\u00e7\u00e3o para simplificar o processo.</p> <p>Os seguintes exemplos mostram como usar os modelos YOLO-NAS com o pacote <code>ultralytics</code> para infer\u00eancia e valida\u00e7\u00e3o:</p>"},{"location":"models/yolo-nas/#exemplos-de-inferencia-e-validacao","title":"Exemplos de Infer\u00eancia e Valida\u00e7\u00e3o","text":"<p>Neste exemplo, validamos o YOLO-NAS-s no conjunto de dados COCO8.</p> <p>Exemplo</p> <p>Este exemplo fornece um c\u00f3digo simples de infer\u00eancia e valida\u00e7\u00e3o para o YOLO-NAS. Para lidar com os resultados da infer\u00eancia, consulte o modo Predict. Para usar o YOLO-NAS com modos adicionais, consulte Val e Export. O YOLO-NAS no pacote <code>ultralytics</code> n\u00e3o suporta treinamento.</p> PythonCLI <p>Arquivos de modelos pr\u00e9-treinados <code>*.pt</code> do PyTorch podem ser passados para a classe <code>NAS()</code> para criar uma inst\u00e2ncia do modelo em Python:</p> <pre><code>from ultralytics import NAS\n\n# Carrega um modelo YOLO-NAS-s pr\u00e9-treinado no COCO\nmodel = NAS('yolo_nas_s.pt')\n\n# Exibe informa\u00e7\u00f5es do modelo (opcional)\nmodel.info()\n\n# Valida o modelo no conjunto de dados de exemplo COCO8\nresults = model.val(data='coco8.yaml')\n\n# Executa infer\u00eancia com o modelo YOLO-NAS-s na imagem 'bus.jpg'\nresults = model('caminho/para/bus.jpg')\n</code></pre> <p>Comandos de CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Carrega um modelo YOLO-NAS-s pr\u00e9-treinado no COCO e valida seu desempenho no conjunto de dados de exemplo COCO8\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Carrega um modelo YOLO-NAS-s pr\u00e9-treinado no COCO e executa infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolo_nas_s.pt source=caminho/para/bus.jpg\n</code></pre>"},{"location":"models/yolo-nas/#tarefas-e-modos-compativeis","title":"Tarefas e Modos Compat\u00edveis","text":"<p>Oferecemos tr\u00eas variantes dos modelos YOLO-NAS: Pequeno (s), M\u00e9dio (m) e Grande (l). Cada variante foi projetada para atender a diferentes necessidades computacionais e de desempenho:</p> <ul> <li>YOLO-NAS-s: Otimizado para ambientes com recursos computacionais limitados, mas efici\u00eancia \u00e9 fundamental.</li> <li>YOLO-NAS-m: Oferece uma abordagem equilibrada, adequada para detec\u00e7\u00e3o de objetos em geral com maior precis\u00e3o.</li> <li>YOLO-NAS-l: Adaptado para cen\u00e1rios que requerem a maior precis\u00e3o, onde os recursos computacionais s\u00e3o menos restritos.</li> </ul> <p>Abaixo est\u00e1 uma vis\u00e3o geral detalhada de cada modelo, incluindo links para seus pesos pr\u00e9-treinados, as tarefas que eles suportam e sua compatibilidade com diferentes modos de opera\u00e7\u00e3o.</p> Tipo de Modelo Pesos Pr\u00e9-Treinados Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o YOLO-NAS-s yolo_nas_s.pt Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u274c \u2705 YOLO-NAS-m yolo_nas_m.pt Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u274c \u2705 YOLO-NAS-l yolo_nas_l.pt Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u274c \u2705"},{"location":"models/yolo-nas/#citacoes-e-agradecimentos","title":"Cita\u00e7\u00f5es e Agradecimentos","text":"<p>Se voc\u00ea utilizar o YOLO-NAS em seus estudos ou trabalho de desenvolvimento, por favor, cite o SuperGradients:</p> BibTeX <pre><code>@misc{supergradients,\n      doi = {10.5281/ZENODO.7789328},\n      url = {https://zenodo.org/record/7789328},\n      author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\n      title = {Super-Gradients},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      year = {2021},\n}\n</code></pre> <p>Expressamos nossa gratid\u00e3o \u00e0 equipe SuperGradients da Deci AI por seus esfor\u00e7os na cria\u00e7\u00e3o e manuten\u00e7\u00e3o deste recurso valioso para a comunidade de vis\u00e3o computacional. Acreditamos que o YOLO-NAS, com sua arquitetura inovadora e capacidades superiores de detec\u00e7\u00e3o de objetos, se tornar\u00e1 uma ferramenta fundamental para desenvolvedores e pesquisadores.</p> <p>keywords: YOLO-NAS, Deci AI, detec\u00e7\u00e3o de objetos, aprendizado profundo, busca de arquitetura neural, API do Ultralytics Python, modelo YOLO, SuperGradients, modelos pr\u00e9-treinados, bloco b\u00e1sico amig\u00e1vel para quantiza\u00e7\u00e3o, esquemas avan\u00e7ados de treinamento, quantiza\u00e7\u00e3o p\u00f3s-treinamento, otimiza\u00e7\u00e3o AutoNAC, COCO, Objects365, Roboflow 100</p>"},{"location":"models/yolov3/","title":"YOLOv3, YOLOv3-Ultralytics, e YOLOv3u","text":""},{"location":"models/yolov3/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este documento apresenta uma vis\u00e3o geral de tr\u00eas modelos de detec\u00e7\u00e3o de objetos intimamente relacionados, nomeadamente o YOLOv3, YOLOv3-Ultralytics e YOLOv3u.</p> <ol> <li> <p>YOLOv3: Esta \u00e9 a terceira vers\u00e3o do algoritmo de detec\u00e7\u00e3o de objetos You Only Look Once (YOLO). Originalmente desenvolvido por Joseph Redmon, o YOLOv3 melhorou seus predecessores ao introduzir recursos como previs\u00f5es em v\u00e1rias escalas e tr\u00eas tamanhos diferentes de kernels de detec\u00e7\u00e3o.</p> </li> <li> <p>YOLOv3-Ultralytics: Esta \u00e9 a implementa\u00e7\u00e3o do YOLOv3 pela Ultralytics. Ela reproduz a arquitetura original do YOLOv3 e oferece funcionalidades adicionais, como suporte para mais modelos pr\u00e9-treinados e op\u00e7\u00f5es de personaliza\u00e7\u00e3o mais f\u00e1ceis.</p> </li> <li> <p>YOLOv3u: Esta \u00e9 uma vers\u00e3o atualizada do YOLOv3-Ultralytics que incorpora o cabe\u00e7alho dividido livre de \u00e2ncoras e sem \"objectness\" usado nos modelos YOLOv8. O YOLOv3u mant\u00e9m a mesma arquitetura de \"backbone\" e \"neck\" do YOLOv3, mas com o cabe\u00e7alho de detec\u00e7\u00e3o atualizado do YOLOv8.</p> </li> </ol> <p></p>"},{"location":"models/yolov3/#principais-caracteristicas","title":"Principais Caracter\u00edsticas","text":"<ul> <li> <p>YOLOv3: Introduziu o uso de tr\u00eas escalas diferentes para detec\u00e7\u00e3o, aproveitando tr\u00eas tamanhos diferentes de kernels de detec\u00e7\u00e3o: 13x13, 26x26 e 52x52. Isso melhorou significativamente a precis\u00e3o da detec\u00e7\u00e3o para objetos de diferentes tamanhos. Al\u00e9m disso, o YOLOv3 adicionou recursos como previs\u00f5es multi-r\u00f3tulos para cada caixa delimitadora e uma rede de extra\u00e7\u00e3o de caracter\u00edsticas melhor.</p> </li> <li> <p>YOLOv3-Ultralytics: A implementa\u00e7\u00e3o do YOLOv3 pela Ultralytics oferece o mesmo desempenho do modelo original, por\u00e9m possui suporte adicional para mais modelos pr\u00e9-treinados, m\u00e9todos de treinamento adicionais e op\u00e7\u00f5es de personaliza\u00e7\u00e3o mais f\u00e1ceis. Isso torna o modelo mais vers\u00e1til e f\u00e1cil de usar para aplica\u00e7\u00f5es pr\u00e1ticas.</p> </li> <li> <p>YOLOv3u: Este modelo atualizado incorpora o cabe\u00e7alho dividido livre de \u00e2ncoras e \"objectness\" do YOLOv8. Ao eliminar a necessidade de caixas de \u00e2ncoras pr\u00e9-definidas e pontua\u00e7\u00f5es de \"objectness\", esse design de cabe\u00e7alho de detec\u00e7\u00e3o pode melhorar a capacidade do modelo de detectar objetos de tamanhos e formatos variados. Isso torna o YOLOv3u mais robusto e preciso para tarefas de detec\u00e7\u00e3o de objetos.</p> </li> </ul>"},{"location":"models/yolov3/#tarefas-e-modos-suportados","title":"Tarefas e Modos Suportados","text":"<p>A s\u00e9rie YOLOv3, incluindo YOLOv3, YOLOv3-Ultralytics e YOLOv3u, foi projetada especificamente para tarefas de detec\u00e7\u00e3o de objetos. Esses modelos s\u00e3o conhecidos por sua efic\u00e1cia em v\u00e1rios cen\u00e1rios do mundo real, equilibrando precis\u00e3o e velocidade. Cada variante oferece recursos e otimiza\u00e7\u00f5es \u00fanicos, tornando-os adequados para uma variedade de aplica\u00e7\u00f5es.</p> <p>Os tr\u00eas modelos suportam um conjunto abrangente de modos, garantindo versatilidade em v\u00e1rias etapas do desenvolvimento e implanta\u00e7\u00e3o de modelos. Esses modos incluem Infer\u00eancia, Valida\u00e7\u00e3o, Treinamento e Exporta\u00e7\u00e3o, fornecendo aos usu\u00e1rios um conjunto completo de ferramentas para detec\u00e7\u00e3o eficaz de objetos.</p> Tipo de Modelo Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o YOLOv3 Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv3-Ultralytics Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv3u Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 <p>Esta tabela fornece uma vis\u00e3o r\u00e1pida das capacidades de cada variante do YOLOv3, destacando sua versatilidade e adequa\u00e7\u00e3o para v\u00e1rias tarefas e modos operacionais em fluxos de trabalho de detec\u00e7\u00e3o de objetos.</p>"},{"location":"models/yolov3/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Este exemplo apresenta exemplos simples de treinamento e infer\u00eancia do YOLOv3. Para obter documenta\u00e7\u00e3o completa sobre esses e outros modos, consulte as p\u00e1ginas de documenta\u00e7\u00e3o do Predict, Train, Val e Export.</p> <p>Exemplo</p> PythonCLI <p>Modelos pr\u00e9-treinados do PyTorch <code>*.pt</code>, bem como arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados para a classe <code>YOLO()</code> para criar uma inst\u00e2ncia do modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carregue um modelo YOLOv3n pr\u00e9-treinado na COCO\nmodel = YOLO('yolov3n.pt')\n\n# Exiba informa\u00e7\u00f5es sobre o modelo (opcional)\nmodel.info()\n\n# Treine o modelo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Execute infer\u00eancia com o modelo YOLOv3n na imagem 'bus.jpg'\nresults = model('caminho/para/bus.jpg')\n</code></pre> <p>Comandos CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Carregue um modelo YOLOv3n pr\u00e9-treinado na COCO e treine-o no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=yolov3n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carregue um modelo YOLOv3n pr\u00e9-treinado na COCO e execute infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov3n.pt source=caminho/para/bus.jpg\n</code></pre>"},{"location":"models/yolov3/#citacoes-e-reconhecimentos","title":"Cita\u00e7\u00f5es e Reconhecimentos","text":"<p>Se voc\u00ea utilizar o YOLOv3 em sua pesquisa, por favor, cite os artigos originais do YOLO e o reposit\u00f3rio Ultralytics YOLOv3:</p> BibTeX <pre><code>@article{redmon2018yolov3,\n  title={YOLOv3: An Incremental Improvement},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1804.02767},\n  year={2018}\n}\n</code></pre> <p>Agradecemos a Joseph Redmon e Ali Farhadi por desenvolverem o YOLOv3 original.</p>"},{"location":"models/yolov4/","title":"YOLOv4: Detec\u00e7\u00e3o de Objetos R\u00e1pida e Precisa","text":"<p>Bem-vindo \u00e0 p\u00e1gina de documenta\u00e7\u00e3o do Ultralytics para o YOLOv4, um detector de objetos em tempo real de \u00faltima gera\u00e7\u00e3o lan\u00e7ado em 2020 por Alexey Bochkovskiy em https://github.com/AlexeyAB/darknet. O YOLOv4 foi projetado para fornecer o equil\u00edbrio ideal entre velocidade e precis\u00e3o, tornando-o uma excelente escolha para muitas aplica\u00e7\u00f5es.</p> <p> Diagrama da arquitetura do YOLOv4. Mostra o design intricado da rede do YOLOv4, incluindo os componentes backbone, neck e head, bem como suas camadas interconectadas para uma detec\u00e7\u00e3o de objetos em tempo real otimizada.</p>"},{"location":"models/yolov4/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>YOLOv4 significa You Only Look Once vers\u00e3o 4. \u00c9 um modelo de detec\u00e7\u00e3o de objetos em tempo real desenvolvido para superar as limita\u00e7\u00f5es de vers\u00f5es anteriores do YOLO, como YOLOv3 e outros modelos de detec\u00e7\u00e3o de objetos. Ao contr\u00e1rio de outros detectores de objetos baseados em redes neurais convolucionais (CNN), o YOLOv4 \u00e9 aplic\u00e1vel n\u00e3o apenas a sistemas de recomenda\u00e7\u00e3o, mas tamb\u00e9m ao gerenciamento de processos independentes e \u00e0 redu\u00e7\u00e3o da entrada humana. Sua opera\u00e7\u00e3o em unidades de processamento gr\u00e1fico (GPUs) convencionais permite o uso em massa a um pre\u00e7o acess\u00edvel, e foi projetado para funcionar em tempo real em uma GPU convencional, exigindo apenas uma GPU para treinamento.</p>"},{"location":"models/yolov4/#arquitetura","title":"Arquitetura","text":"<p>O YOLOv4 faz uso de v\u00e1rias caracter\u00edsticas inovadoras que trabalham juntas para otimizar seu desempenho. Estas incluem Conex\u00f5es Residuais Ponderadas (WRC), Conex\u00f5es Parciais Cruzadas de Est\u00e1gio (CSP), Normaliza\u00e7\u00e3o Cruzada em Mini Lote (CmBN), Treinamento Autoadvers\u00e1rio (SAT), Ativa\u00e7\u00e3o Mish, Aumento de Dados Mosaic, Regulariza\u00e7\u00e3o DropBlock e Perda CIoU. Essas caracter\u00edsticas s\u00e3o combinadas para obter resultados de \u00faltima gera\u00e7\u00e3o.</p> <p>Um detector de objetos t\u00edpico \u00e9 composto por v\u00e1rias partes, incluindo a entrada, o backbone, o neck e o head. O backbone do YOLOv4 \u00e9 pr\u00e9-treinado no ImageNet e \u00e9 usado para prever as classes e caixas delimitadoras dos objetos. O backbone pode ser de v\u00e1rios modelos, incluindo VGG, ResNet, ResNeXt ou DenseNet. A parte neck do detector \u00e9 usada para coletar mapas de caracter\u00edsticas de diferentes est\u00e1gios e geralmente inclui v\u00e1rias caminhadas bottom-up e v\u00e1rias caminhadas top-down. A parte head \u00e9 respons\u00e1vel por fazer as detec\u00e7\u00f5es e classifica\u00e7\u00f5es finais dos objetos.</p>"},{"location":"models/yolov4/#bag-of-freebies","title":"Bag of Freebies","text":"<p>O YOLOv4 tamb\u00e9m faz uso de m\u00e9todos conhecidos como \"bag of freebies\" (saco de brindes), que s\u00e3o t\u00e9cnicas que melhoram a precis\u00e3o do modelo durante o treinamento sem aumentar o custo da infer\u00eancia. O aumento de dados \u00e9 uma t\u00e9cnica comum de \"bag of freebies\" usada na detec\u00e7\u00e3o de objetos, que aumenta a variabilidade das imagens de entrada para melhorar a robustez do modelo. Alguns exemplos de aumento de dados incluem distor\u00e7\u00f5es fotom\u00e9tricas (ajustando o brilho, contraste, matiz, satura\u00e7\u00e3o e ru\u00eddo de uma imagem) e distor\u00e7\u00f5es geom\u00e9tricas (adicionando dimensionamento aleat\u00f3rio, recorte, espelhamento e rota\u00e7\u00e3o). Essas t\u00e9cnicas ajudam o modelo a generalizar melhor para diferentes tipos de imagens.</p>"},{"location":"models/yolov4/#recursos-e-desempenho","title":"Recursos e Desempenho","text":"<p>O YOLOv4 foi projetado para oferecer velocidade e precis\u00e3o ideais na detec\u00e7\u00e3o de objetos. A arquitetura do YOLOv4 inclui o CSPDarknet53 como o backbone, o PANet como o neck e o YOLOv3 como a cabe\u00e7a de detec\u00e7\u00e3o. Esse design permite que o YOLOv4 realize detec\u00e7\u00e3o de objetos em uma velocidade impressionante, tornando-o adequado para aplica\u00e7\u00f5es em tempo real. O YOLOv4 tamb\u00e9m se destaca em termos de precis\u00e3o, alcan\u00e7ando resultados de \u00faltima gera\u00e7\u00e3o em benchmarks de detec\u00e7\u00e3o de objetos.</p>"},{"location":"models/yolov4/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>No momento da escrita, o Ultralytics n\u00e3o oferece suporte a modelos YOLOv4. Portanto, os usu\u00e1rios interessados em usar o YOLOv4 dever\u00e3o consultar diretamente o reposit\u00f3rio YOLOv4 no GitHub para instru\u00e7\u00f5es de instala\u00e7\u00e3o e uso.</p> <p>Aqui est\u00e1 uma breve vis\u00e3o geral das etapas t\u00edpicas que voc\u00ea pode seguir para usar o YOLOv4:</p> <ol> <li> <p>Visite o reposit\u00f3rio YOLOv4 no GitHub: https://github.com/AlexeyAB/darknet.</p> </li> <li> <p>Siga as instru\u00e7\u00f5es fornecidas no arquivo README para a instala\u00e7\u00e3o. Isso geralmente envolve clonar o reposit\u00f3rio, instalar as depend\u00eancias necess\u00e1rias e configurar as vari\u00e1veis de ambiente necess\u00e1rias.</p> </li> <li> <p>Uma vez que a instala\u00e7\u00e3o esteja completa, voc\u00ea pode treinar e usar o modelo de acordo com as instru\u00e7\u00f5es de uso fornecidas no reposit\u00f3rio. Isso geralmente envolve a prepara\u00e7\u00e3o do seu conjunto de dados, a configura\u00e7\u00e3o dos par\u00e2metros do modelo, o treinamento do modelo e, em seguida, o uso do modelo treinado para realizar a detec\u00e7\u00e3o de objetos.</p> </li> </ol> <p>Observe que as etapas espec\u00edficas podem variar dependendo do seu caso de uso espec\u00edfico e do estado atual do reposit\u00f3rio YOLOv4. Portanto, \u00e9 altamente recomend\u00e1vel consultar diretamente as instru\u00e7\u00f5es fornecidas no reposit\u00f3rio YOLOv4 do GitHub.</p> <p>Lamentamos qualquer inconveniente que isso possa causar e nos esfor\u00e7aremos para atualizar este documento com exemplos de uso para o Ultralytics assim que o suporte para o YOLOv4 for implementado.</p>"},{"location":"models/yolov4/#conclusao","title":"Conclus\u00e3o","text":"<p>O YOLOv4 \u00e9 um modelo poderoso e eficiente de detec\u00e7\u00e3o de objetos que oferece um equil\u00edbrio entre velocidade e precis\u00e3o. O uso de recursos exclusivos e t\u00e9cnicas \"Bag of Freebies\" durante o treinamento permite que ele tenha um excelente desempenho em tarefas de detec\u00e7\u00e3o de objetos em tempo real. O YOLOv4 pode ser treinado e usado por qualquer pessoa com uma GPU convencional, tornando-o acess\u00edvel e pr\u00e1tico para uma ampla variedade de aplica\u00e7\u00f5es.</p>"},{"location":"models/yolov4/#referencias-e-agradecimentos","title":"Refer\u00eancias e Agradecimentos","text":"<p>Gostar\u00edamos de agradecer aos autores do YOLOv4 por suas contribui\u00e7\u00f5es significativas no campo da detec\u00e7\u00e3o de objetos em tempo real:</p> BibTeX <pre><code>@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>O artigo original do YOLOv4 pode ser encontrado no arXiv. Os autores disponibilizaram seu trabalho publicamente, e o c\u00f3digo pode ser acessado no GitHub. Agradecemos seus esfor\u00e7os em avan\u00e7ar o campo e tornar seu trabalho acess\u00edvel \u00e0 comunidade em geral.</p>"},{"location":"models/yolov5/","title":"YOLOv5","text":""},{"location":"models/yolov5/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O YOLOv5u representa um avan\u00e7o nas metodologias de detec\u00e7\u00e3o de objetos. Origin\u00e1rio da arquitetura fundamental do modelo YOLOv5 desenvolvido pela Ultralytics, o YOLOv5u integra a divis\u00e3o da cabe\u00e7a do Ultralytics sem \u00e2ncora e sem certeza de objectness, uma forma\u00e7\u00e3o introduzida anteriormente nos modelos YOLOv8. Essa adapta\u00e7\u00e3o aprimora a arquitetura do modelo, resultando em uma rela\u00e7\u00e3o aprimorada entre precis\u00e3o e velocidade em tarefas de detec\u00e7\u00e3o de objetos. Com base nos resultados emp\u00edricos e em suas caracter\u00edsticas derivadas, o YOLOv5u oferece uma alternativa eficiente para aqueles que procuram solu\u00e7\u00f5es robustas tanto na pesquisa quanto em aplica\u00e7\u00f5es pr\u00e1ticas.</p> <p></p>"},{"location":"models/yolov5/#principais-recursos","title":"Principais Recursos","text":"<ul> <li> <p>Cabe\u00e7a do Ultralytics sem \u00c2ncora: Modelos tradicionais de detec\u00e7\u00e3o de objetos dependem de caixas \u00e2ncora predefinidas para prever as localiza\u00e7\u00f5es dos objetos. No entanto, o YOLOv5u moderniza essa abordagem. Ao adotar uma cabe\u00e7a do Ultralytics sem \u00e2ncora, ele garante um mecanismo de detec\u00e7\u00e3o mais flex\u00edvel e adapt\u00e1vel, melhorando consequentemente o desempenho em cen\u00e1rios diversos.</p> </li> <li> <p>Equil\u00edbrio otimizado entre precis\u00e3o e velocidade: Velocidade e precis\u00e3o muitas vezes puxam em dire\u00e7\u00f5es opostas. Mas o YOLOv5u desafia esse equil\u00edbrio. Ele oferece um equil\u00edbrio calibrado, garantindo detec\u00e7\u00f5es em tempo real sem comprometer a precis\u00e3o. Esse recurso \u00e9 particularmente valioso para aplicativos que exigem respostas r\u00e1pidas, como ve\u00edculos aut\u00f4nomos, rob\u00f3tica e an\u00e1lise de v\u00eddeo em tempo real.</p> </li> <li> <p>Variedade de Modelos Pr\u00e9-Treinados: Entendendo que diferentes tarefas exigem conjuntos de ferramentas diferentes, o YOLOv5u oferece uma variedade de modelos pr\u00e9-treinados. Se voc\u00ea est\u00e1 focado em Infer\u00eancia, Valida\u00e7\u00e3o ou Treinamento, h\u00e1 um modelo personalizado esperando por voc\u00ea. Essa variedade garante que voc\u00ea n\u00e3o esteja apenas usando uma solu\u00e7\u00e3o gen\u00e9rica, mas sim um modelo ajustado especificamente para o seu desafio \u00fanico.</p> </li> </ul>"},{"location":"models/yolov5/#tarefas-e-modos-suportados","title":"Tarefas e Modos Suportados","text":"<p>Os modelos YOLOv5u, com v\u00e1rios pesos pr\u00e9-treinados, se destacam nas tarefas de Detec\u00e7\u00e3o de Objetos. Eles suportam uma ampla gama de modos, tornando-os adequados para aplica\u00e7\u00f5es diversas, desde o desenvolvimento at\u00e9 a implanta\u00e7\u00e3o.</p> Tipo de Modelo Pesos Pr\u00e9-Treinados Tarefa Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o YOLOv5u <code>yolov5nu</code>, <code>yolov5su</code>, <code>yolov5mu</code>, <code>yolov5lu</code>, <code>yolov5xu</code>, <code>yolov5n6u</code>, <code>yolov5s6u</code>, <code>yolov5m6u</code>, <code>yolov5l6u</code>, <code>yolov5x6u</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 <p>Essa tabela oferece uma vis\u00e3o detalhada das variantes do modelo YOLOv5u, destacando sua aplicabilidade em tarefas de detec\u00e7\u00e3o de objetos e suporte a diversos modos operacionais, como Infer\u00eancia, Valida\u00e7\u00e3o, Treinamento e Exporta\u00e7\u00e3o. Esse suporte abrangente garante que os usu\u00e1rios possam aproveitar totalmente as capacidades dos modelos YOLOv5u em uma ampla gama de cen\u00e1rios de detec\u00e7\u00e3o de objetos.</p>"},{"location":"models/yolov5/#metricas-de-desempenho","title":"M\u00e9tricas de Desempenho","text":"<p>Desempenho</p> Detec\u00e7\u00e3o <p>Consulte a Documenta\u00e7\u00e3o de Detec\u00e7\u00e3o para exemplos de uso com esses modelos treinados no conjunto de dados COCO, que incluem 80 classes pr\u00e9-treinadas.</p> Modelo YAML tamanho<sup>(pixels) mAP<sup>val50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) yolov5nu.pt yolov5n.yaml 640 34.3 73.6 1.06 2.6 7.7 yolov5su.pt yolov5s.yaml 640 43.0 120.7 1.27 9.1 24.0 yolov5mu.pt yolov5m.yaml 640 49.0 233.9 1.86 25.1 64.2 yolov5lu.pt yolov5l.yaml 640 52.2 408.4 2.50 53.2 135.0 yolov5xu.pt yolov5x.yaml 640 53.2 763.2 3.81 97.2 246.4 yolov5n6u.pt yolov5n6.yaml 1280 42.1 211.0 1.83 4.3 7.8 yolov5s6u.pt yolov5s6.yaml 1280 48.6 422.6 2.34 15.3 24.6 yolov5m6u.pt yolov5m6.yaml 1280 53.6 810.9 4.36 41.2 65.7 yolov5l6u.pt yolov5l6.yaml 1280 55.7 1470.9 5.47 86.1 137.4 yolov5x6u.pt yolov5x6.yaml 1280 56.8 2436.5 8.98 155.4 250.7"},{"location":"models/yolov5/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Este exemplo fornece exemplos simples de treinamento e infer\u00eancia do YOLOv5. Para documenta\u00e7\u00e3o completa sobre esses e outros modos, consulte as p\u00e1ginas de documenta\u00e7\u00e3o Predict, Train, Val e Export.</p> <p>Exemplo</p> PythonCLI <p>Modelos pr\u00e9-treinados <code>*.pt</code> do PyTorch, assim como os arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados para a classe <code>YOLO()</code> para criar uma inst\u00e2ncia do modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv5n pr\u00e9-treinado no COCO\nmodelo = YOLO('yolov5n.pt')\n\n# Mostra informa\u00e7\u00f5es do modelo (opcional)\nmodelo.info()\n\n# Treina o modelo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresultados = modelo.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Executa a infer\u00eancia com o modelo YOLOv5n na imagem 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Comandos CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Carrega um modelo YOLOv5n pr\u00e9-treinado no COCO e o treina no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carrega um modelo YOLOv5n pr\u00e9-treinado no COCO e executa a infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov5/#citacoes-e-agradecimentos","title":"Cita\u00e7\u00f5es e Agradecimentos","text":"<p>Se voc\u00ea usar o YOLOv5 ou YOLOv5u em sua pesquisa, por favor, cite o reposit\u00f3rio YOLOv5 da Ultralytics da seguinte forma:</p> BibTeX <pre><code>@software{yolov5,\n  title = {Ultralytics YOLOv5},\n  author = {Glenn Jocher},\n  year = {2020},\n  version = {7.0},\n  license = {AGPL-3.0},\n  url = {https://github.com/ultralytics/yolov5},\n  doi = {10.5281/zenodo.3908559},\n  orcid = {0000-0001-5950-6979}\n}\n</code></pre> <p>Observe que os modelos YOLOv5 s\u00e3o fornecidos sob licen\u00e7as AGPL-3.0 e Enterprise.</p>"},{"location":"models/yolov6/","title":"Meituan YOLOv6","text":""},{"location":"models/yolov6/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O Meituan YOLOv6 \u00e9 um detector de objetos de ponta que oferece um equil\u00edbrio not\u00e1vel entre velocidade e precis\u00e3o, tornando-se uma escolha popular para aplica\u00e7\u00f5es em tempo real. Este modelo apresenta v\u00e1rias melhorias em sua arquitetura e esquema de treinamento, incluindo a implementa\u00e7\u00e3o de um m\u00f3dulo de Concatena\u00e7\u00e3o Bidirecional (BiC), uma estrat\u00e9gia de treinamento assistido por \u00e2ncora (AAT) e um design aprimorado de espinha dorsal e pesco\u00e7o para obter precis\u00e3o de \u00faltima gera\u00e7\u00e3o no conjunto de dados COCO.</p> <p> Vis\u00e3o geral do YOLOv6. Diagrama da arquitetura do modelo mostrando os componentes de rede redesenhados e as estrat\u00e9gias de treinamento que levaram a melhorias significativas no desempenho. (a) O pesco\u00e7o do YOLOv6 (N e S s\u00e3o mostrados). RepBlocks \u00e9 substitu\u00edda por CSPStackRep para M/L. (b) A estrutura de um m\u00f3dulo BiC. (c) Um bloco SimCSPSPPF. (fonte).</p>"},{"location":"models/yolov6/#principais-caracteristicas","title":"Principais Caracter\u00edsticas","text":"<ul> <li>M\u00f3dulo de Concatena\u00e7\u00e3o Bidirecional (BiC): O YOLOv6 introduz um m\u00f3dulo BiC no pesco\u00e7o do detector, aprimorando os sinais de localiza\u00e7\u00e3o e oferecendo ganhos de desempenho com uma degrada\u00e7\u00e3o de velocidade insignificante.</li> <li>Estrat\u00e9gia de Treinamento Assistido por \u00c2ncora (AAT): Este modelo prop\u00f5e AAT para aproveitar os benef\u00edcios dos paradigmas baseados em \u00e2ncoras e sem \u00e2ncoras sem comprometer a efici\u00eancia da infer\u00eancia.</li> <li>Design de Espinha Dorsal e Pesco\u00e7o Aprimorado: Ao aprofundar o YOLOv6 para incluir mais uma etapa na espinha dorsal e no pesco\u00e7o, este modelo alcan\u00e7a desempenho de \u00faltima gera\u00e7\u00e3o no conjunto de dados COCO com entrada de alta resolu\u00e7\u00e3o.</li> <li>Estrat\u00e9gia de Auto-Destila\u00e7\u00e3o: Uma nova estrat\u00e9gia de auto-destila\u00e7\u00e3o \u00e9 implementada para aumentar o desempenho de modelos menores do YOLOv6, aprimorando o ramo auxiliar de regress\u00e3o durante o treinamento e removendo-o durante a infer\u00eancia para evitar uma queda significativa na velocidade.</li> </ul>"},{"location":"models/yolov6/#metricas-de-desempenho","title":"M\u00e9tricas de Desempenho","text":"<p>O YOLOv6 fornece v\u00e1rios modelos pr\u00e9-treinados com diferentes escalas:</p> <ul> <li>YOLOv6-N: 37,5% AP na val2017 do COCO a 1187 FPS com GPU NVIDIA Tesla T4.</li> <li>YOLOv6-S: 45,0% de AP a 484 FPS.</li> <li>YOLOv6-M: 50,0% de AP a 226 FPS.</li> <li>YOLOv6-L: 52,8% de AP a 116 FPS.</li> <li>YOLOv6-L6: Precis\u00e3o de \u00faltima gera\u00e7\u00e3o em tempo real.</li> </ul> <p>O YOLOv6 tamb\u00e9m fornece modelos quantizados para diferentes precis\u00f5es e modelos otimizados para plataformas m\u00f3veis.</p>"},{"location":"models/yolov6/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Este exemplo fornece exemplos simples de treinamento e infer\u00eancia do YOLOv6. Para documenta\u00e7\u00e3o completa sobre esses e outros modos, consulte as p\u00e1ginas de documenta\u00e7\u00e3o Predict, Train, Val e Export.</p> <p>Exemplo</p> PythonCLI <p>Modelos pr\u00e9-treinados <code>*.pt</code> do PyTorch, assim como arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados \u00e0 classe <code>YOLO()</code> para criar uma inst\u00e2ncia do modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Constr\u00f3i um modelo YOLOv6n do zero\nmodel = YOLO('yolov6n.yaml')\n\n# Exibe informa\u00e7\u00f5es do modelo (opcional)\nmodel.info()\n\n# Treina o modelo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Executa infer\u00eancia com o modelo YOLOv6n na imagem 'bus.jpg'\nresults = model('caminho/para/onibus.jpg')\n</code></pre> <p>Comandos da CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Constr\u00f3i um modelo YOLOv6n do zero e o treina no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Constr\u00f3i um modelo YOLOv6n do zero e executa infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov6n.yaml source=caminho/para/onibus.jpg\n</code></pre>"},{"location":"models/yolov6/#tarefas-e-modos-suportados","title":"Tarefas e Modos Suportados","text":"<p>A s\u00e9rie YOLOv6 oferece uma variedade de modelos, cada um otimizado para Detec\u00e7\u00e3o de Objetos de alta performance. Esses modelos atendem a diferentes necessidades computacionais e requisitos de precis\u00e3o, tornando-os vers\u00e1teis para uma ampla variedade de aplica\u00e7\u00f5es.</p> Tipo de Modelo Pesos Pr\u00e9-treinados Tarefas Suportadas Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o YOLOv6-N <code>yolov6-n.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-S <code>yolov6-s.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-M <code>yolov6-m.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-L <code>yolov6-l.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-L6 <code>yolov6-l6.pt</code> Detec\u00e7\u00e3o de Objetos \u2705 \u2705 \u2705 \u2705 <p>Esta tabela fornece uma vis\u00e3o geral detalhada das variantes do modelo YOLOv6, destacando suas capacidades em tarefas de detec\u00e7\u00e3o de objetos e sua compatibilidade com v\u00e1rios modos operacionais, como infer\u00eancia, valida\u00e7\u00e3o, treinamento e exporta\u00e7\u00e3o. Esse suporte abrangente garante que os usu\u00e1rios possam aproveitar totalmente as capacidades dos modelos YOLOv6 em uma ampla gama de cen\u00e1rios de detec\u00e7\u00e3o de objetos.</p>"},{"location":"models/yolov6/#citacoes-e-agradecimentos","title":"Cita\u00e7\u00f5es e Agradecimentos","text":"<p>Gostar\u00edamos de agradecer aos autores por suas contribui\u00e7\u00f5es significativas no campo da detec\u00e7\u00e3o de objetos em tempo real:</p> BibTeX <pre><code>@misc{li2023yolov6,\n      title={YOLOv6 v3.0: A Full-Scale Reloading},\n      author={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\n      year={2023},\n      eprint={2301.05586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>O artigo original do YOLOv6 pode ser encontrado no arXiv. Os autores disponibilizaram publicamente seu trabalho, e o c\u00f3digo pode ser acessado no GitHub. Agradecemos seus esfor\u00e7os em avan\u00e7ar no campo e disponibilizar seu trabalho para a comunidade em geral.</p>"},{"location":"models/yolov7/","title":"YOLOv7: Trein\u00e1vel Bag-of-Freebies","text":"<p>O YOLOv7 \u00e9 um detector de objetos em tempo real state-of-the-art que supera todos os detectores de objetos conhecidos em termos de velocidade e precis\u00e3o na faixa de 5 FPS a 160 FPS. Ele possui a maior precis\u00e3o (56,8% de AP) entre todos os detectores de objetos em tempo real conhecidos com 30 FPS ou mais no GPU V100. Al\u00e9m disso, o YOLOv7 supera outros detectores de objetos, como YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5 e muitos outros em velocidade e precis\u00e3o. O modelo \u00e9 treinado no conjunto de dados MS COCO do zero, sem usar outros conjuntos de dados ou pesos pr\u00e9-treinados. O c\u00f3digo-fonte para o YOLOv7 est\u00e1 dispon\u00edvel no GitHub.</p> <p> **Compara\u00e7\u00e3o de detectores de objetos state-of-the-art. ** A partir dos resultados na Tabela 2, sabemos que o m\u00e9todo proposto tem a melhor rela\u00e7\u00e3o velocidade-precis\u00e3o de forma abrangente. Se compararmos o YOLOv7-tiny-SiLU com o YOLOv5-N (r6.1), nosso m\u00e9todo \u00e9 127 FPS mais r\u00e1pido e 10,7% mais preciso em AP. Al\u00e9m disso, o YOLOv7 tem 51,4% de AP em uma taxa de quadros de 161 FPS, enquanto o PPYOLOE-L com o mesmo AP tem apenas uma taxa de quadros de 78 FPS. Em termos de uso de par\u00e2metros, o YOLOv7 \u00e9 41% menor do que o PPYOLOE-L. Se compararmos o YOLOv7-X com uma velocidade de infer\u00eancia de 114 FPS com o YOLOv5-L (r6.1) com uma velocidade de infer\u00eancia de 99 FPS, o YOLOv7-X pode melhorar o AP em 3,9%. Se o YOLOv7-X for comparado com o YOLOv5-X (r6.1) de escala similar, a velocidade de infer\u00eancia do YOLOv7-X \u00e9 31 FPS mais r\u00e1pida. Al\u00e9m disso, em termos da quantidade de par\u00e2metros e c\u00e1lculos, o YOLOv7-X reduz 22% dos par\u00e2metros e 8% dos c\u00e1lculos em compara\u00e7\u00e3o com o YOLOv5-X (r6.1), mas melhora o AP em 2,2% (Fonte).</p>"},{"location":"models/yolov7/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>A detec\u00e7\u00e3o de objetos em tempo real \u00e9 um componente importante em muitos sistemas de vis\u00e3o computacional, incluindo rastreamento de m\u00faltiplos objetos, dire\u00e7\u00e3o aut\u00f4noma, rob\u00f3tica e an\u00e1lise de imagens m\u00e9dicas. Nos \u00faltimos anos, o desenvolvimento de detec\u00e7\u00e3o de objetos em tempo real tem se concentrado em projetar arquiteturas eficientes e melhorar a velocidade de infer\u00eancia de v\u00e1rias CPUs, GPUs e unidades de processamento neural (NPUs). O YOLOv7 suporta tanto GPUs m\u00f3veis quanto dispositivos GPU, desde a borda at\u00e9 a nuvem.</p> <p>Ao contr\u00e1rio dos detectores de objetos em tempo real tradicionais que se concentram na otimiza\u00e7\u00e3o de arquitetura, o YOLOv7 introduz um foco na otimiza\u00e7\u00e3o do processo de treinamento. Isso inclui m\u00f3dulos e m\u00e9todos de otimiza\u00e7\u00e3o projetados para melhorar a precis\u00e3o da detec\u00e7\u00e3o de objetos sem aumentar o custo de infer\u00eancia, um conceito conhecido como \"trein\u00e1vel bag-of-freebies\".</p>"},{"location":"models/yolov7/#recursos-principais","title":"Recursos Principais","text":"<p>O YOLOv7 apresenta v\u00e1rios recursos principais:</p> <ol> <li> <p>Reparametriza\u00e7\u00e3o do Modelo: O YOLOv7 prop\u00f5e um modelo reparametrizado planejado, que \u00e9 uma estrat\u00e9gia aplic\u00e1vel a camadas em diferentes redes com o conceito de caminho de propaga\u00e7\u00e3o de gradiente.</p> </li> <li> <p>Atribui\u00e7\u00e3o Din\u00e2mica de R\u00f3tulo: O treinamento do modelo com v\u00e1rias camadas de sa\u00edda apresenta um novo problema: \"Como atribuir alvos din\u00e2micos para as sa\u00eddas de diferentes ramifica\u00e7\u00f5es?\" Para resolver esse problema, o YOLOv7 introduz um novo m\u00e9todo de atribui\u00e7\u00e3o de r\u00f3tulo chamado atribui\u00e7\u00e3o de r\u00f3tulo orientada por lideran\u00e7a de granularidade fina (coarse-to-fine).</p> </li> <li> <p>Escalonamento Estendido e Composto: O YOLOv7 prop\u00f5e m\u00e9todos de \"escalonamento estendido\" e \"escalonamento composto\" para o detector de objetos em tempo real que podem utilizar efetivamente par\u00e2metros e c\u00e1lculos.</p> </li> <li> <p>Efici\u00eancia: O m\u00e9todo proposto pelo YOLOv7 pode reduzir efetivamente cerca de 40% dos par\u00e2metros e 50% dos c\u00e1lculos do detector de objetos em tempo real state-of-the-art, al\u00e9m de apresentar uma velocidade de infer\u00eancia mais r\u00e1pida e maior precis\u00e3o de detec\u00e7\u00e3o.</p> </li> </ol>"},{"location":"models/yolov7/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>No momento em que este texto foi escrito, a Ultralytics ainda n\u00e3o oferece suporte aos modelos YOLOv7. Portanto, qualquer usu\u00e1rio interessado em usar o YOLOv7 precisar\u00e1 se referir diretamente ao reposit\u00f3rio do YOLOv7 no GitHub para obter instru\u00e7\u00f5es de instala\u00e7\u00e3o e uso.</p> <p>Aqui est\u00e1 uma breve vis\u00e3o geral das etapas t\u00edpicas que voc\u00ea pode seguir para usar o YOLOv7:</p> <ol> <li> <p>Acesse o reposit\u00f3rio do YOLOv7 no GitHub: https://github.com/WongKinYiu/yolov7.</p> </li> <li> <p>Siga as instru\u00e7\u00f5es fornecidas no arquivo README para a instala\u00e7\u00e3o. Isso normalmente envolve clonar o reposit\u00f3rio, instalar as depend\u00eancias necess\u00e1rias e configurar quaisquer vari\u00e1veis de ambiente necess\u00e1rias.</p> </li> <li> <p>Ap\u00f3s a conclus\u00e3o da instala\u00e7\u00e3o, voc\u00ea pode treinar e usar o modelo conforme as instru\u00e7\u00f5es de uso fornecidas no reposit\u00f3rio. Isso geralmente envolve a prepara\u00e7\u00e3o do conjunto de dados, a configura\u00e7\u00e3o dos par\u00e2metros do modelo, o treinamento do modelo e, em seguida, o uso do modelo treinado para realizar a detec\u00e7\u00e3o de objetos.</p> </li> </ol> <p>Observe que as etapas espec\u00edficas podem variar dependendo do caso de uso espec\u00edfico e do estado atual do reposit\u00f3rio do YOLOv7. Portanto, \u00e9 altamente recomend\u00e1vel consultar diretamente as instru\u00e7\u00f5es fornecidas no reposit\u00f3rio do YOLOv7 no GitHub.</p> <p>Lamentamos qualquer inconveniente que isso possa causar e nos esfor\u00e7aremos para atualizar este documento com exemplos de uso para a Ultralytics assim que o suporte para o YOLOv7 for implementado.</p>"},{"location":"models/yolov7/#citacoes-e-agradecimentos","title":"Cita\u00e7\u00f5es e Agradecimentos","text":"<p>Gostar\u00edamos de agradecer aos autores do YOLOv7 por suas contribui\u00e7\u00f5es significativas no campo da detec\u00e7\u00e3o de objetos em tempo real:</p> BibTeX <pre><code>@article{wang2022yolov7,\n  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  journal={arXiv preprint arXiv:2207.02696},\n  year={2022}\n}\n</code></pre> <p>O artigo original do YOLOv7 pode ser encontrado no arXiv. Os autores disponibilizaram publicamente seu trabalho, e o c\u00f3digo pode ser acessado no GitHub. Agradecemos seus esfor\u00e7os em avan\u00e7ar o campo e tornar seu trabalho acess\u00edvel \u00e0 comunidade em geral.</p>"},{"location":"models/yolov8/","title":"YOLOv8","text":""},{"location":"models/yolov8/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O YOLOv8 \u00e9 a vers\u00e3o mais recente da s\u00e9rie YOLO de detectores de objetos em tempo real, oferecendo um desempenho de ponta em termos de precis\u00e3o e velocidade. Construindo sobre as inova\u00e7\u00f5es das vers\u00f5es anteriores do YOLO, o YOLOv8 introduz novas caracter\u00edsticas e otimiza\u00e7\u00f5es que o tornam uma escolha ideal para diversas tarefas de detec\u00e7\u00e3o de objetos em uma ampla variedade de aplica\u00e7\u00f5es.</p> <p></p>"},{"location":"models/yolov8/#principais-caracteristicas","title":"Principais Caracter\u00edsticas","text":"<ul> <li>Arquiteturas Avan\u00e7adas de Backbone e Neck: O YOLOv8 utiliza arquiteturas avan\u00e7adas de backbone e neck, resultando em uma melhor extra\u00e7\u00e3o de caracter\u00edsticas e desempenho na detec\u00e7\u00e3o de objetos.</li> <li>Anchor-free Split Ultralytics Head: O YOLOv8 adota um head Ultralytics dividido sem ancoragem, o que contribui para uma melhor precis\u00e3o e um processo de detec\u00e7\u00e3o mais eficiente em compara\u00e7\u00e3o com abordagens baseadas em \u00e2ncoras.</li> <li>Equil\u00edbrio Otimizado entre Precis\u00e3o e Velocidade: Com foco em manter um equil\u00edbrio ideal entre precis\u00e3o e velocidade, o YOLOv8 \u00e9 adequado para tarefas de detec\u00e7\u00e3o de objetos em tempo real em diversas \u00e1reas de aplica\u00e7\u00e3o.</li> <li>Variedade de Modelos Pr\u00e9-treinados: O YOLOv8 oferece uma variedade de modelos pr\u00e9-treinados para atender a diversas tarefas e requisitos de desempenho, tornando mais f\u00e1cil encontrar o modelo adequado para o seu caso de uso espec\u00edfico.</li> </ul>"},{"location":"models/yolov8/#tarefas-e-modos-suportados","title":"Tarefas e Modos Suportados","text":"<p>A s\u00e9rie YOLOv8 oferece uma variedade de modelos, cada um especializado em tarefas espec\u00edficas de vis\u00e3o computacional. Esses modelos s\u00e3o projetados para atender a diversos requisitos, desde a detec\u00e7\u00e3o de objetos at\u00e9 tarefas mais complexas, como segmenta\u00e7\u00e3o de inst\u00e2ncias, detec\u00e7\u00e3o de poses/pontos-chave e classifica\u00e7\u00e3o.</p> <p>Cada variante da s\u00e9rie YOLOv8 \u00e9 otimizada para a respectiva tarefa, garantindo alto desempenho e precis\u00e3o. Al\u00e9m disso, esses modelos s\u00e3o compat\u00edveis com diversos modos operacionais, incluindo Infer\u00eancia, Valida\u00e7\u00e3o, Treinamento e Exporta\u00e7\u00e3o, facilitando o uso em diferentes est\u00e1gios de implanta\u00e7\u00e3o e desenvolvimento.</p> Modelo Nomes de Arquivo Tarefa Infer\u00eancia Valida\u00e7\u00e3o Treinamento Exporta\u00e7\u00e3o YOLOv8 <code>yolov8n.pt</code> <code>yolov8s.pt</code> <code>yolov8m.pt</code> <code>yolov8l.pt</code> <code>yolov8x.pt</code> Detec\u00e7\u00e3o \u2705 \u2705 \u2705 \u2705 YOLOv8-seg <code>yolov8n-seg.pt</code> <code>yolov8s-seg.pt</code> <code>yolov8m-seg.pt</code> <code>yolov8l-seg.pt</code> <code>yolov8x-seg.pt</code> Segmenta\u00e7\u00e3o de Inst\u00e2ncias \u2705 \u2705 \u2705 \u2705 YOLOv8-pose <code>yolov8n-pose.pt</code> <code>yolov8s-pose.pt</code> <code>yolov8m-pose.pt</code> <code>yolov8l-pose.pt</code> <code>yolov8x-pose.pt</code> <code>yolov8x-pose-p6.pt</code> Pose/Pontos-chave \u2705 \u2705 \u2705 \u2705 YOLOv8-cls <code>yolov8n-cls.pt</code> <code>yolov8s-cls.pt</code> <code>yolov8m-cls.pt</code> <code>yolov8l-cls.pt</code> <code>yolov8x-cls.pt</code> Classifica\u00e7\u00e3o \u2705 \u2705 \u2705 \u2705 <p>Esta tabela fornece uma vis\u00e3o geral das variantes de modelos YOLOv8, destacando suas aplica\u00e7\u00f5es em tarefas espec\u00edficas e sua compatibilidade com diversos modos operacionais, como infer\u00eancia, valida\u00e7\u00e3o, treinamento e exporta\u00e7\u00e3o. Ela demonstra a versatilidade e robustez da s\u00e9rie YOLOv8, tornando-os adequados para diversas aplica\u00e7\u00f5es em vis\u00e3o computacional.</p>"},{"location":"models/yolov8/#metricas-de-desempenho","title":"M\u00e9tricas de Desempenho","text":"<p>Desempenho</p> Detec\u00e7\u00e3o (COCO)Detec\u00e7\u00e3o (Open Images V7)Segmenta\u00e7\u00e3o (COCO)Classifica\u00e7\u00e3o (ImageNet)Pose (COCO) <p>Consulte a Documenta\u00e7\u00e3o de Detec\u00e7\u00e3o para exemplos de uso com esses modelos treinados no conjunto de dados COCO, que inclui 80 classes pr\u00e9-treinadas.</p> Modelo tamanho<sup>(pixels) mAP<sup>val50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37,3 80,4 0,99 3,2 8,7 YOLOv8s 640 44,9 128,4 1,20 11,2 28,6 YOLOv8m 640 50,2 234,7 1,83 25,9 78,9 YOLOv8l 640 52,9 375,2 2,39 43,7 165,2 YOLOv8x 640 53,9 479,1 3,53 68,2 257,8 <p>Consulte a Documenta\u00e7\u00e3o de Detec\u00e7\u00e3o para exemplos de uso com esses modelos treinados no conjunto de dados Open Images V7, que inclui 600 classes pr\u00e9-treinadas.</p> Modelo tamanho<sup>(pixels) mAP<sup>val50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 18,4 142,4 1,21 3,5 10,5 YOLOv8s 640 27,7 183,1 1,40 11,4 29,7 YOLOv8m 640 33,6 408,5 2,26 26,2 80,6 YOLOv8l 640 34,9 596,9 2,43 44,1 167,4 YOLOv8x 640 36,3 860,6 3,56 68,7 260,6 <p>Consulte a Documenta\u00e7\u00e3o de Segmenta\u00e7\u00e3o para exemplos de uso com esses modelos treinados no conjunto de dados COCO, que inclui 80 classes pr\u00e9-treinadas.</p> Modelo tamanho<sup>(pixels) mAP<sup>box50-95 mAP<sup>m\u00e1scara50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36,7 30,5 96,1 1,21 3,4 12,6 YOLOv8s-seg 640 44,6 36,8 155,7 1,47 11,8 42,6 YOLOv8m-seg 640 49,9 40,8 317,0 2,18 27,3 110,2 YOLOv8l-seg 640 52,3 42,6 572,4 2,79 46,0 220,5 YOLOv8x-seg 640 53,4 43,4 712,1 4,02 71,8 344,1 <p>Consulte a Documenta\u00e7\u00e3o de Classifica\u00e7\u00e3o para exemplos de uso com esses modelos treinados no conjunto de dados ImageNet, que inclui 1000 classes pr\u00e9-treinadas.</p> Modelo tamanho<sup>(pixels) acur\u00e1cia<sup>top1 acur\u00e1cia<sup>top5 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) a 640 YOLOv8n-cls 224 66,6 87,0 12,9 0,31 2,7 4,3 YOLOv8s-cls 224 72,3 91,1 23,4 0,35 6,4 13,5 YOLOv8m-cls 224 76,4 93,2 85,4 0,62 17,0 42,7 YOLOv8l-cls 224 78,0 94,1 163,0 0,87 37,5 99,7 YOLOv8x-cls 224 78,4 94,3 232,0 1,01 57,4 154,8 <p>Consulte a Documenta\u00e7\u00e3o de Estimativa de Pose para exemplos de uso com esses modelos treinados no conjunto de dados COCO, que inclui 1 classe pr\u00e9-treinada, 'person'.</p> Modelo tamanho<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50,4 80,1 131,8 1,18 3,3 9,2 YOLOv8s-pose 640 60,0 86,2 233,2 1,42 11,6 30,2 YOLOv8m-pose 640 65,0 88,8 456,3 2,00 26,4 81,0 YOLOv8l-pose 640 67,6 90,0 784,5 2,59 44,4 168,6 YOLOv8x-pose 640 69,2 90,2 1607,1 3,73 69,4 263,2 YOLOv8x-pose-p6 1280 71,6 91,2 4088,7 10,04 99,1 1066,4"},{"location":"models/yolov8/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Este exemplo fornece exemplos simples de treinamento e infer\u00eancia do YOLOv8. Para a documenta\u00e7\u00e3o completa desses e outros modos, consulte as p\u00e1ginas de documenta\u00e7\u00e3o Predict, Train, Val e Export.</p> <p>Observe que o exemplo abaixo \u00e9 para modelos YOLOv8 de Detec\u00e7\u00e3o para detec\u00e7\u00e3o de objetos. Para outras tarefas suportadas, consulte a documenta\u00e7\u00e3o de Segmenta\u00e7\u00e3o, Classifica\u00e7\u00e3o e Pose.</p> <p>Exemplo</p> PythonCLI <p>Modelos pr\u00e9-treinados <code>*.pt</code> PyTorch, bem como arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados para a classe <code>YOLO()</code> para criar uma inst\u00e2ncia do modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado para COCO\nmodel = YOLO('yolov8n.pt')\n\n# Exibir informa\u00e7\u00f5es do modelo (opcional)\nmodel.info()\n\n# Treinar o modelo no exemplo de conjunto de dados COCO8 por 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Executar infer\u00eancia com o modelo YOLOv8n na imagem 'bus.jpg'\nresults = model('caminho/para/bus.jpg')\n</code></pre> <p>Comandos da CLI est\u00e3o dispon\u00edveis para executar os modelos diretamente:</p> <pre><code># Carregar um modelo YOLOv8n pr\u00e9-treinado para COCO e trein\u00e1-lo no exemplo de conjunto de dados COCO8 por 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado para COCO e executar infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov8n.pt source=caminho/para/bus.jpg\n</code></pre>"},{"location":"models/yolov8/#citacoes-e-reconhecimentos","title":"Cita\u00e7\u00f5es e Reconhecimentos","text":"<p>Se voc\u00ea utilizar o modelo YOLOv8 ou qualquer outro software deste reposit\u00f3rio em seu trabalho, por favor cite-o utilizando o formato abaixo:</p> BibTeX <pre><code>@software{yolov8_ultralytics,\n  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n  title = {Ultralytics YOLOv8},\n  version = {8.0.0},\n  year = {2023},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n</code></pre> <p>Observe que o DOI est\u00e1 pendente e ser\u00e1 adicionado \u00e0 cita\u00e7\u00e3o assim que estiver dispon\u00edvel. Os modelos YOLOv8 s\u00e3o disponibilizados sob as licen\u00e7as AGPL-3.0 e Enterprise.</p>"},{"location":"modes/","title":"Modos Ultralytics YOLOv8","text":""},{"location":"modes/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O Ultralytics YOLOv8 n\u00e3o \u00e9 apenas mais um modelo de detec\u00e7\u00e3o de objetos; \u00e9 um framework vers\u00e1til projetado para cobrir todo o ciclo de vida dos modelos de aprendizado de m\u00e1quina \u2014 desde a ingest\u00e3o de dados e treinamento do modelo at\u00e9 a valida\u00e7\u00e3o, implanta\u00e7\u00e3o e rastreamento no mundo real. Cada modo serve a um prop\u00f3sito espec\u00edfico e \u00e9 projetado para oferecer a flexibilidade e efici\u00eancia necess\u00e1rias para diferentes tarefas e casos de uso.</p> <p> Assista: Tutorial dos Modos Ultralytics: Treinar, Validar, Prever, Exportar e Benchmark. </p>"},{"location":"modes/#visao-geral-dos-modos","title":"Vis\u00e3o Geral dos Modos","text":"<p>Entender os diferentes modos que o Ultralytics YOLOv8 suporta \u00e9 cr\u00edtico para tirar o m\u00e1ximo proveito de seus modelos:</p> <ul> <li>Modo Treino: Ajuste fino do seu modelo em conjuntos de dados personalizados ou pr\u00e9-carregados.</li> <li>Modo Valida\u00e7\u00e3o (Val): Um checkpoint p\u00f3s-treinamento para validar o desempenho do modelo.</li> <li>Modo Predi\u00e7\u00e3o (Predict): Libere o poder preditivo do seu modelo em dados do mundo real.</li> <li>Modo Exporta\u00e7\u00e3o (Export): Prepare seu modelo para implanta\u00e7\u00e3o em v\u00e1rios formatos.</li> <li>Modo Rastreamento (Track): Estenda seu modelo de detec\u00e7\u00e3o de objetos para aplica\u00e7\u00f5es de rastreamento em tempo real.</li> <li>Modo Benchmarking: Analise a velocidade e precis\u00e3o do seu modelo em diversos ambientes de implanta\u00e7\u00e3o.</li> </ul> <p>Este guia abrangente visa fornecer uma vis\u00e3o geral e insights pr\u00e1ticos para cada modo, ajudando voc\u00ea a aproveitar o potencial total do YOLOv8.</p>"},{"location":"modes/#treinar","title":"Treinar","text":"<p>O modo Treinar \u00e9 utilizado para treinar um modelo YOLOv8 em um conjunto de dados personalizado. Neste modo, o modelo \u00e9 treinado usando o conjunto de dados especificado e os hiperpar\u00e2metros escolhidos. O processo de treinamento envolve otimizar os par\u00e2metros do modelo para que ele possa prever com precis\u00e3o as classes e localiza\u00e7\u00f5es de objetos em uma imagem.</p> <p>Exemplos de Treino</p>"},{"location":"modes/#validar","title":"Validar","text":"<p>O modo Validar \u00e9 utilizado para validar um modelo YOLOv8 ap\u00f3s ter sido treinado. Neste modo, o modelo \u00e9 avaliado em um conjunto de valida\u00e7\u00e3o para medir sua precis\u00e3o e desempenho de generaliza\u00e7\u00e3o. Este modo pode ser usado para ajustar os hiperpar\u00e2metros do modelo para melhorar seu desempenho.</p> <p>Exemplos de Valida\u00e7\u00e3o</p>"},{"location":"modes/#prever","title":"Prever","text":"<p>O modo Prever \u00e9 utilizado para fazer previs\u00f5es usando um modelo YOLOv8 treinado em novas imagens ou v\u00eddeos. Neste modo, o modelo \u00e9 carregado de um arquivo de checkpoint, e o usu\u00e1rio pode fornecer imagens ou v\u00eddeos para realizar a infer\u00eancia. O modelo prev\u00ea as classes e localiza\u00e7\u00f5es dos objetos nas imagens ou v\u00eddeos fornecidos.</p> <p>Exemplos de Predi\u00e7\u00e3o</p>"},{"location":"modes/#exportar","title":"Exportar","text":"<p>O modo Exportar \u00e9 utilizado para exportar um modelo YOLOv8 para um formato que possa ser utilizado para implanta\u00e7\u00e3o. Neste modo, o modelo \u00e9 convertido para um formato que possa ser utilizado por outras aplica\u00e7\u00f5es de software ou dispositivos de hardware. Este modo \u00e9 \u00fatil ao implantar o modelo em ambientes de produ\u00e7\u00e3o.</p> <p>Exemplos de Exporta\u00e7\u00e3o</p>"},{"location":"modes/#rastrear","title":"Rastrear","text":"<p>O modo Rastrear \u00e9 utilizado para rastrear objetos em tempo real usando um modelo YOLOv8. Neste modo, o modelo \u00e9 carregado de um arquivo de checkpoint, e o usu\u00e1rio pode fornecer um fluxo de v\u00eddeo ao vivo para realizar o rastreamento de objetos em tempo real. Este modo \u00e9 \u00fatil para aplica\u00e7\u00f5es como sistemas de vigil\u00e2ncia ou carros aut\u00f4nomos.</p> <p>Exemplos de Rastreamento</p>"},{"location":"modes/#benchmark","title":"Benchmark","text":"<p>O modo Benchmark \u00e9 utilizado para fazer um perfil da velocidade e precis\u00e3o de v\u00e1rios formatos de exporta\u00e7\u00e3o para o YOLOv8. Os benchmarks fornecem informa\u00e7\u00f5es sobre o tamanho do formato exportado, suas m\u00e9tricas <code>mAP50-95</code> (para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e pose) ou <code>accuracy_top5</code> (para classifica\u00e7\u00e3o), e o tempo de infer\u00eancia em milissegundos por imagem em diversos formatos de exporta\u00e7\u00e3o, como ONNX, OpenVINO, TensorRT e outros. Essas informa\u00e7\u00f5es podem ajudar os usu\u00e1rios a escolher o formato de exporta\u00e7\u00e3o \u00f3timo para seu caso de uso espec\u00edfico, com base em seus requisitos de velocidade e precis\u00e3o.</p> <p>Exemplos de Benchmark</p>"},{"location":"modes/benchmark/","title":"Benchmarking de Modelos com o Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Uma vez que seu modelo esteja treinado e validado, o pr\u00f3ximo passo l\u00f3gico \u00e9 avaliar seu desempenho em diversos cen\u00e1rios do mundo real. O modo de benchmark no Ultralytics YOLOv8 serve a esse prop\u00f3sito, oferecendo uma estrutura robusta para avaliar a velocidade e a precis\u00e3o do seu modelo em uma gama de formatos de exporta\u00e7\u00e3o.</p>"},{"location":"modes/benchmark/#por-que-o-benchmarking-e-crucial","title":"Por Que o Benchmarking \u00e9 Crucial?","text":"<ul> <li>Decis\u00f5es Informadas: Obtenha insights sobre o equil\u00edbrio entre velocidade e precis\u00e3o.</li> <li>Aloca\u00e7\u00e3o de Recursos: Entenda como diferentes formatos de exporta\u00e7\u00e3o se comportam em diferentes hardwares.</li> <li>Otimiza\u00e7\u00e3o: Aprenda qual formato de exporta\u00e7\u00e3o oferece o melhor desempenho para o seu caso espec\u00edfico.</li> <li>Efici\u00eancia de Custos: Fa\u00e7a uso mais eficiente dos recursos de hardware com base nos resultados do benchmark.</li> </ul>"},{"location":"modes/benchmark/#metricas-chave-no-modo-de-benchmark","title":"M\u00e9tricas Chave no Modo de Benchmark","text":"<ul> <li>mAP50-95: Para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e estimativa de pose.</li> <li>accuracy_top5: Para classifica\u00e7\u00e3o de imagens.</li> <li>Tempo de Infer\u00eancia: Tempo levado para cada imagem em milissegundos.</li> </ul>"},{"location":"modes/benchmark/#formatos-de-exportacao-suportados","title":"Formatos de Exporta\u00e7\u00e3o Suportados","text":"<ul> <li>ONNX: Para desempenho \u00f3timo em CPU</li> <li>TensorRT: Para efici\u00eancia m\u00e1xima em GPU</li> <li>OpenVINO: Para otimiza\u00e7\u00e3o em hardware Intel</li> <li>CoreML, TensorFlow SavedModel e Mais: Para uma variedade de necessidades de implanta\u00e7\u00e3o.</li> </ul> <p>Dica</p> <ul> <li>Exporte para ONNX ou OpenVINO para acelerar at\u00e9 3x a velocidade em CPU.</li> <li>Exporte para TensorRT para acelerar at\u00e9 5x em GPU.</li> </ul>"},{"location":"modes/benchmark/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Execute benchmarks do YOLOv8n em todos os formatos de exporta\u00e7\u00e3o suportados incluindo ONNX, TensorRT etc. Consulte a se\u00e7\u00e3o Argumentos abaixo para ver uma lista completa de argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark na GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumentos","title":"Argumentos","text":"<p>Argumentos como <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> e <code>verbose</code> proporcionam aos usu\u00e1rios flexibilidade para ajustar os benchmarks \u00e0s suas necessidades espec\u00edficas e comparar o desempenho de diferentes formatos de exporta\u00e7\u00e3o com facilidade.</p> Chave Valor Descri\u00e7\u00e3o <code>model</code> <code>None</code> caminho para o arquivo do modelo, ou seja, yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> caminho para o YAML com dataset de benchmarking (sob o r\u00f3tulo <code>val</code>) <code>imgsz</code> <code>640</code> tamanho da imagem como um escalar ou lista (h, w), ou seja, (640, 480) <code>half</code> <code>False</code> quantiza\u00e7\u00e3o FP16 <code>int8</code> <code>False</code> quantiza\u00e7\u00e3o INT8 <code>device</code> <code>None</code> dispositivo para execu\u00e7\u00e3o, ou seja, dispositivo cuda=0 ou device=0,1,2,3 ou device=cpu <code>verbose</code> <code>False</code> n\u00e3o continuar em erro (bool), ou limiar m\u00ednimo para val (float)"},{"location":"modes/benchmark/#formatos-de-exportacao","title":"Formatos de Exporta\u00e7\u00e3o","text":"<p>Os benchmarks tentar\u00e3o executar automaticamente em todos os poss\u00edveis formatos de exporta\u00e7\u00e3o listados abaixo.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> Modelo Salvo do TF <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> GraphDef do TF <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja os detalhes completos de <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Export.</p>"},{"location":"modes/export/","title":"Exporta\u00e7\u00e3o de Modelo com Ultralytics YOLO","text":""},{"location":"modes/export/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O objetivo final de treinar um modelo \u00e9 implant\u00e1-lo para aplica\u00e7\u00f5es no mundo real. O modo de exporta\u00e7\u00e3o no Ultralytics YOLOv8 oferece uma ampla gama de op\u00e7\u00f5es para exportar seu modelo treinado para diferentes formatos, tornando-o implant\u00e1vel em v\u00e1rias plataformas e dispositivos. Este guia abrangente visa orient\u00e1-lo atrav\u00e9s das nuances da exporta\u00e7\u00e3o de modelos, mostrando como alcan\u00e7ar a m\u00e1xima compatibilidade e performance.</p> <p> Assista: Como Exportar Modelo Treinado Customizado do Ultralytics YOLOv8 e Executar Infer\u00eancia ao Vivo na Webcam. </p>"},{"location":"modes/export/#por-que-escolher-o-modo-de-exportacao-do-yolov8","title":"Por Que Escolher o Modo de Exporta\u00e7\u00e3o do YOLOv8?","text":"<ul> <li>Versatilidade: Exporte para m\u00faltiplos formatos incluindo ONNX, TensorRT, CoreML e mais.</li> <li>Performance: Ganhe at\u00e9 5x acelera\u00e7\u00e3o em GPU com TensorRT e 3x acelera\u00e7\u00e3o em CPU com ONNX ou OpenVINO.</li> <li>Compatibilidade: Torne seu modelo universalmente implant\u00e1vel em numerosos ambientes de hardware e software.</li> <li>Facilidade de Uso: Interface de linha de comando simples e API Python para exporta\u00e7\u00e3o r\u00e1pida e direta de modelos.</li> </ul>"},{"location":"modes/export/#principais-recursos-do-modo-de-exportacao","title":"Principais Recursos do Modo de Exporta\u00e7\u00e3o","text":"<p>Aqui est\u00e3o algumas das funcionalidades de destaque:</p> <ul> <li>Exporta\u00e7\u00e3o com Um Clique: Comandos simples para exporta\u00e7\u00e3o em diferentes formatos.</li> <li>Exporta\u00e7\u00e3o em Lote: Exporte modelos capazes de infer\u00eancia em lote.</li> <li>Infer\u00eancia Otimizada: Modelos exportados s\u00e3o otimizados para tempos de infer\u00eancia mais r\u00e1pidos.</li> <li>V\u00eddeos Tutoriais: Guias e tutoriais detalhados para uma experi\u00eancia de exporta\u00e7\u00e3o tranquila.</li> </ul> <p>Dica</p> <ul> <li>Exporte para ONNX ou OpenVINO para at\u00e9 3x acelera\u00e7\u00e3o em CPU.</li> <li>Exporte para TensorRT para at\u00e9 5x acelera\u00e7\u00e3o em GPU.</li> </ul>"},{"location":"modes/export/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Exporte um modelo YOLOv8n para um formato diferente como ONNX ou TensorRT. Veja a se\u00e7\u00e3o de Argumentos abaixo para uma lista completa dos argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carrega um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carrega um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporta modelo oficial\nyolo export model=caminho/para/best.pt format=onnx  # exporta modelo treinado personalizado\n</code></pre>"},{"location":"modes/export/#argumentos","title":"Argumentos","text":"<p>Configura\u00e7\u00f5es de exporta\u00e7\u00e3o para modelos YOLO referem-se \u00e0s v\u00e1rias configura\u00e7\u00f5es e op\u00e7\u00f5es usadas para salvar ou exportar o modelo para uso em outros ambientes ou plataformas. Essas configura\u00e7\u00f5es podem afetar a performance, tamanho e compatibilidade do modelo com diferentes sistemas. Algumas configura\u00e7\u00f5es comuns de exporta\u00e7\u00e3o de YOLO incluem o formato do arquivo de modelo exportado (por exemplo, ONNX, TensorFlow SavedModel), o dispositivo em que o modelo ser\u00e1 executado (por exemplo, CPU, GPU) e a presen\u00e7a de recursos adicionais como m\u00e1scaras ou m\u00faltiplos r\u00f3tulos por caixa. Outros fatores que podem afetar o processo de exporta\u00e7\u00e3o incluem a tarefa espec\u00edfica para a qual o modelo est\u00e1 sendo usado e os requisitos ou restri\u00e7\u00f5es do ambiente ou plataforma alvo. \u00c9 importante considerar e configurar cuidadosamente essas configura\u00e7\u00f5es para garantir que o modelo exportado seja otimizado para o caso de uso pretendido e possa ser usado eficazmente no ambiente alvo.</p> Chave Valor Descri\u00e7\u00e3o <code>format</code> <code>'torchscript'</code> formato para exporta\u00e7\u00e3o <code>imgsz</code> <code>640</code> tamanho da imagem como escalar ou lista (h, w), ou seja, (640, 480) <code>keras</code> <code>False</code> usar Keras para exporta\u00e7\u00e3o TF SavedModel <code>optimize</code> <code>False</code> TorchScript: otimizar para mobile <code>half</code> <code>False</code> quantiza\u00e7\u00e3o FP16 <code>int8</code> <code>False</code> quantiza\u00e7\u00e3o INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT: eixos din\u00e2micos <code>simplify</code> <code>False</code> ONNX/TensorRT: simplificar modelo <code>opset</code> <code>None</code> ONNX: vers\u00e3o do opset (opcional, padr\u00e3o para a mais recente) <code>workspace</code> <code>4</code> TensorRT: tamanho do espa\u00e7o de trabalho (GB) <code>nms</code> <code>False</code> CoreML: adicionar NMS"},{"location":"modes/export/#formatos-de-exportacao","title":"Formatos de Exporta\u00e7\u00e3o","text":"<p>Os formatos de exporta\u00e7\u00e3o dispon\u00edveis para YOLOv8 est\u00e3o na tabela abaixo. Voc\u00ea pode exportar para qualquer formato usando o argumento <code>format</code>, ou seja, <code>format='onnx'</code> ou <code>format='engine'</code>.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Predi\u00e7\u00e3o de Modelo com Ultralytics YOLO","text":""},{"location":"modes/predict/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>No mundo do aprendizado de m\u00e1quina e vis\u00e3o computacional, o processo de fazer sentido a partir de dados visuais \u00e9 chamado de 'infer\u00eancia' ou 'predi\u00e7\u00e3o'. O Ultralytics YOLOv8 oferece um recurso poderoso conhecido como modo predict que \u00e9 personalizado para infer\u00eancia em tempo real de alto desempenho em uma ampla gama de fontes de dados.</p> <p> Assista: Como Extrair as Sa\u00eddas do Modelo Ultralytics YOLOv8 para Projetos Personalizados. </p>"},{"location":"modes/predict/#aplicacoes-no-mundo-real","title":"Aplica\u00e7\u00f5es no Mundo Real","text":"Manufatura Esportes Seguran\u00e7a Detec\u00e7\u00e3o de Pe\u00e7as de Reposi\u00e7\u00e3o de Ve\u00edculo Detec\u00e7\u00e3o de Jogador de Futebol Detec\u00e7\u00e3o de Queda de Pessoas"},{"location":"modes/predict/#por-que-usar-o-ultralytics-yolo-para-inferencia","title":"Por Que Usar o Ultralytics YOLO para Infer\u00eancia?","text":"<p>Aqui est\u00e1 o porqu\u00ea de voc\u00ea considerar o modo predict do YOLOv8 para suas diversas necessidades de infer\u00eancia:</p> <ul> <li>Versatilidade: Capaz de fazer infer\u00eancias em imagens, v\u00eddeos e at\u00e9 transmiss\u00f5es ao vivo.</li> <li>Desempenho: Projetado para processamento em tempo real e de alta velocidade sem sacrificar a precis\u00e3o.</li> <li>Facilidade de Uso: Interfaces Python e CLI intuitivas para implanta\u00e7\u00e3o e testes r\u00e1pidos.</li> <li>Altamente Customiz\u00e1vel: V\u00e1rias configura\u00e7\u00f5es e par\u00e2metros para ajustar o comportamento de infer\u00eancia do modelo de acordo com suas necessidades espec\u00edficas.</li> </ul>"},{"location":"modes/predict/#recursos-chave-do-modo-predict","title":"Recursos Chave do Modo Predict","text":"<p>O modo predict do YOLOv8 \u00e9 projetado para ser robusto e vers\u00e1til, apresentando:</p> <ul> <li>Compatibilidade com M\u00faltiplas Fontes de Dados: Se seus dados est\u00e3o na forma de imagens individuais, uma cole\u00e7\u00e3o de imagens, arquivos de v\u00eddeo ou transmiss\u00f5es de v\u00eddeo em tempo real, o modo predict atende a todas as necessidades.</li> <li>Modo de Streaming: Use o recurso de streaming para gerar um gerador eficiente de mem\u00f3ria de objetos <code>Results</code>. Ative isso definindo <code>stream=True</code> no m\u00e9todo de chamada do preditor.</li> <li>Processamento em Lote: A capacidade de processar v\u00e1rias imagens ou quadros de v\u00eddeo em um \u00fanico lote, acelerando ainda mais o tempo de infer\u00eancia.</li> <li>Integra\u00e7\u00e3o Amig\u00e1vel: Integra\u00e7\u00e3o f\u00e1cil com pipelines de dados existentes e outros componentes de software, gra\u00e7as \u00e0 sua API flex\u00edvel.</li> </ul> <p>Os modelos Ultralytics YOLO retornam ou uma lista de objetos <code>Results</code> em Python, ou um gerador em Python eficiente de mem\u00f3ria de objetos <code>Results</code> quando <code>stream=True</code> \u00e9 passado para o modelo durante a infer\u00eancia:</p> <p>Predict</p> Retorna uma lista com <code>stream=False</code>Retorna um gerador com <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n pr\u00e9-treinado\n\n# Executa a infer\u00eancia em lote em uma lista de imagens\nresults = model(['im1.jpg', 'im2.jpg'])  # retorna uma lista de objetos Results\n\n# Processa a lista de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para sa\u00eddas de bbox\n    masks = result.masks  # Objeto Masks para sa\u00eddas de m\u00e1scaras de segmenta\u00e7\u00e3o\n    keypoints = result.keypoints  # Objeto Keypoints para sa\u00eddas de pose\n    probs = result.probs  # Objeto Probs para sa\u00eddas de classifica\u00e7\u00e3o\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n pr\u00e9-treinado\n\n# Executa a infer\u00eancia em lote em uma lista de imagens\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # retorna um gerador de objetos Results\n\n# Processa o gerador de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para sa\u00eddas de bbox\n    masks = result.masks  # Objeto Masks para sa\u00eddas de m\u00e1scaras de segmenta\u00e7\u00e3o\n    keypoints = result.keypoints  # Objeto Keypoints para sa\u00eddas de pose\n    probs = result.probs  # Objeto Probs para sa\u00eddas de classifica\u00e7\u00e3o\n</code></pre>"},{"location":"modes/predict/#fontes-de-inferencia","title":"Fontes de Infer\u00eancia","text":"<p>O YOLOv8 pode processar diferentes tipos de fontes de entrada para infer\u00eancia, conforme mostrado na tabela abaixo. As fontes incluem imagens est\u00e1ticas, transmiss\u00f5es de v\u00eddeo e v\u00e1rios formatos de dados. A tabela tamb\u00e9m indica se cada fonte pode ser usada no modo de streaming com o argumento <code>stream=True</code> \u2705. O modo de streaming \u00e9 ben\u00e9fico para processar v\u00eddeos ou transmiss\u00f5es ao vivo, pois cria um gerador de resultados em vez de carregar todos os quadros na mem\u00f3ria.</p> <p>Dica</p> <p>Use <code>stream=True</code> para processar v\u00eddeos longos ou grandes conjuntos de dados para gerenciar a mem\u00f3ria de forma eficiente. Quando <code>stream=False</code>, os resultados de todos os quadros ou pontos de dados s\u00e3o armazenados na mem\u00f3ria, o que pode aumentar rapidamente e causar erros de falta de mem\u00f3ria para grandes entradas. Em contraste, <code>stream=True</code> utiliza um gerador, que mant\u00e9m apenas os resultados do quadro atual ou ponto de dados na mem\u00f3ria, reduzindo significativamente o consumo de mem\u00f3ria e prevenindo problemas de falta dela.</p> Fonte Argumento Tipo Notas imagem <code>'image.jpg'</code> <code>str</code> ou <code>Path</code> Arquivo de imagem \u00fanico. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL para uma imagem. captura de tela <code>'screen'</code> <code>str</code> Captura uma captura de tela. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Formato HWC com canais RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Formato HWC com canais BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Formato HWC com canais BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Formato BCHW com canais RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> ou <code>Path</code> Arquivo CSV contendo caminhos para imagens, v\u00eddeos ou diret\u00f3rios. v\u00eddeo \u2705 <code>'video.mp4'</code> <code>str</code> ou <code>Path</code> Arquivo de v\u00eddeo em formatos como MP4, AVI, etc. diret\u00f3rio \u2705 <code>'path/'</code> <code>str</code> ou <code>Path</code> Caminho para um diret\u00f3rio contendo imagens ou v\u00eddeos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Padr\u00e3o glob para combinar v\u00e1rios arquivos. Use o caractere <code>*</code> como curinga. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL para um v\u00eddeo do YouTube. stream \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL para protocolos de streaming como RTSP, RTMP, TCP ou um endere\u00e7o IP. multi-stream \u2705 <code>'list.streams'</code> <code>str</code> ou <code>Path</code> Arquivo de texto <code>*.streams</code> com uma URL de stream por linha, ou seja, 8 streams ser\u00e3o executados em lote de tamanho 8. <p>Abaixo est\u00e3o exemplos de c\u00f3digo para usar cada tipo de fonte:</p> <p>Fontes de previs\u00e3o</p> imagemcaptura de telaURLPILOpenCVnumpytorch <p>Executa a infer\u00eancia em um arquivo de imagem. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define o caminho para o arquivo de imagem\nsource = 'caminho/para/imagem.jpg'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia no conte\u00fado atual da tela como uma captura de tela. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define a captura de tela atual como fonte\nsource = 'screen'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem ou v\u00eddeo hospedado remotamente via URL. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define a URL remota da imagem ou v\u00eddeo\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem aberta com a Biblioteca de Imagens do Python (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Abre uma imagem usando PIL\nsource = Image.open('caminho/para/imagem.jpg')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem lida com OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# L\u00ea uma imagem usando OpenCV\nsource = cv2.imread('caminho/para/imagem.jpg')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem representada como um array numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Cria um array random de numpy com forma HWC (640, 640, 3) com valores no intervalo [0, 255] e tipo uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem representada como um tensor PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#carrega-um-modelo-yolov8n-pre-treinado","title":"Carrega um modelo YOLOv8n pr\u00e9-treinado","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#cria-um-tensor-random-de-torch-com-forma-bchw-1-3-640-640-com-valores-no-intervalo-0-1-e-tipo-float32","title":"Cria um tensor random de torch com forma BCHW (1, 3, 640, 640) com valores no intervalo [0, 1] e tipo float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#executa-a-inferencia-na-fonte","title":"Executa a infer\u00eancia na fonte","text":"<p>results = model(source)  # lista de objetos Results</p>"},{"location":"modes/track/","title":"Rastreamento de M\u00faltiplos Objetos com Ultralytics YOLO","text":"<p>Rastreamento de objetos no \u00e2mbito da an\u00e1lise de v\u00eddeo \u00e9 uma tarefa crucial que n\u00e3o apenas identifica a localiza\u00e7\u00e3o e classe dos objetos dentro do quadro, mas tamb\u00e9m mant\u00e9m um ID \u00fanico para cada objeto detectado \u00e0 medida que o v\u00eddeo avan\u00e7a. As aplica\u00e7\u00f5es s\u00e3o ilimitadas \u2014 variando desde vigil\u00e2ncia e seguran\u00e7a at\u00e9 an\u00e1lises esportivas em tempo real.</p>"},{"location":"modes/track/#por-que-escolher-ultralytics-yolo-para-rastreamento-de-objetos","title":"Por Que Escolher Ultralytics YOLO para Rastreamento de Objetos?","text":"<p>A sa\u00edda dos rastreadores da Ultralytics \u00e9 consistente com a detec\u00e7\u00e3o de objetos padr\u00e3o, mas com o valor agregado dos IDs dos objetos. Isso facilita o rastreamento de objetos em fluxos de v\u00eddeo e a realiza\u00e7\u00e3o de an\u00e1lises subsequentes. Aqui est\u00e1 o porqu\u00ea de considerar usar Ultralytics YOLO para suas necessidades de rastreamento de objetos:</p> <ul> <li>Efici\u00eancia: Processa fluxos de v\u00eddeo em tempo real sem comprometer a precis\u00e3o.</li> <li>Flexibilidade: Suporta m\u00faltiplos algoritmos de rastreamento e configura\u00e7\u00f5es.</li> <li>Facilidade de Uso: Simples API em Python e op\u00e7\u00f5es CLI para r\u00e1pida integra\u00e7\u00e3o e implanta\u00e7\u00e3o.</li> <li>Personaliza\u00e7\u00e3o: F\u00e1cil de usar com modelos YOLO treinados personalizados, permitindo integra\u00e7\u00e3o em aplica\u00e7\u00f5es espec\u00edficas de dom\u00ednio.</li> </ul> <p> Assistir: Detec\u00e7\u00e3o e Rastreamento de Objetos com Ultralytics YOLOv8. </p>"},{"location":"modes/track/#aplicacoes-no-mundo-real","title":"Aplica\u00e7\u00f5es no Mundo Real","text":"Transporte Varejo Aquicultura Rastreamento de Ve\u00edculos Rastreamento de Pessoas Rastreamento de Peixes"},{"location":"modes/track/#caracteristicas-em-destaque","title":"Caracter\u00edsticas em Destaque","text":"<p>Ultralytics YOLO estende suas funcionalidades de detec\u00e7\u00e3o de objetos para fornecer rastreamento de objetos robusto e vers\u00e1til:</p> <ul> <li>Rastreamento em Tempo Real: Acompanha objetos de forma cont\u00ednua em v\u00eddeos de alta taxa de quadros.</li> <li>Suporte a M\u00faltiplos Rastreadores: Escolha dentre uma variedade de algoritmos de rastreamento estabelecidos.</li> <li>Configura\u00e7\u00f5es de Rastreador Personaliz\u00e1veis: Adapte o algoritmo de rastreamento para atender requisitos espec\u00edficos ajustando v\u00e1rios par\u00e2metros.</li> </ul>"},{"location":"modes/track/#rastreadores-disponiveis","title":"Rastreadores Dispon\u00edveis","text":"<p>Ultralytics YOLO suporta os seguintes algoritmos de rastreamento. Eles podem ser ativados passando o respectivo arquivo de configura\u00e7\u00e3o YAML, como <code>tracker=tracker_type.yaml</code>:</p> <ul> <li>BoT-SORT - Use <code>botsort.yaml</code> para ativar este rastreador.</li> <li>ByteTrack - Use <code>bytetrack.yaml</code> para ativar este rastreador.</li> </ul> <p>O rastreador padr\u00e3o \u00e9 o BoT-SORT.</p>"},{"location":"modes/track/#rastreamento","title":"Rastreamento","text":"<p>Para executar o rastreador em fluxos de v\u00eddeo, use um modelo Detect, Segment ou Pose treinado, como YOLOv8n, YOLOv8n-seg e YOLOv8n-pose.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo oficial ou personalizado\nmodel = YOLO('yolov8n.pt')  # Carregar um modelo Detect oficial\nmodel = YOLO('yolov8n-seg.pt')  # Carregar um modelo Segment oficial\nmodel = YOLO('yolov8n-pose.pt')  # Carregar um modelo Pose oficial\nmodel = YOLO('caminho/para/melhor.pt')  # Carregar um modelo treinado personalizado\n\n# Realizar rastreamento com o modelo\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Rastreamento com rastreador padr\u00e3o\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Rastreamento com o rastreador ByteTrack\n</code></pre> <pre><code># Realizar rastreamento com v\u00e1rios modelos usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Detect oficial\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Segment oficial\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Pose oficial\nyolo track model=caminho/para/melhor.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo treinado personalizado\n\n# Rastrear usando o rastreador ByteTrack\nyolo track model=caminho/para/melhor.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Como pode ser visto no uso acima, o rastreamento est\u00e1 dispon\u00edvel para todos os modelos Detect, Segment e Pose executados em v\u00eddeos ou fontes de streaming.</p>"},{"location":"modes/track/#configuracao","title":"Configura\u00e7\u00e3o","text":""},{"location":"modes/track/#argumentos-de-rastreamento","title":"Argumentos de Rastreamento","text":"<p>A configura\u00e7\u00e3o de rastreamento compartilha propriedades com o modo Predict, como <code>conf</code>, <code>iou</code>, e <code>show</code>. Para mais configura\u00e7\u00f5es, consulte a p\u00e1gina de Predict model page.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurar os par\u00e2metros de rastreamento e executar o rastreador\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurar par\u00e2metros de rastreamento e executar o rastreador usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#selecao-de-rastreador","title":"Sele\u00e7\u00e3o de Rastreador","text":"<p>A Ultralytics tamb\u00e9m permite que voc\u00ea use um arquivo de configura\u00e7\u00e3o de rastreador modificado. Para fazer isso, simplesmente fa\u00e7a uma c\u00f3pia de um arquivo de configura\u00e7\u00e3o de rastreador (por exemplo, <code>custom_tracker.yaml</code>) de ultralytics/cfg/trackers e modifique quaisquer configura\u00e7\u00f5es (exceto <code>tracker_type</code>) conforme suas necessidades.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar o modelo e executar o rastreador com um arquivo de configura\u00e7\u00e3o personalizado\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Carregar o modelo e executar o rastreador com um arquivo de configura\u00e7\u00e3o personalizado usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Para uma lista completa de argumentos de rastreamento, consulte a p\u00e1gina ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#exemplos-em-python","title":"Exemplos em Python","text":""},{"location":"modes/track/#loop-de-persistencia-de-rastreamentos","title":"Loop de Persist\u00eancia de Rastreamentos","text":"<p>Aqui est\u00e1 um script em Python usando OpenCV (<code>cv2</code>) e YOLOv8 para executar rastreamento de objetos em quadros de v\u00eddeo. Este script ainda pressup\u00f5e que voc\u00ea j\u00e1 instalou os pacotes necess\u00e1rios (<code>opencv-python</code> e <code>ultralytics</code>). O argumento <code>persist=True</code> indica ao rastreador que a imagem ou quadro atual \u00e9 o pr\u00f3ximo de uma sequ\u00eancia e que espera rastreamentos da imagem anterior na imagem atual.</p> <p>Loop de fluxo com rastreamento</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Carregar o modelo YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Abrir o arquivo de v\u00eddeo\nvideo_path = \"caminho/para/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Repetir atrav\u00e9s dos quadros de v\u00eddeo\nwhile cap.isOpened():\n    # Ler um quadro do v\u00eddeo\n    success, frame = cap.read()\n\n    if success:\n        # Executar rastreamento YOLOv8 no quadro, persistindo rastreamentos entre quadros\n        results = model.track(frame, persist=True)\n\n        # Visualizar os resultados no quadro\n        annotated_frame = results[0].plot()\n\n        # Exibir o quadro anotado\n        cv2.imshow(\"Rastreamento YOLOv8\", annotated_frame)\n\n        # Interromper o loop se 'q' for pressionado\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Interromper o loop se o fim do v\u00eddeo for atingido\n        break\n\n# Liberar o objeto de captura de v\u00eddeo e fechar a janela de exibi\u00e7\u00e3o\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Note a mudan\u00e7a de <code>model(frame)</code> para <code>model.track(frame)</code>, que habilita o rastreamento de objetos ao inv\u00e9s de detec\u00e7\u00e3o simples. Este script modificado ir\u00e1 executar o rastreador em cada quadro do v\u00eddeo, visualizar os resultados e exibi-los em uma janela. O loop pode ser encerrado pressionando 'q'.</p>"},{"location":"modes/track/#contribuir-com-novos-rastreadores","title":"Contribuir com Novos Rastreadores","text":"<p>Voc\u00ea \u00e9 proficiente em rastreamento de m\u00faltiplos objetos e implementou ou adaptou com sucesso um algoritmo de rastreamento com Ultralytics YOLO? Convidamos voc\u00ea a contribuir para nossa se\u00e7\u00e3o de Rastreadores em ultralytics/cfg/trackers! Suas aplica\u00e7\u00f5es do mundo real e solu\u00e7\u00f5es podem ser inestim\u00e1veis para usu\u00e1rios trabalhando em tarefas de rastreamento.</p> <p>Ao contribuir para esta se\u00e7\u00e3o, voc\u00ea ajuda a expandir o escopo de solu\u00e7\u00f5es de rastreamento dispon\u00edveis dentro do framework Ultralytics YOLO, adicionando outra camada de funcionalidade e utilidade para a comunidade.</p> <p>Para iniciar sua contribui\u00e7\u00e3o, por favor, consulte nosso Guia de Contribui\u00e7\u00e3o para instru\u00e7\u00f5es completas sobre como enviar um Pedido de Pull (PR) \ud83d\udee0\ufe0f. Estamos ansiosos para ver o que voc\u00ea traz para a mesa!</p> <p>Juntos, vamos aprimorar as capacidades de rastreamento do ecossistema Ultralytics YOLO \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Treinamento de Modelos com a YOLO da Ultralytics","text":""},{"location":"modes/train/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O treinamento de um modelo de aprendizado profundo envolve fornecer dados e ajustar seus par\u00e2metros para que ele possa fazer previs\u00f5es precisas. O modo de treino na YOLOv8 da Ultralytics \u00e9 projetado para um treinamento eficaz e eficiente de modelos de detec\u00e7\u00e3o de objetos, aproveitando totalmente as capacidades do hardware moderno. Este guia visa cobrir todos os detalhes que voc\u00ea precisa para come\u00e7ar a treinar seus pr\u00f3prios modelos usando o robusto conjunto de recursos da YOLOv8.</p> <p> Assista: Como Treinar um modelo YOLOv8 no Seu Conjunto de Dados Personalizado no Google Colab. </p>"},{"location":"modes/train/#por-que-escolher-a-yolo-da-ultralytics-para-treinamento","title":"Por Que Escolher a YOLO da Ultralytics para Treinamento?","text":"<p>Aqui est\u00e3o algumas raz\u00f5es convincentes para optar pelo modo de Treino da YOLOv8:</p> <ul> <li>Efici\u00eancia: Aproveite ao m\u00e1ximo seu hardware, seja em um setup com uma \u00fanica GPU ou expandindo para m\u00faltiplas GPUs.</li> <li>Versatilidade: Treine em conjuntos de dados personalizados, al\u00e9m dos j\u00e1 dispon\u00edveis, como COCO, VOC e ImageNet.</li> <li>Facilidade de Uso: Interfaces de linha de comando (CLI) e em Python simples, por\u00e9m poderosas, para uma experi\u00eancia de treinamento direta.</li> <li>Flexibilidade de Hiperpar\u00e2metros: Uma ampla gama de hiperpar\u00e2metros personaliz\u00e1veis para ajustar o desempenho do modelo.</li> </ul>"},{"location":"modes/train/#principais-recursos-do-modo-de-treino","title":"Principais Recursos do Modo de Treino","text":"<p>Os seguintes s\u00e3o alguns recursos not\u00e1veis \u200b\u200bdo modo de Treino da YOLOv8:</p> <ul> <li>Download Autom\u00e1tico de Datasets: Datasets padr\u00f5es como COCO, VOC e ImageNet s\u00e3o baixados automaticamente na primeira utiliza\u00e7\u00e3o.</li> <li>Suporte a Multi-GPU: Escalone seus esfor\u00e7os de treinamento de maneira uniforme entre v\u00e1rias GPUs para acelerar o processo.</li> <li>Configura\u00e7\u00e3o de Hiperpar\u00e2metros: Op\u00e7\u00e3o de modificar hiperpar\u00e2metros atrav\u00e9s de arquivos de configura\u00e7\u00e3o YAML ou argumentos de CLI.</li> <li>Visualiza\u00e7\u00e3o e Monitoramento: Acompanhamento em tempo real das m\u00e9tricas de treinamento e visualiza\u00e7\u00e3o do processo de aprendizagem para obter melhores insights.</li> </ul> <p>Dica</p> <ul> <li>Conjuntos de dados YOLOv8 como COCO, VOC, ImageNet e muitos outros s\u00e3o baixados automaticamente na primeira utiliza\u00e7\u00e3o, ou seja, <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Treine o YOLOv8n no conjunto de dados COCO128 por 100 \u00e9pocas com tamanho de imagem de 640. O dispositivo de treinamento pode ser especificado usando o argumento <code>device</code>. Se nenhum argumento for passado, a GPU <code>device=0</code> ser\u00e1 usado se dispon\u00edvel, caso contr\u00e1rio, <code>device=cpu</code> ser\u00e1 usado. Veja a se\u00e7\u00e3o Argumentos abaixo para uma lista completa dos argumentos de treinamento.</p> <p>Exemplo de Treinamento em Uma \u00danica GPU e CPU</p> <p>O dispositivo \u00e9 determinado automaticamente. Se uma GPU estiver dispon\u00edvel, ela ser\u00e1 usada, caso contr\u00e1rio, o treinamento come\u00e7ar\u00e1 na CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#treinamento-com-multi-gpu","title":"Treinamento com Multi-GPU","text":"<p>O treinamento com m\u00faltiplas GPUs permite uma utiliza\u00e7\u00e3o mais eficiente dos recursos de hardware dispon\u00edveis, distribuindo a carga de treinamento entre v\u00e1rias GPUs. Esse recurso est\u00e1 dispon\u00edvel por meio da API do Python e da interface de linha de comando. Para habilitar o treinamento com v\u00e1rias GPUs, especifique os IDs dos dispositivos de GPU que deseja usar.</p> <p>Exemplo de Treinamento com Multi-GPU</p> <p>Para treinar com 2 GPUs, dispositivos CUDA 0 e 1 use os seguintes comandos. Expanda para GPUs adicionais conforme necess\u00e1rio.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\n\n# Treinar o modelo com 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado usando as GPUs 0 e 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#treinamento-com-apple-m1-e-m2-mps","title":"Treinamento com Apple M1 e M2 MPS","text":"<p>Com a integra\u00e7\u00e3o do suporte para os chips Apple M1 e M2 nos modelos YOLO da Ultralytics, agora \u00e9 poss\u00edvel treinar seus modelos em dispositivos que utilizam o poderoso framework Metal Performance Shaders (MPS). O MPS oferece uma forma de alto desempenho de executar tarefas de computa\u00e7\u00e3o e processamento de imagens no sil\u00edcio personalizado da Apple.</p> <p>Para habilitar o treinamento nos chips Apple M1 e M2, voc\u00ea deve especificar 'mps' como seu dispositivo ao iniciar o processo de treinamento. Abaixo est\u00e1 um exemplo de como voc\u00ea pode fazer isso em Python e via linha de comando:</p> <p>Exemplo de Treinamento com MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\n\n# Treinar o modelo com MPS\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado usando MPS\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Ao aproveitar o poder computacional dos chips M1/M2, isso possibilita o processamento mais eficiente das tarefas de treinamento. Para orienta\u00e7\u00f5es mais detalhadas e op\u00e7\u00f5es avan\u00e7adas de configura\u00e7\u00e3o, consulte a documenta\u00e7\u00e3o do PyTorch MPS.</p>"},{"location":"modes/train/#registro-de-logs","title":"Registro de Logs","text":"<p>Ao treinar um modelo YOLOv8, voc\u00ea pode achar valioso acompanhar o desempenho do modelo ao longo do tempo. \u00c9 aqui que o registro de logs se torna \u00fatil. O YOLO da Ultralytics oferece suporte para tr\u00eas tipos de loggers - Comet, ClearML e TensorBoard.</p> <p>Para usar um logger, selecione-o no menu suspenso no trecho de c\u00f3digo acima e execute-o. O logger escolhido ser\u00e1 instalado e inicializado.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet \u00e9 uma plataforma que permite a cientistas de dados e desenvolvedores rastrear, comparar, explicar e otimizar experimentos e modelos. Oferece funcionalidades como m\u00e9tricas em tempo real, diffs de c\u00f3digo e acompanhamento de hiperpar\u00e2metros.</p> <p>Para usar o Comet:</p> <p>Exemplo</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Lembre-se de fazer login na sua conta Comet no site deles e obter sua chave de API. Voc\u00ea precisar\u00e1 adicionar isso \u00e0s suas vari\u00e1veis de ambiente ou ao seu script para registrar seus experimentos.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML \u00e9 uma plataforma de c\u00f3digo aberto que automatiza o rastreamento de experimentos e ajuda com o compartilhamento eficiente de recursos. \u00c9 projetada para ajudar as equipes a gerenciar, executar e reproduzir seus trabalhos de ML de maneira mais eficiente.</p> <p>Para usar o ClearML:</p> <p>Exemplo</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Ap\u00f3s executar este script, voc\u00ea precisar\u00e1 fazer login na sua conta ClearML no navegador e autenticar sua sess\u00e3o.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard \u00e9 um kit de ferramentas de visualiza\u00e7\u00e3o para TensorFlow. Permite visualizar o seu gr\u00e1fico TensorFlow, plotar m\u00e9tricas quantitativas sobre a execu\u00e7\u00e3o do seu gr\u00e1fico e mostrar dados adicionais como imagens que passam por ele.</p> <p>Para usar o TensorBoard em Google Colab:</p> <p>Exemplo</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # substitua pelo diret\u00f3rio 'runs'\n</code></pre> <p>Para usar o TensorBoard localmente, execute o comando abaixo e veja os resultados em http://localhost:6006/:</p> <p>Exemplo</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # substitua pelo diret\u00f3rio 'runs'\n</code></pre> <p>Isso ir\u00e1 carregar o TensorBoard e direcion\u00e1-lo para o diret\u00f3rio onde seus logs de treinamento est\u00e3o salvos.</p> <p>Depois de configurar o seu logger, voc\u00ea pode ent\u00e3o prosseguir com o treinamento do seu modelo. Todas as m\u00e9tricas de treinamento ser\u00e3o registradas automaticamente na sua plataforma escolhida, e voc\u00ea pode acessar esses logs para monitorar o desempenho do seu modelo ao longo do tempo, comparar diferentes modelos e identificar \u00e1reas para melhoria.</p>"},{"location":"modes/val/","title":"Valida\u00e7\u00e3o de Modelos com Ultralytics YOLO","text":""},{"location":"modes/val/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>A valida\u00e7\u00e3o \u00e9 um passo cr\u00edtico no pipeline de aprendizado de m\u00e1quina, permitindo que voc\u00ea avalie a qualidade dos seus modelos treinados. O modo Val no Ultralytics YOLOv8 fornece um robusto conjunto de ferramentas e m\u00e9tricas para avaliar o desempenho dos seus modelos de detec\u00e7\u00e3o de objetos. Este guia serve como um recurso completo para entender como usar efetivamente o modo Val para garantir que seus modelos sejam precisos e confi\u00e1veis.</p>"},{"location":"modes/val/#por-que-validar-com-o-ultralytics-yolo","title":"Por Que Validar com o Ultralytics YOLO?","text":"<p>Aqui est\u00e3o as vantagens de usar o modo Val no YOLOv8:</p> <ul> <li>Precis\u00e3o: Obtenha m\u00e9tricas precisas como mAP50, mAP75 e mAP50-95 para avaliar seu modelo de forma abrangente.</li> <li>Conveni\u00eancia: Utilize recursos integrados que lembram as configura\u00e7\u00f5es de treinamento, simplificando o processo de valida\u00e7\u00e3o.</li> <li>Flexibilidade: Valide seu modelo com os mesmos ou diferentes conjuntos de dados e tamanhos de imagem.</li> <li>Ajuste de Hiperpar\u00e2metros: Utilize as m\u00e9tricas de valida\u00e7\u00e3o para refinar seu modelo e obter um desempenho melhor.</li> </ul>"},{"location":"modes/val/#principais-recursos-do-modo-val","title":"Principais Recursos do Modo Val","text":"<p>Estas s\u00e3o as funcionalidades not\u00e1veis oferecidas pelo modo Val do YOLOv8:</p> <ul> <li>Configura\u00e7\u00f5es Automatizadas: Os modelos lembram suas configura\u00e7\u00f5es de treinamento para valida\u00e7\u00e3o direta.</li> <li>Suporte Multi-M\u00e9trico: Avalie seu modelo com base em uma variedade de m\u00e9tricas de precis\u00e3o.</li> <li>API em Python e CLI: Escolha entre a interface de linha de comando ou API em Python com base na sua prefer\u00eancia de valida\u00e7\u00e3o.</li> <li>Compatibilidade de Dados: Funciona perfeitamente com conjuntos de dados usados durante a fase de treinamento, bem como conjuntos de dados personalizados.</li> </ul> <p>Dica</p> <ul> <li>Os modelos YOLOv8 lembram automaticamente suas configura\u00e7\u00f5es de treinamento, ent\u00e3o voc\u00ea pode validar um modelo no mesmo tamanho de imagem e no conjunto de dados original facilmente com apenas <code>yolo val model=yolov8n.pt</code> ou <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Validar a precis\u00e3o do modelo YOLOv8n treinado no conjunto de dados COCO128. Nenhum argumento precisa ser passado, pois o <code>model</code> ret\u00e9m os dados de treinamento e argumentos como atributos do modelo. Veja a se\u00e7\u00e3o de Argumentos abaixo para uma lista completa dos argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # nenhum argumento necess\u00e1rio, conjunto de dados e configura\u00e7\u00f5es lembrados\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # validar modelo oficial\nyolo detect val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"modes/val/#argumentos","title":"Argumentos","text":"<p>As configura\u00e7\u00f5es de valida\u00e7\u00e3o para os modelos YOLO referem-se aos v\u00e1rios hiperpar\u00e2metros e configura\u00e7\u00f5es usados para avaliar o desempenho do modelo em um conjunto de dados de valida\u00e7\u00e3o. Essas configura\u00e7\u00f5es podem afetar o desempenho, velocidade e precis\u00e3o do modelo. Algumas configura\u00e7\u00f5es comuns de valida\u00e7\u00e3o do YOLO incluem o tamanho do lote, a frequ\u00eancia com que a valida\u00e7\u00e3o \u00e9 realizada durante o treinamento e as m\u00e9tricas usadas para avaliar o desempenho do modelo. Outros fatores que podem afetar o processo de valida\u00e7\u00e3o incluem o tamanho e a composi\u00e7\u00e3o do conjunto de dados de valida\u00e7\u00e3o e a tarefa espec\u00edfica para a qual o modelo est\u00e1 sendo usado. \u00c9 importante ajustar e experimentar cuidadosamente essas configura\u00e7\u00f5es para garantir que o modelo apresente um bom desempenho no conjunto de dados de valida\u00e7\u00e3o e para detectar e prevenir o sobreajuste.</p> Chave Valor Descri\u00e7\u00e3o <code>data</code> <code>None</code> caminho para o arquivo de dados, ex. coco128.yaml <code>imgsz</code> <code>640</code> tamanho das imagens de entrada como inteiro <code>batch</code> <code>16</code> n\u00famero de imagens por lote (-1 para AutoBatch) <code>save_json</code> <code>False</code> salvar resultados em arquivo JSON <code>save_hybrid</code> <code>False</code> salvar vers\u00e3o h\u00edbrida das etiquetas (etiquetas + previs\u00f5es adicionais) <code>conf</code> <code>0.001</code> limite de confian\u00e7a do objeto para detec\u00e7\u00e3o <code>iou</code> <code>0.6</code> limiar de interse\u00e7\u00e3o sobre uni\u00e3o (IoU) para NMS <code>max_det</code> <code>300</code> n\u00famero m\u00e1ximo de detec\u00e7\u00f5es por imagem <code>half</code> <code>True</code> usar precis\u00e3o meia (FP16) <code>device</code> <code>None</code> dispositivo para execu\u00e7\u00e3o, ex. dispositivo cuda=0/1/2/3 ou device=cpu <code>dnn</code> <code>False</code> usar OpenCV DNN para infer\u00eancia ONNX <code>plots</code> <code>False</code> mostrar gr\u00e1ficos durante o treinamento <code>rect</code> <code>False</code> val retangular com cada lote colado para minimizar o preenchimento <code>split</code> <code>val</code> divis\u00e3o do conjunto de dados para usar na valida\u00e7\u00e3o, ex. 'val', 'test' ou 'train'"},{"location":"tasks/","title":"Tarefas do Ultralytics YOLOv8","text":"<p>YOLOv8 \u00e9 um framework de IA que suporta m\u00faltiplas tarefas de vis\u00e3o computacional. O framework pode ser usado para realizar detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, classifica\u00e7\u00e3o e estimativa de pose. Cada uma dessas tarefas tem um objetivo e caso de uso diferente.</p> <p>Nota</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o multil\u00edngue est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando para aprimor\u00e1-la. Agradecemos sua paci\u00eancia! \ud83d\ude4f</p> <p> Assista: Explore as Tarefas do Ultralytics YOLO: Detec\u00e7\u00e3o de Objetos, Segmenta\u00e7\u00e3o, Rastreamento e Estimativa de Pose. </p>"},{"location":"tasks/#deteccao","title":"Detec\u00e7\u00e3o","text":"<p>A detec\u00e7\u00e3o \u00e9 a principal tarefa suportada pelo YOLOv8. Envolve detectar objetos em uma imagem ou quadro de v\u00eddeo e desenhar caixas delimitadoras ao redor deles. Os objetos detectados s\u00e3o classificados em diferentes categorias com base em suas caracter\u00edsticas. YOLOv8 pode detectar m\u00faltiplos objetos em uma \u00fanica imagem ou quadro de v\u00eddeo com alta precis\u00e3o e velocidade.</p> <p>Exemplos de Detec\u00e7\u00e3o</p>"},{"location":"tasks/#segmentacao","title":"Segmenta\u00e7\u00e3o","text":"<p>Segmenta\u00e7\u00e3o \u00e9 uma tarefa que envolve segmentar uma imagem em diferentes regi\u00f5es com base no conte\u00fado da imagem. Cada regi\u00e3o recebe um r\u00f3tulo com base em seu conte\u00fado. Essa tarefa \u00e9 \u00fatil em aplica\u00e7\u00f5es como segmenta\u00e7\u00e3o de imagens e imagiologia m\u00e9dica. YOLOv8 usa uma variante da arquitetura U-Net para realizar a segmenta\u00e7\u00e3o.</p> <p>Exemplos de Segmenta\u00e7\u00e3o</p>"},{"location":"tasks/#classificacao","title":"Classifica\u00e7\u00e3o","text":"<p>Classifica\u00e7\u00e3o \u00e9 uma tarefa que envolve classificar uma imagem em diferentes categorias. YOLOv8 pode ser usado para classificar imagens com base em seu conte\u00fado. Utiliza uma variante da arquitetura EfficientNet para realizar a classifica\u00e7\u00e3o.</p> <p>Exemplos de Classifica\u00e7\u00e3o</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>A detec\u00e7\u00e3o de pose/pontos-chave \u00e9 uma tarefa que envolve detectar pontos espec\u00edficos em uma imagem ou quadro de v\u00eddeo. Esses pontos s\u00e3o chamados de keypoints e s\u00e3o usados para rastrear movimento ou estimar poses. YOLOv8 pode detectar keypoints em uma imagem ou quadro de v\u00eddeo com alta precis\u00e3o e velocidade.</p> <p>Exemplos de Pose</p>"},{"location":"tasks/#conclusao","title":"Conclus\u00e3o","text":"<p>YOLOv8 suporta m\u00faltiplas tarefas, incluindo detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, classifica\u00e7\u00e3o e detec\u00e7\u00e3o de keypoints. Cada uma dessas tarefas tem objetivos e casos de uso diferentes. Ao entender as diferen\u00e7as entre essas tarefas, voc\u00ea pode escolher a tarefa apropriada para sua aplica\u00e7\u00e3o de vis\u00e3o computacional.</p>"},{"location":"tasks/classify/","title":"Classifica\u00e7\u00e3o de Imagens","text":"<p>A classifica\u00e7\u00e3o de imagens \u00e9 a tarefa mais simples das tr\u00eas e envolve classificar uma imagem inteira em uma de um conjunto de classes pr\u00e9-definidas.</p> <p>A sa\u00edda de um classificador de imagem \u00e9 um \u00fanico r\u00f3tulo de classe e uma pontua\u00e7\u00e3o de confian\u00e7a. A classifica\u00e7\u00e3o de imagem \u00e9 \u00fatil quando voc\u00ea precisa saber apenas a qual classe uma imagem pertence e n\u00e3o precisa conhecer a localiza\u00e7\u00e3o dos objetos dessa classe ou o formato exato deles.</p> <p>Dica</p> <p>Os modelos YOLOv8 Classify usam o sufixo <code>-cls</code>, ou seja, <code>yolov8n-cls.pt</code> e s\u00e3o pr\u00e9-treinados na ImageNet.</p>"},{"location":"tasks/classify/#modelos","title":"Modelos","text":"<p>Aqui s\u00e3o mostrados os modelos pr\u00e9-treinados YOLOv8 Classify. Modelos de Detec\u00e7\u00e3o, Segmenta\u00e7\u00e3o e Pose s\u00e3o pr\u00e9-treinados no dataset COCO, enquanto que os modelos de Classifica\u00e7\u00e3o s\u00e3o pr\u00e9-treinados no dataset ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo Tamanho<sup>(pixels) acur\u00e1cia<sup>top1 acur\u00e1cia<sup>top5 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) a 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Os valores de acc s\u00e3o as acur\u00e1cias dos modelos no conjunto de valida\u00e7\u00e3o do dataset ImageNet.   Reproduza com <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Velocidade m\u00e9dia observada sobre imagens de valida\u00e7\u00e3o da ImageNet usando uma inst\u00e2ncia Amazon EC2 P4d.   Reproduza com <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#treino","title":"Treino","text":"<p>Treine o modelo YOLOv8n-cls no dataset MNIST160 por 100 \u00e9pocas com tamanho de imagem 64. Para uma lista completa de argumentos dispon\u00edveis, veja a p\u00e1gina de Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treino)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar treino do zero\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Come\u00e7ar treino de um modelo pr\u00e9-treinado *.pt\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construir um novo modelo do YAML, transferir pesos pr\u00e9-treinados e come\u00e7ar treino\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#formato-do-dataset","title":"Formato do dataset","text":"<p>O formato do dataset de classifica\u00e7\u00e3o YOLO pode ser encontrado em detalhes no Guia de Datasets.</p>"},{"location":"tasks/classify/#val","title":"Val","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-cls treinado no dataset MNIST160. N\u00e3o \u00e9 necess\u00e1rio passar argumento, pois o <code>modelo</code> ret\u00e9m seus dados de treinamento e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem argumentos necess\u00e1rios, dataset e configura\u00e7\u00f5es lembrados\nmetrics.top1   # acur\u00e1cia top1\nmetrics.top5   # acur\u00e1cia top5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # validar modelo oficial\nyolo classify val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/classify/#previsao","title":"Previs\u00e3o","text":"<p>Use um modelo YOLOv8n-cls treinado para realizar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Prever com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo oficial\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo de <code>previs\u00e3o</code> na p\u00e1gina Predict.</p>"},{"location":"tasks/classify/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n-cls para um formato diferente, como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8-cls dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente nos modelos exportados, ou seja, <code>yolo predict model=yolov8n-cls.onnx</code>. Exemplos de uso s\u00e3o mostrados para seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadata Argumentos PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Export.</p>"},{"location":"tasks/detect/","title":"Detec\u00e7\u00e3o de Objetos","text":"<p>Detec\u00e7\u00e3o de objetos \u00e9 uma tarefa que envolve identificar a localiza\u00e7\u00e3o e a classe de objetos em uma imagem ou fluxo de v\u00eddeo.</p> <p>A sa\u00edda de um detector de objetos \u00e9 um conjunto de caixas delimitadoras que cercam os objetos na imagem, junto com r\u00f3tulos de classe e pontua\u00e7\u00f5es de confian\u00e7a para cada caixa. A detec\u00e7\u00e3o de objetos \u00e9 uma boa escolha quando voc\u00ea precisa identificar objetos de interesse em uma cena, mas n\u00e3o precisa saber exatamente onde o objeto est\u00e1 ou seu formato exato.</p> <p> Assista: Detec\u00e7\u00e3o de Objetos com Modelo Pre-treinado Ultralytics YOLOv8. </p> <p>Dica</p> <p>Os modelos YOLOv8 Detect s\u00e3o os modelos padr\u00e3o do YOLOv8, ou seja, <code>yolov8n.pt</code> e s\u00e3o pr\u00e9-treinados no COCO.</p>"},{"location":"tasks/detect/#modelos","title":"Modelos","text":"<p>Os modelos pr\u00e9-treinados YOLOv8 Detect s\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no dataset COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no dataset ImageNet.</p> <p>Os Modelos s\u00e3o baixados automaticamente a partir do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo Tamanho<sup>(pixels) mAP<sup>val50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) Par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Os valores de mAP<sup>val</sup> s\u00e3o para um \u00fanico modelo e uma \u00fanica escala no dataset COCO val2017.   Reproduza usando <code>yolo val detect data=coco.yaml device=0</code></li> <li>A Velocidade \u00e9 m\u00e9dia tirada sobre as imagens do COCO val num Amazon EC2 P4d   inst\u00e2ncia.   Reproduza usando <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#treinar","title":"Treinar","text":"<p>Treine o YOLOv8n no dataset COCO128 por 100 \u00e9pocas com tamanho de imagem 640. Para uma lista completa de argumentos dispon\u00edveis, veja a p\u00e1gina Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.yaml')  # construir um novo modelo pelo YAML\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir pelo YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo pelo YAML e come\u00e7ar o treinamento do zero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo pr\u00e9-treinado *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir um novo modelo pelo YAML, transferir pesos pr\u00e9-treinados e come\u00e7ar o treinamento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#formato-do-dataset","title":"Formato do Dataset","text":"<p>O formato do dataset de detec\u00e7\u00e3o do YOLO pode ser encontrado em detalhes no Guia de Datasets. Para converter seu dataset existente de outros formatos (como COCO, etc.) para o formato YOLO, por favor utilize a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/detect/#validar","title":"Validar","text":"<p>Valide a precis\u00e3o do modelo YOLOv8n treinado no dataset COCO128. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>modelo</code> mant\u00e9m seus <code>dados</code> de treino e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem a necessidade de argumentos, dataset e configura\u00e7\u00f5es lembradas\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # valida\u00e7\u00e3o do modelo oficial\nyolo detect val model=caminho/para/best.pt  # valida\u00e7\u00e3o do modelo personalizado\n</code></pre>"},{"location":"tasks/detect/#predizer","title":"Predizer","text":"<p>Use um modelo YOLOv8n treinado para fazer predi\u00e7\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo personalizado\n\n# Predizer com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predizer em uma imagem\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # predizer com modelo oficial\nyolo detect predict model=caminho/para/best.pt source='https://ultralytics.com/images/bus.jpg'  # predizer com modelo personalizado\n</code></pre> <p>Veja os detalhes completos do modo <code>predict</code> na p\u00e1gina Predi\u00e7\u00e3o.</p>"},{"location":"tasks/detect/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n para um formato diferente, como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exportar modelo oficial\nyolo export model=caminho/para/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8 dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode fazer predi\u00e7\u00f5es ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a exporta\u00e7\u00e3o ser conclu\u00edda.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja os detalhes completos de <code>exportar</code> na p\u00e1gina Exporta\u00e7\u00e3o.</p>"},{"location":"tasks/pose/","title":"Estimativa de Pose","text":"<p>A estimativa de pose \u00e9 uma tarefa que envolve identificar a localiza\u00e7\u00e3o de pontos espec\u00edficos em uma imagem, geralmente referidos como pontos-chave. Os pontos-chave podem representar v\u00e1rias partes do objeto como articula\u00e7\u00f5es, pontos de refer\u00eancia ou outras caracter\u00edsticas distintas. As localiza\u00e7\u00f5es dos pontos-chave s\u00e3o geralmente representadas como um conjunto de coordenadas 2D <code>[x, y]</code> ou 3D <code>[x, y, vis\u00edvel]</code>.</p> <p>A sa\u00edda de um modelo de estimativa de pose \u00e9 um conjunto de pontos que representam os pontos-chave em um objeto na imagem, geralmente junto com os escores de confian\u00e7a para cada ponto. A estimativa de pose \u00e9 uma boa escolha quando voc\u00ea precisa identificar partes espec\u00edficas de um objeto em uma cena, e sua localiza\u00e7\u00e3o relativa entre si.</p> <p> Assista: Estimativa de Pose com Ultralytics YOLOv8. </p> <p>Dica</p> <p>Modelos YOLOv8 pose usam o sufixo <code>-pose</code>, isto \u00e9 <code>yolov8n-pose.pt</code>. Esses modelos s\u00e3o treinados no conjunto de dados COCO keypoints e s\u00e3o adequados para uma variedade de tarefas de estimativa de pose.</p>"},{"location":"tasks/pose/#modelos","title":"Modelos","text":"<p>Os modelos YOLOv8 Pose pr\u00e9-treinados s\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no conjunto de dados COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no conjunto de dados ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo tamanho<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>mAP<sup>val</sup> valores s\u00e3o para um \u00fanico modelo em escala \u00fanica no conjunto de dados COCO Keypoints val2017   .   Reproduza <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>Velocidade m\u00e9dia em imagens COCO val usando uma inst\u00e2ncia Amazon EC2 P4d   .   Reproduza <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#treinar","title":"Treinar","text":"<p>Treine um modelo YOLOv8-pose no conjunto de dados COCO128-pose.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar treinamento de um modelo *.pt pr\u00e9-treinado\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#formato-do-conjunto-de-dados","title":"Formato do conjunto de dados","text":"<p>O formato do conjunto de dados de pose YOLO pode ser encontrado em detalhes no Guia de Conjuntos de Dados. Para converter seu conjunto de dados existente de outros formatos (como COCO etc.) para o formato YOLO, por favor, use a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/pose/#validar","title":"Validar","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-pose treinado no conjunto de dados COCO128-pose. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>model</code> ret\u00e9m seus <code>data</code> de treinamento e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # nenhum argumento necess\u00e1rio, conjunto de dados e configura\u00e7\u00f5es lembradas\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # validar modelo oficial\nyolo pose val model=caminho/para/melhor.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/pose/#prever","title":"Prever","text":"<p>Use um modelo YOLOv8n-pose treinado para executar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo personalizado\n\n# Prever com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo oficial\nyolo pose predict model=caminho/para/melhor.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo <code>predict</code> na p\u00e1gina Prever.</p>"},{"location":"tasks/pose/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n Pose para um formato diferente como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exportar modelo oficial\nyolo export model=caminho/para/melhor.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8-pose dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n-pose.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Exportar.</p>"},{"location":"tasks/segment/","title":"Segmenta\u00e7\u00e3o de Inst\u00e2ncias","text":"<p>A segmenta\u00e7\u00e3o de inst\u00e2ncias vai al\u00e9m da detec\u00e7\u00e3o de objetos e envolve a identifica\u00e7\u00e3o de objetos individuais em uma imagem e a sua segmenta\u00e7\u00e3o do resto da imagem.</p> <p>A sa\u00edda de um modelo de segmenta\u00e7\u00e3o de inst\u00e2ncias \u00e9 um conjunto de m\u00e1scaras ou contornos que delineiam cada objeto na imagem, juntamente com r\u00f3tulos de classe e pontua\u00e7\u00f5es de confian\u00e7a para cada objeto. A segmenta\u00e7\u00e3o de inst\u00e2ncias \u00e9 \u00fatil quando voc\u00ea precisa saber n\u00e3o apenas onde os objetos est\u00e3o em uma imagem, mas tamb\u00e9m qual \u00e9 a forma exata deles.</p> <p> Assista: Executar Segmenta\u00e7\u00e3o com o Modelo Treinado Ultralytics YOLOv8 em Python. </p> <p>Dica</p> <p>Modelos YOLOv8 Segment usam o sufixo <code>-seg</code>, ou seja, <code>yolov8n-seg.pt</code> e s\u00e3o pr\u00e9-treinados no COCO.</p>"},{"location":"tasks/segment/#modelos","title":"Modelos","text":"<p>Os modelos Segment pr\u00e9-treinados do YOLOv8 est\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no conjunto de dados COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no conjunto de dados ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release na primeira utiliza\u00e7\u00e3o.</p> Modelo Tamanho<sup>(pixels) mAP<sup>box50-95 mAP<sup>m\u00e1scara50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) Par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Os valores de mAP<sup>val</sup> s\u00e3o para um \u00fanico modelo em uma \u00fanica escala no conjunto de dados COCO val2017.   Reproduza por meio de <code>yolo val segment data=coco.yaml device=0</code></li> <li>Velocidade m\u00e9dia em imagens COCO val usando uma inst\u00e2ncia Amazon EC2 P4d.   Reproduza por meio de <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#treinar","title":"Treinar","text":"<p>Treine o modelo YOLOv8n-seg no conjunto de dados COCO128-seg por 100 \u00e9pocas com tamanho de imagem 640. Para uma lista completa de argumentos dispon\u00edveis, consulte a p\u00e1gina Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construir a partir do YAML e transferir os pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#formato-do-conjunto-de-dados","title":"Formato do conjunto de dados","text":"<p>O formato do conjunto de dados de segmenta\u00e7\u00e3o YOLO pode ser encontrado em detalhes no Guia de Conjuntos de Dados. Para converter seu conjunto de dados existente de outros formatos (como COCO etc.) para o formato YOLO, utilize a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/segment/#val","title":"Val","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-seg treinado no conjunto de dados COCO128-seg. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>modelo</code> ret\u00e9m seus <code>dados</code> de treino e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem necessidade de argumentos, conjunto de dados e configura\u00e7\u00f5es s\u00e3o lembrados\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # uma lista contendo map50-95(B) de cada categoria\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # uma lista contendo map50-95(M) de cada categoria\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # val modelo oficial\nyolo segment val model=path/to/best.pt  # val modelo personalizado\n</code></pre>"},{"location":"tasks/segment/#prever","title":"Prever","text":"<p>Use um modelo YOLOv8n-seg treinado para realizar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Realizar previs\u00e3o com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # previs\u00e3o com modelo oficial\nyolo segment predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # previs\u00e3o com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo <code>predict</code> na p\u00e1gina Prever.</p>"},{"location":"tasks/segment/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n-seg para um formato diferente como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o dispon\u00edveis para YOLOv8-seg est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n-seg.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Exportar.</p>"}]}