{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"P\u00e1gina Inicial","text":"<p>Apresentamos o Ultralytics YOLOv8, a mais recente vers\u00e3o do aclamado modelo de detec\u00e7\u00e3o de objetos em tempo real e segmenta\u00e7\u00e3o de imagens. O YOLOv8 \u00e9 baseado nos mais recentes avan\u00e7os do aprendizado profundo e vis\u00e3o computacional, oferecendo um desempenho sem paralelo em termos de velocidade e precis\u00e3o. Seu design simplificado o torna adequado para v\u00e1rias aplica\u00e7\u00f5es e facilmente adapt\u00e1vel a diferentes plataformas de hardware, desde dispositivos de borda at\u00e9 APIs na nuvem.</p> <p>Explore os Documentos do YOLOv8, um recurso abrangente projetado para ajud\u00e1-lo a entender e utilizar suas caracter\u00edsticas e capacidades. Seja voc\u00ea um praticante experiente de aprendizado de m\u00e1quina ou novo no campo, este hub tem como objetivo maximizar o potencial do YOLOv8 em seus projetos</p> <p>Note</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o em v\u00e1rios idiomas est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando arduamente para aprimor\u00e1-la. Agradecemos sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"#por-onde-comecar","title":"Por Onde Come\u00e7ar","text":"<ul> <li>Instalar <code>ultralytics</code> com pip e come\u00e7ar a funcionar em minutos \u00a0  Come\u00e7ar</li> <li>Prever novas imagens e v\u00eddeos com o YOLOv8 \u00a0  Prever em Imagens</li> <li>Treinar um novo modelo YOLOv8 em seu pr\u00f3prio conjunto de dados personalizado \u00a0  Treinar um Modelo</li> <li>Explorar tarefas do YOLOv8 como segmentar, classificar, estimar pose e rastrear \u00a0  Explorar Tarefas</li> </ul> <p> Assistir: Como Treinar um Modelo YOLOv8 em Seu Conjunto de Dados Personalizado no Google Colab. </p>"},{"location":"#yolo-uma-breve-historia","title":"YOLO: Uma Breve Hist\u00f3ria","text":"<p>YOLO (You Only Look Once), um popular modelo de detec\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o de imagens, foi desenvolvido por Joseph Redmon e Ali Farhadi na Universidade de Washington. Lan\u00e7ado em 2015, o YOLO rapidamente ganhou popularidade por sua alta velocidade e precis\u00e3o.</p> <ul> <li>YOLOv2, lan\u00e7ado em 2016, aprimorou o modelo original incorporando normaliza\u00e7\u00e3o em lote, caixas \u00e2ncora e aglomerados dimensionais.</li> <li>YOLOv3, lan\u00e7ado em 2018, melhorou ainda mais o desempenho do modelo usando uma rede dorsal mais eficiente, m\u00faltiplas \u00e2ncoras e pooling piramidal espacial.</li> <li>YOLOv4 foi lan\u00e7ado em 2020, introduzindo inova\u00e7\u00f5es como a amplia\u00e7\u00e3o de dados Mosaic, uma nova cabe\u00e7a de detec\u00e7\u00e3o sem \u00e2ncoras e uma nova fun\u00e7\u00e3o de perda.</li> <li>YOLOv5 melhorou ainda mais o desempenho do modelo e adicionou novos recursos, como otimiza\u00e7\u00e3o de hiperpar\u00e2metros, rastreamento integrado de experimentos e exporta\u00e7\u00e3o autom\u00e1tica para formatos de exporta\u00e7\u00e3o populares.</li> <li>YOLOv6 foi disponibilizado em c\u00f3digo aberto por Meituan em 2022 e est\u00e1 em uso em muitos dos rob\u00f4s aut\u00f4nomos de entrega da empresa.</li> <li>YOLOv7 adicionou tarefas adicionais, como estimativa de pose no conjunto de dados de keypoints COCO.</li> <li>YOLOv8, a mais recente vers\u00e3o do YOLO pela Ultralytics. Como um modelo de \u00faltima gera\u00e7\u00e3o, o YOLOv8 baseia-se no sucesso das vers\u00f5es anteriores, introduzindo novos recursos e melhorias para desempenho, flexibilidade e efici\u00eancia aprimorados. O YOLOv8 suporta uma gama completa de tarefas de IA de vis\u00e3o, incluindo detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, estimativa de pose, rastreamento e classifica\u00e7\u00e3o. Essa versatilidade permite que os usu\u00e1rios aproveitem as capacidades do YOLOv8 em diversas aplica\u00e7\u00f5es e dom\u00ednios.</li> </ul>"},{"location":"#licencas-yolo-como-o-yolo-da-ultralytics-e-licenciado","title":"Licen\u00e7as YOLO: Como o YOLO da Ultralytics \u00e9 licenciado?","text":"<p>A Ultralytics oferece duas op\u00e7\u00f5es de licen\u00e7a para acomodar casos de uso diversos:</p> <ul> <li>Licen\u00e7a AGPL-3.0: Essa licen\u00e7a de c\u00f3digo aberto aprovada pela OSI \u00e9 ideal para estudantes e entusiastas, promovendo colabora\u00e7\u00e3o aberta e compartilhamento de conhecimento. Veja o arquivo LICENSE para mais detalhes.</li> <li>Licen\u00e7a Empresarial: Projetada para uso comercial, esta licen\u00e7a permite a integra\u00e7\u00e3o perfeita do software Ultralytics e modelos de IA em bens e servi\u00e7os comerciais, contornando os requisitos de c\u00f3digo aberto da AGPL-3.0. Se o seu cen\u00e1rio envolver a incorpora\u00e7\u00e3o de nossas solu\u00e7\u00f5es em uma oferta comercial, entre em contato atrav\u00e9s do Licenciamento da Ultralytics.</li> </ul> <p>Nossa estrat\u00e9gia de licenciamento \u00e9 projetada para garantir que qualquer melhoria em nossos projetos de c\u00f3digo aberto retorne \u00e0 comunidade. Mantemos os princ\u00edpios de c\u00f3digo aberto pr\u00f3ximos ao nosso cora\u00e7\u00e3o \u2764\ufe0f, e nossa miss\u00e3o \u00e9 garantir que nossas contribui\u00e7\u00f5es possam ser utilizadas e expandidas de formas que beneficiem todos.</p>"},{"location":"quickstart/","title":"In\u00edcio R\u00e1pido","text":""},{"location":"quickstart/#instalacao-do-ultralytics","title":"Instala\u00e7\u00e3o do Ultralytics","text":"<p>O Ultralytics oferece diversos m\u00e9todos de instala\u00e7\u00e3o, incluindo pip, conda e Docker. Instale o YOLOv8 atrav\u00e9s do pacote <code>ultralytics</code> pip para a vers\u00e3o est\u00e1vel mais recente ou clonando o reposit\u00f3rio GitHub do Ultralytics para obter a vers\u00e3o mais atualizada. O Docker pode ser usado para executar o pacote em um cont\u00eainer isolado, evitando a instala\u00e7\u00e3o local.</p> <p>Instalar</p> Pip install (recomendado)Conda installGit clone <p>Instale o pacote <code>ultralytics</code> usando pip, ou atualize uma instala\u00e7\u00e3o existente executando <code>pip install -U ultralytics</code>. Visite o \u00cdndice de Pacotes Python (PyPI) para mais detalhes sobre o pacote <code>ultralytics</code>: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Instalar o pacote ultralytics do PyPI\npip install ultralytics\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode instalar o pacote <code>ultralytics</code> diretamente do reposit\u00f3rio GitHub. Isso pode ser \u00fatil se voc\u00ea desejar a vers\u00e3o de desenvolvimento mais recente. Certifique-se de ter a ferramenta de linha de comando Git instalada no seu sistema. O comando <code>@main</code> instala a branch <code>main</code> e pode ser modificado para outra branch, ou seja, <code>@my-branch</code>, ou removido completamente para padr\u00e3o na branch <code>main</code>.</p> <pre><code># Instalar o pacote ultralytics do GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda \u00e9 um gerenciador de pacotes alternativo ao pip que tamb\u00e9m pode ser usado para instala\u00e7\u00e3o. Visite Anaconda para mais detalhes em https://anaconda.org/conda-forge/ultralytics. O reposit\u00f3rio de feedstock do Ultralytics para atualizar o pacote conda est\u00e1 em https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Instalar o pacote ultralytics usando conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Note</p> <p>Se voc\u00ea est\u00e1 instalando em um ambiente CUDA a pr\u00e1tica recomendada \u00e9 instalar <code>ultralytics</code>, <code>pytorch</code> e <code>pytorch-cuda</code> no mesmo comando para permitir que o gerenciador de pacotes conda resolva quaisquer conflitos, ou instalar <code>pytorch-cuda</code> por \u00faltimo para permitir que ele substitua o pacote espec\u00edfico para CPU <code>pytorch</code>, se necess\u00e1rio. <pre><code># Instalar todos os pacotes juntos usando conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clone o reposit\u00f3rio <code>ultralytics</code> se voc\u00ea est\u00e1 interessado em contribuir para o desenvolvimento ou deseja experimentar com o c\u00f3digo-fonte mais recente. Ap\u00f3s clonar, navegue at\u00e9 o diret\u00f3rio e instale o pacote em modo edit\u00e1vel <code>-e</code> usando pip. <pre><code># Clonar o reposit\u00f3rio ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navegar para o diret\u00f3rio clonado\ncd ultralytics\n\n# Instalar o pacote em modo edit\u00e1vel para desenvolvimento\npip install -e .\n</code></pre></p> <p>Veja o arquivo requirements.txt do <code>ultralytics</code> para uma lista de depend\u00eancias. Note que todos os exemplos acima instalam todas as depend\u00eancias necess\u00e1rias.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Dica</p> <p>Os requisitos do PyTorch variam pelo sistema operacional e pelos requisitos de CUDA, ent\u00e3o \u00e9 recomendado instalar o PyTorch primeiro seguindo as instru\u00e7\u00f5es em https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#imagem-docker-conda","title":"Imagem Docker Conda","text":"<p>As imagens Docker Conda do Ultralytics tamb\u00e9m est\u00e3o dispon\u00edveis em DockerHub. Estas imagens s\u00e3o baseadas em Miniconda3 e s\u00e3o um modo simples de come\u00e7ar a usar <code>ultralytics</code> em um ambiente Conda.</p> <pre><code># Definir o nome da imagem como uma vari\u00e1vel\nt=ultralytics/ultralytics:latest-conda\n\n# Puxar a imagem mais recente do ultralytics do Docker Hub\nsudo docker pull $t\n\n# Executar a imagem ultralytics em um cont\u00eainer com suporte a GPU\nsudo docker run -it --ipc=host --gpus all $t  # todas as GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # especificar GPUs\n</code></pre>"},{"location":"quickstart/#use-o-ultralytics-com-cli","title":"Use o Ultralytics com CLI","text":"<p>A interface de linha de comando (CLI) do Ultralytics permite comandos simples de uma \u00fanica linha sem a necessidade de um ambiente Python. O CLI n\u00e3o requer personaliza\u00e7\u00e3o ou c\u00f3digo Python. Voc\u00ea pode simplesmente rodar todas as tarefas do terminal com o comando <code>yolo</code>. Confira o Guia CLI para aprender mais sobre o uso do YOLOv8 pela linha de comando.</p> <p>Example</p> SintaxeTrainPredictValExportSpecial <p>Os comandos <code>yolo</code> do Ultralytics usam a seguinte sintaxe: <pre><code>yolo TAREFA MODO ARGUMENTOS\n\nOnde   TAREFA (opcional) \u00e9 um entre [detect, segment, classify]\n        MODO (obrigat\u00f3rio) \u00e9 um entre [train, val, predict, export, track]\n        ARGUMENTOS (opcional) s\u00e3o qualquer n\u00famero de pares personalizados 'arg=valor' como 'imgsz=320' que substituem os padr\u00f5es.\n</code></pre> Veja todos os ARGUMENTOS no guia completo de Configura\u00e7\u00e3o ou com <code>yolo cfg</code></p> <p>Treinar um modelo de detec\u00e7\u00e3o por 10 \u00e9pocas com uma taxa de aprendizado inicial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Prever um v\u00eddeo do YouTube usando um modelo de segmenta\u00e7\u00e3o pr\u00e9-treinado com tamanho de imagem 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validar um modelo de detec\u00e7\u00e3o pr\u00e9-treinado com tamanho de lote 1 e tamanho de imagem 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportar um modelo de classifica\u00e7\u00e3o YOLOv8n para formato ONNX com tamanho de imagem 224 por 128 (nenhuma TAREFA necess\u00e1ria) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Executar comandos especiais para ver vers\u00e3o, visualizar configura\u00e7\u00f5es, rodar verifica\u00e7\u00f5es e mais: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Aviso</p> <p>Argumentos devem ser passados como pares <code>arg=valor</code>, separados por um sinal de igual <code>=</code> e delimitados por espa\u00e7os <code></code> entre pares. N\u00e3o use prefixos de argumentos <code>--</code> ou v\u00edrgulas <code>,</code> entre os argumentos.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Guia CLI</p>"},{"location":"quickstart/#use-o-ultralytics-com-python","title":"Use o Ultralytics com Python","text":"<p>A interface Python do YOLOv8 permite uma integra\u00e7\u00e3o tranquila em seus projetos Python, tornando f\u00e1cil carregar, executar e processar a sa\u00edda do modelo. Projetada com simplicidade e facilidade de uso em mente, a interface Python permite que os usu\u00e1rios implementem rapidamente detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e classifica\u00e7\u00e3o em seus projetos. Isto torna a interface Python do YOLOv8 uma ferramenta inestim\u00e1vel para qualquer pessoa buscando incorporar essas funcionalidades em seus projetos Python.</p> <p>Por exemplo, os usu\u00e1rios podem carregar um modelo, trein\u00e1-lo, avaliar o seu desempenho em um conjunto de valida\u00e7\u00e3o e at\u00e9 export\u00e1-lo para o formato ONNX com apenas algumas linhas de c\u00f3digo. Confira o Guia Python para aprender mais sobre o uso do YOLOv8 dentro dos seus projetos Python.</p> <p>Example</p> <pre><code>from ultralytics import YOLO\n\n# Criar um novo modelo YOLO do zero\nmodel = YOLO('yolov8n.yaml')\n\n# Carregar um modelo YOLO pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.pt')\n\n# Treinar o modelo usando o conjunto de dados 'coco128.yaml' por 3 \u00e9pocas\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Avaliar o desempenho do modelo no conjunto de valida\u00e7\u00e3o\nresults = model.val()\n\n# Realizar detec\u00e7\u00e3o de objetos em uma imagem usando o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportar o modelo para formato ONNX\nsuccess = model.export(format='onnx')\n</code></pre> <p>Guia Python</p>"},{"location":"datasets/","title":"Vis\u00e3o Geral de Conjuntos de Dados","text":"<p>A Ultralytics oferece suporte para diversos conjuntos de dados para facilitar tarefas de vis\u00e3o computacional, como detec\u00e7\u00e3o, segmenta\u00e7\u00e3o de inst\u00e2ncia, estimativa de pose, classifica\u00e7\u00e3o e rastreamento de m\u00faltiplos objetos. Abaixo est\u00e1 uma lista dos principais conjuntos de dados da Ultralytics, seguidos por um resumo de cada tarefa de vis\u00e3o computacional e os respectivos conjuntos de dados.</p> <p>Note</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o multil\u00edngue est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando arduamente para melhor\u00e1-la. Obrigado pela sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"datasets/#conjuntos-de-dados-de-deteccao","title":"Conjuntos de Dados de Detec\u00e7\u00e3o","text":"<p>A t\u00e9cnica de detec\u00e7\u00e3o de objetos com caixas delimitadoras envolve detectar e localizar objetos em uma imagem desenhando uma caixa delimitadora ao redor de cada objeto.</p> <ul> <li>Argoverse: Um conjunto de dados contendo dados de rastreamento 3D e previs\u00e3o de movimento de ambientes urbanos com anota\u00e7\u00f5es detalhadas.</li> <li>COCO: Um conjunto de dados em grande escala projetado para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e legendagem com mais de 200 mil imagens etiquetadas.</li> <li>COCO8: Cont\u00e9m as primeiras 4 imagens do COCO train e COCO val, adequado para testes r\u00e1pidos.</li> <li>Global Wheat 2020: Um conjunto de dados de imagens de espiga de trigo coletadas ao redor do mundo para tarefas de detec\u00e7\u00e3o e localiza\u00e7\u00e3o de objetos.</li> <li>Objects365: Um conjunto de dados de alta qualidade de grande escala para detec\u00e7\u00e3o de objetos com 365 categorias e mais de 600 mil imagens anotadas.</li> <li>OpenImagesV7: Um conjunto de dados abrangente do Google com 1,7 milh\u00e3o de imagens de treino e 42 mil imagens de valida\u00e7\u00e3o.</li> <li>SKU-110K: Um conjunto de dados apresentando detec\u00e7\u00e3o de objetos densos em ambientes de varejo com mais de 11 mil imagens e 1,7 milh\u00e3o de caixas delimitadoras.</li> <li>VisDrone: Um conjunto de dados que cont\u00e9m informa\u00e7\u00e3o de detec\u00e7\u00e3o de objetos e rastreamento de m\u00faltiplos objetos a partir de imagens capturadas por drones com mais de 10 mil imagens e sequ\u00eancias de v\u00eddeo.</li> <li>VOC: O conjunto de dados Visual Object Classes (VOC) Pascal para detec\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o com 20 classes de objetos e mais de 11 mil imagens.</li> <li>xView: Um conjunto de dados para detec\u00e7\u00e3o de objetos em imagens a\u00e9reas com 60 categorias de objetos e mais de 1 milh\u00e3o de objetos anotados.</li> </ul>"},{"location":"datasets/#conjuntos-de-dados-de-segmentacao-de-instancia","title":"Conjuntos de Dados de Segmenta\u00e7\u00e3o de Inst\u00e2ncia","text":"<p>A segmenta\u00e7\u00e3o de inst\u00e2ncia \u00e9 uma t\u00e9cnica de vis\u00e3o computacional que identifica e localiza objetos em uma imagem ao n\u00edvel de pixel.</p> <ul> <li>COCO: Um conjunto de dados em grande escala projetado para detec\u00e7\u00e3o de objetos, tarefas de segmenta\u00e7\u00e3o e legendagem com mais de 200 mil imagens etiquetadas.</li> <li>COCO8-seg: Um conjunto de dados menor para tarefas de segmenta\u00e7\u00e3o de inst\u00e2ncias, contendo um subconjunto de 8 imagens COCO com anota\u00e7\u00f5es de segmenta\u00e7\u00e3o.</li> </ul>"},{"location":"datasets/#estimativa-de-pose","title":"Estimativa de Pose","text":"<p>A estimativa de pose \u00e9 uma t\u00e9cnica usada para determinar a pose do objeto em rela\u00e7\u00e3o \u00e0 c\u00e2mera ou ao sistema de coordenadas do mundo.</p> <ul> <li>COCO: Um conjunto de dados em grande escala com anota\u00e7\u00f5es de pose humana projetado para tarefas de estimativa de pose.</li> <li>COCO8-pose: Um conjunto de dados menor para tarefas de estimativa de pose, contendo um subconjunto de 8 imagens COCO com anota\u00e7\u00f5es de pose humana.</li> <li>Tiger-pose: Um conjunto de dados compacto consistindo de 263 imagens focadas em tigres, anotadas com 12 pontos-chave por tigre para tarefas de estimativa de pose.</li> </ul>"},{"location":"datasets/#classificacao","title":"Classifica\u00e7\u00e3o","text":"<p>Classifica\u00e7\u00e3o de imagens \u00e9 uma tarefa de vis\u00e3o computacional que envolve categorizar uma imagem em uma ou mais classes ou categorias predefinidas com base em seu conte\u00fado visual.</p> <ul> <li>Caltech 101: Um conjunto de dados contendo imagens de 101 categorias de objetos para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>Caltech 256: Uma vers\u00e3o estendida do Caltech 101 com 256 categorias de objetos e imagens mais desafiadoras.</li> <li>CIFAR-10: Um conjunto de dados de 60 mil imagens coloridas de 32x32 em 10 classes, com 6 mil imagens por classe.</li> <li>CIFAR-100: Uma vers\u00e3o estendida do CIFAR-10 com 100 categorias de objetos e 600 imagens por classe.</li> <li>Fashion-MNIST: Um conjunto de dados consistindo de 70 mil imagens em escala de cinza de 10 categorias de moda para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>ImageNet: Um conjunto de dados em grande escala para detec\u00e7\u00e3o de objetos e classifica\u00e7\u00e3o de imagens com mais de 14 milh\u00f5es de imagens e 20 mil categorias.</li> <li>ImageNet-10: Um subconjunto menor do ImageNet com 10 categorias para experimenta\u00e7\u00e3o e teste mais r\u00e1pidos.</li> <li>Imagenette: Um subconjunto menor do ImageNet que cont\u00e9m 10 classes facilmente distingu\u00edveis para treinamento e teste mais r\u00e1pidos.</li> <li>Imagewoof: Um subconjunto do ImageNet mais desafiador contendo 10 categorias de ra\u00e7as de c\u00e3es para tarefas de classifica\u00e7\u00e3o de imagens.</li> <li>MNIST: Um conjunto de dados de 70 mil imagens em escala de cinza de d\u00edgitos manuscritos para tarefas de classifica\u00e7\u00e3o de imagens.</li> </ul>"},{"location":"datasets/#caixas-delimitadoras-orientadas-obb","title":"Caixas Delimitadoras Orientadas (OBB)","text":"<p>As Caixas Delimitadoras Orientadas (OBB) \u00e9 um m\u00e9todo em vis\u00e3o computacional para detectar objetos angulados em imagens usando caixas delimitadoras rotacionadas, muitas vezes aplicado em imagens a\u00e9reas e de sat\u00e9lite.</p> <ul> <li>DOTAv2: Um popular conjunto de dados de imagens a\u00e9reas OBB com 1,7 milh\u00e3o de inst\u00e2ncias e 11.268 imagens.</li> </ul>"},{"location":"datasets/#rastreamento-de-multiplos-objetos","title":"Rastreamento de M\u00faltiplos Objetos","text":"<p>O rastreamento de m\u00faltiplos objetos \u00e9 uma t\u00e9cnica de vis\u00e3o computacional que envolve detectar e rastrear v\u00e1rios objetos ao longo do tempo em uma sequ\u00eancia de v\u00eddeo.</p> <ul> <li>Argoverse: Um conjunto de dados contendo dados de rastreamento 3D e previs\u00e3o de movimento de ambientes urbanos com anota\u00e7\u00f5es ricas para tarefas de rastreamento de m\u00faltiplos objetos.</li> <li>VisDrone: Um conjunto de dados que cont\u00e9m informa\u00e7\u00e3o de detec\u00e7\u00e3o de objetos e rastreamento de m\u00faltiplos objetos a partir de imagens capturadas por drones com mais de 10 mil imagens e sequ\u00eancias de v\u00eddeo.</li> </ul>"},{"location":"datasets/#contribuir-com-novos-conjuntos-de-dados","title":"Contribuir com Novos Conjuntos de Dados","text":"<p>Contribuir com um novo conjunto de dados envolve v\u00e1rias etapas para garantir que ele se alinhe bem com a infraestrutura existente. Abaixo est\u00e3o as etapas necess\u00e1rias:</p>"},{"location":"datasets/#etapas-para-contribuir-com-um-novo-conjunto-de-dados","title":"Etapas para Contribuir com um Novo Conjunto de Dados","text":"<ol> <li> <p>Coletar Imagens: Re\u00fana as imagens que pertencem ao conjunto de dados. Estas podem ser coletadas de v\u00e1rias fontes, como bancos de dados p\u00fablicos ou sua pr\u00f3pria cole\u00e7\u00e3o.</p> </li> <li> <p>Anotar Imagens: Anote essas imagens com caixas delimitadoras, segmentos ou pontos-chave, dependendo da tarefa.</p> </li> <li> <p>Exportar Anota\u00e7\u00f5es: Converta essas anota\u00e7\u00f5es no formato de arquivo *.txt YOLO que a Ultralytics suporta.</p> </li> <li> <p>Organizar Conjunto de Dados: Organize seu conjunto de dados na estrutura de pastas correta. Voc\u00ea deve ter diret\u00f3rios de topo <code>train/</code> e <code>val/</code>, e dentro de cada um, um subdiret\u00f3rio <code>images/</code> e <code>labels/</code>.</p> <pre><code>conjunto_de_dados/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Criar um Arquivo <code>data.yaml</code>: No diret\u00f3rio raiz do seu conjunto de dados, crie um arquivo <code>data.yaml</code> que descreva o conjunto de dados, as classes e outras informa\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>Otimizar Imagens (Opcional): Se voc\u00ea quiser reduzir o tamanho do conjunto de dados para um processamento mais eficiente, pode otimizar as imagens usando o c\u00f3digo abaixo. Isso n\u00e3o \u00e9 obrigat\u00f3rio, mas recomendado para tamanhos menores de conjunto de dados e velocidades de download mais r\u00e1pidas.</p> </li> <li> <p>Compactar Conjunto de Dados: Compacte toda a pasta do conjunto de dados em um arquivo zip.</p> </li> <li> <p>Documentar e PR: Crie uma p\u00e1gina de documenta\u00e7\u00e3o descrevendo seu conjunto de dados e como ele se encaixa no framework existente. Depois disso, submeta um Pull Request (PR). Consulte Diretrizes de Contribui\u00e7\u00e3o da Ultralytics para mais detalhes sobre como submeter um PR.</p> </li> </ol>"},{"location":"datasets/#exemplo-de-codigo-para-otimizar-e-compactar-um-conjunto-de-dados","title":"Exemplo de C\u00f3digo para Otimizar e Compactar um Conjunto de Dados","text":"<p>Otimizar e Compactar um Conjunto de Dados</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definir diret\u00f3rio do conjunto de dados\npath = Path('caminho/para/conjunto_de_dados')\n\n# Otimizar imagens no conjunto de dados (opcional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Compactar conjunto de dados em 'caminho/para/conjunto_de_dados.zip'\nzip_directory(path)\n</code></pre> <p>Seguindo esses passos, voc\u00ea poder\u00e1 contribuir com um novo conjunto de dados que se integra bem com a estrutura existente da Ultralytics.</p>"},{"location":"models/","title":"Modelos Suportados pela Ultralytics","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o de modelos da Ultralytics! Oferecemos suporte para uma ampla gama de modelos, cada um adaptado para tarefas espec\u00edficas como detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o de inst\u00e2ncias, classifica\u00e7\u00e3o de imagens, estimativa de pose e rastreamento de m\u00faltiplos objetos. Se voc\u00ea est\u00e1 interessado em contribuir com sua arquitetura de modelo para a Ultralytics, confira nosso Guia de Contribui\u00e7\u00e3o.</p> <p>Note</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o multil\u00edngue est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando duro para melhor\u00e1-la. Obrigado pela sua paci\u00eancia! \ud83d\ude4f</p>"},{"location":"models/#modelos-em-destaque","title":"Modelos em Destaque","text":"<p>Aqui est\u00e3o alguns dos principais modelos suportados:</p> <ol> <li>YOLOv3: A terceira itera\u00e7\u00e3o da fam\u00edlia de modelos YOLO, originalmente por Joseph Redmon, conhecida por suas capacidades eficientes de detec\u00e7\u00e3o de objetos em tempo real.</li> <li>YOLOv4: Uma atualiza\u00e7\u00e3o nativa do darknet para o YOLOv3, lan\u00e7ada por Alexey Bochkovskiy em 2020.</li> <li>YOLOv5: Uma vers\u00e3o aprimorada da arquitetura YOLO pela Ultralytics, oferecendo melhores trade-offs de desempenho e velocidade comparado \u00e0s vers\u00f5es anteriores.</li> <li>YOLOv6: Lan\u00e7ado pela Meituan em 2022, e em uso em muitos dos rob\u00f4s aut\u00f4nomos de entrega da empresa.</li> <li>YOLOv7: Modelos YOLO atualizados lan\u00e7ados em 2022 pelos autores do YOLOv4.</li> <li>YOLOv8: A vers\u00e3o mais recente da fam\u00edlia YOLO, com capacidades aprimoradas como segmenta\u00e7\u00e3o de inst\u00e2ncias, estimativa de pose/pontos-chave e classifica\u00e7\u00e3o.</li> <li>Segment Anything Model (SAM): Modelo de Segment Everything (SAM) do Meta.</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM para aplica\u00e7\u00f5es m\u00f3veis, pela Universidade Kyung Hee.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM pelo Grupo de An\u00e1lise de Imagem e V\u00eddeo, Instituto de Automa\u00e7\u00e3o, Academia Chinesa de Ci\u00eancias.</li> <li>YOLO-NAS: Modelos YOLO de Pesquisa de Arquitetura Neural (NAS).</li> <li>Realtime Detection Transformers (RT-DETR): Modelos do Transformer de Detec\u00e7\u00e3o em Tempo Real (RT-DETR) da PaddlePaddle da Baidu.</li> </ol> <p> Assista: Execute modelos YOLO da Ultralytics em apenas algumas linhas de c\u00f3digo. </p>"},{"location":"models/#comecando-exemplos-de-uso","title":"Come\u00e7ando: Exemplos de Uso","text":"<p>Exemplo</p> PythonCLI <p>Modelos <code>*.pt</code> pr\u00e9-treinados com PyTorch, bem como arquivos de configura\u00e7\u00e3o <code>*.yaml</code>, podem ser passados para as classes <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> e <code>RTDETR()</code> para criar uma inst\u00e2ncia de modelo em Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO\nmodel = YOLO('yolov8n.pt')\n\n# Exibir informa\u00e7\u00f5es do modelo (opcional)\nmodel.info()\n\n# Treinar o modelo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Executar infer\u00eancia com o modelo YOLOv8n na imagem 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Comandos CLI est\u00e3o dispon\u00edveis para executar diretamente os modelos:</p> <pre><code># Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO e trein\u00e1-lo no conjunto de dados de exemplo COCO8 por 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carregar um modelo YOLOv8n pr\u00e9-treinado no COCO e executar infer\u00eancia na imagem 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribuindo-com-novos-modelos","title":"Contribuindo com Novos Modelos","text":"<p>Interessado em contribuir com o seu modelo para a Ultralytics? \u00d3timo! Estamos sempre abertos \u00e0 expans\u00e3o de nosso portf\u00f3lio de modelos.</p> <ol> <li> <p>Fork no Reposit\u00f3rio: Comece fazendo um fork do reposit\u00f3rio GitHub da Ultralytics.</p> </li> <li> <p>Clone o Seu Fork: Clone o seu fork para a sua m\u00e1quina local e crie uma nova branch para trabalhar.</p> </li> <li> <p>Implemente Seu Modelo: Adicione o seu modelo seguindo os padr\u00f5es de codifica\u00e7\u00e3o e diretrizes fornecidos em nosso Guia de Contribui\u00e7\u00e3o.</p> </li> <li> <p>Teste Completamente: Certifique-se de testar seu modelo rigorosamente, isoladamente e como parte do pipeline.</p> </li> <li> <p>Crie um Pull Request: Uma vez que esteja satisfeito com seu modelo, crie um pull request para o reposit\u00f3rio principal para revis\u00e3o.</p> </li> <li> <p>Revis\u00e3o de C\u00f3digo &amp; Merge: Ap\u00f3s a revis\u00e3o, se o seu modelo atender os nossos crit\u00e9rios, ele ser\u00e1 combinado com o reposit\u00f3rio principal.</p> </li> </ol> <p>Para etapas detalhadas, consulte nosso Guia de Contribui\u00e7\u00e3o.</p>"},{"location":"modes/","title":"Modos Ultralytics YOLOv8","text":""},{"location":"modes/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O Ultralytics YOLOv8 n\u00e3o \u00e9 apenas mais um modelo de detec\u00e7\u00e3o de objetos; \u00e9 um framework vers\u00e1til projetado para cobrir todo o ciclo de vida dos modelos de aprendizado de m\u00e1quina \u2014 desde a ingest\u00e3o de dados e treinamento do modelo at\u00e9 a valida\u00e7\u00e3o, implanta\u00e7\u00e3o e rastreamento no mundo real. Cada modo serve a um prop\u00f3sito espec\u00edfico e \u00e9 projetado para oferecer a flexibilidade e efici\u00eancia necess\u00e1rias para diferentes tarefas e casos de uso.</p> <p> Assista: Tutorial dos Modos Ultralytics: Treinar, Validar, Prever, Exportar e Benchmark. </p>"},{"location":"modes/#visao-geral-dos-modos","title":"Vis\u00e3o Geral dos Modos","text":"<p>Entender os diferentes modos que o Ultralytics YOLOv8 suporta \u00e9 cr\u00edtico para tirar o m\u00e1ximo proveito de seus modelos:</p> <ul> <li>Modo Treino: Ajuste fino do seu modelo em conjuntos de dados personalizados ou pr\u00e9-carregados.</li> <li>Modo Valida\u00e7\u00e3o (Val): Um checkpoint p\u00f3s-treinamento para validar o desempenho do modelo.</li> <li>Modo Predi\u00e7\u00e3o (Predict): Libere o poder preditivo do seu modelo em dados do mundo real.</li> <li>Modo Exporta\u00e7\u00e3o (Export): Prepare seu modelo para implanta\u00e7\u00e3o em v\u00e1rios formatos.</li> <li>Modo Rastreamento (Track): Estenda seu modelo de detec\u00e7\u00e3o de objetos para aplica\u00e7\u00f5es de rastreamento em tempo real.</li> <li>Modo Benchmarking: Analise a velocidade e precis\u00e3o do seu modelo em diversos ambientes de implanta\u00e7\u00e3o.</li> </ul> <p>Este guia abrangente visa fornecer uma vis\u00e3o geral e insights pr\u00e1ticos para cada modo, ajudando voc\u00ea a aproveitar o potencial total do YOLOv8.</p>"},{"location":"modes/#treinar","title":"Treinar","text":"<p>O modo Treinar \u00e9 utilizado para treinar um modelo YOLOv8 em um conjunto de dados personalizado. Neste modo, o modelo \u00e9 treinado usando o conjunto de dados especificado e os hiperpar\u00e2metros escolhidos. O processo de treinamento envolve otimizar os par\u00e2metros do modelo para que ele possa prever com precis\u00e3o as classes e localiza\u00e7\u00f5es de objetos em uma imagem.</p> <p>Exemplos de Treino</p>"},{"location":"modes/#validar","title":"Validar","text":"<p>O modo Validar \u00e9 utilizado para validar um modelo YOLOv8 ap\u00f3s ter sido treinado. Neste modo, o modelo \u00e9 avaliado em um conjunto de valida\u00e7\u00e3o para medir sua precis\u00e3o e desempenho de generaliza\u00e7\u00e3o. Este modo pode ser usado para ajustar os hiperpar\u00e2metros do modelo para melhorar seu desempenho.</p> <p>Exemplos de Valida\u00e7\u00e3o</p>"},{"location":"modes/#prever","title":"Prever","text":"<p>O modo Prever \u00e9 utilizado para fazer previs\u00f5es usando um modelo YOLOv8 treinado em novas imagens ou v\u00eddeos. Neste modo, o modelo \u00e9 carregado de um arquivo de checkpoint, e o usu\u00e1rio pode fornecer imagens ou v\u00eddeos para realizar a infer\u00eancia. O modelo prev\u00ea as classes e localiza\u00e7\u00f5es dos objetos nas imagens ou v\u00eddeos fornecidos.</p> <p>Exemplos de Predi\u00e7\u00e3o</p>"},{"location":"modes/#exportar","title":"Exportar","text":"<p>O modo Exportar \u00e9 utilizado para exportar um modelo YOLOv8 para um formato que possa ser utilizado para implanta\u00e7\u00e3o. Neste modo, o modelo \u00e9 convertido para um formato que possa ser utilizado por outras aplica\u00e7\u00f5es de software ou dispositivos de hardware. Este modo \u00e9 \u00fatil ao implantar o modelo em ambientes de produ\u00e7\u00e3o.</p> <p>Exemplos de Exporta\u00e7\u00e3o</p>"},{"location":"modes/#rastrear","title":"Rastrear","text":"<p>O modo Rastrear \u00e9 utilizado para rastrear objetos em tempo real usando um modelo YOLOv8. Neste modo, o modelo \u00e9 carregado de um arquivo de checkpoint, e o usu\u00e1rio pode fornecer um fluxo de v\u00eddeo ao vivo para realizar o rastreamento de objetos em tempo real. Este modo \u00e9 \u00fatil para aplica\u00e7\u00f5es como sistemas de vigil\u00e2ncia ou carros aut\u00f4nomos.</p> <p>Exemplos de Rastreamento</p>"},{"location":"modes/#benchmark","title":"Benchmark","text":"<p>O modo Benchmark \u00e9 utilizado para fazer um perfil da velocidade e precis\u00e3o de v\u00e1rios formatos de exporta\u00e7\u00e3o para o YOLOv8. Os benchmarks fornecem informa\u00e7\u00f5es sobre o tamanho do formato exportado, suas m\u00e9tricas <code>mAP50-95</code> (para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e pose) ou <code>accuracy_top5</code> (para classifica\u00e7\u00e3o), e o tempo de infer\u00eancia em milissegundos por imagem em diversos formatos de exporta\u00e7\u00e3o, como ONNX, OpenVINO, TensorRT e outros. Essas informa\u00e7\u00f5es podem ajudar os usu\u00e1rios a escolher o formato de exporta\u00e7\u00e3o \u00f3timo para seu caso de uso espec\u00edfico, com base em seus requisitos de velocidade e precis\u00e3o.</p> <p>Exemplos de Benchmark</p>"},{"location":"modes/benchmark/","title":"Benchmarking de Modelos com o Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Uma vez que seu modelo esteja treinado e validado, o pr\u00f3ximo passo l\u00f3gico \u00e9 avaliar seu desempenho em diversos cen\u00e1rios do mundo real. O modo de benchmark no Ultralytics YOLOv8 serve a esse prop\u00f3sito, oferecendo uma estrutura robusta para avaliar a velocidade e a precis\u00e3o do seu modelo em uma gama de formatos de exporta\u00e7\u00e3o.</p>"},{"location":"modes/benchmark/#por-que-o-benchmarking-e-crucial","title":"Por Que o Benchmarking \u00e9 Crucial?","text":"<ul> <li>Decis\u00f5es Informadas: Obtenha insights sobre o equil\u00edbrio entre velocidade e precis\u00e3o.</li> <li>Aloca\u00e7\u00e3o de Recursos: Entenda como diferentes formatos de exporta\u00e7\u00e3o se comportam em diferentes hardwares.</li> <li>Otimiza\u00e7\u00e3o: Aprenda qual formato de exporta\u00e7\u00e3o oferece o melhor desempenho para o seu caso espec\u00edfico.</li> <li>Efici\u00eancia de Custos: Fa\u00e7a uso mais eficiente dos recursos de hardware com base nos resultados do benchmark.</li> </ul>"},{"location":"modes/benchmark/#metricas-chave-no-modo-de-benchmark","title":"M\u00e9tricas Chave no Modo de Benchmark","text":"<ul> <li>mAP50-95: Para detec\u00e7\u00e3o de objetos, segmenta\u00e7\u00e3o e estimativa de pose.</li> <li>accuracy_top5: Para classifica\u00e7\u00e3o de imagens.</li> <li>Tempo de Infer\u00eancia: Tempo levado para cada imagem em milissegundos.</li> </ul>"},{"location":"modes/benchmark/#formatos-de-exportacao-suportados","title":"Formatos de Exporta\u00e7\u00e3o Suportados","text":"<ul> <li>ONNX: Para desempenho \u00f3timo em CPU</li> <li>TensorRT: Para efici\u00eancia m\u00e1xima em GPU</li> <li>OpenVINO: Para otimiza\u00e7\u00e3o em hardware Intel</li> <li>CoreML, TensorFlow SavedModel e Mais: Para uma variedade de necessidades de implanta\u00e7\u00e3o.</li> </ul> <p>Dica</p> <ul> <li>Exporte para ONNX ou OpenVINO para acelerar at\u00e9 3x a velocidade em CPU.</li> <li>Exporte para TensorRT para acelerar at\u00e9 5x em GPU.</li> </ul>"},{"location":"modes/benchmark/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Execute benchmarks do YOLOv8n em todos os formatos de exporta\u00e7\u00e3o suportados incluindo ONNX, TensorRT etc. Consulte a se\u00e7\u00e3o Argumentos abaixo para ver uma lista completa de argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark na GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumentos","title":"Argumentos","text":"<p>Argumentos como <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> e <code>verbose</code> proporcionam aos usu\u00e1rios flexibilidade para ajustar os benchmarks \u00e0s suas necessidades espec\u00edficas e comparar o desempenho de diferentes formatos de exporta\u00e7\u00e3o com facilidade.</p> Chave Valor Descri\u00e7\u00e3o <code>model</code> <code>None</code> caminho para o arquivo do modelo, ou seja, yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> caminho para o YAML com dataset de benchmarking (sob o r\u00f3tulo <code>val</code>) <code>imgsz</code> <code>640</code> tamanho da imagem como um escalar ou lista (h, w), ou seja, (640, 480) <code>half</code> <code>False</code> quantiza\u00e7\u00e3o FP16 <code>int8</code> <code>False</code> quantiza\u00e7\u00e3o INT8 <code>device</code> <code>None</code> dispositivo para execu\u00e7\u00e3o, ou seja, dispositivo cuda=0 ou device=0,1,2,3 ou device=cpu <code>verbose</code> <code>False</code> n\u00e3o continuar em erro (bool), ou limiar m\u00ednimo para val (float)"},{"location":"modes/benchmark/#formatos-de-exportacao","title":"Formatos de Exporta\u00e7\u00e3o","text":"<p>Os benchmarks tentar\u00e3o executar automaticamente em todos os poss\u00edveis formatos de exporta\u00e7\u00e3o listados abaixo.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> Modelo Salvo do TF <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> GraphDef do TF <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja os detalhes completos de <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Export.</p>"},{"location":"modes/export/","title":"Exporta\u00e7\u00e3o de Modelo com Ultralytics YOLO","text":""},{"location":"modes/export/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O objetivo final de treinar um modelo \u00e9 implant\u00e1-lo para aplica\u00e7\u00f5es no mundo real. O modo de exporta\u00e7\u00e3o no Ultralytics YOLOv8 oferece uma ampla gama de op\u00e7\u00f5es para exportar seu modelo treinado para diferentes formatos, tornando-o implant\u00e1vel em v\u00e1rias plataformas e dispositivos. Este guia abrangente visa orient\u00e1-lo atrav\u00e9s das nuances da exporta\u00e7\u00e3o de modelos, mostrando como alcan\u00e7ar a m\u00e1xima compatibilidade e performance.</p> <p> Assista: Como Exportar Modelo Treinado Customizado do Ultralytics YOLOv8 e Executar Infer\u00eancia ao Vivo na Webcam. </p>"},{"location":"modes/export/#por-que-escolher-o-modo-de-exportacao-do-yolov8","title":"Por Que Escolher o Modo de Exporta\u00e7\u00e3o do YOLOv8?","text":"<ul> <li>Versatilidade: Exporte para m\u00faltiplos formatos incluindo ONNX, TensorRT, CoreML e mais.</li> <li>Performance: Ganhe at\u00e9 5x acelera\u00e7\u00e3o em GPU com TensorRT e 3x acelera\u00e7\u00e3o em CPU com ONNX ou OpenVINO.</li> <li>Compatibilidade: Torne seu modelo universalmente implant\u00e1vel em numerosos ambientes de hardware e software.</li> <li>Facilidade de Uso: Interface de linha de comando simples e API Python para exporta\u00e7\u00e3o r\u00e1pida e direta de modelos.</li> </ul>"},{"location":"modes/export/#principais-recursos-do-modo-de-exportacao","title":"Principais Recursos do Modo de Exporta\u00e7\u00e3o","text":"<p>Aqui est\u00e3o algumas das funcionalidades de destaque:</p> <ul> <li>Exporta\u00e7\u00e3o com Um Clique: Comandos simples para exporta\u00e7\u00e3o em diferentes formatos.</li> <li>Exporta\u00e7\u00e3o em Lote: Exporte modelos capazes de infer\u00eancia em lote.</li> <li>Infer\u00eancia Otimizada: Modelos exportados s\u00e3o otimizados para tempos de infer\u00eancia mais r\u00e1pidos.</li> <li>V\u00eddeos Tutoriais: Guias e tutoriais detalhados para uma experi\u00eancia de exporta\u00e7\u00e3o tranquila.</li> </ul> <p>Dica</p> <ul> <li>Exporte para ONNX ou OpenVINO para at\u00e9 3x acelera\u00e7\u00e3o em CPU.</li> <li>Exporte para TensorRT para at\u00e9 5x acelera\u00e7\u00e3o em GPU.</li> </ul>"},{"location":"modes/export/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Exporte um modelo YOLOv8n para um formato diferente como ONNX ou TensorRT. Veja a se\u00e7\u00e3o de Argumentos abaixo para uma lista completa dos argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carrega um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carrega um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporta modelo oficial\nyolo export model=caminho/para/best.pt format=onnx  # exporta modelo treinado personalizado\n</code></pre>"},{"location":"modes/export/#argumentos","title":"Argumentos","text":"<p>Configura\u00e7\u00f5es de exporta\u00e7\u00e3o para modelos YOLO referem-se \u00e0s v\u00e1rias configura\u00e7\u00f5es e op\u00e7\u00f5es usadas para salvar ou exportar o modelo para uso em outros ambientes ou plataformas. Essas configura\u00e7\u00f5es podem afetar a performance, tamanho e compatibilidade do modelo com diferentes sistemas. Algumas configura\u00e7\u00f5es comuns de exporta\u00e7\u00e3o de YOLO incluem o formato do arquivo de modelo exportado (por exemplo, ONNX, TensorFlow SavedModel), o dispositivo em que o modelo ser\u00e1 executado (por exemplo, CPU, GPU) e a presen\u00e7a de recursos adicionais como m\u00e1scaras ou m\u00faltiplos r\u00f3tulos por caixa. Outros fatores que podem afetar o processo de exporta\u00e7\u00e3o incluem a tarefa espec\u00edfica para a qual o modelo est\u00e1 sendo usado e os requisitos ou restri\u00e7\u00f5es do ambiente ou plataforma alvo. \u00c9 importante considerar e configurar cuidadosamente essas configura\u00e7\u00f5es para garantir que o modelo exportado seja otimizado para o caso de uso pretendido e possa ser usado eficazmente no ambiente alvo.</p> Chave Valor Descri\u00e7\u00e3o <code>format</code> <code>'torchscript'</code> formato para exporta\u00e7\u00e3o <code>imgsz</code> <code>640</code> tamanho da imagem como escalar ou lista (h, w), ou seja, (640, 480) <code>keras</code> <code>False</code> usar Keras para exporta\u00e7\u00e3o TF SavedModel <code>optimize</code> <code>False</code> TorchScript: otimizar para mobile <code>half</code> <code>False</code> quantiza\u00e7\u00e3o FP16 <code>int8</code> <code>False</code> quantiza\u00e7\u00e3o INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT: eixos din\u00e2micos <code>simplify</code> <code>False</code> ONNX/TensorRT: simplificar modelo <code>opset</code> <code>None</code> ONNX: vers\u00e3o do opset (opcional, padr\u00e3o para a mais recente) <code>workspace</code> <code>4</code> TensorRT: tamanho do espa\u00e7o de trabalho (GB) <code>nms</code> <code>False</code> CoreML: adicionar NMS"},{"location":"modes/export/#formatos-de-exportacao","title":"Formatos de Exporta\u00e7\u00e3o","text":"<p>Os formatos de exporta\u00e7\u00e3o dispon\u00edveis para YOLOv8 est\u00e3o na tabela abaixo. Voc\u00ea pode exportar para qualquer formato usando o argumento <code>format</code>, ou seja, <code>format='onnx'</code> ou <code>format='engine'</code>.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Predi\u00e7\u00e3o de Modelo com Ultralytics YOLO","text":""},{"location":"modes/predict/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>No mundo do aprendizado de m\u00e1quina e vis\u00e3o computacional, o processo de fazer sentido a partir de dados visuais \u00e9 chamado de 'infer\u00eancia' ou 'predi\u00e7\u00e3o'. O Ultralytics YOLOv8 oferece um recurso poderoso conhecido como modo predict que \u00e9 personalizado para infer\u00eancia em tempo real de alto desempenho em uma ampla gama de fontes de dados.</p> <p> Assista: Como Extrair as Sa\u00eddas do Modelo Ultralytics YOLOv8 para Projetos Personalizados. </p>"},{"location":"modes/predict/#aplicacoes-no-mundo-real","title":"Aplica\u00e7\u00f5es no Mundo Real","text":"Manufatura Esportes Seguran\u00e7a Detec\u00e7\u00e3o de Pe\u00e7as de Reposi\u00e7\u00e3o de Ve\u00edculo Detec\u00e7\u00e3o de Jogador de Futebol Detec\u00e7\u00e3o de Queda de Pessoas"},{"location":"modes/predict/#por-que-usar-o-ultralytics-yolo-para-inferencia","title":"Por Que Usar o Ultralytics YOLO para Infer\u00eancia?","text":"<p>Aqui est\u00e1 o porqu\u00ea de voc\u00ea considerar o modo predict do YOLOv8 para suas diversas necessidades de infer\u00eancia:</p> <ul> <li>Versatilidade: Capaz de fazer infer\u00eancias em imagens, v\u00eddeos e at\u00e9 transmiss\u00f5es ao vivo.</li> <li>Desempenho: Projetado para processamento em tempo real e de alta velocidade sem sacrificar a precis\u00e3o.</li> <li>Facilidade de Uso: Interfaces Python e CLI intuitivas para implanta\u00e7\u00e3o e testes r\u00e1pidos.</li> <li>Altamente Customiz\u00e1vel: V\u00e1rias configura\u00e7\u00f5es e par\u00e2metros para ajustar o comportamento de infer\u00eancia do modelo de acordo com suas necessidades espec\u00edficas.</li> </ul>"},{"location":"modes/predict/#recursos-chave-do-modo-predict","title":"Recursos Chave do Modo Predict","text":"<p>O modo predict do YOLOv8 \u00e9 projetado para ser robusto e vers\u00e1til, apresentando:</p> <ul> <li>Compatibilidade com M\u00faltiplas Fontes de Dados: Se seus dados est\u00e3o na forma de imagens individuais, uma cole\u00e7\u00e3o de imagens, arquivos de v\u00eddeo ou transmiss\u00f5es de v\u00eddeo em tempo real, o modo predict atende a todas as necessidades.</li> <li>Modo de Streaming: Use o recurso de streaming para gerar um gerador eficiente de mem\u00f3ria de objetos <code>Results</code>. Ative isso definindo <code>stream=True</code> no m\u00e9todo de chamada do preditor.</li> <li>Processamento em Lote: A capacidade de processar v\u00e1rias imagens ou quadros de v\u00eddeo em um \u00fanico lote, acelerando ainda mais o tempo de infer\u00eancia.</li> <li>Integra\u00e7\u00e3o Amig\u00e1vel: Integra\u00e7\u00e3o f\u00e1cil com pipelines de dados existentes e outros componentes de software, gra\u00e7as \u00e0 sua API flex\u00edvel.</li> </ul> <p>Os modelos Ultralytics YOLO retornam ou uma lista de objetos <code>Results</code> em Python, ou um gerador em Python eficiente de mem\u00f3ria de objetos <code>Results</code> quando <code>stream=True</code> \u00e9 passado para o modelo durante a infer\u00eancia:</p> <p>Predict</p> Retorna uma lista com <code>stream=False</code>Retorna um gerador com <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n pr\u00e9-treinado\n\n# Executa a infer\u00eancia em lote em uma lista de imagens\nresults = model(['im1.jpg', 'im2.jpg'])  # retorna uma lista de objetos Results\n\n# Processa a lista de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para sa\u00eddas de bbox\n    masks = result.masks  # Objeto Masks para sa\u00eddas de m\u00e1scaras de segmenta\u00e7\u00e3o\n    keypoints = result.keypoints  # Objeto Keypoints para sa\u00eddas de pose\n    probs = result.probs  # Objeto Probs para sa\u00eddas de classifica\u00e7\u00e3o\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n pr\u00e9-treinado\n\n# Executa a infer\u00eancia em lote em uma lista de imagens\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # retorna um gerador de objetos Results\n\n# Processa o gerador de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para sa\u00eddas de bbox\n    masks = result.masks  # Objeto Masks para sa\u00eddas de m\u00e1scaras de segmenta\u00e7\u00e3o\n    keypoints = result.keypoints  # Objeto Keypoints para sa\u00eddas de pose\n    probs = result.probs  # Objeto Probs para sa\u00eddas de classifica\u00e7\u00e3o\n</code></pre>"},{"location":"modes/predict/#fontes-de-inferencia","title":"Fontes de Infer\u00eancia","text":"<p>O YOLOv8 pode processar diferentes tipos de fontes de entrada para infer\u00eancia, conforme mostrado na tabela abaixo. As fontes incluem imagens est\u00e1ticas, transmiss\u00f5es de v\u00eddeo e v\u00e1rios formatos de dados. A tabela tamb\u00e9m indica se cada fonte pode ser usada no modo de streaming com o argumento <code>stream=True</code> \u2705. O modo de streaming \u00e9 ben\u00e9fico para processar v\u00eddeos ou transmiss\u00f5es ao vivo, pois cria um gerador de resultados em vez de carregar todos os quadros na mem\u00f3ria.</p> <p>Dica</p> <p>Use <code>stream=True</code> para processar v\u00eddeos longos ou grandes conjuntos de dados para gerenciar a mem\u00f3ria de forma eficiente. Quando <code>stream=False</code>, os resultados de todos os quadros ou pontos de dados s\u00e3o armazenados na mem\u00f3ria, o que pode aumentar rapidamente e causar erros de falta de mem\u00f3ria para grandes entradas. Em contraste, <code>stream=True</code> utiliza um gerador, que mant\u00e9m apenas os resultados do quadro atual ou ponto de dados na mem\u00f3ria, reduzindo significativamente o consumo de mem\u00f3ria e prevenindo problemas de falta dela.</p> Fonte Argumento Tipo Notas imagem <code>'image.jpg'</code> <code>str</code> ou <code>Path</code> Arquivo de imagem \u00fanico. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL para uma imagem. captura de tela <code>'screen'</code> <code>str</code> Captura uma captura de tela. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Formato HWC com canais RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Formato HWC com canais BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Formato HWC com canais BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Formato BCHW com canais RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> ou <code>Path</code> Arquivo CSV contendo caminhos para imagens, v\u00eddeos ou diret\u00f3rios. v\u00eddeo \u2705 <code>'video.mp4'</code> <code>str</code> ou <code>Path</code> Arquivo de v\u00eddeo em formatos como MP4, AVI, etc. diret\u00f3rio \u2705 <code>'path/'</code> <code>str</code> ou <code>Path</code> Caminho para um diret\u00f3rio contendo imagens ou v\u00eddeos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Padr\u00e3o glob para combinar v\u00e1rios arquivos. Use o caractere <code>*</code> como curinga. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL para um v\u00eddeo do YouTube. stream \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL para protocolos de streaming como RTSP, RTMP, TCP ou um endere\u00e7o IP. multi-stream \u2705 <code>'list.streams'</code> <code>str</code> ou <code>Path</code> Arquivo de texto <code>*.streams</code> com uma URL de stream por linha, ou seja, 8 streams ser\u00e3o executados em lote de tamanho 8. <p>Abaixo est\u00e3o exemplos de c\u00f3digo para usar cada tipo de fonte:</p> <p>Fontes de previs\u00e3o</p> imagemcaptura de telaURLPILOpenCVnumpytorch <p>Executa a infer\u00eancia em um arquivo de imagem. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define o caminho para o arquivo de imagem\nsource = 'caminho/para/imagem.jpg'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia no conte\u00fado atual da tela como uma captura de tela. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define a captura de tela atual como fonte\nsource = 'screen'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem ou v\u00eddeo hospedado remotamente via URL. <pre><code>from ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Define a URL remota da imagem ou v\u00eddeo\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem aberta com a Biblioteca de Imagens do Python (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Abre uma imagem usando PIL\nsource = Image.open('caminho/para/imagem.jpg')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem lida com OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# L\u00ea uma imagem usando OpenCV\nsource = cv2.imread('caminho/para/imagem.jpg')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem representada como um array numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Carrega um modelo YOLOv8n pr\u00e9-treinado\nmodel = YOLO('yolov8n.pt')\n\n# Cria um array random de numpy com forma HWC (640, 640, 3) com valores no intervalo [0, 255] e tipo uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Executa a infer\u00eancia na fonte\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Executa a infer\u00eancia em uma imagem representada como um tensor PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#carrega-um-modelo-yolov8n-pre-treinado","title":"Carrega um modelo YOLOv8n pr\u00e9-treinado","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#cria-um-tensor-random-de-torch-com-forma-bchw-1-3-640-640-com-valores-no-intervalo-0-1-e-tipo-float32","title":"Cria um tensor random de torch com forma BCHW (1, 3, 640, 640) com valores no intervalo [0, 1] e tipo float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#executa-a-inferencia-na-fonte","title":"Executa a infer\u00eancia na fonte","text":"<p>results = model(source)  # lista de objetos Results</p>"},{"location":"modes/track/","title":"Rastreamento de M\u00faltiplos Objetos com Ultralytics YOLO","text":"<p>Rastreamento de objetos no \u00e2mbito da an\u00e1lise de v\u00eddeo \u00e9 uma tarefa crucial que n\u00e3o apenas identifica a localiza\u00e7\u00e3o e classe dos objetos dentro do quadro, mas tamb\u00e9m mant\u00e9m um ID \u00fanico para cada objeto detectado \u00e0 medida que o v\u00eddeo avan\u00e7a. As aplica\u00e7\u00f5es s\u00e3o ilimitadas \u2014 variando desde vigil\u00e2ncia e seguran\u00e7a at\u00e9 an\u00e1lises esportivas em tempo real.</p>"},{"location":"modes/track/#por-que-escolher-ultralytics-yolo-para-rastreamento-de-objetos","title":"Por Que Escolher Ultralytics YOLO para Rastreamento de Objetos?","text":"<p>A sa\u00edda dos rastreadores da Ultralytics \u00e9 consistente com a detec\u00e7\u00e3o de objetos padr\u00e3o, mas com o valor agregado dos IDs dos objetos. Isso facilita o rastreamento de objetos em fluxos de v\u00eddeo e a realiza\u00e7\u00e3o de an\u00e1lises subsequentes. Aqui est\u00e1 o porqu\u00ea de considerar usar Ultralytics YOLO para suas necessidades de rastreamento de objetos:</p> <ul> <li>Efici\u00eancia: Processa fluxos de v\u00eddeo em tempo real sem comprometer a precis\u00e3o.</li> <li>Flexibilidade: Suporta m\u00faltiplos algoritmos de rastreamento e configura\u00e7\u00f5es.</li> <li>Facilidade de Uso: Simples API em Python e op\u00e7\u00f5es CLI para r\u00e1pida integra\u00e7\u00e3o e implanta\u00e7\u00e3o.</li> <li>Personaliza\u00e7\u00e3o: F\u00e1cil de usar com modelos YOLO treinados personalizados, permitindo integra\u00e7\u00e3o em aplica\u00e7\u00f5es espec\u00edficas de dom\u00ednio.</li> </ul> <p> Assistir: Detec\u00e7\u00e3o e Rastreamento de Objetos com Ultralytics YOLOv8. </p>"},{"location":"modes/track/#aplicacoes-no-mundo-real","title":"Aplica\u00e7\u00f5es no Mundo Real","text":"Transporte Varejo Aquicultura Rastreamento de Ve\u00edculos Rastreamento de Pessoas Rastreamento de Peixes"},{"location":"modes/track/#caracteristicas-em-destaque","title":"Caracter\u00edsticas em Destaque","text":"<p>Ultralytics YOLO estende suas funcionalidades de detec\u00e7\u00e3o de objetos para fornecer rastreamento de objetos robusto e vers\u00e1til:</p> <ul> <li>Rastreamento em Tempo Real: Acompanha objetos de forma cont\u00ednua em v\u00eddeos de alta taxa de quadros.</li> <li>Suporte a M\u00faltiplos Rastreadores: Escolha dentre uma variedade de algoritmos de rastreamento estabelecidos.</li> <li>Configura\u00e7\u00f5es de Rastreador Personaliz\u00e1veis: Adapte o algoritmo de rastreamento para atender requisitos espec\u00edficos ajustando v\u00e1rios par\u00e2metros.</li> </ul>"},{"location":"modes/track/#rastreadores-disponiveis","title":"Rastreadores Dispon\u00edveis","text":"<p>Ultralytics YOLO suporta os seguintes algoritmos de rastreamento. Eles podem ser ativados passando o respectivo arquivo de configura\u00e7\u00e3o YAML, como <code>tracker=tracker_type.yaml</code>:</p> <ul> <li>BoT-SORT - Use <code>botsort.yaml</code> para ativar este rastreador.</li> <li>ByteTrack - Use <code>bytetrack.yaml</code> para ativar este rastreador.</li> </ul> <p>O rastreador padr\u00e3o \u00e9 o BoT-SORT.</p>"},{"location":"modes/track/#rastreamento","title":"Rastreamento","text":"<p>Para executar o rastreador em fluxos de v\u00eddeo, use um modelo Detect, Segment ou Pose treinado, como YOLOv8n, YOLOv8n-seg e YOLOv8n-pose.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo oficial ou personalizado\nmodel = YOLO('yolov8n.pt')  # Carregar um modelo Detect oficial\nmodel = YOLO('yolov8n-seg.pt')  # Carregar um modelo Segment oficial\nmodel = YOLO('yolov8n-pose.pt')  # Carregar um modelo Pose oficial\nmodel = YOLO('caminho/para/melhor.pt')  # Carregar um modelo treinado personalizado\n\n# Realizar rastreamento com o modelo\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Rastreamento com rastreador padr\u00e3o\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Rastreamento com o rastreador ByteTrack\n</code></pre> <pre><code># Realizar rastreamento com v\u00e1rios modelos usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Detect oficial\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Segment oficial\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo Pose oficial\nyolo track model=caminho/para/melhor.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo treinado personalizado\n\n# Rastrear usando o rastreador ByteTrack\nyolo track model=caminho/para/melhor.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Como pode ser visto no uso acima, o rastreamento est\u00e1 dispon\u00edvel para todos os modelos Detect, Segment e Pose executados em v\u00eddeos ou fontes de streaming.</p>"},{"location":"modes/track/#configuracao","title":"Configura\u00e7\u00e3o","text":""},{"location":"modes/track/#argumentos-de-rastreamento","title":"Argumentos de Rastreamento","text":"<p>A configura\u00e7\u00e3o de rastreamento compartilha propriedades com o modo Predict, como <code>conf</code>, <code>iou</code>, e <code>show</code>. Para mais configura\u00e7\u00f5es, consulte a p\u00e1gina de Predict model page.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurar os par\u00e2metros de rastreamento e executar o rastreador\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurar par\u00e2metros de rastreamento e executar o rastreador usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#selecao-de-rastreador","title":"Sele\u00e7\u00e3o de Rastreador","text":"<p>A Ultralytics tamb\u00e9m permite que voc\u00ea use um arquivo de configura\u00e7\u00e3o de rastreador modificado. Para fazer isso, simplesmente fa\u00e7a uma c\u00f3pia de um arquivo de configura\u00e7\u00e3o de rastreador (por exemplo, <code>custom_tracker.yaml</code>) de ultralytics/cfg/trackers e modifique quaisquer configura\u00e7\u00f5es (exceto <code>tracker_type</code>) conforme suas necessidades.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar o modelo e executar o rastreador com um arquivo de configura\u00e7\u00e3o personalizado\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Carregar o modelo e executar o rastreador com um arquivo de configura\u00e7\u00e3o personalizado usando a interface de linha de comando\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Para uma lista completa de argumentos de rastreamento, consulte a p\u00e1gina ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#exemplos-em-python","title":"Exemplos em Python","text":""},{"location":"modes/track/#loop-de-persistencia-de-rastreamentos","title":"Loop de Persist\u00eancia de Rastreamentos","text":"<p>Aqui est\u00e1 um script em Python usando OpenCV (<code>cv2</code>) e YOLOv8 para executar rastreamento de objetos em quadros de v\u00eddeo. Este script ainda pressup\u00f5e que voc\u00ea j\u00e1 instalou os pacotes necess\u00e1rios (<code>opencv-python</code> e <code>ultralytics</code>). O argumento <code>persist=True</code> indica ao rastreador que a imagem ou quadro atual \u00e9 o pr\u00f3ximo de uma sequ\u00eancia e que espera rastreamentos da imagem anterior na imagem atual.</p> <p>Loop de fluxo com rastreamento</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Carregar o modelo YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Abrir o arquivo de v\u00eddeo\nvideo_path = \"caminho/para/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Repetir atrav\u00e9s dos quadros de v\u00eddeo\nwhile cap.isOpened():\n    # Ler um quadro do v\u00eddeo\n    success, frame = cap.read()\n\n    if success:\n        # Executar rastreamento YOLOv8 no quadro, persistindo rastreamentos entre quadros\n        results = model.track(frame, persist=True)\n\n        # Visualizar os resultados no quadro\n        annotated_frame = results[0].plot()\n\n        # Exibir o quadro anotado\n        cv2.imshow(\"Rastreamento YOLOv8\", annotated_frame)\n\n        # Interromper o loop se 'q' for pressionado\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Interromper o loop se o fim do v\u00eddeo for atingido\n        break\n\n# Liberar o objeto de captura de v\u00eddeo e fechar a janela de exibi\u00e7\u00e3o\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Note a mudan\u00e7a de <code>model(frame)</code> para <code>model.track(frame)</code>, que habilita o rastreamento de objetos ao inv\u00e9s de detec\u00e7\u00e3o simples. Este script modificado ir\u00e1 executar o rastreador em cada quadro do v\u00eddeo, visualizar os resultados e exibi-los em uma janela. O loop pode ser encerrado pressionando 'q'.</p>"},{"location":"modes/track/#contribuir-com-novos-rastreadores","title":"Contribuir com Novos Rastreadores","text":"<p>Voc\u00ea \u00e9 proficiente em rastreamento de m\u00faltiplos objetos e implementou ou adaptou com sucesso um algoritmo de rastreamento com Ultralytics YOLO? Convidamos voc\u00ea a contribuir para nossa se\u00e7\u00e3o de Rastreadores em ultralytics/cfg/trackers! Suas aplica\u00e7\u00f5es do mundo real e solu\u00e7\u00f5es podem ser inestim\u00e1veis para usu\u00e1rios trabalhando em tarefas de rastreamento.</p> <p>Ao contribuir para esta se\u00e7\u00e3o, voc\u00ea ajuda a expandir o escopo de solu\u00e7\u00f5es de rastreamento dispon\u00edveis dentro do framework Ultralytics YOLO, adicionando outra camada de funcionalidade e utilidade para a comunidade.</p> <p>Para iniciar sua contribui\u00e7\u00e3o, por favor, consulte nosso Guia de Contribui\u00e7\u00e3o para instru\u00e7\u00f5es completas sobre como enviar um Pedido de Pull (PR) \ud83d\udee0\ufe0f. Estamos ansiosos para ver o que voc\u00ea traz para a mesa!</p> <p>Juntos, vamos aprimorar as capacidades de rastreamento do ecossistema Ultralytics YOLO \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Treinamento de Modelos com a YOLO da Ultralytics","text":""},{"location":"modes/train/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O treinamento de um modelo de aprendizado profundo envolve fornecer dados e ajustar seus par\u00e2metros para que ele possa fazer previs\u00f5es precisas. O modo de treino na YOLOv8 da Ultralytics \u00e9 projetado para um treinamento eficaz e eficiente de modelos de detec\u00e7\u00e3o de objetos, aproveitando totalmente as capacidades do hardware moderno. Este guia visa cobrir todos os detalhes que voc\u00ea precisa para come\u00e7ar a treinar seus pr\u00f3prios modelos usando o robusto conjunto de recursos da YOLOv8.</p> <p> Assista: Como Treinar um modelo YOLOv8 no Seu Conjunto de Dados Personalizado no Google Colab. </p>"},{"location":"modes/train/#por-que-escolher-a-yolo-da-ultralytics-para-treinamento","title":"Por Que Escolher a YOLO da Ultralytics para Treinamento?","text":"<p>Aqui est\u00e3o algumas raz\u00f5es convincentes para optar pelo modo de Treino da YOLOv8:</p> <ul> <li>Efici\u00eancia: Aproveite ao m\u00e1ximo seu hardware, seja em um setup com uma \u00fanica GPU ou expandindo para m\u00faltiplas GPUs.</li> <li>Versatilidade: Treine em conjuntos de dados personalizados, al\u00e9m dos j\u00e1 dispon\u00edveis, como COCO, VOC e ImageNet.</li> <li>Facilidade de Uso: Interfaces de linha de comando (CLI) e em Python simples, por\u00e9m poderosas, para uma experi\u00eancia de treinamento direta.</li> <li>Flexibilidade de Hiperpar\u00e2metros: Uma ampla gama de hiperpar\u00e2metros personaliz\u00e1veis para ajustar o desempenho do modelo.</li> </ul>"},{"location":"modes/train/#principais-recursos-do-modo-de-treino","title":"Principais Recursos do Modo de Treino","text":"<p>Os seguintes s\u00e3o alguns recursos not\u00e1veis \u200b\u200bdo modo de Treino da YOLOv8:</p> <ul> <li>Download Autom\u00e1tico de Datasets: Datasets padr\u00f5es como COCO, VOC e ImageNet s\u00e3o baixados automaticamente na primeira utiliza\u00e7\u00e3o.</li> <li>Suporte a Multi-GPU: Escalone seus esfor\u00e7os de treinamento de maneira uniforme entre v\u00e1rias GPUs para acelerar o processo.</li> <li>Configura\u00e7\u00e3o de Hiperpar\u00e2metros: Op\u00e7\u00e3o de modificar hiperpar\u00e2metros atrav\u00e9s de arquivos de configura\u00e7\u00e3o YAML ou argumentos de CLI.</li> <li>Visualiza\u00e7\u00e3o e Monitoramento: Acompanhamento em tempo real das m\u00e9tricas de treinamento e visualiza\u00e7\u00e3o do processo de aprendizagem para obter melhores insights.</li> </ul> <p>Dica</p> <ul> <li>Conjuntos de dados YOLOv8 como COCO, VOC, ImageNet e muitos outros s\u00e3o baixados automaticamente na primeira utiliza\u00e7\u00e3o, ou seja, <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Treine o YOLOv8n no conjunto de dados COCO128 por 100 \u00e9pocas com tamanho de imagem de 640. O dispositivo de treinamento pode ser especificado usando o argumento <code>device</code>. Se nenhum argumento for passado, a GPU <code>device=0</code> ser\u00e1 usado se dispon\u00edvel, caso contr\u00e1rio, <code>device=cpu</code> ser\u00e1 usado. Veja a se\u00e7\u00e3o Argumentos abaixo para uma lista completa dos argumentos de treinamento.</p> <p>Exemplo de Treinamento em Uma \u00danica GPU e CPU</p> <p>O dispositivo \u00e9 determinado automaticamente. Se uma GPU estiver dispon\u00edvel, ela ser\u00e1 usada, caso contr\u00e1rio, o treinamento come\u00e7ar\u00e1 na CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#treinamento-com-multi-gpu","title":"Treinamento com Multi-GPU","text":"<p>O treinamento com m\u00faltiplas GPUs permite uma utiliza\u00e7\u00e3o mais eficiente dos recursos de hardware dispon\u00edveis, distribuindo a carga de treinamento entre v\u00e1rias GPUs. Esse recurso est\u00e1 dispon\u00edvel por meio da API do Python e da interface de linha de comando. Para habilitar o treinamento com v\u00e1rias GPUs, especifique os IDs dos dispositivos de GPU que deseja usar.</p> <p>Exemplo de Treinamento com Multi-GPU</p> <p>Para treinar com 2 GPUs, dispositivos CUDA 0 e 1 use os seguintes comandos. Expanda para GPUs adicionais conforme necess\u00e1rio.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\n\n# Treinar o modelo com 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado usando as GPUs 0 e 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#treinamento-com-apple-m1-e-m2-mps","title":"Treinamento com Apple M1 e M2 MPS","text":"<p>Com a integra\u00e7\u00e3o do suporte para os chips Apple M1 e M2 nos modelos YOLO da Ultralytics, agora \u00e9 poss\u00edvel treinar seus modelos em dispositivos que utilizam o poderoso framework Metal Performance Shaders (MPS). O MPS oferece uma forma de alto desempenho de executar tarefas de computa\u00e7\u00e3o e processamento de imagens no sil\u00edcio personalizado da Apple.</p> <p>Para habilitar o treinamento nos chips Apple M1 e M2, voc\u00ea deve especificar 'mps' como seu dispositivo ao iniciar o processo de treinamento. Abaixo est\u00e1 um exemplo de como voc\u00ea pode fazer isso em Python e via linha de comando:</p> <p>Exemplo de Treinamento com MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\n\n# Treinar o modelo com MPS\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado usando MPS\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Ao aproveitar o poder computacional dos chips M1/M2, isso possibilita o processamento mais eficiente das tarefas de treinamento. Para orienta\u00e7\u00f5es mais detalhadas e op\u00e7\u00f5es avan\u00e7adas de configura\u00e7\u00e3o, consulte a documenta\u00e7\u00e3o do PyTorch MPS.</p>"},{"location":"modes/train/#registro-de-logs","title":"Registro de Logs","text":"<p>Ao treinar um modelo YOLOv8, voc\u00ea pode achar valioso acompanhar o desempenho do modelo ao longo do tempo. \u00c9 aqui que o registro de logs se torna \u00fatil. O YOLO da Ultralytics oferece suporte para tr\u00eas tipos de loggers - Comet, ClearML e TensorBoard.</p> <p>Para usar um logger, selecione-o no menu suspenso no trecho de c\u00f3digo acima e execute-o. O logger escolhido ser\u00e1 instalado e inicializado.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet \u00e9 uma plataforma que permite a cientistas de dados e desenvolvedores rastrear, comparar, explicar e otimizar experimentos e modelos. Oferece funcionalidades como m\u00e9tricas em tempo real, diffs de c\u00f3digo e acompanhamento de hiperpar\u00e2metros.</p> <p>Para usar o Comet:</p> <p>Exemplo</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Lembre-se de fazer login na sua conta Comet no site deles e obter sua chave de API. Voc\u00ea precisar\u00e1 adicionar isso \u00e0s suas vari\u00e1veis de ambiente ou ao seu script para registrar seus experimentos.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML \u00e9 uma plataforma de c\u00f3digo aberto que automatiza o rastreamento de experimentos e ajuda com o compartilhamento eficiente de recursos. \u00c9 projetada para ajudar as equipes a gerenciar, executar e reproduzir seus trabalhos de ML de maneira mais eficiente.</p> <p>Para usar o ClearML:</p> <p>Exemplo</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Ap\u00f3s executar este script, voc\u00ea precisar\u00e1 fazer login na sua conta ClearML no navegador e autenticar sua sess\u00e3o.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard \u00e9 um kit de ferramentas de visualiza\u00e7\u00e3o para TensorFlow. Permite visualizar o seu gr\u00e1fico TensorFlow, plotar m\u00e9tricas quantitativas sobre a execu\u00e7\u00e3o do seu gr\u00e1fico e mostrar dados adicionais como imagens que passam por ele.</p> <p>Para usar o TensorBoard em Google Colab:</p> <p>Exemplo</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # substitua pelo diret\u00f3rio 'runs'\n</code></pre> <p>Para usar o TensorBoard localmente, execute o comando abaixo e veja os resultados em http://localhost:6006/:</p> <p>Exemplo</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # substitua pelo diret\u00f3rio 'runs'\n</code></pre> <p>Isso ir\u00e1 carregar o TensorBoard e direcion\u00e1-lo para o diret\u00f3rio onde seus logs de treinamento est\u00e3o salvos.</p> <p>Depois de configurar o seu logger, voc\u00ea pode ent\u00e3o prosseguir com o treinamento do seu modelo. Todas as m\u00e9tricas de treinamento ser\u00e3o registradas automaticamente na sua plataforma escolhida, e voc\u00ea pode acessar esses logs para monitorar o desempenho do seu modelo ao longo do tempo, comparar diferentes modelos e identificar \u00e1reas para melhoria.</p>"},{"location":"modes/val/","title":"Valida\u00e7\u00e3o de Modelos com Ultralytics YOLO","text":""},{"location":"modes/val/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>A valida\u00e7\u00e3o \u00e9 um passo cr\u00edtico no pipeline de aprendizado de m\u00e1quina, permitindo que voc\u00ea avalie a qualidade dos seus modelos treinados. O modo Val no Ultralytics YOLOv8 fornece um robusto conjunto de ferramentas e m\u00e9tricas para avaliar o desempenho dos seus modelos de detec\u00e7\u00e3o de objetos. Este guia serve como um recurso completo para entender como usar efetivamente o modo Val para garantir que seus modelos sejam precisos e confi\u00e1veis.</p>"},{"location":"modes/val/#por-que-validar-com-o-ultralytics-yolo","title":"Por Que Validar com o Ultralytics YOLO?","text":"<p>Aqui est\u00e3o as vantagens de usar o modo Val no YOLOv8:</p> <ul> <li>Precis\u00e3o: Obtenha m\u00e9tricas precisas como mAP50, mAP75 e mAP50-95 para avaliar seu modelo de forma abrangente.</li> <li>Conveni\u00eancia: Utilize recursos integrados que lembram as configura\u00e7\u00f5es de treinamento, simplificando o processo de valida\u00e7\u00e3o.</li> <li>Flexibilidade: Valide seu modelo com os mesmos ou diferentes conjuntos de dados e tamanhos de imagem.</li> <li>Ajuste de Hiperpar\u00e2metros: Utilize as m\u00e9tricas de valida\u00e7\u00e3o para refinar seu modelo e obter um desempenho melhor.</li> </ul>"},{"location":"modes/val/#principais-recursos-do-modo-val","title":"Principais Recursos do Modo Val","text":"<p>Estas s\u00e3o as funcionalidades not\u00e1veis oferecidas pelo modo Val do YOLOv8:</p> <ul> <li>Configura\u00e7\u00f5es Automatizadas: Os modelos lembram suas configura\u00e7\u00f5es de treinamento para valida\u00e7\u00e3o direta.</li> <li>Suporte Multi-M\u00e9trico: Avalie seu modelo com base em uma variedade de m\u00e9tricas de precis\u00e3o.</li> <li>API em Python e CLI: Escolha entre a interface de linha de comando ou API em Python com base na sua prefer\u00eancia de valida\u00e7\u00e3o.</li> <li>Compatibilidade de Dados: Funciona perfeitamente com conjuntos de dados usados durante a fase de treinamento, bem como conjuntos de dados personalizados.</li> </ul> <p>Dica</p> <ul> <li>Os modelos YOLOv8 lembram automaticamente suas configura\u00e7\u00f5es de treinamento, ent\u00e3o voc\u00ea pode validar um modelo no mesmo tamanho de imagem e no conjunto de dados original facilmente com apenas <code>yolo val model=yolov8n.pt</code> ou <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Validar a precis\u00e3o do modelo YOLOv8n treinado no conjunto de dados COCO128. Nenhum argumento precisa ser passado, pois o <code>model</code> ret\u00e9m os dados de treinamento e argumentos como atributos do modelo. Veja a se\u00e7\u00e3o de Argumentos abaixo para uma lista completa dos argumentos de exporta\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # nenhum argumento necess\u00e1rio, conjunto de dados e configura\u00e7\u00f5es lembrados\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # validar modelo oficial\nyolo detect val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"modes/val/#argumentos","title":"Argumentos","text":"<p>As configura\u00e7\u00f5es de valida\u00e7\u00e3o para os modelos YOLO referem-se aos v\u00e1rios hiperpar\u00e2metros e configura\u00e7\u00f5es usados para avaliar o desempenho do modelo em um conjunto de dados de valida\u00e7\u00e3o. Essas configura\u00e7\u00f5es podem afetar o desempenho, velocidade e precis\u00e3o do modelo. Algumas configura\u00e7\u00f5es comuns de valida\u00e7\u00e3o do YOLO incluem o tamanho do lote, a frequ\u00eancia com que a valida\u00e7\u00e3o \u00e9 realizada durante o treinamento e as m\u00e9tricas usadas para avaliar o desempenho do modelo. Outros fatores que podem afetar o processo de valida\u00e7\u00e3o incluem o tamanho e a composi\u00e7\u00e3o do conjunto de dados de valida\u00e7\u00e3o e a tarefa espec\u00edfica para a qual o modelo est\u00e1 sendo usado. \u00c9 importante ajustar e experimentar cuidadosamente essas configura\u00e7\u00f5es para garantir que o modelo apresente um bom desempenho no conjunto de dados de valida\u00e7\u00e3o e para detectar e prevenir o sobreajuste.</p> Chave Valor Descri\u00e7\u00e3o <code>data</code> <code>None</code> caminho para o arquivo de dados, ex. coco128.yaml <code>imgsz</code> <code>640</code> tamanho das imagens de entrada como inteiro <code>batch</code> <code>16</code> n\u00famero de imagens por lote (-1 para AutoBatch) <code>save_json</code> <code>False</code> salvar resultados em arquivo JSON <code>save_hybrid</code> <code>False</code> salvar vers\u00e3o h\u00edbrida das etiquetas (etiquetas + previs\u00f5es adicionais) <code>conf</code> <code>0.001</code> limite de confian\u00e7a do objeto para detec\u00e7\u00e3o <code>iou</code> <code>0.6</code> limiar de interse\u00e7\u00e3o sobre uni\u00e3o (IoU) para NMS <code>max_det</code> <code>300</code> n\u00famero m\u00e1ximo de detec\u00e7\u00f5es por imagem <code>half</code> <code>True</code> usar precis\u00e3o meia (FP16) <code>device</code> <code>None</code> dispositivo para execu\u00e7\u00e3o, ex. dispositivo cuda=0/1/2/3 ou device=cpu <code>dnn</code> <code>False</code> usar OpenCV DNN para infer\u00eancia ONNX <code>plots</code> <code>False</code> mostrar gr\u00e1ficos durante o treinamento <code>rect</code> <code>False</code> val retangular com cada lote colado para minimizar o preenchimento <code>split</code> <code>val</code> divis\u00e3o do conjunto de dados para usar na valida\u00e7\u00e3o, ex. 'val', 'test' ou 'train'"},{"location":"tasks/","title":"Tarefas do Ultralytics YOLOv8","text":"<p>YOLOv8 \u00e9 um framework de IA que suporta m\u00faltiplas tarefas de vis\u00e3o computacional. O framework pode ser usado para realizar detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, classifica\u00e7\u00e3o e estimativa de pose. Cada uma dessas tarefas tem um objetivo e caso de uso diferente.</p> <p>Note</p> <p>\ud83d\udea7 Nossa documenta\u00e7\u00e3o multil\u00edngue est\u00e1 atualmente em constru\u00e7\u00e3o e estamos trabalhando para aprimor\u00e1-la. Agradecemos sua paci\u00eancia! \ud83d\ude4f</p> <p> Assista: Explore as Tarefas do Ultralytics YOLO: Detec\u00e7\u00e3o de Objetos, Segmenta\u00e7\u00e3o, Rastreamento e Estimativa de Pose. </p>"},{"location":"tasks/#deteccao","title":"Detec\u00e7\u00e3o","text":"<p>A detec\u00e7\u00e3o \u00e9 a principal tarefa suportada pelo YOLOv8. Envolve detectar objetos em uma imagem ou quadro de v\u00eddeo e desenhar caixas delimitadoras ao redor deles. Os objetos detectados s\u00e3o classificados em diferentes categorias com base em suas caracter\u00edsticas. YOLOv8 pode detectar m\u00faltiplos objetos em uma \u00fanica imagem ou quadro de v\u00eddeo com alta precis\u00e3o e velocidade.</p> <p>Exemplos de Detec\u00e7\u00e3o</p>"},{"location":"tasks/#segmentacao","title":"Segmenta\u00e7\u00e3o","text":"<p>Segmenta\u00e7\u00e3o \u00e9 uma tarefa que envolve segmentar uma imagem em diferentes regi\u00f5es com base no conte\u00fado da imagem. Cada regi\u00e3o recebe um r\u00f3tulo com base em seu conte\u00fado. Essa tarefa \u00e9 \u00fatil em aplica\u00e7\u00f5es como segmenta\u00e7\u00e3o de imagens e imagiologia m\u00e9dica. YOLOv8 usa uma variante da arquitetura U-Net para realizar a segmenta\u00e7\u00e3o.</p> <p>Exemplos de Segmenta\u00e7\u00e3o</p>"},{"location":"tasks/#classificacao","title":"Classifica\u00e7\u00e3o","text":"<p>Classifica\u00e7\u00e3o \u00e9 uma tarefa que envolve classificar uma imagem em diferentes categorias. YOLOv8 pode ser usado para classificar imagens com base em seu conte\u00fado. Utiliza uma variante da arquitetura EfficientNet para realizar a classifica\u00e7\u00e3o.</p> <p>Exemplos de Classifica\u00e7\u00e3o</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>A detec\u00e7\u00e3o de pose/pontos-chave \u00e9 uma tarefa que envolve detectar pontos espec\u00edficos em uma imagem ou quadro de v\u00eddeo. Esses pontos s\u00e3o chamados de keypoints e s\u00e3o usados para rastrear movimento ou estimar poses. YOLOv8 pode detectar keypoints em uma imagem ou quadro de v\u00eddeo com alta precis\u00e3o e velocidade.</p> <p>Exemplos de Pose</p>"},{"location":"tasks/#conclusao","title":"Conclus\u00e3o","text":"<p>YOLOv8 suporta m\u00faltiplas tarefas, incluindo detec\u00e7\u00e3o, segmenta\u00e7\u00e3o, classifica\u00e7\u00e3o e detec\u00e7\u00e3o de keypoints. Cada uma dessas tarefas tem objetivos e casos de uso diferentes. Ao entender as diferen\u00e7as entre essas tarefas, voc\u00ea pode escolher a tarefa apropriada para sua aplica\u00e7\u00e3o de vis\u00e3o computacional.</p>"},{"location":"tasks/classify/","title":"Classifica\u00e7\u00e3o de Imagens","text":"<p>A classifica\u00e7\u00e3o de imagens \u00e9 a tarefa mais simples das tr\u00eas e envolve classificar uma imagem inteira em uma de um conjunto de classes pr\u00e9-definidas.</p> <p>A sa\u00edda de um classificador de imagem \u00e9 um \u00fanico r\u00f3tulo de classe e uma pontua\u00e7\u00e3o de confian\u00e7a. A classifica\u00e7\u00e3o de imagem \u00e9 \u00fatil quando voc\u00ea precisa saber apenas a qual classe uma imagem pertence e n\u00e3o precisa conhecer a localiza\u00e7\u00e3o dos objetos dessa classe ou o formato exato deles.</p> <p>Dica</p> <p>Os modelos YOLOv8 Classify usam o sufixo <code>-cls</code>, ou seja, <code>yolov8n-cls.pt</code> e s\u00e3o pr\u00e9-treinados na ImageNet.</p>"},{"location":"tasks/classify/#modelos","title":"Modelos","text":"<p>Aqui s\u00e3o mostrados os modelos pr\u00e9-treinados YOLOv8 Classify. Modelos de Detec\u00e7\u00e3o, Segmenta\u00e7\u00e3o e Pose s\u00e3o pr\u00e9-treinados no dataset COCO, enquanto que os modelos de Classifica\u00e7\u00e3o s\u00e3o pr\u00e9-treinados no dataset ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo Tamanho<sup>(pixels) acur\u00e1cia<sup>top1 acur\u00e1cia<sup>top5 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) a 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Os valores de acc s\u00e3o as acur\u00e1cias dos modelos no conjunto de valida\u00e7\u00e3o do dataset ImageNet.   Reproduza com <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Velocidade m\u00e9dia observada sobre imagens de valida\u00e7\u00e3o da ImageNet usando uma inst\u00e2ncia Amazon EC2 P4d.   Reproduza com <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#treino","title":"Treino","text":"<p>Treine o modelo YOLOv8n-cls no dataset MNIST160 por 100 \u00e9pocas com tamanho de imagem 64. Para uma lista completa de argumentos dispon\u00edveis, veja a p\u00e1gina de Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treino)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar treino do zero\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Come\u00e7ar treino de um modelo pr\u00e9-treinado *.pt\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construir um novo modelo do YAML, transferir pesos pr\u00e9-treinados e come\u00e7ar treino\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#formato-do-dataset","title":"Formato do dataset","text":"<p>O formato do dataset de classifica\u00e7\u00e3o YOLO pode ser encontrado em detalhes no Guia de Datasets.</p>"},{"location":"tasks/classify/#val","title":"Val","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-cls treinado no dataset MNIST160. N\u00e3o \u00e9 necess\u00e1rio passar argumento, pois o <code>modelo</code> ret\u00e9m seus dados de treinamento e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem argumentos necess\u00e1rios, dataset e configura\u00e7\u00f5es lembrados\nmetrics.top1   # acur\u00e1cia top1\nmetrics.top5   # acur\u00e1cia top5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # validar modelo oficial\nyolo classify val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/classify/#previsao","title":"Previs\u00e3o","text":"<p>Use um modelo YOLOv8n-cls treinado para realizar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Prever com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo oficial\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo de <code>previs\u00e3o</code> na p\u00e1gina Predict.</p>"},{"location":"tasks/classify/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n-cls para um formato diferente, como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-cls.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8-cls dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente nos modelos exportados, ou seja, <code>yolo predict model=yolov8n-cls.onnx</code>. Exemplos de uso s\u00e3o mostrados para seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadata Argumentos PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Export.</p>"},{"location":"tasks/detect/","title":"Detec\u00e7\u00e3o de Objetos","text":"<p>Detec\u00e7\u00e3o de objetos \u00e9 uma tarefa que envolve identificar a localiza\u00e7\u00e3o e a classe de objetos em uma imagem ou fluxo de v\u00eddeo.</p> <p>A sa\u00edda de um detector de objetos \u00e9 um conjunto de caixas delimitadoras que cercam os objetos na imagem, junto com r\u00f3tulos de classe e pontua\u00e7\u00f5es de confian\u00e7a para cada caixa. A detec\u00e7\u00e3o de objetos \u00e9 uma boa escolha quando voc\u00ea precisa identificar objetos de interesse em uma cena, mas n\u00e3o precisa saber exatamente onde o objeto est\u00e1 ou seu formato exato.</p> <p> Assista: Detec\u00e7\u00e3o de Objetos com Modelo Pre-treinado Ultralytics YOLOv8. </p> <p>Dica</p> <p>Os modelos YOLOv8 Detect s\u00e3o os modelos padr\u00e3o do YOLOv8, ou seja, <code>yolov8n.pt</code> e s\u00e3o pr\u00e9-treinados no COCO.</p>"},{"location":"tasks/detect/#modelos","title":"Modelos","text":"<p>Os modelos pr\u00e9-treinados YOLOv8 Detect s\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no dataset COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no dataset ImageNet.</p> <p>Os Modelos s\u00e3o baixados automaticamente a partir do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo Tamanho<sup>(pixels) mAP<sup>val50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) Par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Os valores de mAP<sup>val</sup> s\u00e3o para um \u00fanico modelo e uma \u00fanica escala no dataset COCO val2017.   Reproduza usando <code>yolo val detect data=coco.yaml device=0</code></li> <li>A Velocidade \u00e9 m\u00e9dia tirada sobre as imagens do COCO val num Amazon EC2 P4d   inst\u00e2ncia.   Reproduza usando <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#treinar","title":"Treinar","text":"<p>Treine o YOLOv8n no dataset COCO128 por 100 \u00e9pocas com tamanho de imagem 640. Para uma lista completa de argumentos dispon\u00edveis, veja a p\u00e1gina Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.yaml')  # construir um novo modelo pelo YAML\nmodel = YOLO('yolov8n.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir pelo YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo pelo YAML e come\u00e7ar o treinamento do zero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo pr\u00e9-treinado *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir um novo modelo pelo YAML, transferir pesos pr\u00e9-treinados e come\u00e7ar o treinamento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#formato-do-dataset","title":"Formato do Dataset","text":"<p>O formato do dataset de detec\u00e7\u00e3o do YOLO pode ser encontrado em detalhes no Guia de Datasets. Para converter seu dataset existente de outros formatos (como COCO, etc.) para o formato YOLO, por favor utilize a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/detect/#validar","title":"Validar","text":"<p>Valide a precis\u00e3o do modelo YOLOv8n treinado no dataset COCO128. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>modelo</code> mant\u00e9m seus <code>dados</code> de treino e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem a necessidade de argumentos, dataset e configura\u00e7\u00f5es lembradas\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # valida\u00e7\u00e3o do modelo oficial\nyolo detect val model=caminho/para/best.pt  # valida\u00e7\u00e3o do modelo personalizado\n</code></pre>"},{"location":"tasks/detect/#predizer","title":"Predizer","text":"<p>Use um modelo YOLOv8n treinado para fazer predi\u00e7\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo personalizado\n\n# Predizer com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predizer em uma imagem\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # predizer com modelo oficial\nyolo detect predict model=caminho/para/best.pt source='https://ultralytics.com/images/bus.jpg'  # predizer com modelo personalizado\n</code></pre> <p>Veja os detalhes completos do modo <code>predict</code> na p\u00e1gina Predi\u00e7\u00e3o.</p>"},{"location":"tasks/detect/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n para um formato diferente, como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exportar modelo oficial\nyolo export model=caminho/para/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8 dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode fazer predi\u00e7\u00f5es ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a exporta\u00e7\u00e3o ser conclu\u00edda.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja os detalhes completos de <code>exportar</code> na p\u00e1gina Exporta\u00e7\u00e3o.</p>"},{"location":"tasks/pose/","title":"Estimativa de Pose","text":"<p>A estimativa de pose \u00e9 uma tarefa que envolve identificar a localiza\u00e7\u00e3o de pontos espec\u00edficos em uma imagem, geralmente referidos como pontos-chave. Os pontos-chave podem representar v\u00e1rias partes do objeto como articula\u00e7\u00f5es, pontos de refer\u00eancia ou outras caracter\u00edsticas distintas. As localiza\u00e7\u00f5es dos pontos-chave s\u00e3o geralmente representadas como um conjunto de coordenadas 2D <code>[x, y]</code> ou 3D <code>[x, y, vis\u00edvel]</code>.</p> <p>A sa\u00edda de um modelo de estimativa de pose \u00e9 um conjunto de pontos que representam os pontos-chave em um objeto na imagem, geralmente junto com os escores de confian\u00e7a para cada ponto. A estimativa de pose \u00e9 uma boa escolha quando voc\u00ea precisa identificar partes espec\u00edficas de um objeto em uma cena, e sua localiza\u00e7\u00e3o relativa entre si.</p> <p> Assista: Estimativa de Pose com Ultralytics YOLOv8. </p> <p>Dica</p> <p>Modelos YOLOv8 pose usam o sufixo <code>-pose</code>, isto \u00e9 <code>yolov8n-pose.pt</code>. Esses modelos s\u00e3o treinados no conjunto de dados COCO keypoints e s\u00e3o adequados para uma variedade de tarefas de estimativa de pose.</p>"},{"location":"tasks/pose/#modelos","title":"Modelos","text":"<p>Os modelos YOLOv8 Pose pr\u00e9-treinados s\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no conjunto de dados COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no conjunto de dados ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release no primeiro uso.</p> Modelo tamanho<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>mAP<sup>val</sup> valores s\u00e3o para um \u00fanico modelo em escala \u00fanica no conjunto de dados COCO Keypoints val2017   .   Reproduza <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>Velocidade m\u00e9dia em imagens COCO val usando uma inst\u00e2ncia Amazon EC2 P4d   .   Reproduza <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#treinar","title":"Treinar","text":"<p>Treine um modelo YOLOv8-pose no conjunto de dados COCO128-pose.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construir a partir do YAML e transferir pesos\n\n# Treinar o modelo\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar treinamento de um modelo *.pt pr\u00e9-treinado\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#formato-do-conjunto-de-dados","title":"Formato do conjunto de dados","text":"<p>O formato do conjunto de dados de pose YOLO pode ser encontrado em detalhes no Guia de Conjuntos de Dados. Para converter seu conjunto de dados existente de outros formatos (como COCO etc.) para o formato YOLO, por favor, use a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/pose/#validar","title":"Validar","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-pose treinado no conjunto de dados COCO128-pose. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>model</code> ret\u00e9m seus <code>data</code> de treinamento e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # nenhum argumento necess\u00e1rio, conjunto de dados e configura\u00e7\u00f5es lembradas\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # uma lista cont\u00e9m map50-95 de cada categoria\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # validar modelo oficial\nyolo pose val model=caminho/para/melhor.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/pose/#prever","title":"Prever","text":"<p>Use um modelo YOLOv8n-pose treinado para executar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo personalizado\n\n# Prever com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo oficial\nyolo pose predict model=caminho/para/melhor.pt source='https://ultralytics.com/images/bus.jpg'  # prever com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo <code>predict</code> na p\u00e1gina Prever.</p>"},{"location":"tasks/pose/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n Pose para um formato diferente como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-pose.pt')  # carregar um modelo oficial\nmodel = YOLO('caminho/para/melhor.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exportar modelo oficial\nyolo export model=caminho/para/melhor.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o YOLOv8-pose dispon\u00edveis est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n-pose.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Exportar.</p>"},{"location":"tasks/segment/","title":"Segmenta\u00e7\u00e3o de Inst\u00e2ncias","text":"<p>A segmenta\u00e7\u00e3o de inst\u00e2ncias vai al\u00e9m da detec\u00e7\u00e3o de objetos e envolve a identifica\u00e7\u00e3o de objetos individuais em uma imagem e a sua segmenta\u00e7\u00e3o do resto da imagem.</p> <p>A sa\u00edda de um modelo de segmenta\u00e7\u00e3o de inst\u00e2ncias \u00e9 um conjunto de m\u00e1scaras ou contornos que delineiam cada objeto na imagem, juntamente com r\u00f3tulos de classe e pontua\u00e7\u00f5es de confian\u00e7a para cada objeto. A segmenta\u00e7\u00e3o de inst\u00e2ncias \u00e9 \u00fatil quando voc\u00ea precisa saber n\u00e3o apenas onde os objetos est\u00e3o em uma imagem, mas tamb\u00e9m qual \u00e9 a forma exata deles.</p> <p> Assista: Executar Segmenta\u00e7\u00e3o com o Modelo Treinado Ultralytics YOLOv8 em Python. </p> <p>Dica</p> <p>Modelos YOLOv8 Segment usam o sufixo <code>-seg</code>, ou seja, <code>yolov8n-seg.pt</code> e s\u00e3o pr\u00e9-treinados no COCO.</p>"},{"location":"tasks/segment/#modelos","title":"Modelos","text":"<p>Os modelos Segment pr\u00e9-treinados do YOLOv8 est\u00e3o mostrados aqui. Os modelos Detect, Segment e Pose s\u00e3o pr\u00e9-treinados no conjunto de dados COCO, enquanto os modelos Classify s\u00e3o pr\u00e9-treinados no conjunto de dados ImageNet.</p> <p>Modelos s\u00e3o baixados automaticamente do \u00faltimo lan\u00e7amento da Ultralytics release na primeira utiliza\u00e7\u00e3o.</p> Modelo Tamanho<sup>(pixels) mAP<sup>box50-95 mAP<sup>m\u00e1scara50-95 Velocidade<sup>CPU ONNX(ms) Velocidade<sup>A100 TensorRT(ms) Par\u00e2metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Os valores de mAP<sup>val</sup> s\u00e3o para um \u00fanico modelo em uma \u00fanica escala no conjunto de dados COCO val2017.   Reproduza por meio de <code>yolo val segment data=coco.yaml device=0</code></li> <li>Velocidade m\u00e9dia em imagens COCO val usando uma inst\u00e2ncia Amazon EC2 P4d.   Reproduza por meio de <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#treinar","title":"Treinar","text":"<p>Treine o modelo YOLOv8n-seg no conjunto de dados COCO128-seg por 100 \u00e9pocas com tamanho de imagem 640. Para uma lista completa de argumentos dispon\u00edveis, consulte a p\u00e1gina Configura\u00e7\u00e3o.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.yaml')  # construir um novo modelo a partir do YAML\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo pr\u00e9-treinado (recomendado para treinamento)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construir a partir do YAML e transferir os pesos\n\n# Treinar o modelo\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir um novo modelo a partir do YAML e come\u00e7ar o treinamento do zero\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Come\u00e7ar o treinamento a partir de um modelo *.pt pr\u00e9-treinado\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construir um novo modelo a partir do YAML, transferir pesos pr\u00e9-treinados para ele e come\u00e7ar o treinamento\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#formato-do-conjunto-de-dados","title":"Formato do conjunto de dados","text":"<p>O formato do conjunto de dados de segmenta\u00e7\u00e3o YOLO pode ser encontrado em detalhes no Guia de Conjuntos de Dados. Para converter seu conjunto de dados existente de outros formatos (como COCO etc.) para o formato YOLO, utilize a ferramenta JSON2YOLO da Ultralytics.</p>"},{"location":"tasks/segment/#val","title":"Val","text":"<p>Valide a acur\u00e1cia do modelo YOLOv8n-seg treinado no conjunto de dados COCO128-seg. N\u00e3o \u00e9 necess\u00e1rio passar nenhum argumento, pois o <code>modelo</code> ret\u00e9m seus <code>dados</code> de treino e argumentos como atributos do modelo.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Validar o modelo\nmetrics = model.val()  # sem necessidade de argumentos, conjunto de dados e configura\u00e7\u00f5es s\u00e3o lembrados\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # uma lista contendo map50-95(B) de cada categoria\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # uma lista contendo map50-95(M) de cada categoria\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # val modelo oficial\nyolo segment val model=path/to/best.pt  # val modelo personalizado\n</code></pre>"},{"location":"tasks/segment/#prever","title":"Prever","text":"<p>Use um modelo YOLOv8n-seg treinado para realizar previs\u00f5es em imagens.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo personalizado\n\n# Realizar previs\u00e3o com o modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # prever em uma imagem\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # previs\u00e3o com modelo oficial\nyolo segment predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # previs\u00e3o com modelo personalizado\n</code></pre> <p>Veja detalhes completos do modo <code>predict</code> na p\u00e1gina Prever.</p>"},{"location":"tasks/segment/#exportar","title":"Exportar","text":"<p>Exporte um modelo YOLOv8n-seg para um formato diferente como ONNX, CoreML, etc.</p> <p>Exemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carregar um modelo\nmodel = YOLO('yolov8n-seg.pt')  # carregar um modelo oficial\nmodel = YOLO('path/to/best.pt')  # carregar um modelo treinado personalizado\n\n# Exportar o modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo treinado personalizado\n</code></pre> <p>Os formatos de exporta\u00e7\u00e3o dispon\u00edveis para YOLOv8-seg est\u00e3o na tabela abaixo. Voc\u00ea pode prever ou validar diretamente em modelos exportados, ou seja, <code>yolo predict model=yolov8n-seg.onnx</code>. Exemplos de uso s\u00e3o mostrados para o seu modelo ap\u00f3s a conclus\u00e3o da exporta\u00e7\u00e3o.</p> Formato Argumento <code>format</code> Modelo Metadados Argumentos PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Veja detalhes completos da <code>exporta\u00e7\u00e3o</code> na p\u00e1gina Exportar.</p>"}]}