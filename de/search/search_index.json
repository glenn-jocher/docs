{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Startseite","text":"<p>Wir stellen Ultralytics YOLOv8 vor, die neueste Version des renommierten Echtzeit-Modells zur Objekterkennung und Bildsegmentierung. YOLOv8 basiert auf den neuesten Erkenntnissen im Bereich Deep Learning und Computer Vision und bietet eine unvergleichliche Leistung hinsichtlich Geschwindigkeit und Genauigkeit. Sein optimiertes Design macht es f\u00fcr verschiedene Anwendungen geeignet und leicht an verschiedene Hardwareplattformen anpassbar, von Edge-Ger\u00e4ten bis hin zu Cloud-APIs.</p> <p>Erkunden Sie die YOLOv8-Dokumentation, eine umfassende Ressource, die Ihnen helfen soll, seine Funktionen und F\u00e4higkeiten zu verstehen und zu nutzen. Ob Sie ein erfahrener Machine-Learning-Praktiker sind oder neu in diesem Bereich, dieses Hub zielt darauf ab, das Potenzial von YOLOv8 in Ihren Projekten zu maximieren</p> <p>Note</p> <p>\ud83d\udea7 Unsere mehrsprachige Dokumentation wird derzeit entwickelt und wir arbeiten intensiv an ihrer Verbesserung. Wir danken f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"#wo-sie-beginnen-sollten","title":"Wo Sie beginnen sollten","text":"<ul> <li>Installieren Sie <code>ultralytics</code> mit pip und starten Sie in wenigen Minuten \u00a0  Loslegen</li> <li>Vorhersagen Sie neue Bilder und Videos mit YOLOv8 \u00a0  Auf Bilder vorhersagen</li> <li>Trainieren Sie ein neues YOLOv8-Modell mit Ihrem eigenen benutzerdefinierten Datensatz \u00a0  Ein Modell trainieren</li> <li>Erforschen Sie YOLOv8-Aufgaben wie Segmentieren, Klassifizieren, Posensch\u00e4tzung und Verfolgen \u00a0  Aufgaben erkunden</li> </ul> <p> Ansehen: Wie Sie ein YOLOv8-Modell auf Ihrem eigenen Datensatz in Google Colab trainieren. </p>"},{"location":"#yolo-eine-kurze-geschichte","title":"YOLO: Eine kurze Geschichte","text":"<p>YOLO (You Only Look Once), ein beliebtes Modell zur Objekterkennung und Bildsegmentierung, wurde von Joseph Redmon und Ali Farhadi an der Universit\u00e4t von Washington entwickelt. Seit seiner Einf\u00fchrung im Jahr 2015 erfreut es sich aufgrund seiner hohen Geschwindigkeit und Genauigkeit gro\u00dfer Beliebtheit.</p> <ul> <li>YOLOv2, ver\u00f6ffentlicht im Jahr 2016, verbesserte das Originalmodell durch die Einf\u00fchrung von Batch-Normalisierung, Ankerk\u00e4sten und Dimensionsclustern.</li> <li>YOLOv3, eingef\u00fchrt im Jahr 2018, erh\u00f6hte die Leistung des Modells weiter mit einem effizienteren Backbone-Netzwerk, mehreren Ankern und r\u00e4umlichem Pyramid-Pooling.</li> <li>YOLOv4 wurde 2020 ver\u00f6ffentlicht und brachte Neuerungen wie Mosaic-Datenerweiterung, einen neuen ankerfreien Erkennungskopf und eine neue Verlustfunktion.</li> <li>YOLOv5 verbesserte die Leistung des Modells weiter und f\u00fchrte neue Funktionen ein, wie Hyperparameter-Optimierung, integriertes Experiment-Tracking und automatischen Export in beliebte Exportformate.</li> <li>YOLOv6 wurde 2022 von Meituan als Open Source zur Verf\u00fcgung gestellt und wird in vielen autonomen Lieferrobotern des Unternehmens eingesetzt.</li> <li>YOLOv7 f\u00fchrte zus\u00e4tzliche Aufgaben ein, wie Posensch\u00e4tzung auf dem COCO-Keypoints-Datensatz.</li> <li>YOLOv8 ist die neueste Version von YOLO von Ultralytics. Als Spitzenmodell der neuesten Generation baut YOLOv8 auf dem Erfolg vorheriger Versionen auf und f\u00fchrt neue Funktionen und Verbesserungen f\u00fcr erh\u00f6hte Leistung, Flexibilit\u00e4t und Effizienz ein. YOLOv8 unterst\u00fctzt eine vollst\u00e4ndige Palette an Vision-KI-Aufgaben, einschlie\u00dflich Erkennung, Segmentierung, Posensch\u00e4tzung, Verfolgung und Klassifizierung. Diese Vielseitigkeit erm\u00f6glicht es Benutzern, die F\u00e4higkeiten von YOLOv8 in verschiedenen Anwendungen und Dom\u00e4nen zu nutzen.</li> </ul>"},{"location":"#yolo-lizenzen-wie-wird-ultralytics-yolo-lizenziert","title":"YOLO-Lizenzen: Wie wird Ultralytics YOLO lizenziert?","text":"<p>Ultralytics bietet zwei Lizenzoptionen, um unterschiedliche Einsatzszenarien zu ber\u00fccksichtigen:</p> <ul> <li>AGPL-3.0-Lizenz: Diese OSI-gepr\u00fcfte Open-Source-Lizenz ist ideal f\u00fcr Studenten und Enthusiasten und f\u00f6rdert offene Zusammenarbeit und Wissensaustausch. Weitere Details finden Sie in der LIZENZ-Datei.</li> <li>Enterprise-Lizenz: F\u00fcr die kommerzielle Nutzung konzipiert, erm\u00f6glicht diese Lizenz die problemlose Integration von Ultralytics-Software und KI-Modellen in kommerzielle Produkte und Dienstleistungen und umgeht die Open-Source-Anforderungen der AGPL-3.0. Wenn Ihr Szenario die Einbettung unserer L\u00f6sungen in ein kommerzielles Angebot beinhaltet, kontaktieren Sie uns \u00fcber Ultralytics-Lizenzierung.</li> </ul> <p>Unsere Lizenzstrategie ist darauf ausgerichtet sicherzustellen, dass jegliche Verbesserungen an unseren Open-Source-Projekten der Gemeinschaft zur\u00fcckgegeben werden. Wir halten die Prinzipien von Open Source in Ehren \u2764\ufe0f und es ist unser Anliegen, dass unsere Beitr\u00e4ge auf Weisen genutzt und erweitert werden k\u00f6nnen, die f\u00fcr alle vorteilhaft sind.</p>"},{"location":"quickstart/","title":"Schnellstart","text":""},{"location":"quickstart/#ultralytics-installieren","title":"Ultralytics installieren","text":"<p>Ultralytics bietet verschiedene Installationsmethoden, darunter Pip, Conda und Docker. Installiere YOLOv8 \u00fcber das <code>ultralytics</code> Pip-Paket f\u00fcr die neueste stabile Ver\u00f6ffentlichung oder indem du das Ultralytics GitHub-Repository klonst f\u00fcr die aktuellste Version. Docker kann verwendet werden, um das Paket in einem isolierten Container auszuf\u00fchren, ohne eine lokale Installation vornehmen zu m\u00fcssen.</p> <p>Installieren</p> Pip-Installation (empfohlen)Conda-InstallationGit klonen <p>Installieren Sie das <code>ultralytics</code> Paket mit Pip oder aktualisieren Sie eine bestehende Installation, indem Sie <code>pip install -U ultralytics</code> ausf\u00fchren. Besuchen Sie den Python Package Index (PyPI) f\u00fcr weitere Details zum <code>ultralytics</code> Paket: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Installiere das ultralytics Paket von PyPI\npip install ultralytics\n</code></pre> <p>Sie k\u00f6nnen auch das <code>ultralytics</code> Paket direkt vom GitHub Repository installieren. Dies k\u00f6nnte n\u00fctzlich sein, wenn Sie die neueste Entwicklerversion m\u00f6chten. Stellen Sie sicher, dass das Git-Kommandozeilen-Tool auf Ihrem System installiert ist. Der Befehl <code>@main</code> installiert den <code>main</code> Branch und kann zu einem anderen Branch ge\u00e4ndert werden, z. B. <code>@my-branch</code>, oder ganz entfernt werden, um auf den <code>main</code> Branch standardm\u00e4\u00dfig zur\u00fcckzugreifen.</p> <pre><code># Installiere das ultralytics Paket von GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda ist ein alternativer Paketmanager zu Pip, der ebenfalls f\u00fcr die Installation verwendet werden kann. Besuche Anaconda f\u00fcr weitere Details unter https://anaconda.org/conda-forge/ultralytics. Ultralytics Feedstock Repository f\u00fcr die Aktualisierung des Conda-Pakets befindet sich unter https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Installiere das ultralytics Paket mit Conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Hinweis</p> <p>Wenn Sie in einer CUDA-Umgebung installieren, ist es am besten, <code>ultralytics</code>, <code>pytorch</code> und <code>pytorch-cuda</code> im selben Befehl zu installieren, um dem Conda-Paketmanager zu erm\u00f6glichen, Konflikte zu l\u00f6sen, oder <code>pytorch-cuda</code> als letztes zu installieren, damit es das CPU-spezifische <code>pytorch</code> Paket bei Bedarf \u00fcberschreiben kann. <pre><code># Installiere alle Pakete zusammen mit Conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Klonen Sie das <code>ultralytics</code> Repository, wenn Sie einen Beitrag zur Entwicklung leisten m\u00f6chten oder mit dem neuesten Quellcode experimentieren wollen. Nach dem Klonen navigieren Sie in das Verzeichnis und installieren das Paket im editierbaren Modus <code>-e</code> mit Pip. <pre><code># Klonen Sie das ultralytics Repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigiere zum geklonten Verzeichnis\ncd ultralytics\n\n# Installiere das Paket im editierbaren Modus f\u00fcr die Entwicklung\npip install -e .\n</code></pre></p> <p>Siehe die <code>ultralytics</code> requirements.txt Datei f\u00fcr eine Liste der Abh\u00e4ngigkeiten. Beachten Sie, dass alle oben genannten Beispiele alle erforderlichen Abh\u00e4ngigkeiten installieren.</p> <p>Tipp</p> <p>PyTorch-Anforderungen variieren je nach Betriebssystem und CUDA-Anforderungen, daher wird empfohlen, PyTorch zuerst gem\u00e4\u00df den Anweisungen unter https://pytorch.org/get-started/locally zu installieren.</p> <p> </p>"},{"location":"quickstart/#conda-docker-image","title":"Conda Docker-Image","text":"<p>Ultralytics Conda Docker-Images sind ebenfalls von DockerHub verf\u00fcgbar. Diese Bilder basieren auf Miniconda3 und bieten eine einfache M\u00f6glichkeit, <code>ultralytics</code> in einer Conda-Umgebung zu nutzen.</p> <pre><code># Setze Image-Name als Variable\nt=ultralytics/ultralytics:latest-conda\n\n# Ziehe das neuste ultralytics Image von Docker Hub\nsudo docker pull $t\n\n# F\u00fchre das ultralytics Image in einem Container mit GPU-Unterst\u00fctzung aus\nsudo docker run -it --ipc=host --gpus all $t  # alle GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # spezifische GPUs angeben\n</code></pre>"},{"location":"quickstart/#ultralytics-mit-cli-verwenden","title":"Ultralytics mit CLI verwenden","text":"<p>Die Befehlszeilenschnittstelle (CLI) von Ultralytics erm\u00f6glicht einfache Einzeilige Befehle ohne die Notwendigkeit einer Python-Umgebung. CLI erfordert keine Anpassung oder Python-Code. Sie k\u00f6nnen alle Aufgaben einfach vom Terminal aus mit dem <code>yolo</code> Befehl ausf\u00fchren. Schauen Sie sich den CLI-Leitfaden an, um mehr \u00fcber die Verwendung von YOLOv8 \u00fcber die Befehlszeile zu erfahren.</p> <p>Beispiel</p> SyntaxTrainierenVorhersagenValExportierenSpeziell <p>Ultralytics <code>yolo</code> Befehle verwenden die folgende Syntax: <pre><code>yolo TASK MODE ARGS\n\nWo   TASK (optional) einer von [detect, segment, classify] ist\n        MODE (erforderlich) einer von [train, val, predict, export, track] ist\n        ARGS (optional) eine beliebige Anzahl von benutzerdefinierten 'arg=value' Paaren wie 'imgsz=320', die Vorgaben \u00fcberschreiben.\n</code></pre> Sehen Sie alle ARGS im vollst\u00e4ndigen Konfigurationsleitfaden oder mit <code>yolo cfg</code></p> <p>Trainieren Sie ein Erkennungsmodell f\u00fcr 10 Epochen mit einer Anfangslernerate von 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Vorhersagen eines YouTube-Videos mit einem vortrainierten Segmentierungsmodell bei einer Bildgr\u00f6\u00dfe von 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Val ein vortrainiertes Erkennungsmodell bei Batch-Gr\u00f6\u00dfe 1 und Bildgr\u00f6\u00dfe 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportieren Sie ein YOLOv8n-Klassifikationsmodell im ONNX-Format bei einer Bildgr\u00f6\u00dfe von 224 mal 128 (kein TASK erforderlich) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>F\u00fchren Sie spezielle Befehle aus, um Version, Einstellungen zu sehen, Checks auszuf\u00fchren und mehr: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Warnung</p> <p>Argumente m\u00fcssen als <code>arg=val</code> Paare \u00fcbergeben werden, getrennt durch ein Gleichheitszeichen <code>=</code> und durch Leerzeichen <code></code> zwischen den Paaren. Verwenden Sie keine <code>--</code> Argumentpr\u00e4fixe oder Kommata <code>,</code> zwischen den Argumenten.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>CLI-Leitfaden</p>"},{"location":"quickstart/#ultralytics-mit-python-verwenden","title":"Ultralytics mit Python verwenden","text":"<p>Die Python-Schnittstelle von YOLOv8 erm\u00f6glicht eine nahtlose Integration in Ihre Python-Projekte und erleichtert das Laden, Ausf\u00fchren und Verarbeiten der Modellausgabe. Konzipiert f\u00fcr Einfachheit und Benutzerfreundlichkeit, erm\u00f6glicht die Python-Schnittstelle Benutzern, Objekterkennung, Segmentierung und Klassifizierung schnell in ihren Projekten zu implementieren. Dies macht die Python-Schnittstelle von YOLOv8 zu einem unsch\u00e4tzbaren Werkzeug f\u00fcr jeden, der diese Funktionalit\u00e4ten in seine Python-Projekte integrieren m\u00f6chte.</p> <p>Benutzer k\u00f6nnen beispielsweise ein Modell laden, es trainieren, seine Leistung an einem Validierungsset auswerten und sogar in das ONNX-Format exportieren, und das alles mit nur wenigen Codezeilen. Schauen Sie sich den Python-Leitfaden an, um mehr \u00fcber die Verwendung von YOLOv8 in Ihren_python_pro_jek_ten zu erfahren.</p> <p>Beispiel</p> <pre><code>from ultralytics import YOLO\n\n# Erstellen Sie ein neues YOLO Modell von Grund auf\nmodel = YOLO('yolov8n.yaml')\n\n# Laden Sie ein vortrainiertes YOLO Modell (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n.pt')\n\n# Trainieren Sie das Modell mit dem Datensatz 'coco128.yaml' f\u00fcr 3 Epochen\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Bewerten Sie die Leistung des Modells am Validierungssatz\nresults = model.val()\n\n# F\u00fchren Sie eine Objekterkennung an einem Bild mit dem Modell durch\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportieren Sie das Modell ins ONNX-Format\nsuccess = model.export(format='onnx')\n</code></pre> <p>Python-Leitfaden</p>"},{"location":"datasets/","title":"\u00dcbersicht \u00fcber Datens\u00e4tze","text":"<p>Ultralytics bietet Unterst\u00fctzung f\u00fcr verschiedene Datens\u00e4tze an, um Computervisionsaufgaben wie Erkennung, Instanzsegmentierung, Posensch\u00e4tzung, Klassifizierung und Verfolgung mehrerer Objekte zu erleichtern. Unten finden Sie eine Liste der wichtigsten Ultralytics-Datens\u00e4tze, gefolgt von einer Zusammenfassung jeder Computervisionsaufgabe und den jeweiligen Datens\u00e4tzen.</p> <p>Note</p> <p>\ud83d\udea7 Unsere mehrsprachige Dokumentation befindet sich derzeit im Aufbau und wir arbeiten intensiv an deren Verbesserung. Vielen Dank f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"datasets/#erkennungsdatensatze","title":"Erkennungsdatens\u00e4tze","text":"<p>Die Objekterkennung mittels Bounding Box ist eine Computervisionstechnik, die das Erkennen und Lokalisieren von Objekten in einem Bild anhand des Zeichnens einer Bounding Box um jedes Objekt beinhaltet.</p> <ul> <li>Argoverse: Ein Datensatz mit 3D-Tracking- und Bewegungsvorhersagedaten aus st\u00e4dtischen Umgebungen mit umfassenden Annotationen.</li> <li>COCO: Ein umfangreicher Datensatz f\u00fcr Objekterkennung, Segmentierung und Beschreibung mit \u00fcber 200.000 beschrifteten Bildern.</li> <li>COCO8: Enth\u00e4lt die ersten 4 Bilder aus COCO Train und COCO Val, geeignet f\u00fcr schnelle Tests.</li> <li>Global Wheat 2020: Ein Datensatz mit Bildern von Weizenk\u00f6pfen aus aller Welt f\u00fcr Objekterkennungs- und Lokalisierungsaufgaben.</li> <li>Objects365: Ein hochwertiger, gro\u00dfer Datensatz f\u00fcr Objekterkennung mit 365 Objektkategorien und \u00fcber 600.000 annotierten Bildern.</li> <li>OpenImagesV7: Ein umfassender Datensatz von Google mit 1,7 Millionen Trainingsbildern und 42.000 Validierungsbildern.</li> <li>SKU-110K: Ein Datensatz mit dichter Objekterkennung in Einzelhandelsumgebungen mit \u00fcber 11.000 Bildern und 1,7 Millionen Bounding Boxen.</li> <li>VisDrone: Ein Datensatz mit Objekterkennungs- und Multi-Objekt-Tracking-Daten aus Drohnenaufnahmen mit \u00fcber 10.000 Bildern und Videosequenzen.</li> <li>VOC: Der Pascal Visual Object Classes (VOC) Datensatz f\u00fcr Objekterkennung und Segmentierung mit 20 Objektklassen und \u00fcber 11.000 Bildern.</li> <li>xView: Ein Datensatz f\u00fcr Objekterkennung in \u00dcberwachungsbildern mit 60 Objektkategorien und \u00fcber 1 Million annotierten Objekten.</li> </ul>"},{"location":"datasets/#datensatze-fur-instanzsegmentierung","title":"Datens\u00e4tze f\u00fcr Instanzsegmentierung","text":"<p>Die Instanzsegmentierung ist eine Computervisionstechnik, die das Identifizieren und Lokalisieren von Objekten in einem Bild auf Pixelebene beinhaltet.</p> <ul> <li>COCO: Ein gro\u00dfer Datensatz f\u00fcr Objekterkennung, Segmentierung und Beschreibungsaufgaben mit \u00fcber 200.000 beschrifteten Bildern.</li> <li>COCO8-seg: Ein kleinerer Datensatz f\u00fcr Instanzsegmentierungsaufgaben, der eine Teilmenge von 8 COCO-Bildern mit Segmentierungsannotationen enth\u00e4lt.</li> </ul>"},{"location":"datasets/#posenschatzung","title":"Posensch\u00e4tzung","text":"<p>Die Posensch\u00e4tzung ist eine Technik, die verwendet wird, um die Position des Objekts relativ zur Kamera oder zum Weltkoordinatensystem zu bestimmen.</p> <ul> <li>COCO: Ein gro\u00dfer Datensatz mit menschlichen Pose-Annotationen f\u00fcr Posensch\u00e4tzungsaufgaben.</li> <li>COCO8-pose: Ein kleinerer Datensatz f\u00fcr Posensch\u00e4tzungsaufgaben, der eine Teilmenge von 8 COCO-Bildern mit menschlichen Pose-Annotationen enth\u00e4lt.</li> <li>Tiger-pose: Ein kompakter Datensatz bestehend aus 263 Bildern, die auf Tiger fokussiert sind, mit Annotationen von 12 Schl\u00fcsselpunkten pro Tiger f\u00fcr Posensch\u00e4tzungsaufgaben.</li> </ul>"},{"location":"datasets/#bildklassifizierung","title":"Bildklassifizierung","text":"<p>Die Bildklassifizierung ist eine Computervisionsaufgabe, bei der ein Bild basierend auf seinem visuellen Inhalt in eine oder mehrere vordefinierte Klassen oder Kategorien eingeteilt wird.</p> <ul> <li>Caltech 101: Enth\u00e4lt Bilder von 101 Objektkategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>Caltech 256: Eine erweiterte Version von Caltech 101 mit 256 Objektkategorien und herausfordernderen Bildern.</li> <li>CIFAR-10: Ein Datensatz mit 60.000 32x32 Farbbildern in 10 Klassen, mit 6.000 Bildern pro Klasse.</li> <li>CIFAR-100: Eine erweiterte Version von CIFAR-10 mit 100 Objektkategorien und 600 Bildern pro Klasse.</li> <li>Fashion-MNIST: Ein Datensatz mit 70.000 Graustufenbildern von 10 Modekategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>ImageNet: Ein gro\u00dfer Datensatz f\u00fcr Objekterkennung und Bildklassifizierung mit \u00fcber 14 Millionen Bildern und 20.000 Kategorien.</li> <li>ImageNet-10: Ein kleinerer Teildatensatz von ImageNet mit 10 Kategorien f\u00fcr schnelleres Experimentieren und Testen.</li> <li>Imagenette: Ein kleinerer Teildatensatz von ImageNet, der 10 leicht unterscheidbare Klassen f\u00fcr ein schnelleres Training und Testen enth\u00e4lt.</li> <li>Imagewoof: Ein herausfordernderer Teildatensatz von ImageNet mit 10 Hundezuchtkategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>MNIST: Ein Datensatz mit 70.000 Graustufenbildern von handgeschriebenen Ziffern f\u00fcr Bildklassifizierungsaufgaben.</li> </ul>"},{"location":"datasets/#orientierte-bounding-boxes-obb","title":"Orientierte Bounding Boxes (OBB)","text":"<p>Orientierte Bounding Boxes (OBB) ist eine Methode in der Computervision f\u00fcr die Erkennung von geneigten Objekten in Bildern mithilfe von rotierten Bounding Boxen, die oft auf Luft- und Satellitenbilder angewendet wird.</p> <ul> <li>DOTAv2: Ein beliebter OBB-Datensatz f\u00fcr Luftbildaufnahmen mit 1,7 Millionen Instanzen und 11.268 Bildern.</li> </ul>"},{"location":"datasets/#multi-objekt-verfolgung","title":"Multi-Objekt-Verfolgung","text":"<p>Die Verfolgung mehrerer Objekte ist eine Computervisionstechnik, die das Erkennen und Verfolgen mehrerer Objekte \u00fcber die Zeit in einer Videosequenz beinhaltet.</p> <ul> <li>Argoverse: Ein Datensatz mit 3D-Tracking- und Bewegungsvorhersagedaten aus st\u00e4dtischen Umgebungen mit umfassenden Annotationen f\u00fcr Multi-Objekt-Verfolgungsaufgaben.</li> <li>VisDrone: Ein Datensatz mit Daten zur Objekterkennung und Multi-Objekt-Verfolgung aus Drohnenaufnahmen mit \u00fcber 10.000 Bildern und Videosequenzen.</li> </ul>"},{"location":"datasets/#neue-datensatze-beitragen","title":"Neue Datens\u00e4tze beitragen","text":"<p>Das Bereitstellen eines neuen Datensatzes umfasst mehrere Schritte, um sicherzustellen, dass er gut in die bestehende Infrastruktur integriert werden kann. Unten finden Sie die notwendigen Schritte:</p>"},{"location":"datasets/#schritte-um-einen-neuen-datensatz-beizutragen","title":"Schritte um einen neuen Datensatz beizutragen","text":"<ol> <li> <p>Bilder sammeln: Sammeln Sie die Bilder, die zum Datensatz geh\u00f6ren. Diese k\u00f6nnen von verschiedenen Quellen gesammelt werden, wie \u00f6ffentlichen Datenbanken oder Ihrer eigenen Sammlung.</p> </li> <li> <p>Bilder annotieren: Annotieren Sie diese Bilder mit Bounding Boxen, Segmenten oder Schl\u00fcsselpunkten, je nach Aufgabe.</p> </li> <li> <p>Annotationen exportieren: Konvertieren Sie diese Annotationen in das von Ultralytics unterst\u00fctzte YOLO *.txt-Dateiformat.</p> </li> <li> <p>Datensatz organisieren: Ordnen Sie Ihren Datensatz in die richtige Ordnerstruktur an. Sie sollten \u00fcbergeordnete Verzeichnisse <code>train/</code> und <code>val/</code> haben, und innerhalb dieser je ein Unterverzeichnis <code>images/</code> und <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Eine <code>data.yaml</code>-Datei erstellen: Erstellen Sie in Ihrem Stammverzeichnis des Datensatzes eine Datei <code>data.yaml</code>, die den Datensatz, die Klassen und andere notwendige Informationen beschreibt.</p> </li> <li> <p>Bilder optimieren (Optional): Wenn Sie die Gr\u00f6\u00dfe des Datensatzes f\u00fcr eine effizientere Verarbeitung reduzieren m\u00f6chten, k\u00f6nnen Sie die Bilder mit dem untenstehenden Code optimieren. Dies ist nicht erforderlich, wird aber f\u00fcr kleinere Datensatzgr\u00f6\u00dfen und schnellere Download-Geschwindigkeiten empfohlen.</p> </li> <li> <p>Datensatz zippen: Komprimieren Sie das gesamte Datensatzverzeichnis in eine Zip-Datei.</p> </li> <li> <p>Dokumentation und PR: Erstellen Sie eine Dokumentationsseite, die Ihren Datensatz beschreibt und wie er in das bestehende Framework passt. Danach reichen Sie einen Pull Request (PR) ein. Weitere Details zur Einreichung eines PR finden Sie in den Ultralytics Beitragshinweisen.</p> </li> </ol>"},{"location":"datasets/#beispielcode-zum-optimieren-und-zippen-eines-datensatzes","title":"Beispielcode zum Optimieren und Zippen eines Datensatzes","text":"<p>Optimieren und Zippen eines Datensatzes</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definieren des Verzeichnisses des Datensatzes\npath = Path('Pfad/zum/Datensatz')\n\n# Bilder im Datensatz optimieren (optional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Datensatz in 'Pfad/zum/Datensatz.zip' zippen\nzip_directory(path)\n</code></pre> <p>Indem Sie diesen Schritten folgen, k\u00f6nnen Sie einen neuen Datensatz beitragen, der gut in die bestehende Struktur von Ultralytics integriert wird.</p>"},{"location":"models/","title":"Von Ultralytics unterst\u00fctzte Modelle","text":"<p>Willkommen in der Modell-Dokumentation von Ultralytics! Wir bieten Unterst\u00fctzung f\u00fcr eine breite Palette von Modellen, die f\u00fcr spezifische Aufgaben wie Objekterkennung, Instanzsegmentierung, Bildklassifizierung, Poseerkennung und Multi-Objekt-Tracking zugeschnitten sind. Wenn Sie daran interessiert sind, Ihre Modellarchitektur an Ultralytics beizutragen, werfen Sie einen Blick auf unseren Beitragenden-Leitfaden.</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere mehrsprachige Dokumentation befindet sich derzeit im Aufbau, und wir arbeiten hart daran, sie zu verbessern. Vielen Dank f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"models/#vorgestellte-modelle","title":"Vorgestellte Modelle","text":"<p>Hier sind einige der wesentlichen unterst\u00fctzten Modelle:</p> <ol> <li>YOLOv3: Die dritte Iteration der YOLO-Modellfamilie, urspr\u00fcnglich von Joseph Redmon entwickelt und bekannt f\u00fcr ihre effiziente Echtzeit-Objekterkennung.</li> <li>YOLOv4: Eine darknet-native Aktualisierung von YOLOv3, die 2020 von Alexey Bochkovskiy ver\u00f6ffentlicht wurde.</li> <li>YOLOv5: Eine verbesserte Version der YOLO-Architektur von Ultralytics, die im Vergleich zu fr\u00fcheren Versionen bessere Leistungs- und Geschwindigkeitstrade-offs bietet.</li> <li>YOLOv6: Im Jahr 2022 von Meituan ver\u00f6ffentlicht und in vielen autonomen Zustellrobotern des Unternehmens verwendet.</li> <li>YOLOv7: Im Jahr 2022 von den Autoren von YOLOv4 aktualisierte YOLO-Modelle.</li> <li>YOLOv8: Die neueste Version der YOLO-Familie mit erweiterten F\u00e4higkeiten wie Instanzsegmentierung, Pose-/Schl\u00fcsselpunktsch\u00e4tzung und Klassifizierung.</li> <li>Segment Anything Model (SAM): Metas Segment Anything Model (SAM).</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM f\u00fcr mobile Anwendungen von der Kyung Hee Universit\u00e4t.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM von der Bild- und Videoanalysegruppe des Instituts f\u00fcr Automatisierung, Chinesische Akademie der Wissenschaften.</li> <li>YOLO-NAS: YOLO Neural Architecture Search (NAS) Modelle.</li> <li>Realtime Detection Transformers (RT-DETR): Baidus PaddlePaddle Realtime Detection Transformer (RT-DETR) Modelle.</li> </ol> <p> Sehen Sie: Ultralytics YOLO-Modelle in nur wenigen Zeilen Code ausf\u00fchren. </p>"},{"location":"models/#erste-schritte-anwendungsbeispiele","title":"Erste Schritte: Anwendungsbeispiele","text":"PythonCLI <p>PyTorch vortrainierte <code>*.pt</code> Modelle sowie Konfigurations-<code>*.yaml</code> Dateien k\u00f6nnen den Klassen <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> und <code>RTDETR()</code> \u00fcbergeben werden, um in Python eine Modellinstanz zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Laden eines auf COCO vortrainierten YOLOv8n-Modells\nmodel = YOLO('yolov8n.pt')\n\n# Modellinformationen anzeigen (optional)\nmodel.info()\n\n# Das Modell mit dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen trainieren\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Inferenz mit dem YOLOv8n-Modell am Bild 'bus.jpg' durchf\u00fchren\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle sind verf\u00fcgbar, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Laden eines auf COCO vortrainierten YOLOv8n-Modells und Trainieren auf dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Laden eines auf COCO vortrainierten YOLOv8n-Modells und Durchf\u00fchrung der Inferenz am Bild 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#neue-modelle-beitragen","title":"Neue Modelle beitragen","text":"<p>Interessiert, Ihr Modell bei Ultralytics beizutragen? Gro\u00dfartig! Wir sind immer offen, unser Modellportfolio zu erweitern.</p> <ol> <li> <p>Das Repository forken: Beginnen Sie damit, das GitHub-Repository von Ultralytics zu forken.</p> </li> <li> <p>Ihren Fork klonen: Klonen Sie Ihren Fork auf Ihre lokale Maschine und erstellen Sie einen neuen Branch, um daran zu arbeiten.</p> </li> <li> <p>Ihr Modell implementieren: F\u00fcgen Sie Ihr Modell gem\u00e4\u00df den in unserem Beitragenden-Leitfaden bereitgestellten Codierstandards und Richtlinien hinzu.</p> </li> <li> <p>Gr\u00fcndlich testen: Stellen Sie sicher, dass Sie Ihr Modell sowohl isoliert als auch als Teil der Pipeline rigoros testen.</p> </li> <li> <p>Einen Pull Request erstellen: Wenn Sie mit Ihrem Modell zufrieden sind, erstellen Sie einen Pull Request zum Hauptrepository zur \u00dcberpr\u00fcfung.</p> </li> <li> <p>Code-\u00dcberpr\u00fcfung und Merging: Nach der \u00dcberpr\u00fcfung wird Ihr Modell, wenn es unseren Kriterien entspricht, in das Hauptrepository \u00fcbernommen.</p> </li> </ol> <p>F\u00fcr detaillierte Schritte konsultieren Sie unseren Beitragenden-Leitfaden.</p>"},{"location":"modes/","title":"Ultralytics YOLOv8 Modi","text":""},{"location":"modes/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Ultralytics YOLOv8 ist nicht nur ein weiteres Objekterkennungsmodell; es ist ein vielseitiges Framework, das den gesamten Lebenszyklus von Machine-Learning-Modellen abdeckt - von der Dateneingabe und dem Modelltraining \u00fcber die Validierung und Bereitstellung bis hin zum Tracking in der realen Welt. Jeder Modus dient einem bestimmten Zweck und ist darauf ausgelegt, Ihnen die Flexibilit\u00e4t und Effizienz zu bieten, die f\u00fcr verschiedene Aufgaben und Anwendungsf\u00e4lle erforderlich ist.</p> <p> Anschauen: Ultralytics Modi Tutorial: Trainieren, Validieren, Vorhersagen, Exportieren &amp; Benchmarking. </p>"},{"location":"modes/#modi-im-uberblick","title":"Modi im \u00dcberblick","text":"<p>Das Verst\u00e4ndnis der verschiedenen Modi, die Ultralytics YOLOv8 unterst\u00fctzt, ist entscheidend, um das Beste aus Ihren Modellen herauszuholen:</p> <ul> <li>Train-Modus: Verfeinern Sie Ihr Modell mit angepassten oder vorgeladenen Datens\u00e4tzen.</li> <li>Val-Modus: Eine Nachtrainingspr\u00fcfung zur Validierung der Modellleistung.</li> <li>Predict-Modus: Entfesseln Sie die Vorhersagekraft Ihres Modells mit realen Daten.</li> <li>Export-Modus: Machen Sie Ihr Modell in verschiedenen Formaten einsatzbereit.</li> <li>Track-Modus: Erweitern Sie Ihr Objekterkennungsmodell um Echtzeit-Tracking-Anwendungen.</li> <li>Benchmark-Modus: Analysieren Sie die Geschwindigkeit und Genauigkeit Ihres Modells in verschiedenen Einsatzumgebungen.</li> </ul> <p>Dieser umfassende Leitfaden soll Ihnen einen \u00dcberblick und praktische Einblicke in jeden Modus geben, um Ihnen zu helfen, das volle Potenzial von YOLOv8 zu nutzen.</p>"},{"location":"modes/#trainieren","title":"Trainieren","text":"<p>Der Trainingsmodus wird verwendet, um ein YOLOv8-Modell mit einem angepassten Datensatz zu trainieren. In diesem Modus wird das Modell mit dem angegebenen Datensatz und den Hyperparametern trainiert. Der Trainingsprozess beinhaltet die Optimierung der Modellparameter, damit es die Klassen und Standorte von Objekten in einem Bild genau vorhersagen kann.</p> <p>Trainingsbeispiele</p>"},{"location":"modes/#validieren","title":"Validieren","text":"<p>Der Validierungsmodus wird genutzt, um ein YOLOv8-Modell nach dem Training zu bewerten. In diesem Modus wird das Modell auf einem Validierungsset getestet, um seine Genauigkeit und Generalisierungsleistung zu messen. Dieser Modus kann verwendet werden, um die Hyperparameter des Modells f\u00fcr eine bessere Leistung zu optimieren.</p> <p>Validierungsbeispiele</p>"},{"location":"modes/#vorhersagen","title":"Vorhersagen","text":"<p>Der Vorhersagemodus wird verwendet, um mit einem trainierten YOLOv8-Modell Vorhersagen f\u00fcr neue Bilder oder Videos zu treffen. In diesem Modus wird das Modell aus einer Checkpoint-Datei geladen, und der Benutzer kann Bilder oder Videos zur Inferenz bereitstellen. Das Modell sagt die Klassen und Standorte von Objekten in den Eingabebildern oder -videos voraus.</p> <p>Vorhersagebeispiele</p>"},{"location":"modes/#exportieren","title":"Exportieren","text":"<p>Der Exportmodus wird verwendet, um ein YOLOv8-Modell in ein Format zu exportieren, das f\u00fcr die Bereitstellung verwendet werden kann. In diesem Modus wird das Modell in ein Format konvertiert, das von anderen Softwareanwendungen oder Hardwareger\u00e4ten verwendet werden kann. Dieser Modus ist n\u00fctzlich, wenn das Modell in Produktionsumgebungen eingesetzt wird.</p> <p>Exportbeispiele</p>"},{"location":"modes/#verfolgen","title":"Verfolgen","text":"<p>Der Trackingmodus wird zur Echtzeitverfolgung von Objekten mit einem YOLOv8-Modell verwendet. In diesem Modus wird das Modell aus einer Checkpoint-Datei geladen, und der Benutzer kann einen Live-Videostream f\u00fcr das Echtzeitobjekttracking bereitstellen. Dieser Modus ist n\u00fctzlich f\u00fcr Anwendungen wie \u00dcberwachungssysteme oder selbstfahrende Autos.</p> <p>Trackingbeispiele</p>"},{"location":"modes/#benchmarking","title":"Benchmarking","text":"<p>Der Benchmark-Modus wird verwendet, um die Geschwindigkeit und Genauigkeit verschiedener Exportformate f\u00fcr YOLOv8 zu profilieren. Die Benchmarks liefern Informationen \u00fcber die Gr\u00f6\u00dfe des exportierten Formats, seine <code>mAP50-95</code>-Metriken (f\u00fcr Objekterkennung, Segmentierung und Pose) oder <code>accuracy_top5</code>-Metriken (f\u00fcr Klassifizierung) und die Inferenzzeit in Millisekunden pro Bild f\u00fcr verschiedene Exportformate wie ONNX, OpenVINO, TensorRT und andere. Diese Informationen k\u00f6nnen den Benutzern dabei helfen, das optimale Exportformat f\u00fcr ihren spezifischen Anwendungsfall basierend auf ihren Anforderungen an Geschwindigkeit und Genauigkeit auszuw\u00e4hlen.</p> <p>Benchmarkbeispiele</p>"},{"location":"modes/benchmark/","title":"Modell-Benchmarking mit Ultralytics YOLO","text":""},{"location":"modes/benchmark/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Nachdem Ihr Modell trainiert und validiert wurde, ist der n\u00e4chste logische Schritt, seine Leistung in verschiedenen realen Szenarien zu bewerten. Der Benchmark-Modus in Ultralytics YOLOv8 dient diesem Zweck, indem er einen robusten Rahmen f\u00fcr die Beurteilung von Geschwindigkeit und Genauigkeit Ihres Modells \u00fcber eine Reihe von Exportformaten hinweg bietet.</p>"},{"location":"modes/benchmark/#warum-ist-benchmarking-entscheidend","title":"Warum ist Benchmarking entscheidend?","text":"<ul> <li>Informierte Entscheidungen: Erhalten Sie Einblicke in die Kompromisse zwischen Geschwindigkeit und Genauigkeit.</li> <li>Ressourcenzuweisung: Verstehen Sie, wie sich verschiedene Exportformate auf unterschiedlicher Hardware verhalten.</li> <li>Optimierung: Erfahren Sie, welches Exportformat die beste Leistung f\u00fcr Ihren spezifischen Anwendungsfall bietet.</li> <li>Kosteneffizienz: Nutzen Sie Hardware-Ressourcen basierend auf den Benchmark-Ergebnissen effizienter.</li> </ul>"},{"location":"modes/benchmark/#schlusselmetriken-im-benchmark-modus","title":"Schl\u00fcsselmetriken im Benchmark-Modus","text":"<ul> <li>mAP50-95: F\u00fcr Objekterkennung, Segmentierung und Posensch\u00e4tzung.</li> <li>accuracy_top5: F\u00fcr die Bildklassifizierung.</li> <li>Inferenzzeit: Zeit, die f\u00fcr jedes Bild in Millisekunden ben\u00f6tigt wird.</li> </ul>"},{"location":"modes/benchmark/#unterstutzte-exportformate","title":"Unterst\u00fctzte Exportformate","text":"<ul> <li>ONNX: F\u00fcr optimale CPU-Leistung</li> <li>TensorRT: F\u00fcr maximale GPU-Effizienz</li> <li>OpenVINO: F\u00fcr die Optimierung von Intel-Hardware</li> <li>CoreML, TensorFlow SavedModel, und mehr: F\u00fcr vielf\u00e4ltige Deployment-Anforderungen.</li> </ul> <p>Tipp</p> <ul> <li>Exportieren Sie in ONNX oder OpenVINO f\u00fcr bis zu 3x CPU-Beschleunigung.</li> <li>Exportieren Sie in TensorRT f\u00fcr bis zu 5x GPU-Beschleunigung.</li> </ul>"},{"location":"modes/benchmark/#anwendungsbeispiele","title":"Anwendungsbeispiele","text":"<p>F\u00fchren Sie YOLOv8n-Benchmarks auf allen unterst\u00fctzten Exportformaten einschlie\u00dflich ONNX, TensorRT usw. durch. Siehe den Abschnitt Argumente unten f\u00fcr eine vollst\u00e4ndige Liste der Exportargumente.</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark auf GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumente","title":"Argumente","text":"<p>Argumente wie <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> und <code>verbose</code> bieten Benutzern die Flexibilit\u00e4t, die Benchmarks auf ihre spezifischen Bed\u00fcrfnisse abzustimmen und die Leistung verschiedener Exportformate m\u00fchelos zu vergleichen.</p> Schl\u00fcssel Wert Beschreibung <code>model</code> <code>None</code> Pfad zur Modelldatei, z. B. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> Pfad zur YAML, die das Benchmarking-Dataset referenziert (unter <code>val</code>-Kennzeichnung) <code>imgsz</code> <code>640</code> Bildgr\u00f6\u00dfe als Skalar oder Liste (h, w), z. B. (640, 480) <code>half</code> <code>False</code> FP16-Quantisierung <code>int8</code> <code>False</code> INT8-Quantisierung <code>device</code> <code>None</code> Ger\u00e4t zum Ausf\u00fchren, z. B. CUDA device=0 oder device=0,1,2,3 oder device=cpu <code>verbose</code> <code>False</code> bei Fehlern nicht fortsetzen (bool), oder Wertebereichsschwelle (float)"},{"location":"modes/benchmark/#exportformate","title":"Exportformate","text":"<p>Benchmarks werden automatisch auf allen m\u00f6glichen Exportformaten unten ausgef\u00fchrt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vollst\u00e4ndige Details zum <code>export</code> finden Sie auf der Export-Seite.</p>"},{"location":"modes/export/","title":"Modell-Export mit Ultralytics YOLO","text":""},{"location":"modes/export/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Das ultimative Ziel des Trainierens eines Modells besteht darin, es f\u00fcr reale Anwendungen einzusetzen. Der Exportmodus in Ultralytics YOLOv8 bietet eine vielseitige Palette von Optionen f\u00fcr den Export Ihres trainierten Modells in verschiedene Formate, sodass es auf verschiedenen Plattformen und Ger\u00e4ten eingesetzt werden kann. Dieser umfassende Leitfaden soll Sie durch die Nuancen des Modell-Exports f\u00fchren und zeigen, wie Sie maximale Kompatibilit\u00e4t und Leistung erzielen k\u00f6nnen.</p> <p> Ansehen: Wie man ein benutzerdefiniertes trainiertes Ultralytics YOLOv8-Modell exportiert und Live-Inferenz auf der Webcam ausf\u00fchrt. </p>"},{"location":"modes/export/#warum-den-exportmodus-von-yolov8-wahlen","title":"Warum den Exportmodus von YOLOv8 w\u00e4hlen?","text":"<ul> <li>Vielseitigkeit: Export in verschiedene Formate einschlie\u00dflich ONNX, TensorRT, CoreML und mehr.</li> <li>Leistung: Bis zu 5-fache GPU-Beschleunigung mit TensorRT und 3-fache CPU-Beschleunigung mit ONNX oder OpenVINO.</li> <li>Kompatibilit\u00e4t: Machen Sie Ihr Modell universell einsetzbar in zahlreichen Hardware- und Softwareumgebungen.</li> <li>Benutzerfreundlichkeit: Einfache CLI- und Python-API f\u00fcr schnellen und unkomplizierten Modell-Export.</li> </ul>"},{"location":"modes/export/#schlusselfunktionen-des-exportmodus","title":"Schl\u00fcsselfunktionen des Exportmodus","text":"<p>Hier sind einige der herausragenden Funktionen:</p> <ul> <li>Ein-Klick-Export: Einfache Befehle f\u00fcr den Export in verschiedene Formate.</li> <li>Batch-Export: Export von Modellen, die Batch-Inferenz unterst\u00fctzen.</li> <li>Optimiertes Inferenzverhalten: Exportierte Modelle sind f\u00fcr schnellere Inferenzzeiten optimiert.</li> <li>Tutorial-Videos: Ausf\u00fchrliche Anleitungen und Tutorials f\u00fcr ein reibungsloses Exporterlebnis.</li> </ul> <p>Tipp</p> <ul> <li>Exportieren Sie nach ONNX oder OpenVINO f\u00fcr bis zu 3-fache CPU-Beschleunigung.</li> <li>Exportieren Sie nach TensorRT f\u00fcr bis zu 5-fache GPU-Beschleunigung.</li> </ul>"},{"location":"modes/export/#nutzungsbeispiele","title":"Nutzungsbeispiele","text":"<p>Exportieren Sie ein YOLOv8n-Modell in ein anderes Format wie ONNX oder TensorRT. Weitere Informationen zu den Exportargumenten finden Sie im Abschnitt \u201eArgumente\u201c unten.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden eines Modells\nmodel = YOLO('yolov8n.pt')  # offizielles Modell laden\nmodel = YOLO('path/to/best.pt')  # benutzerdefiniertes trainiertes Modell laden\n\n# Exportieren des Modells\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # offizielles Modell exportieren\nyolo export model=path/to/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre>"},{"location":"modes/export/#argumente","title":"Argumente","text":"<p>Exporteinstellungen f\u00fcr YOLO-Modelle beziehen sich auf verschiedene Konfigurationen und Optionen, die verwendet werden, um das Modell zu speichern oder f\u00fcr den Einsatz in anderen Umgebungen oder Plattformen zu exportieren. Diese Einstellungen k\u00f6nnen die Leistung, Gr\u00f6\u00dfe und Kompatibilit\u00e4t des Modells mit verschiedenen Systemen beeinflussen. Zu den g\u00e4ngigen Exporteinstellungen von YOLO geh\u00f6ren das Format der exportierten Modelldatei (z. B. ONNX, TensorFlow SavedModel), das Ger\u00e4t, auf dem das Modell ausgef\u00fchrt wird (z. B. CPU, GPU) und das Vorhandensein zus\u00e4tzlicher Funktionen wie Masken oder mehrere Labels pro Box. Andere Faktoren, die den Exportprozess beeinflussen k\u00f6nnen, sind die spezifische Aufgabe, f\u00fcr die das Modell verwendet wird, und die Anforderungen oder Einschr\u00e4nkungen der Zielumgebung oder -plattform. Es ist wichtig, diese Einstellungen sorgf\u00e4ltig zu ber\u00fccksichtigen und zu konfigurieren, um sicherzustellen, dass das exportierte Modell f\u00fcr den beabsichtigten Einsatzzweck optimiert ist und in der Zielumgebung effektiv eingesetzt werden kann.</p> Schl\u00fcssel Wert Beschreibung <code>format</code> <code>'torchscript'</code> Format f\u00fcr den Export <code>imgsz</code> <code>640</code> Bildgr\u00f6\u00dfe als Skalar oder (h, w)-Liste, z.B. (640, 480) <code>keras</code> <code>False</code> Verwendung von Keras f\u00fcr TensorFlow SavedModel-Export <code>optimize</code> <code>False</code> TorchScript: Optimierung f\u00fcr mobile Ger\u00e4te <code>half</code> <code>False</code> FP16-Quantisierung <code>int8</code> <code>False</code> INT8-Quantisierung <code>dynamic</code> <code>False</code> ONNX/TensorRT: dynamische Achsen <code>simplify</code> <code>False</code> ONNX/TensorRT: Vereinfachung des Modells <code>opset</code> <code>None</code> ONNX: Opset-Version (optional, Standardwert ist neueste) <code>workspace</code> <code>4</code> TensorRT: Arbeitsbereichgr\u00f6\u00dfe (GB) <code>nms</code> <code>False</code> CoreML: Hinzuf\u00fcgen von NMS"},{"location":"modes/export/#exportformate","title":"Exportformate","text":"<p>Verf\u00fcgbare YOLOv8-Exportformate finden Sie in der Tabelle unten. Sie k\u00f6nnen in jedes Format exportieren, indem Sie das <code>format</code>-Argument verwenden, z. B. <code>format='onnx'</code> oder <code>format='engine'</code>.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Modellvorhersage mit Ultralytics YOLO","text":""},{"location":"modes/predict/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Im Bereich des maschinellen Lernens und der Computer Vision wird der Prozess des Verstehens visueller Daten als 'Inferenz' oder 'Vorhersage' bezeichnet. Ultralytics YOLOv8 bietet eine leistungsstarke Funktion, die als Prognosemodus bekannt ist und f\u00fcr eine hochleistungsf\u00e4hige, echtzeitf\u00e4hige Inferenz auf einer breiten Palette von Datenquellen zugeschnitten ist.</p> <p> Anschauen: Wie man die Ausgaben vom Ultralytics YOLOv8 Modell f\u00fcr individuelle Projekte extrahiert. </p>"},{"location":"modes/predict/#anwendungen-in-der-realen-welt","title":"Anwendungen in der realen Welt","text":"Herstellung Sport Sicherheit Erkennung von Fahrzeugersatzteilen Erkennung von Fu\u00dfballspielern Erkennung von st\u00fcrzenden Personen"},{"location":"modes/predict/#warum-ultralytics-yolo-fur-inferenz-nutzen","title":"Warum Ultralytics YOLO f\u00fcr Inferenz nutzen?","text":"<p>Hier sind Gr\u00fcnde, warum Sie den Prognosemodus von YOLOv8 f\u00fcr Ihre verschiedenen Inferenzanforderungen in Betracht ziehen sollten:</p> <ul> <li>Vielseitigkeit: F\u00e4hig, Inferenzen auf Bilder, Videos und sogar Live-Streams zu machen.</li> <li>Leistung: Entwickelt f\u00fcr Echtzeit-Hochgeschwindigkeitsverarbeitung ohne Genauigkeitsverlust.</li> <li>Einfache Bedienung: Intuitive Python- und CLI-Schnittstellen f\u00fcr schnelle Einsatzbereitschaft und Tests.</li> <li>Hohe Anpassbarkeit: Verschiedene Einstellungen und Parameter, um das Verhalten der Modellinferenz entsprechend Ihren spezifischen Anforderungen zu optimieren.</li> </ul>"},{"location":"modes/predict/#schlusselfunktionen-des-prognosemodus","title":"Schl\u00fcsselfunktionen des Prognosemodus","text":"<p>Der Prognosemodus von YOLOv8 ist robust und vielseitig konzipiert und verf\u00fcgt \u00fcber:</p> <ul> <li>Kompatibilit\u00e4t mit mehreren Datenquellen: Ganz gleich, ob Ihre Daten in Form von Einzelbildern, einer Bildersammlung, Videodateien oder Echtzeit-Videostreams vorliegen, der Prognosemodus deckt alles ab.</li> <li>Streaming-Modus: Nutzen Sie die Streaming-Funktion, um einen speichereffizienten Generator von <code>Results</code>-Objekten zu erzeugen. Aktivieren Sie dies, indem Sie <code>stream=True</code> in der Aufrufmethode des Predictors einstellen.</li> <li>Batchverarbeitung: Die M\u00f6glichkeit, mehrere Bilder oder Videoframes in einem einzigen Batch zu verarbeiten, wodurch die Inferenzzeit weiter verk\u00fcrzt wird.</li> <li>Integrationsfreundlich: Dank der flexiblen API leicht in bestehende Datenpipelines und andere Softwarekomponenten zu integrieren.</li> </ul> <p>Ultralytics YOLO-Modelle geben entweder eine Python-Liste von <code>Results</code>-Objekten zur\u00fcck, oder einen speichereffizienten Python-Generator von <code>Results</code>-Objekten, wenn <code>stream=True</code> beim Inferenzvorgang an das Modell \u00fcbergeben wird:</p> <p>Predict</p> Gibt eine Liste mit <code>stream=False</code> zur\u00fcckGibt einen Generator mit <code>stream=True</code> zur\u00fcck <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n.pt')  # vortrainiertes YOLOv8n Modell\n\n# Batch-Inferenz auf einer Liste von Bildern ausf\u00fchren\nresults = model(['im1.jpg', 'im2.jpg'])  # gibt eine Liste von Results-Objekten zur\u00fcck\n\n# Ergebnisliste verarbeiten\nfor result in results:\n    boxes = result.boxes  # Boxes-Objekt f\u00fcr Bbox-Ausgaben\n    masks = result.masks  # Masks-Objekt f\u00fcr Segmentierungsmasken-Ausgaben\n    keypoints = result.keypoints  # Keypoints-Objekt f\u00fcr Pose-Ausgaben\n    probs = result.probs  # Probs-Objekt f\u00fcr Klassifizierungs-Ausgaben\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n.pt')  # vortrainiertes YOLOv8n Modell\n\n# Batch-Inferenz auf einer Liste von Bildern ausf\u00fchren\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # gibt einen Generator von Results-Objekten zur\u00fcck\n\n# Generator von Ergebnissen verarbeiten\nfor result in results:\n    boxes = result.boxes  # Boxes-Objekt f\u00fcr Bbox-Ausgaben\n    masks = result.masks  # Masks-Objekt f\u00fcr Segmentierungsmasken-Ausgaben\n    keypoints = result.keypoints  # Keypoints-Objekt f\u00fcr Pose-Ausgaben\n    probs = result.probs  # Probs-Objekt f\u00fcr Klassifizierungs-Ausgaben\n</code></pre>"},{"location":"modes/predict/#inferenzquellen","title":"Inferenzquellen","text":"<p>YOLOv8 kann verschiedene Arten von Eingabequellen f\u00fcr die Inferenz verarbeiten, wie in der folgenden Tabelle gezeigt. Die Quellen umfassen statische Bilder, Videostreams und verschiedene Datenformate. Die Tabelle gibt ebenfalls an, ob jede Quelle im Streaming-Modus mit dem Argument <code>stream=True</code> \u2705 verwendet werden kann. Der Streaming-Modus ist vorteilhaft f\u00fcr die Verarbeitung von Videos oder Live-Streams, da er einen Generator von Ergebnissen statt das Laden aller Frames in den Speicher erzeugt.</p> <p>Tipp</p> <p>Verwenden Sie <code>stream=True</code> f\u00fcr die Verarbeitung langer Videos oder gro\u00dfer Datens\u00e4tze, um den Speicher effizient zu verwalten. Bei <code>stream=False</code> werden die Ergebnisse f\u00fcr alle Frames oder Datenpunkte im Speicher gehalten, was bei gro\u00dfen Eingaben schnell zu Speicher\u00fcberl\u00e4ufen f\u00fchren kann. Im Gegensatz dazu verwendet <code>stream=True</code> einen Generator, der nur die Ergebnisse des aktuellen Frames oder Datenpunkts im Speicher beh\u00e4lt, was den Speicherverbrauch erheblich reduziert und Speicher\u00fcberlaufprobleme verhindert.</p> Quelle Argument Typ Hinweise Bild <code>'image.jpg'</code> <code>str</code> oder <code>Path</code> Einzelbilddatei. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL zu einem Bild. Bildschirmaufnahme <code>'screen'</code> <code>str</code> Eine Bildschirmaufnahme erstellen. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> HWC-Format mit RGB-Kan\u00e4len. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> HWC-Format mit BGR-Kan\u00e4len <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> HWC-Format mit BGR-Kan\u00e4len <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> BCHW-Format mit RGB-Kan\u00e4len <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> oder <code>Path</code> CSV-Datei mit Pfaden zu Bildern, Videos oder Verzeichnissen. video \u2705 <code>'video.mp4'</code> <code>str</code> oder <code>Path</code> Videodatei in Formaten wie MP4, AVI, usw. Verzeichnis \u2705 <code>'path/'</code> <code>str</code> oder <code>Path</code> Pfad zu einem Verzeichnis mit Bildern oder Videos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Glob-Muster, um mehrere Dateien zu finden. Verwenden Sie das <code>*</code> Zeichen als Platzhalter. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL zu einem YouTube-Video. stream \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL f\u00fcr Streaming-Protokolle wie RTSP, RTMP, TCP oder eine IP-Adresse. Multi-Stream \u2705 <code>'list.streams'</code> <code>str</code> oder <code>Path</code> <code>*.streams</code> Textdatei mit einer Stream-URL pro Zeile, z.B. 8 Streams laufen bei Batch-Gr\u00f6\u00dfe 8. <p>Untenstehend finden Sie Codebeispiele f\u00fcr die Verwendung jedes Quelltyps:</p> <p>Vorhersagequellen</p> BildBildschirmaufnahmeURLPILOpenCVnumpytorch <p>F\u00fchren Sie die Inferenz auf einer Bilddatei aus. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Pfad zur Bilddatei definieren\nquell = 'Pfad/zum/Bild.jpg'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf dem aktuellen Bildschirminhalt als Screenshot aus. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Aktuellen Screenshot als Quelle definieren\nquell = 'Bildschirm'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild oder Video aus, das \u00fcber eine URL remote gehostet wird. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Remote-Bild- oder Video-URL definieren\nquell = 'https://ultralytics.com/images/bus.jpg'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das mit der Python Imaging Library (PIL) ge\u00f6ffnet wurde. <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein Bild mit PIL \u00f6ffnen\nquell = Image.open('Pfad/zum/Bild.jpg')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das mit OpenCV gelesen wurde. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein Bild mit OpenCV lesen\nquell = cv2.imread('Pfad/zum/Bild.jpg')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das als numpy-Array dargestellt wird. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein zuf\u00e4lliges numpy-Array der HWC-Form (640, 640, 3) mit Werten im Bereich [0, 255] und Typ uint8 erstellen\nquell = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das als PyTorch-Tensor dargestellt wird. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#ein-vortrainiertes-yolov8n-modell-laden","title":"Ein vortrainiertes YOLOv8n Modell laden","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#ein-zufalliger-torch-tensor-der-bchw-form-1-3-640-640-mit-werten-im-bereich-0-1-und-typ-float32-erstellen","title":"Ein zuf\u00e4lliger torch-Tensor der BCHW-Form (1, 3, 640, 640) mit Werten im Bereich [0, 1] und Typ float32 erstellen","text":"<p>quell = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#inferenz-auf-der-quelle-ausfuhren","title":"Inferenz auf der Quelle ausf\u00fchren","text":"<p>ergebnisse = model(quell)  # Liste von Results-Objekten</p>"},{"location":"modes/track/","title":"Multi-Objektverfolgung mit Ultralytics YOLO","text":"<p>Objektverfolgung im Bereich der Videoanalytik ist eine essentielle Aufgabe, die nicht nur den Standort und die Klasse von Objekten innerhalb des Frames identifiziert, sondern auch eine eindeutige ID f\u00fcr jedes erkannte Objekt, w\u00e4hrend das Video fortschreitet, erh\u00e4lt. Die Anwendungsm\u00f6glichkeiten sind grenzenlos \u2013 von \u00dcberwachung und Sicherheit bis hin zur Echtzeitsportanalytik.</p>"},{"location":"modes/track/#warum-ultralytics-yolo-fur-objektverfolgung-wahlen","title":"Warum Ultralytics YOLO f\u00fcr Objektverfolgung w\u00e4hlen?","text":"<p>Die Ausgabe von Ultralytics Trackern ist konsistent mit der standardm\u00e4\u00dfigen Objekterkennung, bietet aber zus\u00e4tzlich Objekt-IDs. Dies erleichtert das Verfolgen von Objekten in Videostreams und das Durchf\u00fchren nachfolgender Analysen. Hier sind einige Gr\u00fcnde, warum Sie Ultralytics YOLO f\u00fcr Ihre Objektverfolgungsaufgaben in Betracht ziehen sollten:</p> <ul> <li>Effizienz: Verarbeitung von Videostreams in Echtzeit ohne Einbu\u00dfen bei der Genauigkeit.</li> <li>Flexibilit\u00e4t: Unterst\u00fctzt mehrere Tracking-Algorithmen und -Konfigurationen.</li> <li>Benutzerfreundlichkeit: Einfache Python-API und CLI-Optionen f\u00fcr schnelle Integration und Bereitstellung.</li> <li>Anpassbarkeit: Einfache Verwendung mit individuell trainierten YOLO-Modellen, erm\u00f6glicht Integration in branchenspezifische Anwendungen.</li> </ul> <p> Ansehen: Objekterkennung und -verfolgung mit Ultralytics YOLOv8. </p>"},{"location":"modes/track/#anwendungen-in-der-realen-welt","title":"Anwendungen in der realen Welt","text":"Transportwesen Einzelhandel Aquakultur Fahrzeugverfolgung Personenverfolgung Fischverfolgung"},{"location":"modes/track/#eigenschaften-auf-einen-blick","title":"Eigenschaften auf einen Blick","text":"<p>Ultralytics YOLO erweitert seine Objekterkennungsfunktionen, um eine robuste und vielseitige Objektverfolgung bereitzustellen:</p> <ul> <li>Echtzeitverfolgung: Nahtloses Verfolgen von Objekten in Videos mit hoher Bildfrequenz.</li> <li>Unterst\u00fctzung mehrerer Tracker: Auswahl aus einer Vielzahl etablierter Tracking-Algorithmen.</li> <li>Anpassbare Tracker-Konfigurationen: Anpassen des Tracking-Algorithmus an spezifische Anforderungen durch Einstellung verschiedener Parameter.</li> </ul>"},{"location":"modes/track/#verfugbare-tracker","title":"Verf\u00fcgbare Tracker","text":"<p>Ultralytics YOLO unterst\u00fctzt die folgenden Tracking-Algorithmen. Sie k\u00f6nnen aktiviert werden, indem Sie die entsprechende YAML-Konfigurationsdatei wie <code>tracker=tracker_type.yaml</code> \u00fcbergeben:</p> <ul> <li>BoT-SORT - Verwenden Sie <code>botsort.yaml</code>, um diesen Tracker zu aktivieren.</li> <li>ByteTrack - Verwenden Sie <code>bytetrack.yaml</code>, um diesen Tracker zu aktivieren.</li> </ul> <p>Der Standardtracker ist BoT-SORT.</p>"},{"location":"modes/track/#verfolgung","title":"Verfolgung","text":"<p>Um den Tracker auf Videostreams auszuf\u00fchren, verwenden Sie ein trainiertes Erkennungs-, Segmentierungs- oder Posierungsmodell wie YOLOv8n, YOLOv8n-seg und YOLOv8n-pose.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein offizielles oder individuelles Modell\nmodel = YOLO('yolov8n.pt')  # Laden Sie ein offizielles Erkennungsmodell\nmodel = YOLO('yolov8n-seg.pt')  # Laden Sie ein offizielles Segmentierungsmodell\nmodel = YOLO('yolov8n-pose.pt')  # Laden Sie ein offizielles Posierungsmodell\nmodel = YOLO('path/to/best.pt')  # Laden Sie ein individuell trainiertes Modell\n\n# F\u00fchren Sie die Verfolgung mit dem Modell durch\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Verfolgung mit Standardtracker\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Verfolgung mit ByteTrack-Tracker\n</code></pre> <pre><code># F\u00fchren Sie die Verfolgung mit verschiedenen Modellen \u00fcber die Befehlszeilenschnittstelle durch\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Erkennungsmodell\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Segmentierungsmodell\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Posierungsmodell\nyolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Individuell trainiertes Modell\n\n# Verfolgung mit ByteTrack-Tracker\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Wie in der obigen Nutzung zu sehen ist, ist die Verfolgung f\u00fcr alle Detect-, Segment- und Pose-Modelle verf\u00fcgbar, die auf Videos oder Streaming-Quellen ausgef\u00fchrt werden.</p>"},{"location":"modes/track/#konfiguration","title":"Konfiguration","text":""},{"location":"modes/track/#tracking-argumente","title":"Tracking-Argumente","text":"<p>Die Tracking-Konfiguration teilt Eigenschaften mit dem Predict-Modus, wie <code>conf</code>, <code>iou</code> und <code>show</code>. F\u00fcr weitere Konfigurationen siehe die Seite des Predict-Modells.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Konfigurieren Sie die Tracking-Parameter und f\u00fchren Sie den Tracker aus\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Konfigurieren Sie die Tracking-Parameter und f\u00fchren Sie den Tracker \u00fcber die Befehlszeilenschnittstelle aus\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#tracker-auswahl","title":"Tracker-Auswahl","text":"<p>Ultralytics erm\u00f6glicht es Ihnen auch, eine modifizierte Tracker-Konfigurationsdatei zu verwenden. Hierf\u00fcr kopieren Sie einfach eine Tracker-Konfigurationsdatei (zum Beispiel <code>custom_tracker.yaml</code>) von ultralytics/cfg/trackers und \u00e4ndern jede Konfiguration (au\u00dfer dem <code>tracker_type</code>), wie es Ihren Bed\u00fcrfnissen entspricht.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie das Modell und f\u00fchren Sie den Tracker mit einer individuellen Konfigurationsdatei aus\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Laden Sie das Modell und f\u00fchren Sie den Tracker mit einer individuellen Konfigurationsdatei \u00fcber die Befehlszeilenschnittstelle aus\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>F\u00fcr eine umfassende Liste der Tracking-Argumente siehe die Seite ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#python-beispiele","title":"Python-Beispiele","text":""},{"location":"modes/track/#persistierende-tracks-schleife","title":"Persistierende Tracks-Schleife","text":"<p>Hier ist ein Python-Skript, das OpenCV (<code>cv2</code>) und YOLOv8 verwendet, um Objektverfolgung in Videoframes durchzuf\u00fchren. Dieses Skript setzt voraus, dass Sie die notwendigen Pakete (<code>opencv-python</code> und <code>ultralytics</code>) bereits installiert haben. Das Argument <code>persist=True</code> teilt dem Tracker mit, dass das aktuelle Bild oder Frame das n\u00e4chste in einer Sequenz ist und Tracks aus dem vorherigen Bild im aktuellen Bild erwartet werden.</p> <p>Streaming-For-Schleife mit Tracking</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Laden Sie das YOLOv8-Modell\nmodel = YOLO('yolov8n.pt')\n\n# \u00d6ffnen Sie die Videodatei\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Schleife durch die Videoframes\nwhile cap.isOpened():\n    # Einen Frame aus dem Video lesen\n    success, frame = cap.read()\n\n    if success:\n        # F\u00fchren Sie YOLOv8-Tracking im Frame aus, wobei Tracks zwischen Frames beibehalten werden\n        results = model.track(frame, persist=True)\n\n        # Visualisieren Sie die Ergebnisse im Frame\n        annotated_frame = results[0].plot()\n\n        # Zeigen Sie den kommentierten Frame an\n        cv2.imshow(\"YOLOv8-Tracking\", annotated_frame)\n\n        # Beenden Sie die Schleife, wenn 'q' gedr\u00fcckt wird\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Beenden Sie die Schleife, wenn das Ende des Videos erreicht ist\n        break\n\n# Geben Sie das Videoaufnahmeobjekt frei und schlie\u00dfen Sie das Anzeigefenster\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Bitte beachten Sie die \u00c4nderung von <code>model(frame)</code> zu <code>model.track(frame)</code>, welche die Objektverfolgung anstelle der einfachen Erkennung aktiviert. Dieses modifizierte Skript f\u00fchrt den Tracker auf jedem Frame des Videos aus, visualisiert die Ergebnisse und zeigt sie in einem Fenster an. Die Schleife kann durch Dr\u00fccken von 'q' beendet werden.</p>"},{"location":"modes/track/#neue-tracker-beisteuern","title":"Neue Tracker beisteuern","text":"<p>Sind Sie versiert in der Multi-Objektverfolgung und haben erfolgreich einen Tracking-Algorithmus mit Ultralytics YOLO implementiert oder angepasst? Wir laden Sie ein, zu unserem Trackers-Bereich in ultralytics/cfg/trackers beizutragen! Ihre realen Anwendungen und L\u00f6sungen k\u00f6nnten f\u00fcr Benutzer, die an Tracking-Aufgaben arbeiten, von unsch\u00e4tzbarem Wert sein.</p> <p>Indem Sie zu diesem Bereich beitragen, helfen Sie, das Spektrum verf\u00fcgbarer Tracking-L\u00f6sungen innerhalb des Ultralytics YOLO-Frameworks zu erweitern und f\u00fcgen eine weitere Funktionsschicht f\u00fcr die Gemeinschaft hinzu.</p> <p>Um Ihren Beitrag einzuleiten, sehen Sie bitte in unserem Contributing Guide f\u00fcr umfassende Anweisungen zur Einreichung eines Pull Requests (PR) \ud83d\udee0\ufe0f. Wir sind gespannt darauf, was Sie beitragen!</p> <p>Gemeinsam verbessern wir die Tracking-F\u00e4higkeiten des Ultralytics YOLO-\u00d6kosystems \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Modelltraining mit Ultralytics YOLO","text":""},{"location":"modes/train/#einleitung","title":"Einleitung","text":"<p>Das Training eines Deep-Learning-Modells beinhaltet das Einspeisen von Daten und die Anpassung seiner Parameter, so dass es genaue Vorhersagen treffen kann. Der Trainingsmodus in Ultralytics YOLOv8 ist f\u00fcr das effektive und effiziente Training von Objekterkennungsmodellen konzipiert und nutzt dabei die F\u00e4higkeiten moderner Hardware voll aus. Dieser Leitfaden zielt darauf ab, alle Details zu vermitteln, die Sie ben\u00f6tigen, um mit dem Training Ihrer eigenen Modelle unter Verwendung des robusten Funktionssatzes von YOLOv8 zu beginnen.</p> <p> Video anschauen: Wie man ein YOLOv8-Modell auf Ihrem benutzerdefinierten Datensatz in Google Colab trainiert. </p>"},{"location":"modes/train/#warum-ultralytics-yolo-fur-das-training-wahlen","title":"Warum Ultralytics YOLO f\u00fcr das Training w\u00e4hlen?","text":"<p>Hier einige \u00fcberzeugende Gr\u00fcnde, sich f\u00fcr den Trainingsmodus von YOLOv8 zu entscheiden:</p> <ul> <li>Effizienz: Machen Sie das Beste aus Ihrer Hardware, egal ob Sie auf einem Single-GPU-Setup sind oder \u00fcber mehrere GPUs skalieren.</li> <li>Vielseitigkeit: Training auf benutzerdefinierten Datens\u00e4tzen zus\u00e4tzlich zu den bereits verf\u00fcgbaren Datens\u00e4tzen wie COCO, VOC und ImageNet.</li> <li>Benutzerfreundlich: Einfache, aber leistungsstarke CLI- und Python-Schnittstellen f\u00fcr ein unkompliziertes Trainingserlebnis.</li> <li>Flexibilit\u00e4t der Hyperparameter: Eine breite Palette von anpassbaren Hyperparametern, um die Modellleistung zu optimieren.</li> </ul>"},{"location":"modes/train/#schlusselfunktionen-des-trainingsmodus","title":"Schl\u00fcsselfunktionen des Trainingsmodus","text":"<p>Die folgenden sind einige bemerkenswerte Funktionen von YOLOv8s Trainingsmodus:</p> <ul> <li>Automatischer Datensatz-Download: Standarddatens\u00e4tze wie COCO, VOC und ImageNet werden bei der ersten Verwendung automatisch heruntergeladen.</li> <li>Multi-GPU-Unterst\u00fctzung: Skalieren Sie Ihr Training nahtlos \u00fcber mehrere GPUs, um den Prozess zu beschleunigen.</li> <li>Konfiguration der Hyperparameter: Die M\u00f6glichkeit zur Modifikation der Hyperparameter \u00fcber YAML-Konfigurationsdateien oder CLI-Argumente.</li> <li>Visualisierung und \u00dcberwachung: Echtzeit-Tracking von Trainingsmetriken und Visualisierung des Lernprozesses f\u00fcr bessere Einsichten.</li> </ul> <p>Tipp</p> <ul> <li>YOLOv8-Datens\u00e4tze wie COCO, VOC, ImageNet und viele andere werden automatisch bei der ersten Verwendung heruntergeladen, d.h. <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#nutzungsbeispiele","title":"Nutzungsbeispiele","text":"<p>Trainieren Sie YOLOv8n auf dem COCO128-Datensatz f\u00fcr 100 Epochen bei einer Bildgr\u00f6\u00dfe von 640. Das Trainingsger\u00e4t kann mit dem Argument <code>device</code> spezifiziert werden. Wenn kein Argument \u00fcbergeben wird, wird GPU <code>device=0</code> verwendet, wenn verf\u00fcgbar, sonst wird <code>device=cpu</code> verwendet. Siehe den Abschnitt Argumente unten f\u00fcr eine vollst\u00e4ndige Liste der Trainingsargumente.</p> <p>Beispiel f\u00fcr Single-GPU- und CPU-Training</p> <p>Das Ger\u00e4t wird automatisch ermittelt. Wenn eine GPU verf\u00fcgbar ist, dann wird diese verwendet, sonst beginnt das Training auf der CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.yaml')  # bauen Sie ein neues Modell aus YAML\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # bauen Sie aus YAML und \u00fcbertragen Sie Gewichte\n\n# Trainieren Sie das Modell\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Bauen Sie ein neues Modell aus YAML und beginnen Sie das Training von Grund auf\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Beginnen Sie das Training von einem vortrainierten *.pt Modell\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Bauen Sie ein neues Modell aus YAML, \u00fcbertragen Sie vortrainierte Gewichte darauf und beginnen Sie das Training\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#multi-gpu-training","title":"Multi-GPU-Training","text":"<p>Multi-GPU-Training erm\u00f6glicht eine effizientere Nutzung von verf\u00fcgbaren Hardware-Ressourcen, indem die Trainingslast \u00fcber mehrere GPUs verteilt wird. Diese Funktion ist \u00fcber sowohl die Python-API als auch die Befehlszeilenschnittstelle verf\u00fcgbar. Um das Multi-GPU-Training zu aktivieren, geben Sie die GPU-Ger\u00e4te-IDs an, die Sie verwenden m\u00f6chten.</p> <p>Beispiel f\u00fcr Multi-GPU-Training</p> <p>Um mit 2 GPUs zu trainieren, verwenden Sie die folgenden Befehle f\u00fcr CUDA-Ger\u00e4te 0 und 1. Erweitern Sie dies bei Bedarf auf zus\u00e4tzliche GPUs.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\n\n# Trainieren Sie das Modell mit 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Beginnen Sie das Training von einem vortrainierten *.pt Modell unter Verwendung der GPUs 0 und 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#apple-m1-und-m2-mps-training","title":"Apple M1- und M2-MPS-Training","text":"<p>Mit der Unterst\u00fctzung f\u00fcr Apple M1- und M2-Chips, die in den Ultralytics YOLO-Modellen integriert ist, ist es jetzt m\u00f6glich, Ihre Modelle auf Ger\u00e4ten zu trainieren, die das leistungsstarke Metal Performance Shaders (MPS)-Framework nutzen. MPS bietet eine leistungsstarke Methode zur Ausf\u00fchrung von Berechnungs- und Bildverarbeitungsaufgaben auf Apples benutzerdefinierten Siliziumchips.</p> <p>Um das Training auf Apple M1- und M2-Chips zu erm\u00f6glichen, sollten Sie 'mps' als Ihr Ger\u00e4t angeben, wenn Sie den Trainingsprozess starten. Unten ist ein Beispiel, wie Sie dies in Python und \u00fcber die Befehlszeile tun k\u00f6nnten:</p> <p>MPS-Training Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\n\n# Trainieren Sie das Modell mit 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Beginnen Sie das Training von einem vortrainierten *.pt Modell unter Verwendung der GPUs 0 und 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Indem sie die Rechenleistung der M1/M2-Chips nutzen, erm\u00f6glicht dies eine effizientere Verarbeitung der Trainingsaufgaben. F\u00fcr detailliertere Anleitungen und fortgeschrittene Konfigurationsoptionen beziehen Sie sich bitte auf die PyTorch MPS-Dokumentation.</p>"},{"location":"modes/train/#protokollierung","title":"Protokollierung","text":"<p>Beim Training eines YOLOv8-Modells kann es wertvoll sein, die Leistung des Modells im Laufe der Zeit zu verfolgen. Hier kommt die Protokollierung ins Spiel. Ultralytics' YOLO unterst\u00fctzt drei Typen von Loggern - Comet, ClearML und TensorBoard.</p> <p>Um einen Logger zu verwenden, w\u00e4hlen Sie ihn aus dem Dropdown-Men\u00fc im obigen Codeausschnitt aus und f\u00fchren ihn aus. Der ausgew\u00e4hlte Logger wird installiert und initialisiert.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet ist eine Plattform, die Datenwissenschaftlern und Entwicklern erlaubt, Experimente und Modelle zu verfolgen, zu vergleichen, zu erkl\u00e4ren und zu optimieren. Es bietet Funktionen wie Echtzeitmetriken, Code-Diffs und das Verfolgen von Hyperparametern.</p> <p>Um Comet zu verwenden:</p> Python <pre><code># pip installieren comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Vergessen Sie nicht, sich auf der Comet-Website anzumelden und Ihren API-Schl\u00fcssel zu erhalten. Sie m\u00fcssen diesen zu Ihren Umgebungsvariablen oder Ihrem Skript hinzuf\u00fcgen, um Ihre Experimente zu protokollieren.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML ist eine Open-Source-Plattform, die das Verfolgen von Experimenten automatisiert und hilft, Ressourcen effizient zu teilen. Sie ist darauf ausgelegt, Teams bei der Verwaltung, Ausf\u00fchrung und Reproduktion ihrer ML-Arbeiten effizienter zu unterst\u00fctzen.</p> <p>Um ClearML zu verwenden:</p> Python <pre><code># pip installieren clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Nach dem Ausf\u00fchren dieses Skripts m\u00fcssen Sie sich auf dem Browser bei Ihrem ClearML-Konto anmelden und Ihre Sitzung authentifizieren.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard ist ein Visualisierungstoolset f\u00fcr TensorFlow. Es erm\u00f6glicht Ihnen, Ihren TensorFlow-Graphen zu visualisieren, quantitative Metriken \u00fcber die Ausf\u00fchrung Ihres Graphen zu plotten und zus\u00e4tzliche Daten wie Bilder zu zeigen, die durch ihn hindurchgehen.</p> <p>Um TensorBoard in Google Colab zu verwenden:</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # ersetzen Sie mit Ihrem 'runs' Verzeichnis\n</code></pre> <p>Um TensorBoard lokal auszuf\u00fchren, f\u00fchren Sie den folgenden Befehl aus und betrachten Sie die Ergebnisse unter http://localhost:6006/.</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # ersetzen Sie mit Ihrem 'runs' Verzeichnis\n</code></pre> <p>Dies l\u00e4dt TensorBoard und weist es an, das Verzeichnis zu verwenden, in dem Ihre Trainingsprotokolle gespeichert sind.</p> <p>Nachdem Sie Ihren Logger eingerichtet haben, k\u00f6nnen Sie mit Ihrem Modelltraining fortfahren. Alle Trainingsmetriken werden automatisch in Ihrer gew\u00e4hlten Plattform protokolliert, und Sie k\u00f6nnen auf diese Protokolle zugreifen, um die Leistung Ihres Modells im Laufe der Zeit zu \u00fcberwachen, verschiedene Modelle zu vergleichen und Bereiche f\u00fcr Verbesserungen zu identifizieren.</p>"},{"location":"modes/val/","title":"Modellvalidierung mit Ultralytics YOLO","text":""},{"location":"modes/val/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Die Validierung ist ein kritischer Schritt im Machine-Learning-Prozess, der es Ihnen erm\u00f6glicht, die Qualit\u00e4t Ihrer trainierten Modelle zu bewerten. Der Val-Modus in Ultralytics YOLOv8 bietet eine robuste Suite von Tools und Metriken zur Bewertung der Leistung Ihrer Objekterkennungsmodelle. Dieser Leitfaden dient als umfassende Ressource, um zu verstehen, wie Sie den Val-Modus effektiv nutzen k\u00f6nnen, um sicherzustellen, dass Ihre Modelle sowohl genau als auch zuverl\u00e4ssig sind.</p>"},{"location":"modes/val/#warum-mit-ultralytics-yolo-validieren","title":"Warum mit Ultralytics YOLO validieren?","text":"<p>Hier sind die Vorteile der Verwendung des Val-Modus von YOLOv8:</p> <ul> <li>Pr\u00e4zision: Erhalten Sie genaue Metriken wie mAP50, mAP75 und mAP50-95, um Ihr Modell umfassend zu bewerten.</li> <li>Bequemlichkeit: Nutzen Sie integrierte Funktionen, die Trainingseinstellungen speichern und so den Validierungsprozess vereinfachen.</li> <li>Flexibilit\u00e4t: Validieren Sie Ihr Modell mit den gleichen oder verschiedenen Datens\u00e4tzen und Bildgr\u00f6\u00dfen.</li> <li>Hyperparameter-Tuning: Verwenden Sie Validierungsmetriken, um Ihr Modell f\u00fcr eine bessere Leistung zu optimieren.</li> </ul>"},{"location":"modes/val/#schlusselfunktionen-des-val-modus","title":"Schl\u00fcsselfunktionen des Val-Modus","text":"<p>Dies sind die bemerkenswerten Funktionen, die der Val-Modus von YOLOv8 bietet:</p> <ul> <li>Automatisierte Einstellungen: Modelle erinnern sich an ihre Trainingskonfigurationen f\u00fcr eine unkomplizierte Validierung.</li> <li>Unterst\u00fctzung mehrerer Metriken: Bewerten Sie Ihr Modell anhand einer Reihe von Genauigkeitsmetriken.</li> <li>CLI- und Python-API: W\u00e4hlen Sie zwischen Befehlszeilenschnittstelle oder Python-API basierend auf Ihrer Pr\u00e4ferenz f\u00fcr die Validierung.</li> <li>Datenkompatibilit\u00e4t: Funktioniert nahtlos mit Datens\u00e4tzen, die w\u00e4hrend der Trainingsphase sowie mit benutzerdefinierten Datens\u00e4tzen verwendet wurden.</li> </ul> <p>Tipp</p> <ul> <li>YOLOv8-Modelle speichern automatisch ihre Trainingseinstellungen, sodass Sie ein Modell mit der gleichen Bildgr\u00f6\u00dfe und dem urspr\u00fcnglichen Datensatz leicht validieren k\u00f6nnen, indem Sie einfach <code>yolo val model=yolov8n.pt</code> oder <code>model('yolov8n.pt').val()</code> ausf\u00fchren</li> </ul>"},{"location":"modes/val/#beispielverwendung","title":"Beispielverwendung","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-Modells auf dem COCO128-Datensatz. Es muss kein Argument \u00fcbergeben werden, da das <code>model</code> seine Trainings-<code>data</code> und Argumente als Modellattribute speichert. Siehe Abschnitt \u201eArgumente\u201c unten f\u00fcr eine vollst\u00e4ndige Liste der Exportargumente.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('path/to/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell validieren\nmetrics = model.val()  # keine Argumente ben\u00f6tigt, Datensatz und Einstellungen gespeichert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # eine Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # offizielles Modell validieren\nyolo detect val model=path/to/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"modes/val/#argumente","title":"Argumente","text":"<p>Validierungseinstellungen f\u00fcr YOLO-Modelle beziehen sich auf verschiedene Hyperparameter und Konfigurationen, die verwendet werden, um die Leistung des Modells an einem Validierungsdatensatz zu bewerten. Diese Einstellungen k\u00f6nnen die Leistung, Geschwindigkeit und Genauigkeit des Modells beeinflussen. Einige g\u00e4ngige YOLO-Validierungseinstellungen umfassen die Batch-Gr\u00f6\u00dfe, die H\u00e4ufigkeit der Validierung w\u00e4hrend des Trainings und die Metriken zur Bewertung der Modellleistung. Andere Faktoren, die den Validierungsprozess beeinflussen k\u00f6nnen, sind die Gr\u00f6\u00dfe und Zusammensetzung des Validierungsdatensatzes und die spezifische Aufgabe, f\u00fcr die das Modell verwendet wird. Es ist wichtig, diese Einstellungen sorgf\u00e4ltig abzustimmen und zu experimentieren, um sicherzustellen, dass das Modell auf dem Validierungsdatensatz gut funktioniert sowie \u00dcberanpassung zu erkennen und zu verhindern.</p> Key Value Beschreibung <code>data</code> <code>None</code> Pfad zur Datendatei, z.B. coco128.yaml <code>imgsz</code> <code>640</code> Gr\u00f6\u00dfe der Eingabebilder als ganzzahlige Zahl <code>batch</code> <code>16</code> Anzahl der Bilder pro Batch (-1 f\u00fcr AutoBatch) <code>save_json</code> <code>False</code> Ergebnisse in JSON-Datei speichern <code>save_hybrid</code> <code>False</code> hybride Version der Labels speichern (Labels + zus\u00e4tzliche Vorhersagen) <code>conf</code> <code>0.001</code> Objekterkennungsschwelle f\u00fcr Zuversichtlichkeit <code>iou</code> <code>0.6</code> Schwellenwert f\u00fcr IoU (Intersection over Union) f\u00fcr NMS <code>max_det</code> <code>300</code> maximale Anzahl an Vorhersagen pro Bild <code>half</code> <code>True</code> Halbpr\u00e4zision verwenden (FP16) <code>device</code> <code>None</code> Ger\u00e4t zur Ausf\u00fchrung, z.B. CUDA device=0/1/2/3 oder device=cpu <code>dnn</code> <code>False</code> OpenCV DNN f\u00fcr ONNX-Inf erenz nutzen <code>plots</code> <code>False</code> Diagramme w\u00e4hrend des Trainings anzeigen <code>rect</code> <code>False</code> rechteckige Validierung mit jeder Batch-Charge f\u00fcr minimale Polsterung <code>split</code> <code>val</code> Zu verwendende Daten-Teilmenge f\u00fcr Validierung, z.B. 'val', 'test' oder 'train'"},{"location":"tasks/","title":"Ultralytics YOLOv8 Aufgaben","text":"<p>YOLOv8 ist ein KI-Framework, das mehrere Aufgaben im Bereich der Computer Vision unterst\u00fctzt. Das Framework kann f\u00fcr die Erkennung, Segmentierung, Klassifizierung und die Pose-Sch\u00e4tzung verwendet werden. Jede dieser Aufgaben hat ein unterschiedliches Ziel und Anwendungsgebiete.</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere mehrsprachigen Dokumentation befindet sich derzeit im Aufbau und wir arbeiten hart daran, sie zu verbessern. Danke f\u00fcr Ihre Geduld! \ud83d\ude4f</p> <p> Schauen Sie zu: Entdecken Sie Ultralytics YOLO Aufgaben: Objekterkennung, Segmentierung, Verfolgung und Pose-Sch\u00e4tzung. </p>"},{"location":"tasks/#erkennung","title":"Erkennung","text":"<p>Erkennung ist die prim\u00e4re von YOLOv8 unterst\u00fctzte Aufgabe. Sie beinhaltet das Erkennen von Objekten in einem Bild oder Videobild und das Zeichnen von Rahmen um sie herum. Die erkannten Objekte werden anhand ihrer Merkmale in verschiedene Kategorien klassifiziert. YOLOv8 kann mehrere Objekte in einem einzelnen Bild oder Videobild mit hoher Genauigkeit und Geschwindigkeit erkennen.</p> <p>Beispiele f\u00fcr Erkennung</p>"},{"location":"tasks/#segmentierung","title":"Segmentierung","text":"<p>Segmentierung ist eine Aufgabe, die das Aufteilen eines Bildes in unterschiedliche Regionen anhand des Bildinhalts beinhaltet. Jeder Region wird basierend auf ihrem Inhalt eine Markierung zugewiesen. Diese Aufgabe ist n\u00fctzlich in Anwendungen wie der Bildsegmentierung und medizinischen Bildgebung. YOLOv8 verwendet eine Variante der U-Net-Architektur, um die Segmentierung durchzuf\u00fchren.</p> <p>Beispiele f\u00fcr Segmentierung</p>"},{"location":"tasks/#klassifizierung","title":"Klassifizierung","text":"<p>Klassifizierung ist eine Aufgabe, die das Einordnen eines Bildes in verschiedene Kategorien umfasst. YOLOv8 kann genutzt werden, um Bilder anhand ihres Inhalts zu klassifizieren. Es verwendet eine Variante der EfficientNet-Architektur, um die Klassifizierung durchzuf\u00fchren.</p> <p>Beispiele f\u00fcr Klassifizierung</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>Die Pose-/Keypoint-Erkennung ist eine Aufgabe, die das Erkennen von spezifischen Punkten in einem Bild oder Videobild beinhaltet. Diese Punkte werden als Keypoints bezeichnet und werden zur Bewegungsverfolgung oder Pose-Sch\u00e4tzung verwendet. YOLOv8 kann Keypoints in einem Bild oder Videobild mit hoher Genauigkeit und Geschwindigkeit erkennen.</p> <p>Beispiele f\u00fcr Posen</p>"},{"location":"tasks/#fazit","title":"Fazit","text":"<p>YOLOv8 unterst\u00fctzt mehrere Aufgaben, einschlie\u00dflich Erkennung, Segmentierung, Klassifizierung und Keypoint-Erkennung. Jede dieser Aufgaben hat unterschiedliche Ziele und Anwendungsgebiete. Durch das Verst\u00e4ndnis der Unterschiede zwischen diesen Aufgaben k\u00f6nnen Sie die geeignete Aufgabe f\u00fcr Ihre Anwendung im Bereich der Computer Vision ausw\u00e4hlen.</p>"},{"location":"tasks/classify/","title":"Bildklassifizierung","text":"<p>Bildklassifizierung ist die einfachste der drei Aufgaben und besteht darin, ein ganzes Bild in eine von einem Satz vordefinierter Klassen zu klassifizieren.</p> <p>Die Ausgabe eines Bildklassifizierers ist ein einzelnes Klassenlabel und eine Vertrauenspunktzahl. Bildklassifizierung ist n\u00fctzlich, wenn Sie nur wissen m\u00fcssen, zu welcher Klasse ein Bild geh\u00f6rt, und nicht wissen m\u00fcssen, wo sich Objekte dieser Klasse befinden oder wie ihre genaue Form ist.</p> <p>Tipp</p> <p>YOLOv8 Classify-Modelle verwenden den Suffix <code>-cls</code>, z.B. <code>yolov8n-cls.pt</code> und sind auf ImageNet vortrainiert.</p>"},{"location":"tasks/classify/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Classify-Modelle gezeigt. Detect-, Segment- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Classify-Modelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle werden automatisch vom neuesten Ultralytics-Release beim ersten Gebrauch heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) Genauigkeit<sup>top1 Genauigkeit<sup>top5 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) bei 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Genauigkeit-Werte sind Modellgenauigkeiten auf dem ImageNet-Datensatz Validierungsset.   Zur Reproduktion <code>yolo val classify data=pfad/zu/ImageNet device=0 verwenden</code></li> <li>Geschwindigkeit Durchschnitt \u00fcber ImageNet-Validierungsbilder mit einer Amazon EC2 P4d-Instanz.   Zur Reproduktion <code>yolo val classify data=pfad/zu/ImageNet batch=1 device=0|cpu verwenden</code></li> </ul>"},{"location":"tasks/classify/#trainieren","title":"Trainieren","text":"<p>Trainieren Sie das YOLOv8n-cls-Modell auf dem MNIST160-Datensatz f\u00fcr 100 Epochen bei Bildgr\u00f6\u00dfe 64. Eine vollst\u00e4ndige Liste der verf\u00fcgbaren Argumente finden Sie auf der Seite Konfiguration.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.yaml')  # ein neues Modell aus YAML erstellen\nmodel = YOLO('yolov8n-cls.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # aus YAML erstellen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Ein neues Modell aus YAML erstellen und das Training von Grund auf starten\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Das Training von einem vortrainierten *.pt Modell starten\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Ein neues Modell aus YAML erstellen, vortrainierte Gewichte \u00fcbertragen und das Training starten\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#datenformat","title":"Datenformat","text":"<p>Das Datenformat f\u00fcr YOLO-Klassifizierungsdatens\u00e4tze finden Sie im Detail im Datenleitfaden.</p>"},{"location":"tasks/classify/#validieren","title":"Validieren","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-cls-Modells auf dem MNIST160-Datensatz. Kein Argument muss \u00fcbergeben werden, da das <code>modell</code> seine Trainings<code>daten</code> und Argumente als Modellattribute beh\u00e4lt.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # keine Argumente ben\u00f6tigt, Datensatz und Einstellungen gespeichert\nmetrics.top1   # top1 Genauigkeit\nmetrics.top5   # top5 Genauigkeit\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # ein offizielles Modell validieren\nyolo classify val model=pfad/zu/best.pt  # ein benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/classify/#vorhersagen","title":"Vorhersagen","text":"<p>Verwenden Sie ein trainiertes YOLOv8n-cls-Modell, um Vorhersagen auf Bildern durchzuf\u00fchren.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell vorhersagen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # mit offiziellem Modell vorhersagen\nyolo classify predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # mit benutzerdefiniertem Modell vorhersagen\n</code></pre> <p>Vollst\u00e4ndige Details zum <code>predict</code>-Modus finden Sie auf der Seite Vorhersage.</p>"},{"location":"tasks/classify/#exportieren","title":"Exportieren","text":"<p>Exportieren Sie ein YOLOv8n-cls-Modell in ein anderes Format wie ONNX, CoreML usw.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8-cls Exportformate stehen in der folgenden Tabelle. Sie k\u00f6nnen direkt auf exportierten Modellen vorhersagen oder validieren, d.h. <code>yolo predict model=yolov8n-cls.onnx</code>. Nutzungsexempel werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vollst\u00e4ndige Details zum <code>export</code> finden Sie auf der Seite Export.</p>"},{"location":"tasks/detect/","title":"Objekterkennung","text":"<p>Objekterkennung ist eine Aufgabe, die das Identifizieren der Position und Klasse von Objekten in einem Bild oder Videostream umfasst.</p> <p>Die Ausgabe eines Objekterkenners ist eine Menge von Begrenzungsrahmen, die die Objekte im Bild umschlie\u00dfen, zusammen mit Klassenbezeichnungen und Vertrauenswerten f\u00fcr jedes Feld. Objekterkennung ist eine gute Wahl, wenn Sie Objekte von Interesse in einer Szene identifizieren m\u00fcssen, aber nicht genau wissen m\u00fcssen, wo das Objekt ist oder wie seine genaue Form ist.</p> <p> Sehen Sie: Objekterkennung mit vortrainiertem Ultralytics YOLOv8 Modell. </p> <p>Tipp</p> <p>YOLOv8 Detect Modelle sind die Standard YOLOv8 Modelle, zum Beispiel <code>yolov8n.pt</code>, und sind vortrainiert auf dem COCO-Datensatz.</p>"},{"location":"tasks/detect/#modelle","title":"Modelle","text":"<p>Hier werden die vortrainierten YOLOv8 Detect Modelle gezeigt. Detect, Segment und Pose Modelle sind vortrainiert auf dem COCO-Datensatz, w\u00e4hrend die Classify Modelle vortrainiert sind auf dem ImageNet-Datensatz.</p> <p>Modelle werden automatisch von der neuesten Ultralytics Ver\u00f6ffentlichung bei Erstbenutzung heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>val50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>mAP<sup>val</sup> Werte sind f\u00fcr Single-Modell Single-Scale auf dem COCO val2017 Datensatz.   Reproduzieren mit <code>yolo val detect data=coco.yaml device=0</code></li> <li>Geschwindigkeit gemittelt \u00fcber COCO Val Bilder mit einer Amazon EC2 P4d-Instanz.   Reproduzieren mit <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#training","title":"Training","text":"<p>YOLOv8n auf dem COCO128-Datensatz f\u00fcr 100 Epochen bei Bildgr\u00f6\u00dfe 640 trainieren. F\u00fcr eine vollst\u00e4ndige Liste verf\u00fcgbarer Argumente siehe die Konfigurationsseite.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.yaml')  # ein neues Modell aus YAML aufbauen\nmodel = YOLO('yolov8n.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr Training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # aus YAML aufbauen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML aufbauen und Training von Grund auf starten\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Training von einem vortrainierten *.pt Modell starten\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML aufbauen, vortrainierte Gewichte \u00fcbertragen und Training starten\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#datenformat","title":"Datenformat","text":"<p>Das Datenformat f\u00fcr YOLO-Erkennungsdatens\u00e4tze finden Sie detailliert im Dataset Guide. Um Ihren vorhandenen Datensatz von anderen Formaten (wie COCO etc.) in das YOLO-Format zu konvertieren, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/detect/#validierung","title":"Validierung","text":"<p>Genauigkeit des trainierten YOLOv8n-Modells auf dem COCO128-Datensatz validieren. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>modell</code> seine Trainingsdaten und Argumente als Modellattribute beibeh\u00e4lt.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # keine Argumente n\u00f6tig, Datensatz und Einstellungen erinnert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # eine Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # offizielles Modell validieren\nyolo detect val model=pfad/zum/besten.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/detect/#vorhersage","title":"Vorhersage","text":"<p>Ein trainiertes YOLOv8n-Modell verwenden, um Vorhersagen auf Bildern durchzuf\u00fchren.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell vorhersagen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit offiziellem Modell\nyolo detect predict model=pfad/zum/besten.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit benutzerdefiniertem Modell\n</code></pre> <p>Volle Details \u00fcber den <code>predict</code>-Modus finden Sie auf der Predict-Seite.</p>"},{"location":"tasks/detect/#export","title":"Export","text":"<p>Ein YOLOv8n-Modell in ein anderes Format wie ONNX, CoreML usw. exportieren.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniert trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zum/besten.pt format=onnx  # benutzerdefiniert trainiertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8 Exportformate sind in der untenstehenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf den exportierten Modellen Vorhersagen treffen oder diese validieren, zum Beispiel <code>yolo predict model=yolov8n.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Volle Details zum <code>export</code> finden Sie auf der Export-Seite.</p>"},{"location":"tasks/pose/","title":"Pose-Sch\u00e4tzung","text":"<p>Die Pose-Sch\u00e4tzung ist eine Aufgabe, die das Identifizieren der Lage spezifischer Punkte in einem Bild beinhaltet, die normalerweise als Schl\u00fcsselpunkte bezeichnet werden. Die Schl\u00fcsselpunkte k\u00f6nnen verschiedene Teile des Objekts wie Gelenke, Landmarken oder andere charakteristische Merkmale repr\u00e4sentieren. Die Positionen der Schl\u00fcsselpunkte sind \u00fcblicherweise als eine Gruppe von 2D <code>[x, y]</code> oder 3D <code>[x, y, sichtbar]</code> Koordinaten dargestellt.</p> <p>Das Ergebnis eines Pose-Sch\u00e4tzungsmodells ist eine Gruppe von Punkten, die die Schl\u00fcsselpunkte auf einem Objekt im Bild darstellen, normalerweise zusammen mit den Konfidenzwerten f\u00fcr jeden Punkt. Die Pose-Sch\u00e4tzung eignet sich gut, wenn Sie spezifische Teile eines Objekts in einer Szene identifizieren m\u00fcssen und deren Lage zueinander.</p> <p> Ansehen: Pose-Sch\u00e4tzung mit Ultralytics YOLOv8. </p> <p>Tipp</p> <p>YOLOv8 pose-Modelle verwenden den Suffix <code>-pose</code>, z. B. <code>yolov8n-pose.pt</code>. Diese Modelle sind auf dem COCO-Schl\u00fcsselpunkte-Datensatz trainiert und f\u00fcr eine Vielzahl von Pose-Sch\u00e4tzungsaufgaben geeignet.</p>"},{"location":"tasks/pose/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Pose-Modelle gezeigt. Erkennungs-, Segmentierungs- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Klassifizierungsmodelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle werden automatisch aus der neuesten Ultralytics-Ver\u00f6ffentlichung bei erstmaliger Verwendung heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>pose50-95 mAP<sup>pose50 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50,4 80,1 131,8 1,18 3,3 9,2 YOLOv8s-pose 640 60,0 86,2 233,2 1,42 11,6 30,2 YOLOv8m-pose 640 65,0 88,8 456,3 2,00 26,4 81,0 YOLOv8l-pose 640 67,6 90,0 784,5 2,59 44,4 168,6 YOLOv8x-pose 640 69,2 90,2 1607,1 3,73 69,4 263,2 YOLOv8x-pose-p6 1280 71,6 91,2 4088,7 10,04 99,1 1066,4 <ul> <li>mAP<sup>val</sup> Werte gelten f\u00fcr ein einzelnes Modell mit einfacher Skala auf dem COCO Keypoints val2017-Datensatz.   Zu reproduzieren mit <code>yolo val pose data=coco-pose.yaml device=0</code>.</li> <li>Geschwindigkeit gemittelt \u00fcber COCO-Validierungsbilder mit einer Amazon EC2 P4d-Instanz.   Zu reproduzieren mit <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code>.</li> </ul>"},{"location":"tasks/pose/#trainieren","title":"Trainieren","text":"<p>Trainieren Sie ein YOLOv8-Pose-Modell auf dem COCO128-Pose-Datensatz.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.yaml')  # ein neues Modell aus YAML bauen\nmodel = YOLO('yolov8n-pose.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # aus YAML bauen und Gewichte \u00fcbertragen\n\n# Modell trainieren\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML bauen und das Training von Grund auf starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Training von einem vortrainierten *.pt Modell starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML bauen, vortrainierte Gewichte \u00fcbertragen und das Training starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#datensatzformat","title":"Datensatzformat","text":"<p>Das YOLO-Pose-Datensatzformat finden Sie detailliert im Datensatz-Leitfaden. Um Ihren bestehenden Datensatz aus anderen Formaten (wie COCO usw.) in das YOLO-Format zu konvertieren, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/pose/#validieren","title":"Validieren","text":"<p>Die Genauigkeit des trainierten YOLOv8n-Pose-Modells auf dem COCO128-Pose-Datensatz validieren. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>Modell</code> seine Trainings<code>daten</code> und Argumente als Modellattribute beibeh\u00e4lt.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell validieren\nmetrics = model.val()  # keine Argumente n\u00f6tig, Datensatz und Einstellungen sind gespeichert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # offizielles Modell validieren\nyolo pose val model=pfad/zu/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/pose/#vorhersagen","title":"Vorhersagen","text":"<p>Ein trainiertes YOLOv8n-Pose-Modell verwenden, um Vorhersagen auf Bildern zu machen.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell Vorhersagen machen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild machen\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit dem offiziellen Modell machen\nyolo pose predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit dem benutzerdefinierten Modell machen\n</code></pre> <p>Vollst\u00e4ndige <code>predict</code>-Modusdetails finden Sie auf der Vorhersage-Seite.</p>"},{"location":"tasks/pose/#exportieren","title":"Exportieren","text":"<p>Ein YOLOv8n-Pose-Modell in ein anderes Format wie ONNX, CoreML usw. exportieren.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8-Pose-Exportformate sind in der folgenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf exportierten Modellen vorhersagen oder validieren, z. B. <code>yolo predict model=yolov8n-pose.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code> Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimieren</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> <p>Vollst\u00e4ndige <code>export</code>-Details finden Sie auf der Export-Seite.</p>"},{"location":"tasks/segment/","title":"Instanzsegmentierung","text":"<p>Instanzsegmentierung geht einen Schritt weiter als die Objekterkennung und beinhaltet die Identifizierung einzelner Objekte in einem Bild und deren Abtrennung vom Rest des Bildes.</p> <p>Das Ergebnis eines Instanzsegmentierungsmodells ist eine Reihe von Masken oder Konturen, die jedes Objekt im Bild umrei\u00dfen, zusammen mit Klassenbezeichnungen und Vertrauensscores f\u00fcr jedes Objekt. Instanzsegmentierung ist n\u00fctzlich, wenn man nicht nur wissen muss, wo sich Objekte in einem Bild befinden, sondern auch, welche genaue Form sie haben.</p> <p> Schauen Sie: F\u00fchren Sie Segmentierung mit dem vortrainierten Ultralytics YOLOv8 Modell in Python aus. </p> <p>Tipp</p> <p>YOLOv8 Segment-Modelle verwenden das Suffix <code>-seg</code>, d.h. <code>yolov8n-seg.pt</code> und sind auf dem COCO-Datensatz vortrainiert.</p>"},{"location":"tasks/segment/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Segment-Modelle gezeigt. Detect-, Segment- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Classify-Modelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle laden sich automatisch von der neuesten Ultralytics Ver\u00f6ffentlichung beim ersten Gebrauch herunter.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>Kasten50-95 mAP<sup>Masken50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Die mAP<sup>val</sup>-Werte sind f\u00fcr ein einzelnes Modell, einzelne Skala auf dem COCO val2017-Datensatz.   Zum Reproduzieren nutzen Sie <code>yolo val segment data=coco.yaml device=0</code></li> <li>Die Geschwindigkeit ist \u00fcber die COCO-Validierungsbilder gemittelt und verwendet eine Amazon EC2 P4d-Instanz.   Zum Reproduzieren <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#training","title":"Training","text":"<p>Trainieren Sie YOLOv8n-seg auf dem COCO128-seg-Datensatz f\u00fcr 100 Epochen mit einer Bildgr\u00f6\u00dfe von 640. Eine vollst\u00e4ndige Liste der verf\u00fcgbaren Argumente finden Sie auf der Seite Konfiguration.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.yaml')  # ein neues Modell aus YAML erstellen\nmodel = YOLO('yolov8n-seg.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # aus YAML erstellen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML erstellen und das Training von vorne beginnen\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Das Training von einem vortrainierten *.pt Modell aus starten\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML erstellen, vortrainierte Gewichte darauf \u00fcbertragen und das Training beginnen\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#datenformat","title":"Datenformat","text":"<p>Das YOLO Segmentierungsdatenformat finden Sie detailliert im Dataset Guide. Um Ihre vorhandenen Daten aus anderen Formaten (wie COCO usw.) in das YOLO-Format umzuwandeln, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/segment/#val","title":"Val","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-seg-Modells auf dem COCO128-seg-Datensatz. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>Modell</code> seine Trainingsdaten und -argumente als Modellattribute beh\u00e4lt.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # Keine Argumente erforderlich, Datensatz und Einstellungen werden behalten\nmetrics.box.map    # mAP50-95(B)\nmetrics.box.map50  # mAP50(B)\nmetrics.box.map75  # mAP75(B)\nmetrics.box.maps   # eine Liste enth\u00e4lt mAP50-95(B) f\u00fcr jede Kategorie\nmetrics.seg.map    # mAP50-95(M)\nmetrics.seg.map50  # mAP50(M)\nmetrics.seg.map75  # mAP75(M)\nmetrics.seg.maps   # eine Liste enth\u00e4lt mAP50-95(M) f\u00fcr jede Kategorie\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # offizielles Modell validieren\nyolo segment val model=pfad/zu/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/segment/#predict","title":"Predict","text":"<p>Verwenden Sie ein trainiertes YOLOv8n-seg-Modell f\u00fcr Vorhersagen auf Bildern.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes Modell laden\n\n# Mit dem Modell Vorhersagen treffen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit offiziellem Modell treffen\nyolo segment predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit benutzerdefiniertem Modell treffen\n</code></pre> <p>Die vollst\u00e4ndigen Details zum <code>predict</code>-Modus finden Sie auf der Seite Predict.</p>"},{"location":"tasks/segment/#export","title":"Export","text":"<p>Exportieren Sie ein YOLOv8n-seg-Modell in ein anderes Format wie ONNX, CoreML usw.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre> <p>Die verf\u00fcgbaren YOLOv8-seg-Exportformate sind in der folgenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf exportierten Modellen Vorhersagen treffen oder sie validieren, z.B. <code>yolo predict model=yolov8n-seg.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach dem Export angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimieren</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>Arbeitsspeicher</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> <p>Die vollst\u00e4ndigen Details zum <code>export</code> finden Sie auf der Seite Export.</p>"}]}