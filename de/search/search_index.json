{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Startseite","text":"<p>Wir stellen Ultralytics YOLOv8 vor, die neueste Version des renommierten Echtzeit-Modells zur Objekterkennung und Bildsegmentierung. YOLOv8 basiert auf den neuesten Erkenntnissen im Bereich Deep Learning und Computer Vision und bietet eine unvergleichliche Leistung hinsichtlich Geschwindigkeit und Genauigkeit. Sein optimiertes Design macht es f\u00fcr verschiedene Anwendungen geeignet und leicht an verschiedene Hardwareplattformen anpassbar, von Edge-Ger\u00e4ten bis hin zu Cloud-APIs.</p> <p>Erkunden Sie die YOLOv8-Dokumentation, eine umfassende Ressource, die Ihnen helfen soll, seine Funktionen und F\u00e4higkeiten zu verstehen und zu nutzen. Ob Sie ein erfahrener Machine-Learning-Praktiker sind oder neu in diesem Bereich, dieses Hub zielt darauf ab, das Potenzial von YOLOv8 in Ihren Projekten zu maximieren</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere mehrsprachige Dokumentation wird derzeit entwickelt und wir arbeiten intensiv an ihrer Verbesserung. Wir danken f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"#wo-sie-beginnen-sollten","title":"Wo Sie beginnen sollten","text":"<ul> <li>Installieren Sie <code>ultralytics</code> mit pip und starten Sie in wenigen Minuten \u00a0  Loslegen</li> <li>Vorhersagen Sie neue Bilder und Videos mit YOLOv8 \u00a0  Auf Bilder vorhersagen</li> <li>Trainieren Sie ein neues YOLOv8-Modell mit Ihrem eigenen benutzerdefinierten Datensatz \u00a0  Ein Modell trainieren</li> <li>Erforschen Sie YOLOv8-Aufgaben wie Segmentieren, Klassifizieren, Posensch\u00e4tzung und Verfolgen \u00a0  Aufgaben erkunden</li> </ul> <p> Ansehen: Wie Sie ein YOLOv8-Modell auf Ihrem eigenen Datensatz in Google Colab trainieren. </p>"},{"location":"#yolo-eine-kurze-geschichte","title":"YOLO: Eine kurze Geschichte","text":"<p>YOLO (You Only Look Once), ein beliebtes Modell zur Objekterkennung und Bildsegmentierung, wurde von Joseph Redmon und Ali Farhadi an der Universit\u00e4t von Washington entwickelt. Seit seiner Einf\u00fchrung im Jahr 2015 erfreut es sich aufgrund seiner hohen Geschwindigkeit und Genauigkeit gro\u00dfer Beliebtheit.</p> <ul> <li>YOLOv2, ver\u00f6ffentlicht im Jahr 2016, verbesserte das Originalmodell durch die Einf\u00fchrung von Batch-Normalisierung, Ankerk\u00e4sten und Dimensionsclustern.</li> <li>YOLOv3, eingef\u00fchrt im Jahr 2018, erh\u00f6hte die Leistung des Modells weiter mit einem effizienteren Backbone-Netzwerk, mehreren Ankern und r\u00e4umlichem Pyramid-Pooling.</li> <li>YOLOv4 wurde 2020 ver\u00f6ffentlicht und brachte Neuerungen wie Mosaic-Datenerweiterung, einen neuen ankerfreien Erkennungskopf und eine neue Verlustfunktion.</li> <li>YOLOv5 verbesserte die Leistung des Modells weiter und f\u00fchrte neue Funktionen ein, wie Hyperparameter-Optimierung, integriertes Experiment-Tracking und automatischen Export in beliebte Exportformate.</li> <li>YOLOv6 wurde 2022 von Meituan als Open Source zur Verf\u00fcgung gestellt und wird in vielen autonomen Lieferrobotern des Unternehmens eingesetzt.</li> <li>YOLOv7 f\u00fchrte zus\u00e4tzliche Aufgaben ein, wie Posensch\u00e4tzung auf dem COCO-Keypoints-Datensatz.</li> <li>YOLOv8 ist die neueste Version von YOLO von Ultralytics. Als Spitzenmodell der neuesten Generation baut YOLOv8 auf dem Erfolg vorheriger Versionen auf und f\u00fchrt neue Funktionen und Verbesserungen f\u00fcr erh\u00f6hte Leistung, Flexibilit\u00e4t und Effizienz ein. YOLOv8 unterst\u00fctzt eine vollst\u00e4ndige Palette an Vision-KI-Aufgaben, einschlie\u00dflich Erkennung, Segmentierung, Posensch\u00e4tzung, Verfolgung und Klassifizierung. Diese Vielseitigkeit erm\u00f6glicht es Benutzern, die F\u00e4higkeiten von YOLOv8 in verschiedenen Anwendungen und Dom\u00e4nen zu nutzen.</li> </ul>"},{"location":"#yolo-lizenzen-wie-wird-ultralytics-yolo-lizenziert","title":"YOLO-Lizenzen: Wie wird Ultralytics YOLO lizenziert?","text":"<p>Ultralytics bietet zwei Lizenzoptionen, um unterschiedliche Einsatzszenarien zu ber\u00fccksichtigen:</p> <ul> <li>AGPL-3.0-Lizenz: Diese OSI-gepr\u00fcfte Open-Source-Lizenz ist ideal f\u00fcr Studenten und Enthusiasten und f\u00f6rdert offene Zusammenarbeit und Wissensaustausch. Weitere Details finden Sie in der LIZENZ-Datei.</li> <li>Enterprise-Lizenz: F\u00fcr die kommerzielle Nutzung konzipiert, erm\u00f6glicht diese Lizenz die problemlose Integration von Ultralytics-Software und KI-Modellen in kommerzielle Produkte und Dienstleistungen und umgeht die Open-Source-Anforderungen der AGPL-3.0. Wenn Ihr Szenario die Einbettung unserer L\u00f6sungen in ein kommerzielles Angebot beinhaltet, kontaktieren Sie uns \u00fcber Ultralytics-Lizenzierung.</li> </ul> <p>Unsere Lizenzstrategie ist darauf ausgerichtet sicherzustellen, dass jegliche Verbesserungen an unseren Open-Source-Projekten der Gemeinschaft zur\u00fcckgegeben werden. Wir halten die Prinzipien von Open Source in Ehren \u2764\ufe0f und es ist unser Anliegen, dass unsere Beitr\u00e4ge auf Weisen genutzt und erweitert werden k\u00f6nnen, die f\u00fcr alle vorteilhaft sind.</p>"},{"location":"quickstart/","title":"Schnellstart","text":""},{"location":"quickstart/#ultralytics-installieren","title":"Ultralytics installieren","text":"<p>Ultralytics bietet verschiedene Installationsmethoden, darunter Pip, Conda und Docker. Installiere YOLOv8 \u00fcber das <code>ultralytics</code> Pip-Paket f\u00fcr die neueste stabile Ver\u00f6ffentlichung oder indem du das Ultralytics GitHub-Repository klonst f\u00fcr die aktuellste Version. Docker kann verwendet werden, um das Paket in einem isolierten Container auszuf\u00fchren, ohne eine lokale Installation vornehmen zu m\u00fcssen.</p> <p>Installieren</p> Pip-Installation (empfohlen)Conda-InstallationGit klonen <p>Installieren Sie das <code>ultralytics</code> Paket mit Pip oder aktualisieren Sie eine bestehende Installation, indem Sie <code>pip install -U ultralytics</code> ausf\u00fchren. Besuchen Sie den Python Package Index (PyPI) f\u00fcr weitere Details zum <code>ultralytics</code> Paket: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Installiere das ultralytics Paket von PyPI\npip install ultralytics\n</code></pre> <p>Sie k\u00f6nnen auch das <code>ultralytics</code> Paket direkt vom GitHub Repository installieren. Dies k\u00f6nnte n\u00fctzlich sein, wenn Sie die neueste Entwicklerversion m\u00f6chten. Stellen Sie sicher, dass das Git-Kommandozeilen-Tool auf Ihrem System installiert ist. Der Befehl <code>@main</code> installiert den <code>main</code> Branch und kann zu einem anderen Branch ge\u00e4ndert werden, z. B. <code>@my-branch</code>, oder ganz entfernt werden, um auf den <code>main</code> Branch standardm\u00e4\u00dfig zur\u00fcckzugreifen.</p> <pre><code># Installiere das ultralytics Paket von GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda ist ein alternativer Paketmanager zu Pip, der ebenfalls f\u00fcr die Installation verwendet werden kann. Besuche Anaconda f\u00fcr weitere Details unter https://anaconda.org/conda-forge/ultralytics. Ultralytics Feedstock Repository f\u00fcr die Aktualisierung des Conda-Pakets befindet sich unter https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Installiere das ultralytics Paket mit Conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Hinweis</p> <p>Wenn Sie in einer CUDA-Umgebung installieren, ist es am besten, <code>ultralytics</code>, <code>pytorch</code> und <code>pytorch-cuda</code> im selben Befehl zu installieren, um dem Conda-Paketmanager zu erm\u00f6glichen, Konflikte zu l\u00f6sen, oder <code>pytorch-cuda</code> als letztes zu installieren, damit es das CPU-spezifische <code>pytorch</code> Paket bei Bedarf \u00fcberschreiben kann. <pre><code># Installiere alle Pakete zusammen mit Conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Klonen Sie das <code>ultralytics</code> Repository, wenn Sie einen Beitrag zur Entwicklung leisten m\u00f6chten oder mit dem neuesten Quellcode experimentieren wollen. Nach dem Klonen navigieren Sie in das Verzeichnis und installieren das Paket im editierbaren Modus <code>-e</code> mit Pip. <pre><code># Klonen Sie das ultralytics Repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigiere zum geklonten Verzeichnis\ncd ultralytics\n\n# Installiere das Paket im editierbaren Modus f\u00fcr die Entwicklung\npip install -e .\n</code></pre></p> <p>Siehe die <code>ultralytics</code> requirements.txt Datei f\u00fcr eine Liste der Abh\u00e4ngigkeiten. Beachten Sie, dass alle oben genannten Beispiele alle erforderlichen Abh\u00e4ngigkeiten installieren.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Tipp</p> <p>PyTorch-Anforderungen variieren je nach Betriebssystem und CUDA-Anforderungen, daher wird empfohlen, PyTorch zuerst gem\u00e4\u00df den Anweisungen unter https://pytorch.org/get-started/locally zu installieren.</p> <p> </p>"},{"location":"quickstart/#conda-docker-image","title":"Conda Docker-Image","text":"<p>Ultralytics Conda Docker-Images sind ebenfalls von DockerHub verf\u00fcgbar. Diese Bilder basieren auf Miniconda3 und bieten eine einfache M\u00f6glichkeit, <code>ultralytics</code> in einer Conda-Umgebung zu nutzen.</p> <pre><code># Setze Image-Name als Variable\nt=ultralytics/ultralytics:latest-conda\n\n# Ziehe das neuste ultralytics Image von Docker Hub\nsudo docker pull $t\n\n# F\u00fchre das ultralytics Image in einem Container mit GPU-Unterst\u00fctzung aus\nsudo docker run -it --ipc=host --gpus all $t  # alle GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # spezifische GPUs angeben\n</code></pre>"},{"location":"quickstart/#ultralytics-mit-cli-verwenden","title":"Ultralytics mit CLI verwenden","text":"<p>Die Befehlszeilenschnittstelle (CLI) von Ultralytics erm\u00f6glicht einfache Einzeilige Befehle ohne die Notwendigkeit einer Python-Umgebung. CLI erfordert keine Anpassung oder Python-Code. Sie k\u00f6nnen alle Aufgaben einfach vom Terminal aus mit dem <code>yolo</code> Befehl ausf\u00fchren. Schauen Sie sich den CLI-Leitfaden an, um mehr \u00fcber die Verwendung von YOLOv8 \u00fcber die Befehlszeile zu erfahren.</p> <p>Beispiel</p> SyntaxTrainierenVorhersagenValExportierenSpeziell <p>Ultralytics <code>yolo</code> Befehle verwenden die folgende Syntax: <pre><code>yolo TASK MODE ARGS\n\nWo   TASK (optional) einer von [detect, segment, classify] ist\n        MODE (erforderlich) einer von [train, val, predict, export, track] ist\n        ARGS (optional) eine beliebige Anzahl von benutzerdefinierten 'arg=value' Paaren wie 'imgsz=320', die Vorgaben \u00fcberschreiben.\n</code></pre> Sehen Sie alle ARGS im vollst\u00e4ndigen Konfigurationsleitfaden oder mit <code>yolo cfg</code></p> <p>Trainieren Sie ein Erkennungsmodell f\u00fcr 10 Epochen mit einer Anfangslernerate von 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Vorhersagen eines YouTube-Videos mit einem vortrainierten Segmentierungsmodell bei einer Bildgr\u00f6\u00dfe von 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Val ein vortrainiertes Erkennungsmodell bei Batch-Gr\u00f6\u00dfe 1 und Bildgr\u00f6\u00dfe 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportieren Sie ein YOLOv8n-Klassifikationsmodell im ONNX-Format bei einer Bildgr\u00f6\u00dfe von 224 mal 128 (kein TASK erforderlich) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>F\u00fchren Sie spezielle Befehle aus, um Version, Einstellungen zu sehen, Checks auszuf\u00fchren und mehr: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Warnung</p> <p>Argumente m\u00fcssen als <code>arg=val</code> Paare \u00fcbergeben werden, getrennt durch ein Gleichheitszeichen <code>=</code> und durch Leerzeichen <code></code> zwischen den Paaren. Verwenden Sie keine <code>--</code> Argumentpr\u00e4fixe oder Kommata <code>,</code> zwischen den Argumenten.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>CLI-Leitfaden</p>"},{"location":"quickstart/#ultralytics-mit-python-verwenden","title":"Ultralytics mit Python verwenden","text":"<p>Die Python-Schnittstelle von YOLOv8 erm\u00f6glicht eine nahtlose Integration in Ihre Python-Projekte und erleichtert das Laden, Ausf\u00fchren und Verarbeiten der Modellausgabe. Konzipiert f\u00fcr Einfachheit und Benutzerfreundlichkeit, erm\u00f6glicht die Python-Schnittstelle Benutzern, Objekterkennung, Segmentierung und Klassifizierung schnell in ihren Projekten zu implementieren. Dies macht die Python-Schnittstelle von YOLOv8 zu einem unsch\u00e4tzbaren Werkzeug f\u00fcr jeden, der diese Funktionalit\u00e4ten in seine Python-Projekte integrieren m\u00f6chte.</p> <p>Benutzer k\u00f6nnen beispielsweise ein Modell laden, es trainieren, seine Leistung an einem Validierungsset auswerten und sogar in das ONNX-Format exportieren, und das alles mit nur wenigen Codezeilen. Schauen Sie sich den Python-Leitfaden an, um mehr \u00fcber die Verwendung von YOLOv8 in Ihren_python_pro_jek_ten zu erfahren.</p> <p>Beispiel</p> <pre><code>from ultralytics import YOLO\n\n# Erstellen Sie ein neues YOLO Modell von Grund auf\nmodel = YOLO('yolov8n.yaml')\n\n# Laden Sie ein vortrainiertes YOLO Modell (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n.pt')\n\n# Trainieren Sie das Modell mit dem Datensatz 'coco128.yaml' f\u00fcr 3 Epochen\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Bewerten Sie die Leistung des Modells am Validierungssatz\nresults = model.val()\n\n# F\u00fchren Sie eine Objekterkennung an einem Bild mit dem Modell durch\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportieren Sie das Modell ins ONNX-Format\nsuccess = model.export(format='onnx')\n</code></pre> <p>Python-Leitfaden</p>"},{"location":"datasets/","title":"\u00dcbersicht \u00fcber Datens\u00e4tze","text":"<p>Ultralytics bietet Unterst\u00fctzung f\u00fcr verschiedene Datens\u00e4tze an, um Computervisionsaufgaben wie Erkennung, Instanzsegmentierung, Posensch\u00e4tzung, Klassifizierung und Verfolgung mehrerer Objekte zu erleichtern. Unten finden Sie eine Liste der wichtigsten Ultralytics-Datens\u00e4tze, gefolgt von einer Zusammenfassung jeder Computervisionsaufgabe und den jeweiligen Datens\u00e4tzen.</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere mehrsprachige Dokumentation befindet sich derzeit im Aufbau und wir arbeiten intensiv an deren Verbesserung. Vielen Dank f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"datasets/#erkennungsdatensatze","title":"Erkennungsdatens\u00e4tze","text":"<p>Die Objekterkennung mittels Bounding Box ist eine Computervisionstechnik, die das Erkennen und Lokalisieren von Objekten in einem Bild anhand des Zeichnens einer Bounding Box um jedes Objekt beinhaltet.</p> <ul> <li>Argoverse: Ein Datensatz mit 3D-Tracking- und Bewegungsvorhersagedaten aus st\u00e4dtischen Umgebungen mit umfassenden Annotationen.</li> <li>COCO: Ein umfangreicher Datensatz f\u00fcr Objekterkennung, Segmentierung und Beschreibung mit \u00fcber 200.000 beschrifteten Bildern.</li> <li>COCO8: Enth\u00e4lt die ersten 4 Bilder aus COCO Train und COCO Val, geeignet f\u00fcr schnelle Tests.</li> <li>Global Wheat 2020: Ein Datensatz mit Bildern von Weizenk\u00f6pfen aus aller Welt f\u00fcr Objekterkennungs- und Lokalisierungsaufgaben.</li> <li>Objects365: Ein hochwertiger, gro\u00dfer Datensatz f\u00fcr Objekterkennung mit 365 Objektkategorien und \u00fcber 600.000 annotierten Bildern.</li> <li>OpenImagesV7: Ein umfassender Datensatz von Google mit 1,7 Millionen Trainingsbildern und 42.000 Validierungsbildern.</li> <li>SKU-110K: Ein Datensatz mit dichter Objekterkennung in Einzelhandelsumgebungen mit \u00fcber 11.000 Bildern und 1,7 Millionen Bounding Boxen.</li> <li>VisDrone: Ein Datensatz mit Objekterkennungs- und Multi-Objekt-Tracking-Daten aus Drohnenaufnahmen mit \u00fcber 10.000 Bildern und Videosequenzen.</li> <li>VOC: Der Pascal Visual Object Classes (VOC) Datensatz f\u00fcr Objekterkennung und Segmentierung mit 20 Objektklassen und \u00fcber 11.000 Bildern.</li> <li>xView: Ein Datensatz f\u00fcr Objekterkennung in \u00dcberwachungsbildern mit 60 Objektkategorien und \u00fcber 1 Million annotierten Objekten.</li> </ul>"},{"location":"datasets/#datensatze-fur-instanzsegmentierung","title":"Datens\u00e4tze f\u00fcr Instanzsegmentierung","text":"<p>Die Instanzsegmentierung ist eine Computervisionstechnik, die das Identifizieren und Lokalisieren von Objekten in einem Bild auf Pixelebene beinhaltet.</p> <ul> <li>COCO: Ein gro\u00dfer Datensatz f\u00fcr Objekterkennung, Segmentierung und Beschreibungsaufgaben mit \u00fcber 200.000 beschrifteten Bildern.</li> <li>COCO8-seg: Ein kleinerer Datensatz f\u00fcr Instanzsegmentierungsaufgaben, der eine Teilmenge von 8 COCO-Bildern mit Segmentierungsannotationen enth\u00e4lt.</li> </ul>"},{"location":"datasets/#posenschatzung","title":"Posensch\u00e4tzung","text":"<p>Die Posensch\u00e4tzung ist eine Technik, die verwendet wird, um die Position des Objekts relativ zur Kamera oder zum Weltkoordinatensystem zu bestimmen.</p> <ul> <li>COCO: Ein gro\u00dfer Datensatz mit menschlichen Pose-Annotationen f\u00fcr Posensch\u00e4tzungsaufgaben.</li> <li>COCO8-pose: Ein kleinerer Datensatz f\u00fcr Posensch\u00e4tzungsaufgaben, der eine Teilmenge von 8 COCO-Bildern mit menschlichen Pose-Annotationen enth\u00e4lt.</li> <li>Tiger-pose: Ein kompakter Datensatz bestehend aus 263 Bildern, die auf Tiger fokussiert sind, mit Annotationen von 12 Schl\u00fcsselpunkten pro Tiger f\u00fcr Posensch\u00e4tzungsaufgaben.</li> </ul>"},{"location":"datasets/#bildklassifizierung","title":"Bildklassifizierung","text":"<p>Die Bildklassifizierung ist eine Computervisionsaufgabe, bei der ein Bild basierend auf seinem visuellen Inhalt in eine oder mehrere vordefinierte Klassen oder Kategorien eingeteilt wird.</p> <ul> <li>Caltech 101: Enth\u00e4lt Bilder von 101 Objektkategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>Caltech 256: Eine erweiterte Version von Caltech 101 mit 256 Objektkategorien und herausfordernderen Bildern.</li> <li>CIFAR-10: Ein Datensatz mit 60.000 32x32 Farbbildern in 10 Klassen, mit 6.000 Bildern pro Klasse.</li> <li>CIFAR-100: Eine erweiterte Version von CIFAR-10 mit 100 Objektkategorien und 600 Bildern pro Klasse.</li> <li>Fashion-MNIST: Ein Datensatz mit 70.000 Graustufenbildern von 10 Modekategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>ImageNet: Ein gro\u00dfer Datensatz f\u00fcr Objekterkennung und Bildklassifizierung mit \u00fcber 14 Millionen Bildern und 20.000 Kategorien.</li> <li>ImageNet-10: Ein kleinerer Teildatensatz von ImageNet mit 10 Kategorien f\u00fcr schnelleres Experimentieren und Testen.</li> <li>Imagenette: Ein kleinerer Teildatensatz von ImageNet, der 10 leicht unterscheidbare Klassen f\u00fcr ein schnelleres Training und Testen enth\u00e4lt.</li> <li>Imagewoof: Ein herausfordernderer Teildatensatz von ImageNet mit 10 Hundezuchtkategorien f\u00fcr Bildklassifizierungsaufgaben.</li> <li>MNIST: Ein Datensatz mit 70.000 Graustufenbildern von handgeschriebenen Ziffern f\u00fcr Bildklassifizierungsaufgaben.</li> </ul>"},{"location":"datasets/#orientierte-bounding-boxes-obb","title":"Orientierte Bounding Boxes (OBB)","text":"<p>Orientierte Bounding Boxes (OBB) ist eine Methode in der Computervision f\u00fcr die Erkennung von geneigten Objekten in Bildern mithilfe von rotierten Bounding Boxen, die oft auf Luft- und Satellitenbilder angewendet wird.</p> <ul> <li>DOTAv2: Ein beliebter OBB-Datensatz f\u00fcr Luftbildaufnahmen mit 1,7 Millionen Instanzen und 11.268 Bildern.</li> </ul>"},{"location":"datasets/#multi-objekt-verfolgung","title":"Multi-Objekt-Verfolgung","text":"<p>Die Verfolgung mehrerer Objekte ist eine Computervisionstechnik, die das Erkennen und Verfolgen mehrerer Objekte \u00fcber die Zeit in einer Videosequenz beinhaltet.</p> <ul> <li>Argoverse: Ein Datensatz mit 3D-Tracking- und Bewegungsvorhersagedaten aus st\u00e4dtischen Umgebungen mit umfassenden Annotationen f\u00fcr Multi-Objekt-Verfolgungsaufgaben.</li> <li>VisDrone: Ein Datensatz mit Daten zur Objekterkennung und Multi-Objekt-Verfolgung aus Drohnenaufnahmen mit \u00fcber 10.000 Bildern und Videosequenzen.</li> </ul>"},{"location":"datasets/#neue-datensatze-beitragen","title":"Neue Datens\u00e4tze beitragen","text":"<p>Das Bereitstellen eines neuen Datensatzes umfasst mehrere Schritte, um sicherzustellen, dass er gut in die bestehende Infrastruktur integriert werden kann. Unten finden Sie die notwendigen Schritte:</p>"},{"location":"datasets/#schritte-um-einen-neuen-datensatz-beizutragen","title":"Schritte um einen neuen Datensatz beizutragen","text":"<ol> <li> <p>Bilder sammeln: Sammeln Sie die Bilder, die zum Datensatz geh\u00f6ren. Diese k\u00f6nnen von verschiedenen Quellen gesammelt werden, wie \u00f6ffentlichen Datenbanken oder Ihrer eigenen Sammlung.</p> </li> <li> <p>Bilder annotieren: Annotieren Sie diese Bilder mit Bounding Boxen, Segmenten oder Schl\u00fcsselpunkten, je nach Aufgabe.</p> </li> <li> <p>Annotationen exportieren: Konvertieren Sie diese Annotationen in das von Ultralytics unterst\u00fctzte YOLO *.txt-Dateiformat.</p> </li> <li> <p>Datensatz organisieren: Ordnen Sie Ihren Datensatz in die richtige Ordnerstruktur an. Sie sollten \u00fcbergeordnete Verzeichnisse <code>train/</code> und <code>val/</code> haben, und innerhalb dieser je ein Unterverzeichnis <code>images/</code> und <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Eine <code>data.yaml</code>-Datei erstellen: Erstellen Sie in Ihrem Stammverzeichnis des Datensatzes eine Datei <code>data.yaml</code>, die den Datensatz, die Klassen und andere notwendige Informationen beschreibt.</p> </li> <li> <p>Bilder optimieren (Optional): Wenn Sie die Gr\u00f6\u00dfe des Datensatzes f\u00fcr eine effizientere Verarbeitung reduzieren m\u00f6chten, k\u00f6nnen Sie die Bilder mit dem untenstehenden Code optimieren. Dies ist nicht erforderlich, wird aber f\u00fcr kleinere Datensatzgr\u00f6\u00dfen und schnellere Download-Geschwindigkeiten empfohlen.</p> </li> <li> <p>Datensatz zippen: Komprimieren Sie das gesamte Datensatzverzeichnis in eine Zip-Datei.</p> </li> <li> <p>Dokumentation und PR: Erstellen Sie eine Dokumentationsseite, die Ihren Datensatz beschreibt und wie er in das bestehende Framework passt. Danach reichen Sie einen Pull Request (PR) ein. Weitere Details zur Einreichung eines PR finden Sie in den Ultralytics Beitragshinweisen.</p> </li> </ol>"},{"location":"datasets/#beispielcode-zum-optimieren-und-zippen-eines-datensatzes","title":"Beispielcode zum Optimieren und Zippen eines Datensatzes","text":"<p>Optimieren und Zippen eines Datensatzes</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definieren des Verzeichnisses des Datensatzes\npath = Path('Pfad/zum/Datensatz')\n\n# Bilder im Datensatz optimieren (optional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Datensatz in 'Pfad/zum/Datensatz.zip' zippen\nzip_directory(path)\n</code></pre> <p>Indem Sie diesen Schritten folgen, k\u00f6nnen Sie einen neuen Datensatz beitragen, der gut in die bestehende Struktur von Ultralytics integriert wird.</p>"},{"location":"models/","title":"Von Ultralytics unterst\u00fctzte Modelle","text":"<p>Willkommen bei der Modell-Dokumentation von Ultralytics! Wir bieten Unterst\u00fctzung f\u00fcr eine breite Palette von Modellen, die jeweils f\u00fcr spezifische Aufgaben wie Objekterkennung, Instanzsegmentierung, Bildklassifizierung, Posensch\u00e4tzung und Multi-Objekt-Tracking ma\u00dfgeschneidert sind. Wenn Sie daran interessiert sind, Ihre Modellarchitektur bei Ultralytics beizutragen, sehen Sie sich unseren Beitragenden-Leitfaden an.</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere Dokumentation in verschiedenen Sprachen ist derzeit im Aufbau und wir arbeiten hart daran, sie zu verbessern. Vielen Dank f\u00fcr Ihre Geduld! \ud83d\ude4f</p>"},{"location":"models/#vorgestellte-modelle","title":"Vorgestellte Modelle","text":"<p>Hier sind einige der wichtigsten unterst\u00fctzten Modelle:</p> <ol> <li>YOLOv3: Die dritte Iteration der YOLO-Modellfamilie, urspr\u00fcnglich von Joseph Redmon, bekannt f\u00fcr ihre effiziente Echtzeit-Objekterkennungsf\u00e4higkeiten.</li> <li>YOLOv4: Ein dunkelnetz-natives Update von YOLOv3, ver\u00f6ffentlicht von Alexey Bochkovskiy im Jahr 2020.</li> <li>YOLOv5: Eine verbesserte Version der YOLO-Architektur von Ultralytics, die bessere Leistungs- und Geschwindigkeitskompromisse im Vergleich zu fr\u00fcheren Versionen bietet.</li> <li>YOLOv6: Ver\u00f6ffentlicht von Meituan im Jahr 2022 und in vielen autonomen Lieferrobotern des Unternehmens im Einsatz.</li> <li>YOLOv7: Aktualisierte YOLO-Modelle, die 2022 von den Autoren von YOLOv4 ver\u00f6ffentlicht wurden.</li> <li>YOLOv8 NEU \ud83d\ude80: Die neueste Version der YOLO-Familie, mit erweiterten F\u00e4higkeiten wie Instanzsegmentierung, Pose/Schl\u00fcsselpunktsch\u00e4tzung und Klassifizierung.</li> <li>Segment Anything Model (SAM): Metas Segment Anything Model (SAM).</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM f\u00fcr mobile Anwendungen, von der Kyung Hee University.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM von der Image &amp; Video Analysis Group, Institute of Automation, Chinesische Akademie der Wissenschaften.</li> <li>YOLO-NAS: YOLO Neural Architecture Search (NAS) Modelle.</li> <li>Realtime Detection Transformers (RT-DETR): Baidus PaddlePaddle Realtime Detection Transformer (RT-DETR) Modelle.</li> </ol> <p> Anschauen: F\u00fchren Sie Ultralytics YOLO-Modelle in nur wenigen Codezeilen aus. </p>"},{"location":"models/#einstieg-nutzungbeispiele","title":"Einstieg: Nutzungbeispiele","text":"<p>Dieses Beispiel bietet einfache YOLO-Trainings- und Inferenzbeispiele. F\u00fcr vollst\u00e4ndige Dokumentationen \u00fcber diese und andere Modi siehe die Dokumentationsseiten Predict,  Train, Val und Export.</p> <p>Beachten Sie, dass das folgende Beispiel f\u00fcr YOLOv8 Detect Modelle zur Objekterkennung ist. F\u00fcr zus\u00e4tzliche unterst\u00fctzte Aufgaben siehe die Dokumentation zu Segment, Classify und Pose.</p> <p>Beispiel</p> PythonCLI <p>Vorgefertigte PyTorch <code>*.pt</code> Modelle sowie Konfigurationsdateien <code>*.yaml</code> k\u00f6nnen den Klassen <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> und <code>RTDETR()</code> \u00fcbergeben werden, um eine Modellinstanz in Python zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Laden eines COCO-vortrainierten YOLOv8n Modells\nmodel = YOLO('yolov8n.pt')\n\n# Modellinformationen anzeigen (optional)\nmodel.info()\n\n# Model auf dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen trainieren\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Inferenz mit dem YOLOv8n Modell auf das Bild 'bus.jpg' ausf\u00fchren\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle sind verf\u00fcgbar, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Ein COCO-vortrainiertes YOLOv8n Modell laden und auf dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen trainieren\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Ein COCO-vortrainiertes YOLOv8n Modell laden und Inferenz auf das Bild 'bus.jpg' ausf\u00fchren\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#neue-modelle-beitragen","title":"Neue Modelle beitragen","text":"<p>Sind Sie daran interessiert, Ihr Modell bei Ultralytics beizutragen? Gro\u00dfartig! Wir sind immer offen daf\u00fcr, unser Modellportfolio zu erweitern.</p> <ol> <li> <p>Repository forken: Beginnen Sie mit dem Forken des Ultralytics GitHub-Repositorys.</p> </li> <li> <p>Ihren Fork klonen: Klonen Sie Ihren Fork auf Ihre lokale Maschine und erstellen Sie einen neuen Branch, um daran zu arbeiten.</p> </li> <li> <p>Ihr Modell implementieren: F\u00fcgen Sie Ihr Modell entsprechend den in unserem Beitragenden-Leitfaden bereitgestellten Kodierungsstandards und Richtlinien hinzu.</p> </li> <li> <p>Gr\u00fcndlich testen: Stellen Sie sicher, dass Sie Ihr Modell sowohl isoliert als auch als Teil des Pipelines gr\u00fcndlich testen.</p> </li> <li> <p>Eine Pull-Anfrage erstellen: Sobald Sie mit Ihrem Modell zufrieden sind, erstellen Sie eine Pull-Anfrage zum Hauptrepository zur \u00dcberpr\u00fcfung.</p> </li> <li> <p>Code-Review &amp; Zusammenf\u00fchren: Nach der \u00dcberpr\u00fcfung, wenn Ihr Modell unseren Kriterien entspricht, wird es in das Hauptrepository zusammengef\u00fchrt.</p> </li> </ol> <p>F\u00fcr detaillierte Schritte konsultieren Sie unseren Beitragenden-Leitfaden.</p>"},{"location":"models/fast-sam/","title":"Fast Segment Anything Model (FastSAM)","text":"<p>Das Fast Segment Anything Model (FastSAM) ist eine neuartige, Echtzeit-CNN-basierte L\u00f6sung f\u00fcr die Segment Anything Aufgabe. Diese Aufgabe zielt darauf ab, jedes Objekt in einem Bild auf Basis verschiedener m\u00f6glicher Benutzerinteraktionen zu segmentieren. FastSAM reduziert signifikant den Rechenbedarf, w\u00e4hrend es eine wettbewerbsf\u00e4hige Leistung beibeh\u00e4lt und somit f\u00fcr eine Vielzahl von Vision-Aufgaben praktisch einsetzbar ist.</p> <p></p>"},{"location":"models/fast-sam/#uberblick","title":"\u00dcberblick","text":"<p>FastSAM wurde entwickelt, um die Einschr\u00e4nkungen des Segment Anything Model (SAM) zu beheben, einem schweren Transformer-Modell mit erheblichem Rechenressourcenbedarf. Das FastSAM teilt die Segment Anything Aufgabe in zwei aufeinanderfolgende Stufen auf: die Instanzsegmentierung und die promptgesteuerte Auswahl. In der ersten Stufe wird YOLOv8-seg verwendet, um die Segmentierungsmasken aller Instanzen im Bild zu erzeugen. In der zweiten Stufe gibt es den Bereich von Interesse aus, der dem Prompt entspricht.</p>"},{"location":"models/fast-sam/#hauptmerkmale","title":"Hauptmerkmale","text":"<ol> <li> <p>Echtzeitl\u00f6sung: Durch die Nutzung der Recheneffizienz von CNNs bietet FastSAM eine Echtzeitl\u00f6sung f\u00fcr die Segment Anything Aufgabe und eignet sich somit f\u00fcr industrielle Anwendungen, die schnelle Ergebnisse erfordern.</p> </li> <li> <p>Effizienz und Leistung: FastSAM bietet eine signifikante Reduzierung des Rechen- und Ressourcenbedarfs, ohne die Leistungsqualit\u00e4t zu beeintr\u00e4chtigen. Es erzielt eine vergleichbare Leistung wie SAM, verwendet jedoch drastisch reduzierte Rechenressourcen und erm\u00f6glicht so eine Echtzeitanwendung.</p> </li> <li> <p>Promptgesteuerte Segmentierung: FastSAM kann jedes Objekt in einem Bild anhand verschiedener m\u00f6glicher Benutzerinteraktionsaufforderungen segmentieren. Dies erm\u00f6glicht Flexibilit\u00e4t und Anpassungsf\u00e4higkeit in verschiedenen Szenarien.</p> </li> <li> <p>Basierend auf YOLOv8-seg: FastSAM basiert auf YOLOv8-seg, einem Objektdetektor mit einem Instanzsegmentierungsmodul. Dadurch ist es in der Lage, die Segmentierungsmasken aller Instanzen in einem Bild effektiv zu erzeugen.</p> </li> <li> <p>Wettbewerbsf\u00e4hige Ergebnisse auf Benchmarks: Bei der Objektvorschlagsaufgabe auf MS COCO erzielt FastSAM hohe Punktzahlen bei deutlich schnellerem Tempo als SAM auf einer einzelnen NVIDIA RTX 3090. Dies demonstriert seine Effizienz und Leistungsf\u00e4higkeit.</p> </li> <li> <p>Praktische Anwendungen: Der vorgeschlagene Ansatz bietet eine neue, praktische L\u00f6sung f\u00fcr eine Vielzahl von Vision-Aufgaben mit sehr hoher Geschwindigkeit, die zehn- oder hundertmal schneller ist als vorhandene Methoden.</p> </li> <li> <p>M\u00f6glichkeit zur Modellkompression: FastSAM zeigt, dass der Rechenaufwand erheblich reduziert werden kann, indem ein k\u00fcnstlicher Prior in die Struktur eingef\u00fchrt wird. Dadurch er\u00f6ffnen sich neue M\u00f6glichkeiten f\u00fcr gro\u00dfe Modellarchitekturen f\u00fcr allgemeine Vision-Aufgaben.</p> </li> </ol>"},{"location":"models/fast-sam/#verfugbare-modelle-unterstutzte-aufgaben-und-betriebsmodi","title":"Verf\u00fcgbare Modelle, unterst\u00fctzte Aufgaben und Betriebsmodi","text":"<p>In dieser Tabelle werden die verf\u00fcgbaren Modelle mit ihren spezifischen vorab trainierten Gewichten, den unterst\u00fctzten Aufgaben und ihrer Kompatibilit\u00e4t mit verschiedenen Betriebsmodi wie Inferenz, Validierung, Training und Export angezeigt. Dabei stehen \u2705 Emojis f\u00fcr unterst\u00fctzte Modi und \u274c Emojis f\u00fcr nicht unterst\u00fctzte Modi.</p> Modelltyp Vorab trainierte Gewichte Unterst\u00fctzte Aufgaben Inferenz Validierung Training Export FastSAM-s <code>FastSAM-s.pt</code> Instanzsegmentierung \u2705 \u274c \u274c \u2705 FastSAM-x <code>FastSAM-x.pt</code> Instanzsegmentierung \u2705 \u274c \u274c \u2705"},{"location":"models/fast-sam/#beispiele-fur-die-verwendung","title":"Beispiele f\u00fcr die Verwendung","text":"<p>Die FastSAM-Modelle lassen sich problemlos in Ihre Python-Anwendungen integrieren. Ultralytics bietet eine benutzerfreundliche Python-API und CLI-Befehle zur Vereinfachung der Entwicklung.</p>"},{"location":"models/fast-sam/#verwendung-der-methode-predict","title":"Verwendung der Methode <code>predict</code>","text":"<p>Um eine Objekterkennung auf einem Bild durchzuf\u00fchren, verwenden Sie die Methode <code>predict</code> wie folgt:</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import FastSAM\nfrom ultralytics.models.fastsam import FastSAMPrompt\n\n# Definieren Sie die Quelle f\u00fcr die Inferenz\nsource = 'Pfad/zum/bus.jpg'\n\n# Erstellen Sie ein FastSAM-Modell\nmodel = FastSAM('FastSAM-s.pt')  # oder FastSAM-x.pt\n\n# F\u00fchren Sie die Inferenz auf einem Bild durch\neverything_results = model(source, device='cpu', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n\n# Bereiten Sie ein Prompt-Process-Objekt vor\nprompt_process = FastSAMPrompt(source, everything_results, device='cpu')\n\n# Alles-Prompt\nann = prompt_process.everything_prompt()\n\n# Bbox Standardform [0,0,0,0] -&gt; [x1,y1,x2,y2]\nann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n\n# Text-Prompt\nann = prompt_process.text_prompt(text='ein Foto von einem Hund')\n\n# Punkt-Prompt\n# Punkte Standard [[0,0]] [[x1,y1],[x2,y2]]\n# Punktbezeichnung Standard [0] [1,0] 0:Hintergrund, 1:Vordergrund\nann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\nprompt_process.plot(annotations=ann, output='./')\n</code></pre> <pre><code># Laden Sie ein FastSAM-Modell und segmentieren Sie alles damit\nyolo segment predict model=FastSAM-s.pt source=Pfad/zum/bus.jpg imgsz=640\n</code></pre> <p>Dieser Code-Ausschnitt zeigt die Einfachheit des Ladens eines vorab trainierten Modells und das Durchf\u00fchren einer Vorhersage auf einem Bild.</p>"},{"location":"models/fast-sam/#verwendung-von-val","title":"Verwendung von <code>val</code>","text":"<p>Die Validierung des Modells auf einem Datensatz kann wie folgt durchgef\u00fchrt werden:</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import FastSAM\n\n# Erstellen Sie ein FastSAM-Modell\nmodel = FastSAM('FastSAM-s.pt')  # oder FastSAM-x.pt\n\n# Validieren Sie das Modell\nresults = model.val(data='coco8-seg.yaml')\n</code></pre> <pre><code># Laden Sie ein FastSAM-Modell und validieren Sie es auf dem COCO8-Beispieldatensatz mit Bildgr\u00f6\u00dfe 640\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n</code></pre> <p>Bitte beachten Sie, dass FastSAM nur die Erkennung und Segmentierung einer einzigen Objektklasse unterst\u00fctzt. Das bedeutet, dass es alle Objekte als dieselbe Klasse erkennt und segmentiert. Daher m\u00fcssen Sie beim Vorbereiten des Datensatzes alle Objektkategorie-IDs in 0 umwandeln.</p>"},{"location":"models/fast-sam/#offizielle-verwendung-von-fastsam","title":"Offizielle Verwendung von FastSAM","text":"<p>FastSAM ist auch direkt aus dem https://github.com/CASIA-IVA-Lab/FastSAM Repository erh\u00e4ltlich. Hier ist ein kurzer \u00dcberblick \u00fcber die typischen Schritte, die Sie unternehmen k\u00f6nnten, um FastSAM zu verwenden:</p>"},{"location":"models/fast-sam/#installation","title":"Installation","text":"<ol> <li> <p>Klonen Sie das FastSAM-Repository:    <pre><code>git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n</code></pre></p> </li> <li> <p>Erstellen und aktivieren Sie eine Conda-Umgebung mit Python 3.9:    <pre><code>conda create -n FastSAM python=3.9\nconda activate FastSAM\n</code></pre></p> </li> <li> <p>Navigieren Sie zum geklonten Repository und installieren Sie die erforderlichen Pakete:    <pre><code>cd FastSAM\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Installieren Sie das CLIP-Modell:    <pre><code>pip install git+https://github.com/openai/CLIP.git\n</code></pre></p> </li> </ol>"},{"location":"models/fast-sam/#beispielverwendung","title":"Beispielverwendung","text":"<ol> <li> <p>Laden Sie eine Modell-Sicherung herunter.</p> </li> <li> <p>Verwenden Sie FastSAM f\u00fcr Inferenz. Beispielbefehle:</p> <ul> <li> <p>Segmentieren Sie alles in einem Bild:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n</code></pre></p> </li> <li> <p>Segmentieren Sie bestimmte Objekte anhand eines Textprompts:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"der gelbe Hund\"\n</code></pre></p> </li> <li> <p>Segmentieren Sie Objekte innerhalb eines Begrenzungsrahmens (geben Sie die Boxkoordinaten im xywh-Format an):   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n</code></pre></p> </li> <li> <p>Segmentieren Sie Objekte in der N\u00e4he bestimmter Punkte:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n</code></pre></p> </li> </ul> </li> </ol> <p>Sie k\u00f6nnen FastSAM auch \u00fcber eine Colab-Demo oder die HuggingFace-Web-Demo testen, um eine visuelle Erfahrung zu machen.</p>"},{"location":"models/fast-sam/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wir m\u00f6chten den Autoren von FastSAM f\u00fcr ihre bedeutenden Beitr\u00e4ge auf dem Gebiet der Echtzeit-Instanzsegmentierung danken:</p> BibTeX <pre><code>@misc{zhao2023fast,\n      title={Fast Segment Anything},\n      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\n      year={2023},\n      eprint={2306.12156},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Die urspr\u00fcngliche FastSAM-Arbeit ist auf arXiv zu finden. Die Autoren haben ihre Arbeit \u00f6ffentlich zug\u00e4nglich gemacht, und der Code ist auf GitHub verf\u00fcgbar. Wir sch\u00e4tzen ihre Bem\u00fchungen, das Fachgebiet voranzutreiben und ihre Arbeit der breiteren Gemeinschaft zug\u00e4nglich zu machen.</p>"},{"location":"models/mobile-sam/","title":"MobileSAM (Mobile Segment Anything Model)","text":""},{"location":"models/mobile-sam/#mobile-segment-anything-mobilesam","title":"Mobile Segment Anything (MobileSAM)","text":"<p>Das MobileSAM-Paper ist jetzt auf arXiv verf\u00fcgbar.</p> <p>Eine Demonstration von MobileSAM, das auf einer CPU ausgef\u00fchrt wird, finden Sie unter diesem Demo-Link. Die Leistung auf einer Mac i5 CPU betr\u00e4gt etwa 3 Sekunden. Auf der Hugging Face-Demo f\u00fchrt die Benutzeroberfl\u00e4che und CPUs mit niedrigerer Leistung zu einer langsameren Reaktion, aber die Funktion bleibt effektiv.</p> <p>MobileSAM ist in verschiedenen Projekten implementiert, darunter Grounding-SAM, AnyLabeling und Segment Anything in 3D.</p> <p>MobileSAM wird mit einem einzigen GPU und einem 100K-Datensatz (1% der Originalbilder) in weniger als einem Tag trainiert. Der Code f\u00fcr dieses Training wird in Zukunft verf\u00fcgbar gemacht.</p>"},{"location":"models/mobile-sam/#verfugbarkeit-von-modellen-unterstutzte-aufgaben-und-betriebsarten","title":"Verf\u00fcgbarkeit von Modellen, unterst\u00fctzte Aufgaben und Betriebsarten","text":"<p>Die folgende Tabelle zeigt die verf\u00fcgbaren Modelle mit ihren spezifischen vortrainierten Gewichten, die unterst\u00fctzten Aufgaben und ihre Kompatibilit\u00e4t mit unterschiedlichen Betriebsarten wie Inferenz, Validierung, Training und Export. Unterst\u00fctzte Betriebsarten werden mit \u2705-Emojis und nicht unterst\u00fctzte Betriebsarten mit \u274c-Emojis angezeigt.</p> Modelltyp Vortrainierte Gewichte Unterst\u00fctzte Aufgaben Inferenz Validierung Training Export MobileSAM <code>mobile_sam.pt</code> Instanzsegmentierung \u2705 \u274c \u274c \u2705"},{"location":"models/mobile-sam/#anpassung-von-sam-zu-mobilesam","title":"Anpassung von SAM zu MobileSAM","text":"<p>Da MobileSAM die gleiche Pipeline wie das Original-SAM beibeh\u00e4lt, haben wir das urspr\u00fcngliche Preprocessing, Postprocessing und alle anderen Schnittstellen eingebunden. Personen, die derzeit das urspr\u00fcngliche SAM verwenden, k\u00f6nnen daher mit minimalem Aufwand zu MobileSAM wechseln.</p> <p>MobileSAM bietet vergleichbare Leistungen wie das urspr\u00fcngliche SAM und beh\u00e4lt dieselbe Pipeline, mit Ausnahme eines Wechsels des Bildencoders. Konkret ersetzen wir den urspr\u00fcnglichen, leistungsstarken ViT-H-Encoder (632M) durch einen kleineren Tiny-ViT-Encoder (5M). Auf einem einzelnen GPU arbeitet MobileSAM in etwa 12 ms pro Bild: 8 ms auf dem Bildencoder und 4 ms auf dem Maskendekoder.</p> <p>Die folgende Tabelle bietet einen Vergleich der Bildencoder, die auf ViT basieren:</p> Bildencoder Original-SAM MobileSAM Parameter 611M 5M Geschwindigkeit 452ms 8ms <p>Sowohl das urspr\u00fcngliche SAM als auch MobileSAM verwenden denselben promptgef\u00fchrten Maskendekoder:</p> Maskendekoder Original-SAM MobileSAM Parameter 3.876M 3.876M Geschwindigkeit 4ms 4ms <p>Hier ist ein Vergleich der gesamten Pipeline:</p> Gesamte Pipeline (Enc+Dec) Original-SAM MobileSAM Parameter 615M 9.66M Geschwindigkeit 456ms 12ms <p>Die Leistung von MobileSAM und des urspr\u00fcnglichen SAM werden sowohl mit einem Punkt als auch mit einem Kasten als Prompt demonstriert.</p> <p></p> <p></p> <p>Mit seiner \u00fcberlegenen Leistung ist MobileSAM etwa 5-mal kleiner und 7-mal schneller als das aktuelle FastSAM. Weitere Details finden Sie auf der MobileSAM-Projektseite.</p>"},{"location":"models/mobile-sam/#testen-von-mobilesam-in-ultralytics","title":"Testen von MobileSAM in Ultralytics","text":"<p>Wie beim urspr\u00fcnglichen SAM bieten wir eine unkomplizierte Testmethode in Ultralytics an, einschlie\u00dflich Modi f\u00fcr Punkt- und Kasten-Prompts.</p>"},{"location":"models/mobile-sam/#modell-download","title":"Modell-Download","text":"<p>Sie k\u00f6nnen das Modell hier herunterladen.</p>"},{"location":"models/mobile-sam/#punkt-prompt","title":"Punkt-Prompt","text":"<p>Beispiel</p> Python <pre><code>from ultralytics import SAM\n\n# Laden Sie das Modell\nmodel = SAM('mobile_sam.pt')\n\n# Vorhersage einer Segmentierung basierend auf einem Punkt-Prompt\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre>"},{"location":"models/mobile-sam/#kasten-prompt","title":"Kasten-Prompt","text":"<p>Beispiel</p> Python <pre><code>from ultralytics import SAM\n\n# Laden Sie das Modell\nmodel = SAM('mobile_sam.pt')\n\n# Vorhersage einer Segmentierung basierend auf einem Kasten-Prompt\nmodel.predict('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n</code></pre> <p>Wir haben <code>MobileSAM</code> und <code>SAM</code> mit derselben API implementiert. F\u00fcr weitere Verwendungsinformationen sehen Sie bitte die SAM-Seite.</p>"},{"location":"models/mobile-sam/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wenn Sie MobileSAM in Ihrer Forschungs- oder Entwicklungsarbeit n\u00fctzlich finden, zitieren Sie bitte unser Paper:</p> BibTeX <p>```bibtex @article{mobile_sam,   title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},   author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},   journal={arXiv preprint arXiv:2306.14289},   year={2023} }</p>"},{"location":"models/rtdetr/","title":"Baidus RT-DETR: Ein Echtzeit-Objektdetektor auf Basis von Vision Transformers","text":""},{"location":"models/rtdetr/#uberblick","title":"\u00dcberblick","text":"<p>Der Real-Time Detection Transformer (RT-DETR), entwickelt von Baidu, ist ein moderner End-to-End-Objektdetektor, der Echtzeitleistung mit hoher Genauigkeit bietet. Er nutzt die Leistung von Vision Transformers (ViT), um Multiskalen-Funktionen effizient zu verarbeiten, indem intra-skaliere Interaktion und eine skalen\u00fcbergreifende Fusion entkoppelt werden. RT-DETR ist hoch anpassungsf\u00e4hig und unterst\u00fctzt flexible Anpassung der Inferenzgeschwindigkeit durch Verwendung verschiedener Decoder-Schichten ohne erneutes Training. Das Modell \u00fcbertrifft viele andere Echtzeit-Objektdetektoren auf beschleunigten Backends wie CUDA mit TensorRT.</p> <p> \u00dcbersicht von Baidus RT-DETR. Die Modellarchitekturdiagramm des RT-DETR zeigt die letzten drei Stufen des Backbone {S3, S4, S5} als Eingabe f\u00fcr den Encoder. Der effiziente Hybrid-Encoder verwandelt Multiskalen-Funktionen durch intraskalare Feature-Interaktion (AIFI) und das skalen\u00fcbergreifende Feature-Fusion-Modul (CCFM) in eine Sequenz von Bildmerkmalen. Die IoU-bewusste Query-Auswahl wird verwendet, um eine feste Anzahl von Bildmerkmalen als anf\u00e4ngliche Objekt-Queries f\u00fcr den Decoder auszuw\u00e4hlen. Der Decoder optimiert iterativ Objekt-Queries, um Boxen und Vertrauenswerte zu generieren (Quelle).</p>"},{"location":"models/rtdetr/#hauptmerkmale","title":"Hauptmerkmale","text":"<ul> <li>Effizienter Hybrid-Encoder: Baidus RT-DETR verwendet einen effizienten Hybrid-Encoder, der Multiskalen-Funktionen verarbeitet, indem intra-skaliere Interaktion und eine skalen\u00fcbergreifende Fusion entkoppelt werden. Dieses einzigartige Design auf Basis von Vision Transformers reduziert die Rechenkosten und erm\u00f6glicht die Echtzeit-Objekterkennung.</li> <li>IoU-bewusste Query-Auswahl: Baidus RT-DETR verbessert die Initialisierung von Objekt-Queries, indem IoU-bewusste Query-Auswahl verwendet wird. Dadurch kann das Modell sich auf die relevantesten Objekte in der Szene konzentrieren und die Erkennungsgenauigkeit verbessern.</li> <li>Anpassbare Inferenzgeschwindigkeit: Baidus RT-DETR erm\u00f6glicht flexible Anpassungen der Inferenzgeschwindigkeit durch Verwendung unterschiedlicher Decoder-Schichten ohne erneutes Training. Diese Anpassungsf\u00e4higkeit erleichtert den praktischen Einsatz in verschiedenen Echtzeit-Objekterkennungsszenarien.</li> </ul>"},{"location":"models/rtdetr/#vortrainierte-modelle","title":"Vortrainierte Modelle","text":"<p>Die Ultralytics Python API bietet vortrainierte PaddlePaddle RT-DETR-Modelle in verschiedenen Skalierungen:</p> <ul> <li>RT-DETR-L: 53,0% AP auf COCO val2017, 114 FPS auf T4 GPU</li> <li>RT-DETR-X: 54,8% AP auf COCO val2017, 74 FPS auf T4 GPU</li> </ul>"},{"location":"models/rtdetr/#beispiele-fur-die-verwendung","title":"Beispiele f\u00fcr die Verwendung","text":"<p>Das folgende Beispiel enth\u00e4lt einfache Trainings- und Inferenzbeispiele f\u00fcr RT-DETRR. F\u00fcr die vollst\u00e4ndige Dokumentation zu diesen und anderen Modi siehe die Dokumentationsseiten f\u00fcr Predict, Train, Val und Export.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import RTDETR\n\n# Laden Sie ein vortrainiertes RT-DETR-l Modell auf COCO\nmodel = RTDETR('rtdetr-l.pt')\n\n# Zeigen Sie Informationen \u00fcber das Modell an (optional)\nmodel.info()\n\n# Trainieren Sie das Modell auf dem COCO8-Beispiel-Datensatz f\u00fcr 100 Epochen\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# F\u00fchren Sie die Inferenz mit dem RT-DETR-l Modell auf dem Bild 'bus.jpg' aus\nresults = model('path/to/bus.jpg')\n</code></pre> <pre><code># Laden Sie ein vortrainiertes RT-DETR-l Modell auf COCO und trainieren Sie es auf dem COCO8-Beispiel-Datensatz f\u00fcr 100 Epochen\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Laden Sie ein vortrainiertes RT-DETR-l Modell auf COCO und f\u00fchren Sie die Inferenz auf dem Bild 'bus.jpg' aus\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/rtdetr/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>In dieser Tabelle werden die Modelltypen, die spezifischen vortrainierten Gewichte, die von jedem Modell unterst\u00fctzten Aufgaben und die verschiedenen Modi (Train, Val, Predict, Export), die unterst\u00fctzt werden, mit \u2705-Emoji angezeigt.</p> Modelltyp Vortrainierte Gewichte Unterst\u00fctzte Aufgaben Inferenz Validierung Training Exportieren RT-DETR Gro\u00df <code>rtdetr-l.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 RT-DETR Extra-Gro\u00df <code>rtdetr-x.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705"},{"location":"models/rtdetr/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wenn Sie Baidus RT-DETR in Ihrer Forschungs- oder Entwicklungsarbeit verwenden, zitieren Sie bitte das urspr\u00fcngliche Papier:</p> BibTeX <pre><code>@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Wenyu Lv and Shangliang Xu and Yian Zhao and Guanzhong Wang and Jinman Wei and Cheng Cui and Yuning Du and Qingqing Dang and Yi Liu},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Wir m\u00f6chten Baidu und dem PaddlePaddle-Team f\u00fcr die Erstellung und Pflege dieser wertvollen Ressource f\u00fcr die Computer-Vision-Community danken. Ihre Beitrag zum Gebiet der Entwicklung des Echtzeit-Objekterkenners auf Basis von Vision Transformers, RT-DETR, wird sehr gesch\u00e4tzt.</p> <p>Keywords: RT-DETR, Transformer, ViT, Vision Transformers, Baidu RT-DETR, PaddlePaddle, Paddle Paddle RT-DETR, Objekterkennung in Echtzeit, objekterkennung basierend auf Vision Transformers, vortrainierte PaddlePaddle RT-DETR Modelle, Verwendung von Baidus RT-DETR, Ultralytics Python API</p>"},{"location":"models/sam/","title":"Segment Anything Model (SAM)","text":"<p>Willkommen an der Spitze der Bildsegmentierung mit dem Segment Anything Model (SAM). Dieses revolution\u00e4re Modell hat mit promptabler Bildsegmentierung und Echtzeit-Performance neue Standards in diesem Bereich gesetzt.</p>"},{"location":"models/sam/#einfuhrung-in-sam-das-segment-anything-model","title":"Einf\u00fchrung in SAM: Das Segment Anything Model","text":"<p>Das Segment Anything Model (SAM) ist ein innovatives Bildsegmentierungsmodell, das promptable Segmentierung erm\u00f6glicht und so eine beispiellose Vielseitigkeit bei der Bildanalyse bietet. SAM bildet das Herzst\u00fcck der Segment Anything Initiative, einem bahnbrechenden Projekt, das ein neuartiges Modell, eine neue Aufgabe und einen neuen Datensatz f\u00fcr die Bildsegmentierung einf\u00fchrt.</p> <p>Dank seiner fortschrittlichen Konstruktion kann SAM sich an neue Bildverteilungen und Aufgaben anpassen, auch ohne Vorwissen. Das wird als Zero-Shot-Transfer bezeichnet. Trainiert wurde SAM auf dem umfangreichen SA-1B-Datensatz, der \u00fcber 1 Milliarde Masken auf 11 Millionen sorgf\u00e4ltig kuratierten Bildern enth\u00e4lt. SAM hat beeindruckende Zero-Shot-Performance gezeigt und in vielen F\u00e4llen fr\u00fchere vollst\u00e4ndig \u00fcberwachte Ergebnisse \u00fcbertroffen.</p> <p> Beispielimagen mit \u00fcberlagernden Masken aus unserem neu eingef\u00fchrten Datensatz SA-1B. SA-1B enth\u00e4lt 11 Millionen diverse, hochaufl\u00f6sende, lizenzierte und die Privatsph\u00e4re sch\u00fctzende Bilder und 1,1 Milliarden qualitativ hochwertige Segmentierungsmasken. Diese wurden vollautomatisch von SAM annotiert und sind nach menschlichen Bewertungen und zahlreichen Experimenten von hoher Qualit\u00e4t und Vielfalt. Die Bilder sind nach der Anzahl der Masken pro Bild gruppiert (im Durchschnitt sind es etwa 100 Masken pro Bild).</p>"},{"location":"models/sam/#hauptmerkmale-des-segment-anything-model-sam","title":"Hauptmerkmale des Segment Anything Model (SAM)","text":"<ul> <li>Promptable Segmentierungsaufgabe: SAM wurde mit der Ausf\u00fchrung einer promptable Segmentierungsaufgabe entwickelt, wodurch es valide Segmentierungsmasken aus beliebigen Prompts generieren kann, z. B. r\u00e4umlichen oder textuellen Hinweisen zur Identifizierung eines Objekts.</li> <li>Fortgeschrittene Architektur: Das Segment Anything Model verwendet einen leistungsf\u00e4higen Bild-Encoder, einen Prompt-Encoder und einen leichten Masken-Decoder. Diese einzigartige Architektur erm\u00f6glicht flexibles Prompting, Echtzeitmaskenberechnung und Ber\u00fccksichtigung von Mehrdeutigkeiten in Segmentierungsaufgaben.</li> <li>Der SA-1B-Datensatz: Eingef\u00fchrt durch das Segment Anything Projekt, enth\u00e4lt der SA-1B-Datensatz \u00fcber 1 Milliarde Masken auf 11 Millionen Bildern. Als bisher gr\u00f6\u00dfter Segmentierungsdatensatz liefert er SAM eine vielf\u00e4ltige und umfangreiche Datenquelle f\u00fcr das Training.</li> <li>Zero-Shot-Performance: SAM zeigt herausragende Zero-Shot-Performance in verschiedenen Segmentierungsaufgaben und ist damit ein einsatzbereites Werkzeug f\u00fcr vielf\u00e4ltige Anwendungen mit minimalem Bedarf an prompt engineering.</li> </ul> <p>F\u00fcr eine detaillierte Betrachtung des Segment Anything Models und des SA-1B-Datensatzes besuchen Sie bitte die Segment Anything Website und lesen Sie das Forschungspapier Segment Anything.</p>"},{"location":"models/sam/#verfugbare-modelle-unterstutzte-aufgaben-und-betriebsmodi","title":"Verf\u00fcgbare Modelle, unterst\u00fctzte Aufgaben und Betriebsmodi","text":"<p>Diese Tabelle zeigt die verf\u00fcgbaren Modelle mit ihren spezifischen vortrainierten Gewichten, die unterst\u00fctzten Aufgaben und ihre Kompatibilit\u00e4t mit verschiedenen Betriebsmodi wie Inference, Validierung, Training und Export, wobei \u2705 Emojis f\u00fcr unterst\u00fctzte Modi und \u274c Emojis f\u00fcr nicht unterst\u00fctzte Modi verwendet werden.</p> Modelltyp Vortrainierte Gewichte Unterst\u00fctzte Aufgaben Inference Validierung Training Export SAM base <code>sam_b.pt</code> Instanzsegmentierung \u2705 \u274c \u274c \u2705 SAM large <code>sam_l.pt</code> Instanzsegmentierung \u2705 \u274c \u274c \u2705"},{"location":"models/sam/#wie-man-sam-verwendet-vielseitigkeit-und-power-in-der-bildsegmentierung","title":"Wie man SAM verwendet: Vielseitigkeit und Power in der Bildsegmentierung","text":"<p>Das Segment Anything Model kann f\u00fcr eine Vielzahl von Aufgaben verwendet werden, die \u00fcber die Trainingsdaten hinausgehen. Dazu geh\u00f6ren Kantenerkennung, Generierung von Objektvorschl\u00e4gen, Instanzsegmentierung und vorl\u00e4ufige Text-to-Mask-Vorhersage. Mit prompt engineering kann SAM sich schnell an neue Aufgaben und Datenverteilungen anpassen und sich so als vielseitiges und leistungsstarkes Werkzeug f\u00fcr alle Anforderungen der Bildsegmentierung etablieren.</p>"},{"location":"models/sam/#beispiel-fur-sam-vorhersage","title":"Beispiel f\u00fcr SAM-Vorhersage","text":"<p>Segmentierung mit Prompts</p> <p>Bildsegmentierung mit gegebenen Prompts.</p> Python <pre><code>from ultralytics import SAM\n\n# Modell laden\nmodel = SAM('sam_b.pt')\n\n# Modellinformationen anzeigen (optional)\nmodel.info()\n\n# Inferenz mit Bounding Box Prompt\nmodel('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n\n# Inferenz mit Point Prompt\nmodel('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre> <p>Alles segmentieren</p> <p>Das ganze Bild segmentieren.</p> PythonCLI <pre><code>from ultralytics import SAM\n\n# Modell laden\nmodel = SAM('sam_b.pt')\n\n# Modellinformationen anzeigen (optional)\nmodel.info()\n\n# Inferenz\nmodel('Pfad/zum/Bild.jpg')\n</code></pre> <pre><code># Inferenz mit einem SAM-Modell\nyolo predict model=sam_b.pt source=Pfad/zum/Bild.jpg\n</code></pre> <ul> <li>Die Logik hier besteht darin, das gesamte Bild zu segmentieren, wenn keine Prompts (Bounding Box/Point/Maske) \u00fcbergeben werden.</li> </ul> <p>Beispiel SAMPredictor</p> <p>Dadurch k\u00f6nnen Sie das Bild einmal festlegen und mehrmals Inferenz mit Prompts ausf\u00fchren, ohne den Bild-Encoder mehrfach auszuf\u00fchren.</p> Prompt-Inferenz <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# SAMPredictor erstellen\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# Bild festlegen\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # Festlegung mit Bild-Datei\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # Festlegung mit np.ndarray\nresults = predictor(bboxes=[439, 437, 524, 709])\nresults = predictor(points=[900, 370], labels=[1])\n\n# Bild zur\u00fccksetzen\npredictor.reset_image()\n</code></pre> <p>Alles segmentieren mit zus\u00e4tzlichen Argumenten.</p> Alles segmentieren <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# SAMPredictor erstellen\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# Mit zus\u00e4tzlichen Argumenten segmentieren\nresults = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n</code></pre> <ul> <li>Weitere zus\u00e4tzliche Argumente f\u00fcr <code>Alles segmentieren</code> finden Sie in der <code>Predictor/generate</code> Referenz.</li> </ul>"},{"location":"models/sam/#vergleich-von-sam-und-yolov8","title":"Vergleich von SAM und YOLOv8","text":"<p>Hier vergleichen wir Meta's kleinstes SAM-Modell, SAM-b, mit Ultralytics kleinstem Segmentierungsmodell, YOLOv8n-seg:</p> Modell Gr\u00f6\u00dfe Parameter Geschwindigkeit (CPU) Meta's SAM-b 358 MB 94,7 M 51096 ms/pro Bild MobileSAM 40,7 MB 10,1 M 46122 ms/pro Bild FastSAM-s mit YOLOv8-Backbone 23,7 MB 11,8 M 115 ms/pro Bild Ultralytics YOLOv8n-seg 6,7 MB (53,4-mal kleiner) 3,4 M (27,9-mal kleiner) 59 ms/pro Bild (866-mal schneller) <p>Dieser Vergleich zeigt die Gr\u00f6\u00dfen- und Geschwindigkeitsunterschiede zwischen den Modellen. W\u00e4hrend SAM einzigartige F\u00e4higkeiten f\u00fcr die automatische Segmentierung bietet, konkurriert es nicht direkt mit YOLOv8-Segmentierungsmodellen, die kleiner, schneller und effizienter sind.</p> <p>Die Tests wurden auf einem Apple M2 MacBook aus dem Jahr 2023 mit 16 GB RAM durchgef\u00fchrt. Um diesen Test zu reproduzieren:</p> <p>Beispiel</p> Python <pre><code>from ultralytics import FastSAM, SAM, YOLO\n\n# SAM-b profilieren\nmodel = SAM('sam_b.pt')\nmodel.info()\nmodel('ultralytics/assets')\n\n# MobileSAM profilieren\nmodel = SAM('mobile_sam.pt')\nmodel.info()\nmodel('ultralytics/assets')\n\n# FastSAM-s profilieren\nmodel = FastSAM('FastSAM-s.pt')\nmodel.info()\nmodel('ultralytics/assets')\n\n# YOLOv8n-seg profilieren\nmodel = YOLO('yolov8n-seg.pt')\nmodel.info()\nmodel('ultralytics/assets')\n</code></pre>"},{"location":"models/sam/#auto-annotierung-der-schnelle-weg-zu-segmentierungsdatensatzen","title":"Auto-Annotierung: Der schnelle Weg zu Segmentierungsdatens\u00e4tzen","text":"<p>Die Auto-Annotierung ist eine wichtige Funktion von SAM, mit der Benutzer mithilfe eines vortrainierten Detektionsmodells einen Segmentierungsdatensatz generieren k\u00f6nnen. Diese Funktion erm\u00f6glicht eine schnelle und genaue Annotation einer gro\u00dfen Anzahl von Bildern, ohne dass zeitaufw\u00e4ndiges manuelles Labeling erforderlich ist.</p>"},{"location":"models/sam/#generieren-sie-ihren-segmentierungsdatensatz-mit-einem-detektionsmodell","title":"Generieren Sie Ihren Segmentierungsdatensatz mit einem Detektionsmodell","text":"<p>Um Ihren Datensatz mit dem Ultralytics-Framework automatisch zu annotieren, verwenden Sie die <code>auto_annotate</code> Funktion wie folgt:</p> <p>Beispiel</p> Python <pre><code>from ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"Pfad/zum/Bilderordner\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argument Typ Beschreibung Standard data str Pfad zu einem Ordner, der die zu annotierenden Bilder enth\u00e4lt. det_model str, optional Vortrainiertes YOLO-Detektionsmodell. Standardm\u00e4\u00dfig 'yolov8x.pt'. 'yolov8x.pt' sam_model str, optional Vortrainiertes SAM-Segmentierungsmodell. Standardm\u00e4\u00dfig 'sam_b.pt'. 'sam_b.pt' device str, optional Ger\u00e4t, auf dem die Modelle ausgef\u00fchrt werden. Standardm\u00e4\u00dfig ein leerer String (CPU oder GPU, falls verf\u00fcgbar). output_dir str, None, optional Verzeichnis zum Speichern der annotierten Ergebnisse. Standardm\u00e4\u00dfig ein 'labels'-Ordner im selben Verzeichnis wie 'data'. None <p>Die <code>auto_annotate</code> Funktion nimmt den Pfad zu Ihren Bildern entgegen, mit optionalen Argumenten f\u00fcr das vortrainierte Detektions- und SAM-Segmentierungsmodell, das Ger\u00e4t, auf dem die Modelle ausgef\u00fchrt werden sollen, und das Ausgabeverzeichnis, in dem die annotierten Ergebnisse gespeichert werden sollen.</p> <p>Die Auto-Annotierung mit vortrainierten Modellen kann die Zeit und den Aufwand f\u00fcr die Erstellung hochwertiger Segmentierungsdatens\u00e4tze erheblich reduzieren. Diese Funktion ist besonders vorteilhaft f\u00fcr Forscher und Entwickler, die mit gro\u00dfen Bildersammlungen arbeiten. Sie erm\u00f6glicht es ihnen, sich auf die Modellentwicklung und -bewertung zu konzentrieren, anstatt auf die manuelle Annotation.</p>"},{"location":"models/sam/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wenn Sie SAM in Ihrer Forschungs- oder Entwicklungsarbeit n\u00fctzlich finden, erw\u00e4gen Sie bitte, unser Paper zu zitieren:</p> BibTeX <pre><code>@misc{kirillov2023segment,\n      title={Segment Anything},\n      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross Girshick},\n      year={2023},\n      eprint={2304.02643},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Wir m\u00f6chten Meta AI f\u00fcr die Erstellung und Pflege dieser wertvollen Ressource f\u00fcr die Computer Vision Community danken.</p> <p>Stichworte: Segment Anything, Segment Anything Model, SAM, Meta SAM, Bildsegmentierung, Promptable Segmentierung, Zero-Shot-Performance, SA-1B-Datensatz, fortschrittliche Architektur, Auto-Annotierung, Ultralytics, vortrainierte Modelle, SAM Base, SAM Large, Instanzsegmentierung, Computer Vision, K\u00fcnstliche Intelligenz, maschinelles Lernen, Datenannotation, Segmentierungsmasken, Detektionsmodell, YOLO Detektionsmodell, Bibtex, Meta AI.</p>"},{"location":"models/yolo-nas/","title":"YOLO-NAS","text":""},{"location":"models/yolo-nas/#ubersicht","title":"\u00dcbersicht","text":"<p>Entwickelt von Deci AI, ist YOLO-NAS ein bahnbrechendes Modell f\u00fcr die Objekterkennung. Es ist das Ergebnis fortschrittlicher Technologien zur Neural Architecture Search und wurde sorgf\u00e4ltig entworfen, um die Einschr\u00e4nkungen fr\u00fcherer YOLO-Modelle zu \u00fcberwinden. Mit signifikanten Verbesserungen in der Quantisierungsunterst\u00fctzung und Abw\u00e4gung von Genauigkeit und Latenz stellt YOLO-NAS einen gro\u00dfen Fortschritt in der Objekterkennung dar.</p> <p> \u00dcbersicht \u00fcber YOLO-NAS. YOLO-NAS verwendet Quantisierungsbl\u00f6cke und selektive Quantisierung f\u00fcr optimale Leistung. Das Modell weist bei der Konvertierung in seine quantisierte Version mit INT8 einen minimalen Pr\u00e4zisionsverlust auf, was im Vergleich zu anderen Modellen eine signifikante Verbesserung darstellt. Diese Entwicklungen f\u00fchren zu einer \u00fcberlegenen Architektur mit beispiellosen F\u00e4higkeiten zur Objekterkennung und herausragender Leistung.</p>"},{"location":"models/yolo-nas/#schlusselfunktionen","title":"Schl\u00fcsselfunktionen","text":"<ul> <li>Quantisierungsfreundlicher Basiselement: YOLO-NAS f\u00fchrt ein neues Basiselement ein, das f\u00fcr Quantisierung geeignet ist und eine der wesentlichen Einschr\u00e4nkungen fr\u00fcherer YOLO-Modelle angeht.</li> <li>Raffiniertes Training und Quantisierung: YOLO-NAS nutzt fortschrittliche Trainingsschemata und post-training Quantisierung zur Leistungsverbesserung.</li> <li>AutoNAC-Optimierung und Vortraining: YOLO-NAS verwendet die AutoNAC-Optimierung und wird auf prominenten Datens\u00e4tzen wie COCO, Objects365 und Roboflow 100 vortrainiert. Dieses Vortraining macht es \u00e4u\u00dferst geeignet f\u00fcr die Objekterkennung in Produktionsumgebungen.</li> </ul>"},{"location":"models/yolo-nas/#vortrainierte-modelle","title":"Vortrainierte Modelle","text":"<p>Erleben Sie die Leistungsf\u00e4higkeit der Objekterkennung der n\u00e4chsten Generation mit den vortrainierten YOLO-NAS-Modellen von Ultralytics. Diese Modelle sind darauf ausgelegt, sowohl bei Geschwindigkeit als auch bei Genauigkeit hervorragende Leistung zu liefern. W\u00e4hlen Sie aus einer Vielzahl von Optionen, die auf Ihre spezifischen Anforderungen zugeschnitten sind:</p> Modell mAP Latenz (ms) YOLO-NAS S 47,5 3,21 YOLO-NAS M 51,55 5,85 YOLO-NAS L 52,22 7,87 YOLO-NAS S INT-8 47,03 2,36 YOLO-NAS M INT-8 51,0 3,78 YOLO-NAS L INT-8 52,1 4,78 <p>Jede Modellvariante ist darauf ausgelegt, eine Balance zwischen Mean Average Precision (mAP) und Latenz zu bieten und Ihre Objekterkennungsaufgaben f\u00fcr Performance und Geschwindigkeit zu optimieren.</p>"},{"location":"models/yolo-nas/#beispiele-zur-verwendung","title":"Beispiele zur Verwendung","text":"<p>Ultralytics hat es einfach gemacht, YOLO-NAS-Modelle in Ihre Python-Anwendungen \u00fcber unser <code>ultralytics</code> Python-Paket zu integrieren. Das Paket bietet eine benutzerfreundliche Python-API, um den Prozess zu optimieren.</p> <p>Die folgenden Beispiele zeigen, wie Sie YOLO-NAS-Modelle mit dem <code>ultralytics</code>-Paket f\u00fcr Inferenz und Validierung verwenden:</p>"},{"location":"models/yolo-nas/#beispiele-fur-inferenz-und-validierung","title":"Beispiele f\u00fcr Inferenz und Validierung","text":"<p>In diesem Beispiel validieren wir YOLO-NAS-s auf dem COCO8-Datensatz.</p> <p>Beispiel</p> <p>Dieses Beispiel bietet einfachen Code f\u00fcr Inferenz und Validierung f\u00fcr YOLO-NAS. F\u00fcr die Verarbeitung von Inferenzergebnissen siehe den Predict-Modus. F\u00fcr die Verwendung von YOLO-NAS mit zus\u00e4tzlichen Modi siehe Val und Export. Das YOLO-NAS-Modell im <code>ultralytics</code>-Paket unterst\u00fctzt kein Training.</p> PythonCLI <p>Vorab trainierte <code>*.pt</code>-Modelldateien von PyTorch k\u00f6nnen der Klasse <code>NAS()</code> \u00fcbergeben werden, um eine Modellinstanz in Python zu erstellen:</p> <pre><code>from ultralytics import NAS\n\n# Laden Sie ein auf COCO vortrainiertes YOLO-NAS-s-Modell\nmodel = NAS('yolo_nas_s.pt')\n\n# Modelinformationen anzeigen (optional)\nmodel.info()\n\n# Validieren Sie das Modell am Beispiel des COCO8-Datensatzes\nresults = model.val(data='coco8.yaml')\n\n# F\u00fchren Sie Inferenz mit dem YOLO-NAS-s-Modell auf dem Bild 'bus.jpg' aus\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle sind verf\u00fcgbar, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Laden Sie ein auf COCO vortrainiertes YOLO-NAS-s-Modell und validieren Sie die Leistung am Beispiel des COCO8-Datensatzes\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Laden Sie ein auf COCO vortrainiertes YOLO-NAS-s-Modell und f\u00fchren Sie Inferenz auf dem Bild 'bus.jpg' aus\nyolo predict model=yolo_nas_s.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolo-nas/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>Wir bieten drei Varianten der YOLO-NAS-Modelle an: Small (s), Medium (m) und Large (l). Jede Variante ist dazu gedacht, unterschiedliche Berechnungs- und Leistungsanforderungen zu erf\u00fcllen:</p> <ul> <li>YOLO-NAS-s: Optimiert f\u00fcr Umgebungen mit begrenzten Rechenressourcen, bei denen Effizienz entscheidend ist.</li> <li>YOLO-NAS-m: Bietet einen ausgewogenen Ansatz und ist f\u00fcr die Objekterkennung im Allgemeinen mit h\u00f6herer Genauigkeit geeignet.</li> <li>YOLO-NAS-l: Ma\u00dfgeschneidert f\u00fcr Szenarien, bei denen h\u00f6chste Genauigkeit gefordert ist und Rechenressourcen weniger einschr\u00e4nkend sind.</li> </ul> <p>Im Folgenden finden Sie eine detaillierte \u00dcbersicht \u00fcber jedes Modell, einschlie\u00dflich Links zu den vortrainierten Gewichten, den unterst\u00fctzten Aufgaben und deren Kompatibilit\u00e4t mit verschiedenen Betriebsmodi.</p> Modelltyp Vortrainierte Gewichte Unterst\u00fctzte Aufgaben Inferenz Validierung Training Export YOLO-NAS-s yolo_nas_s.pt Objekterkennung \u2705 \u2705 \u274c \u2705 YOLO-NAS-m yolo_nas_m.pt Objekterkennung \u2705 \u2705 \u274c \u2705 YOLO-NAS-l yolo_nas_l.pt Objekterkennung \u2705 \u2705 \u274c \u2705"},{"location":"models/yolo-nas/#zitierungen-und-danksagungen","title":"Zitierungen und Danksagungen","text":"<p>Wenn Sie YOLO-NAS in Ihrer Forschungs- oder Entwicklungsarbeit verwenden, zitieren Sie bitte SuperGradients:</p> BibTeX <pre><code>@misc{supergradients,\n      doi = {10.5281/ZENODO.7789328},\n      url = {https://zenodo.org/record/7789328},\n      author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\n      title = {Super-Gradients},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      year = {2021},\n}\n</code></pre> <p>Wir m\u00f6chten dem SuperGradients-Team von Deci AI f\u00fcr ihre Bem\u00fchungen bei der Erstellung und Pflege dieser wertvollen Ressource f\u00fcr die Computer Vision Community danken. Wir sind der Meinung, dass YOLO-NAS mit seiner innovativen Architektur und seinen herausragenden F\u00e4higkeiten zur Objekterkennung ein wichtiges Werkzeug f\u00fcr Entwickler und Forscher gleicherma\u00dfen wird.</p> <p>Keywords: YOLO-NAS, Deci AI, Objekterkennung, Deep Learning, Neural Architecture Search, Ultralytics Python API, YOLO-Modell, SuperGradients, vortrainierte Modelle, quantisierungsfreundliches Basiselement, fortschrittliche Trainingsschemata, post-training Quantisierung, AutoNAC-Optimierung, COCO, Objects365, Roboflow 100</p>"},{"location":"models/yolov3/","title":"YOLOv3, YOLOv3-Ultralytics und YOLOv3u","text":""},{"location":"models/yolov3/#ubersicht","title":"\u00dcbersicht","text":"<p>Dieses Dokument bietet eine \u00dcbersicht \u00fcber drei eng verwandte Modelle zur Objekterkennung, n\u00e4mlich YOLOv3, YOLOv3-Ultralytics und YOLOv3u.</p> <ol> <li> <p>YOLOv3: Dies ist die dritte Version des You Only Look Once (YOLO) Objekterkennungsalgorithmus. Urspr\u00fcnglich entwickelt von Joseph Redmon, verbesserte YOLOv3 seine Vorg\u00e4ngermodelle durch die Einf\u00fchrung von Funktionen wie mehrskaligen Vorhersagen und drei verschiedenen Gr\u00f6\u00dfen von Erkennungskernen.</p> </li> <li> <p>YOLOv3-Ultralytics: Dies ist die Implementierung des YOLOv3-Modells von Ultralytics. Es reproduziert die urspr\u00fcngliche YOLOv3-Architektur und bietet zus\u00e4tzliche Funktionalit\u00e4ten, wie die Unterst\u00fctzung f\u00fcr weitere vortrainierte Modelle und einfachere Anpassungsoptionen.</p> </li> <li> <p>YOLOv3u: Dies ist eine aktualisierte Version von YOLOv3-Ultralytics, die den anchor-freien, objektfreien Split Head aus den YOLOv8-Modellen einbezieht. YOLOv3u verwendet die gleiche Backbone- und Neck-Architektur wie YOLOv3, aber mit dem aktualisierten Erkennungskopf von YOLOv8.</p> </li> </ol> <p></p>"},{"location":"models/yolov3/#wichtigste-funktionen","title":"Wichtigste Funktionen","text":"<ul> <li> <p>YOLOv3: Einf\u00fchrung der Verwendung von drei unterschiedlichen Skalen f\u00fcr die Erkennung unter Verwendung von drei verschiedenen Gr\u00f6\u00dfen von Erkennungskernen: 13x13, 26x26 und 52x52. Dadurch wurde die Erkennungsgenauigkeit f\u00fcr Objekte unterschiedlicher Gr\u00f6\u00dfe erheblich verbessert. Dar\u00fcber hinaus f\u00fcgte YOLOv3 Funktionen wie Mehrfachkennzeichnungen f\u00fcr jeden Begrenzungsrahmen und ein besseres Feature-Extraktionsnetzwerk hinzu.</p> </li> <li> <p>YOLOv3-Ultralytics: Ultralytics' Implementierung von YOLOv3 bietet die gleiche Leistung wie das urspr\u00fcngliche Modell, bietet jedoch zus\u00e4tzliche Unterst\u00fctzung f\u00fcr weitere vortrainierte Modelle, zus\u00e4tzliche Trainingsmethoden und einfachere Anpassungsoptionen. Dadurch wird es vielseitiger und benutzerfreundlicher f\u00fcr praktische Anwendungen.</p> </li> <li> <p>YOLOv3u: Dieses aktualisierte Modell enth\u00e4lt den anchor-freien, objektfreien Split Head aus YOLOv8. Durch die Beseitigung der Notwendigkeit vordefinierter Ankerfelder und Objektheitsscores kann dieses Entwurfsmerkmal f\u00fcr den Erkennungskopf die F\u00e4higkeit des Modells verbessern, Objekte unterschiedlicher Gr\u00f6\u00dfe und Form zu erkennen. Dadurch wird YOLOv3u robuster und genauer f\u00fcr Aufgaben der Objekterkennung.</p> </li> </ul>"},{"location":"models/yolov3/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>Die YOLOv3-Serie, einschlie\u00dflich YOLOv3, YOLOv3-Ultralytics und YOLOv3u, ist speziell f\u00fcr Aufgaben der Objekterkennung konzipiert. Diese Modelle sind bekannt f\u00fcr ihre Effektivit\u00e4t in verschiedenen realen Szenarien und kombinieren Genauigkeit und Geschwindigkeit. Jede Variante bietet einzigartige Funktionen und Optimierungen, die sie f\u00fcr eine Vielzahl von Anwendungen geeignet machen.</p> <p>Alle drei Modelle unterst\u00fctzen einen umfangreichen Satz von Modi, um Vielseitigkeit in verschiedenen Phasen der Modellbereitstellung und -entwicklung zu gew\u00e4hrleisten. Zu diesen Modi geh\u00f6ren Inferenz, Validierung, Training und Export, was den Benutzern ein vollst\u00e4ndiges Toolkit f\u00fcr eine effektive Objekterkennung bietet.</p> Modelltyp Unterst\u00fctzte Aufgaben Inferenz Validierung Training Export YOLOv3 Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv3-Ultralytics Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv3u Objekterkennung \u2705 \u2705 \u2705 \u2705 <p>Diese Tabelle bietet einen schnellen \u00dcberblick \u00fcber die F\u00e4higkeiten jeder YOLOv3-Variante und hebt ihre Vielseitigkeit und Eignung f\u00fcr verschiedene Aufgaben und Betriebsmodi in Workflows zur Objekterkennung hervor.</p>"},{"location":"models/yolov3/#beispiele-zur-verwendung","title":"Beispiele zur Verwendung","text":"<p>Dieses Beispiel enth\u00e4lt einfache Trainings- und Inferenzbeispiele f\u00fcr YOLOv3. F\u00fcr die vollst\u00e4ndige Dokumentation zu diesen und anderen Modi siehe die Seiten zur Predict, Train, Val und Export.</p> <p>Beispiel</p> PythonCLI <p>Vorgefertigte PyTorch-Modelle im <code>*.pt</code>-Format sowie Konfigurationsdateien im <code>*.yaml</code>-Format k\u00f6nnen an die <code>YOLO()</code>-Klasse \u00fcbergeben werden, um eine Modellinstanz in Python zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Lade ein vortrainiertes YOLOv3n-Modell f\u00fcr COCO\nmodel = YOLO('yolov3n.pt')\n\n# Zeige Informationen zum Modell an (optional)\nmodel.info()\n\n# Trainiere das Modell mit dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# F\u00fchre Inferenz mit dem YOLOv3n-Modell auf dem Bild \"bus.jpg\" durch\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle stehen zur Verf\u00fcgung, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Lade ein vortrainiertes YOLOv3n-Modell und trainiere es mit dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen\nyolo train model=yolov3n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Lade ein vortrainiertes YOLOv3n-Modell und f\u00fchre Inferenz auf dem Bild \"bus.jpg\" aus\nyolo predict model=yolov3n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov3/#zitate-und-anerkennungen","title":"Zitate und Anerkennungen","text":"<p>Wenn Sie YOLOv3 in Ihrer Forschung verwenden, zitieren Sie bitte die urspr\u00fcnglichen YOLO-Papiere und das Ultralytics YOLOv3-Repository:</p> BibTeX <pre><code>@article{redmon2018yolov3,\n  title={YOLOv3: An Incremental Improvement},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1804.02767},\n  year={2018}\n}\n</code></pre> <p>Vielen Dank an Joseph Redmon und Ali Farhadi f\u00fcr die Entwicklung des originalen YOLOv3.</p>"},{"location":"models/yolov4/","title":"YOLOv4: Schnelle und pr\u00e4zise Objekterkennung","text":"<p>Willkommen auf der Ultralytics-Dokumentationsseite f\u00fcr YOLOv4, einem hochmodernen, Echtzeit-Objektdetektor, der 2020 von Alexey Bochkovskiy unter https://github.com/AlexeyAB/darknet ver\u00f6ffentlicht wurde. YOLOv4 wurde entwickelt, um das optimale Gleichgewicht zwischen Geschwindigkeit und Genauigkeit zu bieten und ist somit eine ausgezeichnete Wahl f\u00fcr viele Anwendungen.</p> <p> YOLOv4 Architekturdiagramm. Zeigt das komplexe Netzwerkdesign von YOLOv4, einschlie\u00dflich der Backbone-, Neck- und Head-Komponenten sowie ihrer verbundenen Schichten f\u00fcr eine optimale Echtzeit-Objekterkennung.</p>"},{"location":"models/yolov4/#einleitung","title":"Einleitung","text":"<p>YOLOv4 steht f\u00fcr You Only Look Once Version 4. Es handelt sich um ein Echtzeit-Objekterkennungsmodell, das entwickelt wurde, um die Grenzen fr\u00fcherer YOLO-Versionen wie YOLOv3 und anderer Objekterkennungsmodelle zu \u00fcberwinden. Im Gegensatz zu anderen konvolutionellen neuronalen Netzwerken (CNN), die auf Objekterkennung basieren, ist YOLOv4 nicht nur f\u00fcr Empfehlungssysteme geeignet, sondern auch f\u00fcr eigenst\u00e4ndiges Prozessmanagement und Reduzierung der Benutzereingabe. Durch den Einsatz von herk\u00f6mmlichen Grafikprozessoreinheiten (GPUs) erm\u00f6glicht es YOLOv4 eine Massennutzung zu einem erschwinglichen Preis und ist so konzipiert, dass es in Echtzeit auf einer herk\u00f6mmlichen GPU funktioniert, wobei nur eine solche GPU f\u00fcr das Training erforderlich ist.</p>"},{"location":"models/yolov4/#architektur","title":"Architektur","text":"<p>YOLOv4 nutzt mehrere innovative Funktionen, die zusammenarbeiten, um seine Leistung zu optimieren. Dazu geh\u00f6ren Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT), Mish-Aktivierung, Mosaic-Datenaugmentation, DropBlock-Regularisierung und CIoU-Verlust. Diese Funktionen werden kombiniert, um erstklassige Ergebnisse zu erzielen.</p> <p>Ein typischer Objektdetektor besteht aus mehreren Teilen, darunter der Eingabe, dem Backbone, dem Neck und dem Head. Das Backbone von YOLOv4 ist auf ImageNet vorgeschult und wird zur Vorhersage von Klassen und Begrenzungsrahmen von Objekten verwendet. Das Backbone kann aus verschiedenen Modellen wie VGG, ResNet, ResNeXt oder DenseNet stammen. Der Neck-Teil des Detektors wird verwendet, um Merkmalskarten von verschiedenen Stufen zu sammeln und umfasst normalerweise mehrere Bottom-up-Pfade und mehrere Top-down-Pfade. Der Head-Teil wird schlie\u00dflich zur Durchf\u00fchrung der endg\u00fcltigen Objekterkennung und Klassifizierung verwendet.</p>"},{"location":"models/yolov4/#bag-of-freebies","title":"Bag of Freebies","text":"<p>YOLOv4 verwendet auch Methoden, die als \"Bag of Freebies\" bekannt sind. Dabei handelt es sich um Techniken, die die Genauigkeit des Modells w\u00e4hrend des Trainings verbessern, ohne die Kosten der Inferenz zu erh\u00f6hen. Datenaugmentation ist eine h\u00e4ufige Bag of Freebies-Technik, die in der Objekterkennung verwendet wird, um die Variabilit\u00e4t der Eingabebilder zu erh\u00f6hen und die Robustheit des Modells zu verbessern. Beispiele f\u00fcr Datenaugmentation sind photometrische Verzerrungen (Anpassung von Helligkeit, Kontrast, Farbton, S\u00e4ttigung und Rauschen eines Bildes) und geometrische Verzerrungen (Hinzuf\u00fcgen von zuf\u00e4lliger Skalierung, Ausschnitt, Spiegelung und Rotation). Diese Techniken helfen dem Modell, sich besser an verschiedene Arten von Bildern anzupassen.</p>"},{"location":"models/yolov4/#funktionen-und-leistung","title":"Funktionen und Leistung","text":"<p>YOLOv4 ist f\u00fcr optimale Geschwindigkeit und Genauigkeit in der Objekterkennung konzipiert. Die Architektur von YOLOv4 umfasst CSPDarknet53 als Backbone, PANet als Neck und YOLOv3 als Detektionskopf. Diese Konstruktion erm\u00f6glicht es YOLOv4, beeindruckend schnelle Objekterkennungen durchzuf\u00fchren und ist somit f\u00fcr Echtzeitanwendungen geeignet. YOLOv4 zeichnet sich auch durch Genauigkeit aus und erzielt erstklassige Ergebnisse in Objekterkennungs-Benchmarks.</p>"},{"location":"models/yolov4/#beispiele-fur-die-verwendung","title":"Beispiele f\u00fcr die Verwendung","text":"<p>Zum Zeitpunkt der Erstellung dieser Dokumentation unterst\u00fctzt Ultralytics derzeit keine YOLOv4-Modelle. Daher m\u00fcssen sich Benutzer, die YOLOv4 verwenden m\u00f6chten, direkt an das YOLOv4 GitHub-Repository f\u00fcr Installations- und Verwendungshinweise wenden.</p> <p>Hier ist ein kurzer \u00dcberblick \u00fcber die typischen Schritte, die Sie unternehmen k\u00f6nnten, um YOLOv4 zu verwenden:</p> <ol> <li> <p>Besuchen Sie das YOLOv4 GitHub-Repository: https://github.com/AlexeyAB/darknet.</p> </li> <li> <p>Befolgen Sie die in der README-Datei bereitgestellten Anweisungen zur Installation. Dies beinhaltet in der Regel das Klonen des Repositories, die Installation der erforderlichen Abh\u00e4ngigkeiten und das Einrichten der erforderlichen Umgebungsvariablen.</p> </li> <li> <p>Sobald die Installation abgeschlossen ist, k\u00f6nnen Sie das Modell gem\u00e4\u00df den in dem Repository bereitgestellten Verwendungshinweisen trainieren und verwenden. Dies beinhaltet in der Regel die Vorbereitung des Datensatzes, die Konfiguration der Modellparameter, das Training des Modells und die anschlie\u00dfende Verwendung des trainierten Modells zur Durchf\u00fchrung der Objekterkennung.</p> </li> </ol> <p>Bitte beachten Sie, dass die spezifischen Schritte je nach Ihrer spezifischen Anwendung und dem aktuellen Stand des YOLOv4-Repositories variieren k\u00f6nnen. Es wird daher dringend empfohlen, sich direkt an die Anweisungen im YOLOv4-GitHub-Repository zu halten.</p> <p>Wir bedauern etwaige Unannehmlichkeiten und werden uns bem\u00fchen, dieses Dokument mit Verwendungsbeispielen f\u00fcr Ultralytics zu aktualisieren, sobald die Unterst\u00fctzung f\u00fcr YOLOv4 implementiert ist.</p>"},{"location":"models/yolov4/#fazit","title":"Fazit","text":"<p>YOLOv4 ist ein leistungsstarkes und effizientes Modell zur Objekterkennung, das eine Balance zwischen Geschwindigkeit und Genauigkeit bietet. Durch den Einsatz einzigartiger Funktionen und Bag of Freebies-Techniken w\u00e4hrend des Trainings erzielt es hervorragende Ergebnisse in Echtzeit-Objekterkennungsaufgaben. YOLOv4 kann von jedem mit einer herk\u00f6mmlichen GPU trainiert und verwendet werden, was es f\u00fcr eine Vielzahl von Anwendungen zug\u00e4nglich und praktisch macht.</p>"},{"location":"models/yolov4/#zitate-und-anerkennungen","title":"Zitate und Anerkennungen","text":"<p>Wir m\u00f6chten den Autoren von YOLOv4 f\u00fcr ihren bedeutenden Beitrag auf dem Gebiet der Echtzeit-Objekterkennung danken:</p> BibTeX <pre><code>@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Die originale YOLOv4-Publikation finden Sie auf arXiv. Die Autoren haben ihre Arbeit \u00f6ffentlich zug\u00e4nglich gemacht und der Code kann auf GitHub abgerufen werden. Wir sch\u00e4tzen ihre Bem\u00fchungen, das Fachgebiet voranzubringen und ihre Arbeit der breiteren Community zug\u00e4nglich zu machen.</p>"},{"location":"models/yolov5/","title":"YOLOv5","text":""},{"location":"models/yolov5/#ubersicht","title":"\u00dcbersicht","text":"<p>YOLOv5u steht f\u00fcr eine Weiterentwicklung der Methoden zur Objekterkennung. Basierend auf der grundlegenden Architektur des von Ultralytics entwickelten YOLOv5-Modells integriert YOLOv5u den ankerfreien, objektlosen Split-Kopf, ein Feature, das zuvor in den YOLOv8-Modellen eingef\u00fchrt wurde. Diese Anpassung verfeinert die Architektur des Modells und f\u00fchrt zu einem optimierten Verh\u00e4ltnis von Genauigkeit und Geschwindigkeit bei der Objekterkennung. Basierend auf den empirischen Ergebnissen und den abgeleiteten Features bietet YOLOv5u eine effiziente Alternative f\u00fcr diejenigen, die robuste L\u00f6sungen sowohl in der Forschung als auch in praktischen Anwendungen suchen.</p> <p></p>"},{"location":"models/yolov5/#hauptmerkmale","title":"Hauptmerkmale","text":"<ul> <li> <p>Ankerfreier Split-Ultralytics-Kopf: Herk\u00f6mmliche Objekterkennungsmodelle verwenden vordefinierte Ankerboxen, um die Position von Objekten vorherzusagen. YOLOv5u modernisiert diesen Ansatz. Durch die Verwendung eines ankerfreien Split-Ultralytics-Kopfes wird ein flexiblerer und anpassungsf\u00e4higerer Detektionsmechanismus gew\u00e4hrleistet, der die Leistung in verschiedenen Szenarien verbessert.</p> </li> <li> <p>Optimiertes Verh\u00e4ltnis von Genauigkeit und Geschwindigkeit: Geschwindigkeit und Genauigkeit ziehen oft in entgegengesetzte Richtungen. Aber YOLOv5u stellt diese Abw\u00e4gung in Frage. Es bietet eine ausgewogene Balance, die Echtzeitdetektionen ohne Einbu\u00dfen bei der Genauigkeit erm\u00f6glicht. Diese Funktion ist besonders wertvoll f\u00fcr Anwendungen, die schnelle Reaktionen erfordern, wie autonome Fahrzeuge, Robotik und Echtzeitanalyse von Videos.</p> </li> <li> <p>Vielfalt an vorab trainierten Modellen: YOLOv5u bietet eine Vielzahl von vorab trainierten Modellen, da verschiedene Aufgaben unterschiedliche Werkzeuge erfordern. Ob Sie sich auf Inferenz, Validierung oder Training konzentrieren, es wartet ein ma\u00dfgeschneidertes Modell auf Sie. Diese Vielfalt gew\u00e4hrleistet, dass Sie nicht nur eine Einheitsl\u00f6sung verwenden, sondern ein speziell f\u00fcr Ihre einzigartige Herausforderung feinabgestimmtes Modell.</p> </li> </ul>"},{"location":"models/yolov5/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>Die YOLOv5u-Modelle mit verschiedenen vorab trainierten Gewichten eignen sich hervorragend f\u00fcr Aufgaben zur Objekterkennung. Sie unterst\u00fctzen eine umfassende Palette von Modi, die sie f\u00fcr verschiedene Anwendungen von der Entwicklung bis zur Bereitstellung geeignet machen.</p> Modelltyp Vorab trainierte Gewichte Aufgabe Inferenz Validierung Training Export YOLOv5u <code>yolov5nu</code>, <code>yolov5su</code>, <code>yolov5mu</code>, <code>yolov5lu</code>, <code>yolov5xu</code>, <code>yolov5n6u</code>, <code>yolov5s6u</code>, <code>yolov5m6u</code>, <code>yolov5l6u</code>, <code>yolov5x6u</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 <p>Diese Tabelle bietet eine detaillierte \u00dcbersicht \u00fcber die verschiedenen Varianten des YOLOv5u-Modells und hebt ihre Anwendbarkeit in der Objekterkennung sowie die Unterst\u00fctzung unterschiedlicher Betriebsmodi wie Inferenz, Validierung, Training und Export hervor. Diese umfassende Unterst\u00fctzung erm\u00f6glicht es Benutzern, die F\u00e4higkeiten der YOLOv5u-Modelle in einer Vielzahl von Objekterkennungsszenarien voll auszusch\u00f6pfen.</p>"},{"location":"models/yolov5/#leistungskennzahlen","title":"Leistungskennzahlen","text":"<p>Leistung</p> Erkennung <p>Siehe Erkennungsdokumentation f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf COCO trainiert wurden und 80 vorab trainierte Klassen enthalten.</p> Modell YAML Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>val50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) yolov5nu.pt yolov5n.yaml 640 34,3 73,6 1,06 2,6 7,7 yolov5su.pt yolov5s.yaml 640 43,0 120,7 1,27 9,1 24,0 yolov5mu.pt yolov5m.yaml 640 49,0 233,9 1,86 25,1 64,2 yolov5lu.pt yolov5l.yaml 640 52,2 408,4 2,50 53,2 135,0 yolov5xu.pt yolov5x.yaml 640 53,2 763,2 3,81 97,2 246,4 yolov5n6u.pt yolov5n6.yaml 1.280 42,1 211,0 1,83 4,3 7,8 yolov5s6u.pt yolov5s6.yaml 1.280 48,6 422,6 2,34 15,3 24,6 yolov5m6u.pt yolov5m6.yaml 1.280 53,6 810,9 4,36 41,2 65,7 yolov5l6u.pt yolov5l6.yaml 1.280 55,7 1.470,9 5,47 86,1 137,4 yolov5x6u.pt yolov5x6.yaml 1.280 56,8 2.436,5 8,98 155,4 250,7"},{"location":"models/yolov5/#beispiele-zur-verwendung","title":"Beispiele zur Verwendung","text":"<p>Dieses Beispiel enth\u00e4lt einfache Beispiele zur Schulung und Inferenz mit YOLOv5. Die vollst\u00e4ndige Dokumentation zu diesen und anderen Modi finden Sie in den Seiten Predict, Train, Val und Export.</p> <p>Beispiel</p> PythonCLI <p>PyTorch-vortrainierte <code>*.pt</code>-Modelle sowie Konfigurationsdateien <code>*.yaml</code> k\u00f6nnen an die <code>YOLO()</code>-Klasse \u00fcbergeben werden, um eine Modellinstanz in Python zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein vortrainiertes YOLOv5n-Modell f\u00fcr COCO-Daten\nmodell = YOLO('yolov5n.pt')\n\n# Informationen zum Modell anzeigen (optional)\nmodel.info()\n\n# Trainieren Sie das Modell anhand des COCO8-Beispieldatensatzes f\u00fcr 100 Epochen\nergebnisse = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# F\u00fchren Sie die Inferenz mit dem YOLOv5n-Modell auf dem Bild 'bus.jpg' durch\nergebnisse = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle sind verf\u00fcgbar, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Laden Sie ein vortrainiertes YOLOv5n-Modell und trainieren Sie es anhand des COCO8-Beispieldatensatzes f\u00fcr 100 Epochen\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Laden Sie ein vortrainiertes YOLOv5n-Modell und f\u00fchren Sie die Inferenz auf dem Bild 'bus.jpg' durch\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov5/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wenn Sie YOLOv5 oder YOLOv5u in Ihrer Forschung verwenden, zitieren Sie bitte das Ultralytics YOLOv5-Repository wie folgt:</p> BibTeX <pre><code>@software{yolov5,\n  title = {Ultralytics YOLOv5},\n  author = {Glenn Jocher},\n  year = {2020},\n  version = {7.0},\n  license = {AGPL-3.0},\n  url = {https://github.com/ultralytics/yolov5},\n  doi = {10.5281/zenodo.3908559},\n  orcid = {0000-0001-5950-6979}\n}\n</code></pre> <p>Bitte beachten Sie, dass die YOLOv5-Modelle unter den Lizenzen AGPL-3.0 und Enterprise bereitgestellt werden.</p>"},{"location":"models/yolov6/","title":"Meituan YOLOv6","text":""},{"location":"models/yolov6/#uberblick","title":"\u00dcberblick","text":"<p>Meituan YOLOv6 ist ein moderner Objekterkenner, der eine bemerkenswerte Balance zwischen Geschwindigkeit und Genauigkeit bietet und somit eine beliebte Wahl f\u00fcr Echtzeitanwendungen darstellt. Dieses Modell bietet mehrere bemerkenswerte Verbesserungen in seiner Architektur und seinem Trainingsschema, einschlie\u00dflich der Implementierung eines Bi-direktionalen Konkatenationsmoduls (BiC), einer anchor-aided training (AAT)-Strategie und einem verbesserten Backpropagation- und Neck-Design f\u00fcr Spitzenleistungen auf dem COCO-Datensatz.</p> <p> \u00dcbersicht \u00fcber YOLOv6. Diagramm der Modellarchitektur, das die neu gestalteten Netzwerkkomponenten und Trainingstrategien zeigt, die zu signifikanten Leistungsverbesserungen gef\u00fchrt haben. (a) Der Nacken von YOLOv6 (N und S sind dargestellt). Beachten Sie, dass bei M/L RepBlocks durch CSPStackRep ersetzt wird. (b) Die Struktur eines BiC-Moduls. (c) Ein SimCSPSPPF-Block. (Quelle).</p>"},{"location":"models/yolov6/#hauptmerkmale","title":"Hauptmerkmale","text":"<ul> <li>Bi-direktionales Konkatenations (BiC) Modul: YOLOv6 f\u00fchrt ein BiC-Modul im Nacken des Erkenners ein, das die Lokalisierungssignale verbessert und eine Leistungssteigerung bei vernachl\u00e4ssigbarem Geschwindigkeitsabfall liefert.</li> <li>Anchor-aided Training (AAT) Strategie: Dieses Modell schl\u00e4gt AAT vor, um die Vorteile sowohl von ankerbasierten als auch von ankerfreien Paradigmen zu nutzen, ohne die Inferenzeffizienz zu beeintr\u00e4chtigen.</li> <li>Verbessertes Backpropagation- und Neck-Design: Durch Vertiefung von YOLOv6 um eine weitere Stufe im Backpropagation und Nacken erreicht dieses Modell Spitzenleistungen auf dem COCO-Datensatz bei hochaufl\u00f6senden Eingaben.</li> <li>Self-Distillation Strategie: Eine neue Self-Distillation-Strategie wird implementiert, um die Leistung von kleineren Modellen von YOLOv6 zu steigern, indem der Hilfsregressionszweig w\u00e4hrend des Trainings verst\u00e4rkt und bei der Inferenz entfernt wird, um einen deutlichen Geschwindigkeitsabfall zu vermeiden.</li> </ul>"},{"location":"models/yolov6/#leistungsmetriken","title":"Leistungsmetriken","text":"<p>YOLOv6 bietet verschiedene vorab trainierte Modelle mit unterschiedlichen Ma\u00dfst\u00e4ben:</p> <ul> <li>YOLOv6-N: 37,5% AP auf COCO val2017 bei 1187 FPS mit NVIDIA Tesla T4 GPU.</li> <li>YOLOv6-S: 45,0% AP bei 484 FPS.</li> <li>YOLOv6-M: 50,0% AP bei 226 FPS.</li> <li>YOLOv6-L: 52,8% AP bei 116 FPS.</li> <li>YOLOv6-L6: Spitzenleistung in Echtzeit.</li> </ul> <p>YOLOv6 bietet auch quantisierte Modelle f\u00fcr verschiedene Genauigkeiten sowie Modelle, die f\u00fcr mobile Plattformen optimiert sind.</p>"},{"location":"models/yolov6/#beispiele-zur-verwendung","title":"Beispiele zur Verwendung","text":"<p>In diesem Beispiel werden einfache Schulungs- und Inferenzbeispiele f\u00fcr YOLOv6 bereitgestellt. Weitere Dokumentation zu diesen und anderen Modi finden Sie auf den Seiten Predict, Train, Val und Export.</p> <p>Beispiel</p> PythonCLI <p>In Python kann PyTorch-vorab trainierte <code>*.pt</code>-Modelle sowie Konfigurations-<code>*.yaml</code>-Dateien an die <code>YOLO()</code>-Klasse \u00fcbergeben werden, um eine Modellinstanz zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Erstellen Sie ein YOLOv6n-Modell von Grund auf\nmodel = YOLO('yolov6n.yaml')\n\n# Zeigen Sie Informationen zum Modell an (optional)\nmodel.info()\n\n# Trainieren Sie das Modell am Beispiel des COCO8-Datensatzes f\u00fcr 100 Epochen\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# F\u00fchren Sie Inferenz mit dem YOLOv6n-Modell auf dem Bild 'bus.jpg' durch\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle stehen zur Verf\u00fcgung, um die Modelle direkt auszuf\u00fchren:</p> <pre><code># Erstellen Sie ein YOLOv6n-Modell von Grund auf und trainieren Sie es am Beispiel des COCO8-Datensatzes f\u00fcr 100 Epochen\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Erstellen Sie ein YOLOv6n-Modell von Grund auf und f\u00fchren Sie Inferenz auf dem Bild 'bus.jpg' durch\nyolo predict model=yolov6n.yaml source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov6/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>Die YOLOv6-Serie bietet eine Reihe von Modellen, die jeweils f\u00fcr die Hochleistungs-Objekterkennung optimiert sind. Diese Modelle erf\u00fcllen unterschiedliche Rechenanforderungen und Genauigkeitsanforderungen und sind daher vielseitig f\u00fcr eine Vielzahl von Anwendungen einsetzbar.</p> Modelltyp Vorab trainierte Gewichte Unterst\u00fctzte Aufgaben Inferenz Validierung Training Exportieren YOLOv6-N <code>yolov6-n.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv6-S <code>yolov6-s.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv6-M <code>yolov6-m.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv6-L <code>yolov6-l.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv6-L6 <code>yolov6-l6.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 <p>Diese Tabelle bietet einen detaillierten \u00dcberblick \u00fcber die YOLOv6-Modellvarianten und hebt ihre F\u00e4higkeiten bei der Objekterkennung sowie ihre Kompatibilit\u00e4t mit verschiedenen Betriebsmodi wie Inferenz, Validierung, Training und Exportieren hervor. Diese umfassende Unterst\u00fctzung erm\u00f6glicht es den Benutzern, die F\u00e4higkeiten von YOLOv6-Modellen in einer Vielzahl von Objekterkennungsszenarien vollst\u00e4ndig zu nutzen.</p>"},{"location":"models/yolov6/#zitate-und-anerkennungen","title":"Zitate und Anerkennungen","text":"<p>Wir m\u00f6chten den Autoren f\u00fcr ihre bedeutenden Beitr\u00e4ge auf dem Gebiet der Echtzeit-Objekterkennung danken:</p> BibTeX <pre><code>@misc{li2023yolov6,\n      title={YOLOv6 v3.0: A Full-Scale Reloading},\n      author={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\n      year={2023},\n      eprint={2301.05586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Das urspr\u00fcngliche YOLOv6-Papier finden Sie auf arXiv. Die Autoren haben ihre Arbeit \u00f6ffentlich zug\u00e4nglich gemacht, und der Code kann auf GitHub abgerufen werden. Wir sch\u00e4tzen ihre Bem\u00fchungen zur Weiterentwicklung des Fachgebiets und zur Zug\u00e4nglichmachung ihrer Arbeit f\u00fcr die breitere Gemeinschaft.</p>"},{"location":"models/yolov7/","title":"YOLOv7: Trainable Bag-of-Freebies","text":"<p>YOLOv7 ist ein echtzeitf\u00e4higer Objektdetektor der Spitzenklasse, der alle bekannten Objektdetektoren in Bezug auf Geschwindigkeit und Genauigkeit im Bereich von 5 FPS bis 160 FPS \u00fcbertrifft. Mit einer Genauigkeit von 56,8% AP ist er der pr\u00e4ziseste Echtzeit-Objektdetektor unter allen bekannten Modellen mit einer FPS von 30 oder h\u00f6her auf der GPU V100. Dar\u00fcber hinaus \u00fcbertrifft YOLOv7 andere Objektdetektoren wie YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5 und viele andere in Bezug auf Geschwindigkeit und Genauigkeit. Das Modell wird ausschlie\u00dflich auf dem MS COCO-Datensatz trainiert, ohne andere Datens\u00e4tze oder vortrainierte Gewichte zu verwenden. Sourcecode f\u00fcr YOLOv7 ist auf GitHub verf\u00fcgbar.</p> <p> **Vergleich von Spitzen-Objektdetektoren. ** Aus den Ergebnissen in Tabelle 2 wissen wir, dass die vorgeschlagene Methode das beste Verh\u00e4ltnis von Geschwindigkeit und Genauigkeit umfassend aufweist. Vergleichen wir YOLOv7-tiny-SiLU mit YOLOv5-N (r6.1), so ist unsere Methode 127 FPS schneller und um 10,7% genauer beim AP. Dar\u00fcber hinaus erreicht YOLOv7 bei einer Bildrate von 161 FPS einen AP von 51,4%, w\u00e4hrend PPYOLOE-L mit demselben AP nur eine Bildrate von 78 FPS aufweist. In Bezug auf die Parameterverwendung ist YOLOv7 um 41% geringer als PPYOLOE-L. Vergleicht man YOLOv7-X mit 114 FPS Inferenzgeschwindigkeit mit YOLOv5-L (r6.1) mit 99 FPS Inferenzgeschwindigkeit, kann YOLOv7-X den AP um 3,9% verbessern. Wenn YOLOv7-X mit YOLOv5-X (r6.1) in \u00e4hnlichem Ma\u00dfstab verglichen wird, ist die Inferenzgeschwindigkeit von YOLOv7-X 31 FPS schneller. Dar\u00fcber hinaus reduziert YOLOv7-X in Bezug auf die Anzahl der Parameter und Berechnungen 22% der Parameter und 8% der Berechnungen im Vergleich zu YOLOv5-X (r6.1), verbessert jedoch den AP um 2,2% (Source).</p>"},{"location":"models/yolov7/#ubersicht","title":"\u00dcbersicht","text":"<p>Echtzeit-Objekterkennung ist eine wichtige Komponente vieler Computersysteme f\u00fcr Bildverarbeitung, einschlie\u00dflich Multi-Object-Tracking, autonomes Fahren, Robotik und medizinische Bildanalyse. In den letzten Jahren konzentrierte sich die Entwicklung der Echtzeit-Objekterkennung auf die Gestaltung effizienter Architekturen und die Verbesserung der Inferenzgeschwindigkeit verschiedener CPUs, GPUs und Neural Processing Units (NPUs). YOLOv7 unterst\u00fctzt sowohl mobile GPUs als auch GPU-Ger\u00e4te, von der Edge bis zur Cloud.</p> <p>Im Gegensatz zu herk\u00f6mmlichen, echtzeitf\u00e4higen Objektdetektoren, die sich auf die Architekturoptimierung konzentrieren, f\u00fchrt YOLOv7 eine Fokussierung auf die Optimierung des Schulungsprozesses ein. Dazu geh\u00f6ren Module und Optimierungsmethoden, die darauf abzielen, die Genauigkeit der Objekterkennung zu verbessern, ohne die Inferenzkosten zu erh\u00f6hen - ein Konzept, das als \"trainable bag-of-freebies\" bekannt ist.</p>"},{"location":"models/yolov7/#hauptmerkmale","title":"Hauptmerkmale","text":"<p>YOLOv7 f\u00fchrt mehrere Schl\u00fcsselfunktionen ein:</p> <ol> <li> <p>Modellumparameterisierung: YOLOv7 schl\u00e4gt ein geplantes umparameterisiertes Modell vor, das eine in verschiedenen Netzwerken anwendbare Strategie darstellt und auf dem Konzept des Gradientenpropagationspfades basiert.</p> </li> <li> <p>Dynamische Labelzuweisung: Das Training des Modells mit mehreren Ausgabeschichten stellt ein neues Problem dar: \"Wie weist man dynamische Ziele f\u00fcr die Ausgaben der verschiedenen Zweige zu?\" Zur L\u00f6sung dieses Problems f\u00fchrt YOLOv7 eine neue Methode zur Labelzuweisung ein, die als coarse-to-fine lead guided label assignment bekannt ist.</p> </li> <li> <p>Erweiterte und umfassende Skalierung: YOLOv7 schl\u00e4gt Methoden zur \"erweiterten\" und \"umfassenden Skalierung\" des echtzeitf\u00e4higen Objektdetektors vor, die Parameter und Berechnungen effektiv nutzen k\u00f6nnen.</p> </li> <li> <p>Effizienz: Die von YOLOv7 vorgeschlagene Methode kann etwa 40 % der Parameter und 50 % der Berechnungen des state-of-the-art echtzeitf\u00e4higen Objektdetektors wirksam reduzieren und weist eine schnellere Inferenzgeschwindigkeit und eine h\u00f6here Detektionsgenauigkeit auf.</p> </li> </ol>"},{"location":"models/yolov7/#beispiele-zur-nutzung","title":"Beispiele zur Nutzung","text":"<p>Zum Zeitpunkt der Erstellung dieses Textes unterst\u00fctzt Ultralytics derzeit keine YOLOv7-Modelle. Daher m\u00fcssen sich alle Benutzer, die YOLOv7 verwenden m\u00f6chten, direkt an das YOLOv7 GitHub-Repository f\u00fcr Installations- und Nutzungshinweise wenden.</p> <p>Hier ist ein kurzer \u00dcberblick \u00fcber die typischen Schritte, die Sie unternehmen k\u00f6nnten, um YOLOv7 zu verwenden:</p> <ol> <li> <p>Besuchen Sie das YOLOv7 GitHub-Repository: https://github.com/WongKinYiu/yolov7.</p> </li> <li> <p>Befolgen Sie die in der README-Datei bereitgestellten Anweisungen zur Installation. Dies beinhaltet in der Regel das Klonen des Repositories, die Installation der erforderlichen Abh\u00e4ngigkeiten und das Einrichten eventuell notwendiger Umgebungsvariablen.</p> </li> <li> <p>Sobald die Installation abgeschlossen ist, k\u00f6nnen Sie das Modell entsprechend den im Repository bereitgestellten Anleitungen trainieren und verwenden. Dies umfasst in der Regel die Vorbereitung des Datensatzes, das Konfigurieren der Modellparameter, das Training des Modells und anschlie\u00dfend die Verwendung des trainierten Modells zur Durchf\u00fchrung der Objekterkennung.</p> </li> </ol> <p>Bitte beachten Sie, dass die spezifischen Schritte je nach Ihrem spezifischen Anwendungsfall und dem aktuellen Stand des YOLOv7-Repositories variieren k\u00f6nnen. Es wird daher dringend empfohlen, sich direkt an die im YOLOv7 GitHub-Repository bereitgestellten Anweisungen zu halten.</p> <p>Wir bedauern etwaige Unannehmlichkeiten und werden uns bem\u00fchen, dieses Dokument mit Anwendungsbeispielen f\u00fcr Ultralytics zu aktualisieren, sobald die Unterst\u00fctzung f\u00fcr YOLOv7 implementiert ist.</p>"},{"location":"models/yolov7/#zitationen-und-danksagungen","title":"Zitationen und Danksagungen","text":"<p>Wir m\u00f6chten den Autoren von YOLOv7 f\u00fcr ihre bedeutenden Beitr\u00e4ge im Bereich der echtzeitf\u00e4higen Objekterkennung danken:</p> BibTeX <pre><code>@article{wang2022yolov7,\n  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  journal={arXiv preprint arXiv:2207.02696},\n  year={2022}\n}\n</code></pre> <p>Die urspr\u00fcngliche YOLOv7-Studie kann auf arXiv gefunden werden. Die Autoren haben ihre Arbeit \u00f6ffentlich zug\u00e4nglich gemacht, und der Code kann auf GitHub abgerufen werden. Wir sch\u00e4tzen ihre Bem\u00fchungen, das Feld voranzubringen und ihre Arbeit der breiteren Gemeinschaft zug\u00e4nglich zu machen.</p>"},{"location":"models/yolov8/","title":"YOLOv8","text":""},{"location":"models/yolov8/#ubersicht","title":"\u00dcbersicht","text":"<p>YOLOv8 ist die neueste Version der YOLO-Serie von Echtzeit-Objekterkennern und bietet modernste Leistung in Bezug auf Genauigkeit und Geschwindigkeit. Basierend auf den Fortschritten fr\u00fcherer YOLO-Versionen bringt YOLOv8 neue Funktionen und Optimierungen mit sich, die ihn zu einer idealen Wahl f\u00fcr verschiedene Objekterkennungsaufgaben in einer Vielzahl von Anwendungen machen.</p> <p></p>"},{"location":"models/yolov8/#schlusselfunktionen","title":"Schl\u00fcsselfunktionen","text":"<ul> <li>Fortschrittliche Backbone- und Neck-Architekturen: YOLOv8 verwendet modernste Backbone- und Neck-Architekturen, die zu einer verbesserten Merkmalsextraktion und Objekterkennungsleistung f\u00fchren.</li> <li>Ankerfreier Split Ultralytics Head: YOLOv8 verwendet einen ankerfreien Split Ultralytics Head, der zu einer besseren Genauigkeit und einem effizienteren Erkennungsprozess im Vergleich zu ankerbasierten Ans\u00e4tzen f\u00fchrt.</li> <li>Optimale Genauigkeits-Geschwindigkeits-Balance: Mit dem Fokus auf die Aufrechterhaltung einer optimalen Balance zwischen Genauigkeit und Geschwindigkeit eignet sich YOLOv8 f\u00fcr Echtzeit-Objekterkennungsaufgaben in verschiedenen Anwendungsbereichen.</li> <li>Vielfalt an vortrainierten Modellen: YOLOv8 bietet eine Vielzahl von vortrainierten Modellen, um verschiedenen Aufgaben und Leistungsanforderungen gerecht zu werden. Dies erleichtert die Suche nach dem richtigen Modell f\u00fcr Ihren spezifischen Anwendungsfall.</li> </ul>"},{"location":"models/yolov8/#unterstutzte-aufgaben-und-modi","title":"Unterst\u00fctzte Aufgaben und Modi","text":"<p>Die YOLOv8-Serie bietet eine Vielzahl von Modellen, von denen jedes auf bestimmte Aufgaben in der Computer Vision spezialisiert ist. Diese Modelle sind so konzipiert, dass sie verschiedenen Anforderungen gerecht werden, von der Objekterkennung bis hin zu komplexeren Aufgaben wie Instanzsegmentierung, Pose/Keypoint-Erkennung und Klassifikation.</p> <p>Jede Variante der YOLOv8-Serie ist auf ihre jeweilige Aufgabe optimiert und gew\u00e4hrleistet damit hohe Leistung und Genauigkeit. Dar\u00fcber hinaus sind diese Modelle kompatibel mit verschiedenen Betriebsmodi, einschlie\u00dflich Inference, Validation, Training und Export. Dadurch wird ihre Verwendung in verschiedenen Phasen der Bereitstellung und Entwicklung erleichtert.</p> Modell Dateinamen Aufgabe Inference Validation Training Export YOLOv8 <code>yolov8n.pt</code> <code>yolov8s.pt</code> <code>yolov8m.pt</code> <code>yolov8l.pt</code> <code>yolov8x.pt</code> Objekterkennung \u2705 \u2705 \u2705 \u2705 YOLOv8-seg <code>yolov8n-seg.pt</code> <code>yolov8s-seg.pt</code> <code>yolov8m-seg.pt</code> <code>yolov8l-seg.pt</code> <code>yolov8x-seg.pt</code> Instanzsegmentierung \u2705 \u2705 \u2705 \u2705 YOLOv8-pose <code>yolov8n-pose.pt</code> <code>yolov8s-pose.pt</code> <code>yolov8m-pose.pt</code> <code>yolov8l-pose.pt</code> <code>yolov8x-pose.pt</code> <code>yolov8x-pose-p6.pt</code> Pose/Keypoints \u2705 \u2705 \u2705 \u2705 YOLOv8-cls <code>yolov8n-cls.pt</code> <code>yolov8s-cls.pt</code> <code>yolov8m-cls.pt</code> <code>yolov8l-cls.pt</code> <code>yolov8x-cls.pt</code> Klassifikation \u2705 \u2705 \u2705 \u2705 <p>Diese Tabelle gibt einen \u00dcberblick \u00fcber die verschiedenen Varianten des YOLOv8-Modells und deren Anwendungsbereiche sowie deren Kompatibilit\u00e4t mit verschiedenen Betriebsmodi wie Inference, Validation, Training und Export. Sie zeigt die Vielseitigkeit und Robustheit der YOLOv8-Serie, was sie f\u00fcr verschiedene Anwendungen in der Computer Vision geeignet macht.</p>"},{"location":"models/yolov8/#leistungskennzahlen","title":"Leistungskennzahlen","text":"<p>Performance</p> Objekterkennung (COCO)Objekterkennung (Open Images V7)Segmentierung (COCO)Klassifikation (ImageNet)Pose (COCO) <p>Siehe Objekterkennungsdokumentation f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf COCO trainiert wurden und 80 vortrainierte Klassen enthalten.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>val50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37,3 80,4 0,99 3,2 8,7 YOLOv8s 640 44,9 128,4 1,20 11,2 28,6 YOLOv8m 640 50,2 234,7 1,83 25,9 78,9 YOLOv8l 640 52,9 375,2 2,39 43,7 165,2 YOLOv8x 640 53,9 479,1 3,53 68,2 257,8 <p>Siehe Objekterkennungsdokumentation f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf Open Image V7 trainiert wurden und 600 vortrainierte Klassen enthalten.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>val50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n 640 18,4 142,4 1,21 3,5 10,5 YOLOv8s 640 27,7 183,1 1,40 11,4 29,7 YOLOv8m 640 33,6 408,5 2,26 26,2 80,6 YOLOv8l 640 34,9 596,9 2,43 44,1 167,4 YOLOv8x 640 36,3 860,6 3,56 68,7 260,6 <p>Siehe Segmentierungsdokumentation f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf COCO trainiert wurden und 80 vortrainierte Klassen enthalten.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>box50-95 mAP<sup>mask50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36,7 30,5 96,1 1,21 3,4 12,6 YOLOv8s-seg 640 44,6 36,8 155,7 1,47 11,8 42,6 YOLOv8m-seg 640 49,9 40,8 317,0 2,18 27,3 110,2 YOLOv8l-seg 640 52,3 42,6 572,4 2,79 46,0 220,5 YOLOv8x-seg 640 53,4 43,4 712,1 4,02 71,8 344,1 <p>Siehe Klassifikationsdokumentation f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf ImageNet trainiert wurden und 1000 vortrainierte Klassen enthalten.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) acc<sup>top1 acc<sup>top5 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) bei 640 YOLOv8n-cls 224 66,6 87,0 12,9 0,31 2,7 4,3 YOLOv8s-cls 224 72,3 91,1 23,4 0,35 6,4 13,5 YOLOv8m-cls 224 76,4 93,2 85,4 0,62 17,0 42,7 YOLOv8l-cls 224 78,0 94,1 163,0 0,87 37,5 99,7 YOLOv8x-cls 224 78,4 94,3 232,0 1,01 57,4 154,8 <p>Siehe Pose Estimation Docs f\u00fcr Beispiele zur Verwendung dieser Modelle, die auf COCO trainiert wurden und 1 vortrainierte Klasse, 'person', enthalten.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>pose50-95 mAP<sup>pose50 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50,4 80,1 131,8 1,18 3,3 9,2 YOLOv8s-pose 640 60,0 86,2 233,2 1,42 11,6 30,2 YOLOv8m-pose 640 65,0 88,8 456,3 2,00 26,4 81,0 YOLOv8l-pose 640 67,6 90,0 784,5 2,59 44,4 168,6 YOLOv8x-pose 640 69,2 90,2 1607,1 3,73 69,4 263,2 YOLOv8x-pose-p6 1280 71,6 91,2 4088,7 10,04 99,1 1066,4"},{"location":"models/yolov8/#beispiele-zur-verwendung","title":"Beispiele zur Verwendung","text":"<p>Dieses Beispiel liefert einfache Trainings- und Inferenzbeispiele f\u00fcr YOLOv8. F\u00fcr die vollst\u00e4ndige Dokumentation zu diesen und anderen Modi siehe die Seiten Predict, Train, Val und Export.</p> <p>Beachten Sie, dass das folgende Beispiel f\u00fcr YOLOv8 Detect Modelle f\u00fcr die Objekterkennung verwendet. F\u00fcr zus\u00e4tzliche unterst\u00fctzte Aufgaben siehe die Dokumentation zur Segmentation, Classification und Pose.</p> <p>Beispiel</p> PythonCLI <p>Vortrainierte PyTorch-<code>*.pt</code>-Modelle sowie Konfigurations-<code>*.yaml</code>-Dateien k\u00f6nnen der Klasse <code>YOLO()</code> in Python \u00fcbergeben werden, um eine Modellinstanz zu erstellen:</p> <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein vortrainiertes YOLOv8n-Modell f\u00fcr COCO\nmodel = YOLO('yolov8n.pt')\n\n# Zeigen Sie Informationen zum Modell an (optional)\nmodel.info()\n\n# Trainieren Sie das Modell mit dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# F\u00fchren Sie eine Inferenz mit dem YOLOv8n-Modell auf dem Bild 'bus.jpg' aus\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI-Befehle stehen zur direkten Ausf\u00fchrung der Modelle zur Verf\u00fcgung:</p> <pre><code># Laden Sie ein vortrainiertes YOLOv8n-Modell f\u00fcr COCO und trainieren Sie es mit dem COCO8-Beispieldatensatz f\u00fcr 100 Epochen\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Laden Sie ein vortrainiertes YOLOv8n-Modell f\u00fcr COCO und f\u00fchren Sie eine Inferenz auf dem Bild 'bus.jpg' aus\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov8/#zitate-und-danksagungen","title":"Zitate und Danksagungen","text":"<p>Wenn Sie das YOLOv8-Modell oder eine andere Software aus diesem Repository in Ihrer Arbeit verwenden, zitieren Sie es bitte in folgendem Format:</p> BibTeX <pre><code>@software{yolov8_ultralytics,\n  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n  title = {Ultralytics YOLOv8},\n  version = {8.0.0},\n  year = {2023},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n</code></pre> <p>Bitte beachten Sie, dass dieDOI aussteht und der Zitation hinzugef\u00fcgt wird, sobald sie verf\u00fcgbar ist. YOLOv8-Modelle werden unter den Lizenzen AGPL-3.0 und Enterprise bereitgestellt.</p>"},{"location":"modes/","title":"Ultralytics YOLOv8 Modi","text":""},{"location":"modes/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Ultralytics YOLOv8 ist nicht nur ein weiteres Objekterkennungsmodell; es ist ein vielseitiges Framework, das den gesamten Lebenszyklus von Machine-Learning-Modellen abdeckt - von der Dateneingabe und dem Modelltraining \u00fcber die Validierung und Bereitstellung bis hin zum Tracking in der realen Welt. Jeder Modus dient einem bestimmten Zweck und ist darauf ausgelegt, Ihnen die Flexibilit\u00e4t und Effizienz zu bieten, die f\u00fcr verschiedene Aufgaben und Anwendungsf\u00e4lle erforderlich ist.</p> <p> Anschauen: Ultralytics Modi Tutorial: Trainieren, Validieren, Vorhersagen, Exportieren &amp; Benchmarking. </p>"},{"location":"modes/#modi-im-uberblick","title":"Modi im \u00dcberblick","text":"<p>Das Verst\u00e4ndnis der verschiedenen Modi, die Ultralytics YOLOv8 unterst\u00fctzt, ist entscheidend, um das Beste aus Ihren Modellen herauszuholen:</p> <ul> <li>Train-Modus: Verfeinern Sie Ihr Modell mit angepassten oder vorgeladenen Datens\u00e4tzen.</li> <li>Val-Modus: Eine Nachtrainingspr\u00fcfung zur Validierung der Modellleistung.</li> <li>Predict-Modus: Entfesseln Sie die Vorhersagekraft Ihres Modells mit realen Daten.</li> <li>Export-Modus: Machen Sie Ihr Modell in verschiedenen Formaten einsatzbereit.</li> <li>Track-Modus: Erweitern Sie Ihr Objekterkennungsmodell um Echtzeit-Tracking-Anwendungen.</li> <li>Benchmark-Modus: Analysieren Sie die Geschwindigkeit und Genauigkeit Ihres Modells in verschiedenen Einsatzumgebungen.</li> </ul> <p>Dieser umfassende Leitfaden soll Ihnen einen \u00dcberblick und praktische Einblicke in jeden Modus geben, um Ihnen zu helfen, das volle Potenzial von YOLOv8 zu nutzen.</p>"},{"location":"modes/#trainieren","title":"Trainieren","text":"<p>Der Trainingsmodus wird verwendet, um ein YOLOv8-Modell mit einem angepassten Datensatz zu trainieren. In diesem Modus wird das Modell mit dem angegebenen Datensatz und den Hyperparametern trainiert. Der Trainingsprozess beinhaltet die Optimierung der Modellparameter, damit es die Klassen und Standorte von Objekten in einem Bild genau vorhersagen kann.</p> <p>Trainingsbeispiele</p>"},{"location":"modes/#validieren","title":"Validieren","text":"<p>Der Validierungsmodus wird genutzt, um ein YOLOv8-Modell nach dem Training zu bewerten. In diesem Modus wird das Modell auf einem Validierungsset getestet, um seine Genauigkeit und Generalisierungsleistung zu messen. Dieser Modus kann verwendet werden, um die Hyperparameter des Modells f\u00fcr eine bessere Leistung zu optimieren.</p> <p>Validierungsbeispiele</p>"},{"location":"modes/#vorhersagen","title":"Vorhersagen","text":"<p>Der Vorhersagemodus wird verwendet, um mit einem trainierten YOLOv8-Modell Vorhersagen f\u00fcr neue Bilder oder Videos zu treffen. In diesem Modus wird das Modell aus einer Checkpoint-Datei geladen, und der Benutzer kann Bilder oder Videos zur Inferenz bereitstellen. Das Modell sagt die Klassen und Standorte von Objekten in den Eingabebildern oder -videos voraus.</p> <p>Vorhersagebeispiele</p>"},{"location":"modes/#exportieren","title":"Exportieren","text":"<p>Der Exportmodus wird verwendet, um ein YOLOv8-Modell in ein Format zu exportieren, das f\u00fcr die Bereitstellung verwendet werden kann. In diesem Modus wird das Modell in ein Format konvertiert, das von anderen Softwareanwendungen oder Hardwareger\u00e4ten verwendet werden kann. Dieser Modus ist n\u00fctzlich, wenn das Modell in Produktionsumgebungen eingesetzt wird.</p> <p>Exportbeispiele</p>"},{"location":"modes/#verfolgen","title":"Verfolgen","text":"<p>Der Trackingmodus wird zur Echtzeitverfolgung von Objekten mit einem YOLOv8-Modell verwendet. In diesem Modus wird das Modell aus einer Checkpoint-Datei geladen, und der Benutzer kann einen Live-Videostream f\u00fcr das Echtzeitobjekttracking bereitstellen. Dieser Modus ist n\u00fctzlich f\u00fcr Anwendungen wie \u00dcberwachungssysteme oder selbstfahrende Autos.</p> <p>Trackingbeispiele</p>"},{"location":"modes/#benchmarking","title":"Benchmarking","text":"<p>Der Benchmark-Modus wird verwendet, um die Geschwindigkeit und Genauigkeit verschiedener Exportformate f\u00fcr YOLOv8 zu profilieren. Die Benchmarks liefern Informationen \u00fcber die Gr\u00f6\u00dfe des exportierten Formats, seine <code>mAP50-95</code>-Metriken (f\u00fcr Objekterkennung, Segmentierung und Pose) oder <code>accuracy_top5</code>-Metriken (f\u00fcr Klassifizierung) und die Inferenzzeit in Millisekunden pro Bild f\u00fcr verschiedene Exportformate wie ONNX, OpenVINO, TensorRT und andere. Diese Informationen k\u00f6nnen den Benutzern dabei helfen, das optimale Exportformat f\u00fcr ihren spezifischen Anwendungsfall basierend auf ihren Anforderungen an Geschwindigkeit und Genauigkeit auszuw\u00e4hlen.</p> <p>Benchmarkbeispiele</p>"},{"location":"modes/benchmark/","title":"Modell-Benchmarking mit Ultralytics YOLO","text":""},{"location":"modes/benchmark/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Nachdem Ihr Modell trainiert und validiert wurde, ist der n\u00e4chste logische Schritt, seine Leistung in verschiedenen realen Szenarien zu bewerten. Der Benchmark-Modus in Ultralytics YOLOv8 dient diesem Zweck, indem er einen robusten Rahmen f\u00fcr die Beurteilung von Geschwindigkeit und Genauigkeit Ihres Modells \u00fcber eine Reihe von Exportformaten hinweg bietet.</p>"},{"location":"modes/benchmark/#warum-ist-benchmarking-entscheidend","title":"Warum ist Benchmarking entscheidend?","text":"<ul> <li>Informierte Entscheidungen: Erhalten Sie Einblicke in die Kompromisse zwischen Geschwindigkeit und Genauigkeit.</li> <li>Ressourcenzuweisung: Verstehen Sie, wie sich verschiedene Exportformate auf unterschiedlicher Hardware verhalten.</li> <li>Optimierung: Erfahren Sie, welches Exportformat die beste Leistung f\u00fcr Ihren spezifischen Anwendungsfall bietet.</li> <li>Kosteneffizienz: Nutzen Sie Hardware-Ressourcen basierend auf den Benchmark-Ergebnissen effizienter.</li> </ul>"},{"location":"modes/benchmark/#schlusselmetriken-im-benchmark-modus","title":"Schl\u00fcsselmetriken im Benchmark-Modus","text":"<ul> <li>mAP50-95: F\u00fcr Objekterkennung, Segmentierung und Posensch\u00e4tzung.</li> <li>accuracy_top5: F\u00fcr die Bildklassifizierung.</li> <li>Inferenzzeit: Zeit, die f\u00fcr jedes Bild in Millisekunden ben\u00f6tigt wird.</li> </ul>"},{"location":"modes/benchmark/#unterstutzte-exportformate","title":"Unterst\u00fctzte Exportformate","text":"<ul> <li>ONNX: F\u00fcr optimale CPU-Leistung</li> <li>TensorRT: F\u00fcr maximale GPU-Effizienz</li> <li>OpenVINO: F\u00fcr die Optimierung von Intel-Hardware</li> <li>CoreML, TensorFlow SavedModel, und mehr: F\u00fcr vielf\u00e4ltige Deployment-Anforderungen.</li> </ul> <p>Tipp</p> <ul> <li>Exportieren Sie in ONNX oder OpenVINO f\u00fcr bis zu 3x CPU-Beschleunigung.</li> <li>Exportieren Sie in TensorRT f\u00fcr bis zu 5x GPU-Beschleunigung.</li> </ul>"},{"location":"modes/benchmark/#anwendungsbeispiele","title":"Anwendungsbeispiele","text":"<p>F\u00fchren Sie YOLOv8n-Benchmarks auf allen unterst\u00fctzten Exportformaten einschlie\u00dflich ONNX, TensorRT usw. durch. Siehe den Abschnitt Argumente unten f\u00fcr eine vollst\u00e4ndige Liste der Exportargumente.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark auf GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumente","title":"Argumente","text":"<p>Argumente wie <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> und <code>verbose</code> bieten Benutzern die Flexibilit\u00e4t, die Benchmarks auf ihre spezifischen Bed\u00fcrfnisse abzustimmen und die Leistung verschiedener Exportformate m\u00fchelos zu vergleichen.</p> Schl\u00fcssel Wert Beschreibung <code>model</code> <code>None</code> Pfad zur Modelldatei, z. B. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> Pfad zur YAML, die das Benchmarking-Dataset referenziert (unter <code>val</code>-Kennzeichnung) <code>imgsz</code> <code>640</code> Bildgr\u00f6\u00dfe als Skalar oder Liste (h, w), z. B. (640, 480) <code>half</code> <code>False</code> FP16-Quantisierung <code>int8</code> <code>False</code> INT8-Quantisierung <code>device</code> <code>None</code> Ger\u00e4t zum Ausf\u00fchren, z. B. CUDA device=0 oder device=0,1,2,3 oder device=cpu <code>verbose</code> <code>False</code> bei Fehlern nicht fortsetzen (bool), oder Wertebereichsschwelle (float)"},{"location":"modes/benchmark/#exportformate","title":"Exportformate","text":"<p>Benchmarks werden automatisch auf allen m\u00f6glichen Exportformaten unten ausgef\u00fchrt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vollst\u00e4ndige Details zum <code>export</code> finden Sie auf der Export-Seite.</p>"},{"location":"modes/export/","title":"Modell-Export mit Ultralytics YOLO","text":""},{"location":"modes/export/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Das ultimative Ziel des Trainierens eines Modells besteht darin, es f\u00fcr reale Anwendungen einzusetzen. Der Exportmodus in Ultralytics YOLOv8 bietet eine vielseitige Palette von Optionen f\u00fcr den Export Ihres trainierten Modells in verschiedene Formate, sodass es auf verschiedenen Plattformen und Ger\u00e4ten eingesetzt werden kann. Dieser umfassende Leitfaden soll Sie durch die Nuancen des Modell-Exports f\u00fchren und zeigen, wie Sie maximale Kompatibilit\u00e4t und Leistung erzielen k\u00f6nnen.</p> <p> Ansehen: Wie man ein benutzerdefiniertes trainiertes Ultralytics YOLOv8-Modell exportiert und Live-Inferenz auf der Webcam ausf\u00fchrt. </p>"},{"location":"modes/export/#warum-den-exportmodus-von-yolov8-wahlen","title":"Warum den Exportmodus von YOLOv8 w\u00e4hlen?","text":"<ul> <li>Vielseitigkeit: Export in verschiedene Formate einschlie\u00dflich ONNX, TensorRT, CoreML und mehr.</li> <li>Leistung: Bis zu 5-fache GPU-Beschleunigung mit TensorRT und 3-fache CPU-Beschleunigung mit ONNX oder OpenVINO.</li> <li>Kompatibilit\u00e4t: Machen Sie Ihr Modell universell einsetzbar in zahlreichen Hardware- und Softwareumgebungen.</li> <li>Benutzerfreundlichkeit: Einfache CLI- und Python-API f\u00fcr schnellen und unkomplizierten Modell-Export.</li> </ul>"},{"location":"modes/export/#schlusselfunktionen-des-exportmodus","title":"Schl\u00fcsselfunktionen des Exportmodus","text":"<p>Hier sind einige der herausragenden Funktionen:</p> <ul> <li>Ein-Klick-Export: Einfache Befehle f\u00fcr den Export in verschiedene Formate.</li> <li>Batch-Export: Export von Modellen, die Batch-Inferenz unterst\u00fctzen.</li> <li>Optimiertes Inferenzverhalten: Exportierte Modelle sind f\u00fcr schnellere Inferenzzeiten optimiert.</li> <li>Tutorial-Videos: Ausf\u00fchrliche Anleitungen und Tutorials f\u00fcr ein reibungsloses Exporterlebnis.</li> </ul> <p>Tipp</p> <ul> <li>Exportieren Sie nach ONNX oder OpenVINO f\u00fcr bis zu 3-fache CPU-Beschleunigung.</li> <li>Exportieren Sie nach TensorRT f\u00fcr bis zu 5-fache GPU-Beschleunigung.</li> </ul>"},{"location":"modes/export/#nutzungsbeispiele","title":"Nutzungsbeispiele","text":"<p>Exportieren Sie ein YOLOv8n-Modell in ein anderes Format wie ONNX oder TensorRT. Weitere Informationen zu den Exportargumenten finden Sie im Abschnitt \u201eArgumente\u201c unten.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden eines Modells\nmodel = YOLO('yolov8n.pt')  # offizielles Modell laden\nmodel = YOLO('path/to/best.pt')  # benutzerdefiniertes trainiertes Modell laden\n\n# Exportieren des Modells\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # offizielles Modell exportieren\nyolo export model=path/to/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre>"},{"location":"modes/export/#argumente","title":"Argumente","text":"<p>Exporteinstellungen f\u00fcr YOLO-Modelle beziehen sich auf verschiedene Konfigurationen und Optionen, die verwendet werden, um das Modell zu speichern oder f\u00fcr den Einsatz in anderen Umgebungen oder Plattformen zu exportieren. Diese Einstellungen k\u00f6nnen die Leistung, Gr\u00f6\u00dfe und Kompatibilit\u00e4t des Modells mit verschiedenen Systemen beeinflussen. Zu den g\u00e4ngigen Exporteinstellungen von YOLO geh\u00f6ren das Format der exportierten Modelldatei (z. B. ONNX, TensorFlow SavedModel), das Ger\u00e4t, auf dem das Modell ausgef\u00fchrt wird (z. B. CPU, GPU) und das Vorhandensein zus\u00e4tzlicher Funktionen wie Masken oder mehrere Labels pro Box. Andere Faktoren, die den Exportprozess beeinflussen k\u00f6nnen, sind die spezifische Aufgabe, f\u00fcr die das Modell verwendet wird, und die Anforderungen oder Einschr\u00e4nkungen der Zielumgebung oder -plattform. Es ist wichtig, diese Einstellungen sorgf\u00e4ltig zu ber\u00fccksichtigen und zu konfigurieren, um sicherzustellen, dass das exportierte Modell f\u00fcr den beabsichtigten Einsatzzweck optimiert ist und in der Zielumgebung effektiv eingesetzt werden kann.</p> Schl\u00fcssel Wert Beschreibung <code>format</code> <code>'torchscript'</code> Format f\u00fcr den Export <code>imgsz</code> <code>640</code> Bildgr\u00f6\u00dfe als Skalar oder (h, w)-Liste, z.B. (640, 480) <code>keras</code> <code>False</code> Verwendung von Keras f\u00fcr TensorFlow SavedModel-Export <code>optimize</code> <code>False</code> TorchScript: Optimierung f\u00fcr mobile Ger\u00e4te <code>half</code> <code>False</code> FP16-Quantisierung <code>int8</code> <code>False</code> INT8-Quantisierung <code>dynamic</code> <code>False</code> ONNX/TensorRT: dynamische Achsen <code>simplify</code> <code>False</code> ONNX/TensorRT: Vereinfachung des Modells <code>opset</code> <code>None</code> ONNX: Opset-Version (optional, Standardwert ist neueste) <code>workspace</code> <code>4</code> TensorRT: Arbeitsbereichgr\u00f6\u00dfe (GB) <code>nms</code> <code>False</code> CoreML: Hinzuf\u00fcgen von NMS"},{"location":"modes/export/#exportformate","title":"Exportformate","text":"<p>Verf\u00fcgbare YOLOv8-Exportformate finden Sie in der Tabelle unten. Sie k\u00f6nnen in jedes Format exportieren, indem Sie das <code>format</code>-Argument verwenden, z. B. <code>format='onnx'</code> oder <code>format='engine'</code>.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Modellvorhersage mit Ultralytics YOLO","text":""},{"location":"modes/predict/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Im Bereich des maschinellen Lernens und der Computer Vision wird der Prozess des Verstehens visueller Daten als 'Inferenz' oder 'Vorhersage' bezeichnet. Ultralytics YOLOv8 bietet eine leistungsstarke Funktion, die als Prognosemodus bekannt ist und f\u00fcr eine hochleistungsf\u00e4hige, echtzeitf\u00e4hige Inferenz auf einer breiten Palette von Datenquellen zugeschnitten ist.</p> <p> Anschauen: Wie man die Ausgaben vom Ultralytics YOLOv8 Modell f\u00fcr individuelle Projekte extrahiert. </p>"},{"location":"modes/predict/#anwendungen-in-der-realen-welt","title":"Anwendungen in der realen Welt","text":"Herstellung Sport Sicherheit Erkennung von Fahrzeugersatzteilen Erkennung von Fu\u00dfballspielern Erkennung von st\u00fcrzenden Personen"},{"location":"modes/predict/#warum-ultralytics-yolo-fur-inferenz-nutzen","title":"Warum Ultralytics YOLO f\u00fcr Inferenz nutzen?","text":"<p>Hier sind Gr\u00fcnde, warum Sie den Prognosemodus von YOLOv8 f\u00fcr Ihre verschiedenen Inferenzanforderungen in Betracht ziehen sollten:</p> <ul> <li>Vielseitigkeit: F\u00e4hig, Inferenzen auf Bilder, Videos und sogar Live-Streams zu machen.</li> <li>Leistung: Entwickelt f\u00fcr Echtzeit-Hochgeschwindigkeitsverarbeitung ohne Genauigkeitsverlust.</li> <li>Einfache Bedienung: Intuitive Python- und CLI-Schnittstellen f\u00fcr schnelle Einsatzbereitschaft und Tests.</li> <li>Hohe Anpassbarkeit: Verschiedene Einstellungen und Parameter, um das Verhalten der Modellinferenz entsprechend Ihren spezifischen Anforderungen zu optimieren.</li> </ul>"},{"location":"modes/predict/#schlusselfunktionen-des-prognosemodus","title":"Schl\u00fcsselfunktionen des Prognosemodus","text":"<p>Der Prognosemodus von YOLOv8 ist robust und vielseitig konzipiert und verf\u00fcgt \u00fcber:</p> <ul> <li>Kompatibilit\u00e4t mit mehreren Datenquellen: Ganz gleich, ob Ihre Daten in Form von Einzelbildern, einer Bildersammlung, Videodateien oder Echtzeit-Videostreams vorliegen, der Prognosemodus deckt alles ab.</li> <li>Streaming-Modus: Nutzen Sie die Streaming-Funktion, um einen speichereffizienten Generator von <code>Results</code>-Objekten zu erzeugen. Aktivieren Sie dies, indem Sie <code>stream=True</code> in der Aufrufmethode des Predictors einstellen.</li> <li>Batchverarbeitung: Die M\u00f6glichkeit, mehrere Bilder oder Videoframes in einem einzigen Batch zu verarbeiten, wodurch die Inferenzzeit weiter verk\u00fcrzt wird.</li> <li>Integrationsfreundlich: Dank der flexiblen API leicht in bestehende Datenpipelines und andere Softwarekomponenten zu integrieren.</li> </ul> <p>Ultralytics YOLO-Modelle geben entweder eine Python-Liste von <code>Results</code>-Objekten zur\u00fcck, oder einen speichereffizienten Python-Generator von <code>Results</code>-Objekten, wenn <code>stream=True</code> beim Inferenzvorgang an das Modell \u00fcbergeben wird:</p> <p>Predict</p> Gibt eine Liste mit <code>stream=False</code> zur\u00fcckGibt einen Generator mit <code>stream=True</code> zur\u00fcck <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n.pt')  # vortrainiertes YOLOv8n Modell\n\n# Batch-Inferenz auf einer Liste von Bildern ausf\u00fchren\nresults = model(['im1.jpg', 'im2.jpg'])  # gibt eine Liste von Results-Objekten zur\u00fcck\n\n# Ergebnisliste verarbeiten\nfor result in results:\n    boxes = result.boxes  # Boxes-Objekt f\u00fcr Bbox-Ausgaben\n    masks = result.masks  # Masks-Objekt f\u00fcr Segmentierungsmasken-Ausgaben\n    keypoints = result.keypoints  # Keypoints-Objekt f\u00fcr Pose-Ausgaben\n    probs = result.probs  # Probs-Objekt f\u00fcr Klassifizierungs-Ausgaben\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n.pt')  # vortrainiertes YOLOv8n Modell\n\n# Batch-Inferenz auf einer Liste von Bildern ausf\u00fchren\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # gibt einen Generator von Results-Objekten zur\u00fcck\n\n# Generator von Ergebnissen verarbeiten\nfor result in results:\n    boxes = result.boxes  # Boxes-Objekt f\u00fcr Bbox-Ausgaben\n    masks = result.masks  # Masks-Objekt f\u00fcr Segmentierungsmasken-Ausgaben\n    keypoints = result.keypoints  # Keypoints-Objekt f\u00fcr Pose-Ausgaben\n    probs = result.probs  # Probs-Objekt f\u00fcr Klassifizierungs-Ausgaben\n</code></pre>"},{"location":"modes/predict/#inferenzquellen","title":"Inferenzquellen","text":"<p>YOLOv8 kann verschiedene Arten von Eingabequellen f\u00fcr die Inferenz verarbeiten, wie in der folgenden Tabelle gezeigt. Die Quellen umfassen statische Bilder, Videostreams und verschiedene Datenformate. Die Tabelle gibt ebenfalls an, ob jede Quelle im Streaming-Modus mit dem Argument <code>stream=True</code> \u2705 verwendet werden kann. Der Streaming-Modus ist vorteilhaft f\u00fcr die Verarbeitung von Videos oder Live-Streams, da er einen Generator von Ergebnissen statt das Laden aller Frames in den Speicher erzeugt.</p> <p>Tipp</p> <p>Verwenden Sie <code>stream=True</code> f\u00fcr die Verarbeitung langer Videos oder gro\u00dfer Datens\u00e4tze, um den Speicher effizient zu verwalten. Bei <code>stream=False</code> werden die Ergebnisse f\u00fcr alle Frames oder Datenpunkte im Speicher gehalten, was bei gro\u00dfen Eingaben schnell zu Speicher\u00fcberl\u00e4ufen f\u00fchren kann. Im Gegensatz dazu verwendet <code>stream=True</code> einen Generator, der nur die Ergebnisse des aktuellen Frames oder Datenpunkts im Speicher beh\u00e4lt, was den Speicherverbrauch erheblich reduziert und Speicher\u00fcberlaufprobleme verhindert.</p> Quelle Argument Typ Hinweise Bild <code>'image.jpg'</code> <code>str</code> oder <code>Path</code> Einzelbilddatei. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL zu einem Bild. Bildschirmaufnahme <code>'screen'</code> <code>str</code> Eine Bildschirmaufnahme erstellen. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> HWC-Format mit RGB-Kan\u00e4len. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> HWC-Format mit BGR-Kan\u00e4len <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> HWC-Format mit BGR-Kan\u00e4len <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> BCHW-Format mit RGB-Kan\u00e4len <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> oder <code>Path</code> CSV-Datei mit Pfaden zu Bildern, Videos oder Verzeichnissen. video \u2705 <code>'video.mp4'</code> <code>str</code> oder <code>Path</code> Videodatei in Formaten wie MP4, AVI, usw. Verzeichnis \u2705 <code>'path/'</code> <code>str</code> oder <code>Path</code> Pfad zu einem Verzeichnis mit Bildern oder Videos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Glob-Muster, um mehrere Dateien zu finden. Verwenden Sie das <code>*</code> Zeichen als Platzhalter. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL zu einem YouTube-Video. stream \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL f\u00fcr Streaming-Protokolle wie RTSP, RTMP, TCP oder eine IP-Adresse. Multi-Stream \u2705 <code>'list.streams'</code> <code>str</code> oder <code>Path</code> <code>*.streams</code> Textdatei mit einer Stream-URL pro Zeile, z.B. 8 Streams laufen bei Batch-Gr\u00f6\u00dfe 8. <p>Untenstehend finden Sie Codebeispiele f\u00fcr die Verwendung jedes Quelltyps:</p> <p>Vorhersagequellen</p> BildBildschirmaufnahmeURLPILOpenCVnumpytorch <p>F\u00fchren Sie die Inferenz auf einer Bilddatei aus. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Pfad zur Bilddatei definieren\nquell = 'Pfad/zum/Bild.jpg'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf dem aktuellen Bildschirminhalt als Screenshot aus. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Aktuellen Screenshot als Quelle definieren\nquell = 'Bildschirm'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild oder Video aus, das \u00fcber eine URL remote gehostet wird. <pre><code>from ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Remote-Bild- oder Video-URL definieren\nquell = 'https://ultralytics.com/images/bus.jpg'\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das mit der Python Imaging Library (PIL) ge\u00f6ffnet wurde. <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein Bild mit PIL \u00f6ffnen\nquell = Image.open('Pfad/zum/Bild.jpg')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das mit OpenCV gelesen wurde. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein Bild mit OpenCV lesen\nquell = cv2.imread('Pfad/zum/Bild.jpg')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das als numpy-Array dargestellt wird. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein zuf\u00e4lliges numpy-Array der HWC-Form (640, 640, 3) mit Werten im Bereich [0, 255] und Typ uint8 erstellen\nquell = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p> <p>F\u00fchren Sie die Inferenz auf einem Bild aus, das als PyTorch-Tensor dargestellt wird. <pre><code>import torch\nfrom ultralytics import YOLO\n\n# Ein vortrainiertes YOLOv8n Modell laden\nmodel = YOLO('yolov8n.pt')\n\n# Ein zuf\u00e4lliger torch-Tensor der BCHW-Form (1, 3, 640, 640) mit Werten im Bereich [0, 1] und Typ float32 erstellen\nquell = torch.rand(1, 3, 640, 640, dtype=torch.float32)\n\n# Inferenz auf der Quelle ausf\u00fchren\nergebnisse = model(quell)  # Liste von Results-Objekten\n</code></pre></p>"},{"location":"modes/track/","title":"Multi-Objektverfolgung mit Ultralytics YOLO","text":"<p>Objektverfolgung im Bereich der Videoanalytik ist eine essentielle Aufgabe, die nicht nur den Standort und die Klasse von Objekten innerhalb des Frames identifiziert, sondern auch eine eindeutige ID f\u00fcr jedes erkannte Objekt, w\u00e4hrend das Video fortschreitet, erh\u00e4lt. Die Anwendungsm\u00f6glichkeiten sind grenzenlos \u2013 von \u00dcberwachung und Sicherheit bis hin zur Echtzeitsportanalytik.</p>"},{"location":"modes/track/#warum-ultralytics-yolo-fur-objektverfolgung-wahlen","title":"Warum Ultralytics YOLO f\u00fcr Objektverfolgung w\u00e4hlen?","text":"<p>Die Ausgabe von Ultralytics Trackern ist konsistent mit der standardm\u00e4\u00dfigen Objekterkennung, bietet aber zus\u00e4tzlich Objekt-IDs. Dies erleichtert das Verfolgen von Objekten in Videostreams und das Durchf\u00fchren nachfolgender Analysen. Hier sind einige Gr\u00fcnde, warum Sie Ultralytics YOLO f\u00fcr Ihre Objektverfolgungsaufgaben in Betracht ziehen sollten:</p> <ul> <li>Effizienz: Verarbeitung von Videostreams in Echtzeit ohne Einbu\u00dfen bei der Genauigkeit.</li> <li>Flexibilit\u00e4t: Unterst\u00fctzt mehrere Tracking-Algorithmen und -Konfigurationen.</li> <li>Benutzerfreundlichkeit: Einfache Python-API und CLI-Optionen f\u00fcr schnelle Integration und Bereitstellung.</li> <li>Anpassbarkeit: Einfache Verwendung mit individuell trainierten YOLO-Modellen, erm\u00f6glicht Integration in branchenspezifische Anwendungen.</li> </ul> <p> Ansehen: Objekterkennung und -verfolgung mit Ultralytics YOLOv8. </p>"},{"location":"modes/track/#anwendungen-in-der-realen-welt","title":"Anwendungen in der realen Welt","text":"Transportwesen Einzelhandel Aquakultur Fahrzeugverfolgung Personenverfolgung Fischverfolgung"},{"location":"modes/track/#eigenschaften-auf-einen-blick","title":"Eigenschaften auf einen Blick","text":"<p>Ultralytics YOLO erweitert seine Objekterkennungsfunktionen, um eine robuste und vielseitige Objektverfolgung bereitzustellen:</p> <ul> <li>Echtzeitverfolgung: Nahtloses Verfolgen von Objekten in Videos mit hoher Bildfrequenz.</li> <li>Unterst\u00fctzung mehrerer Tracker: Auswahl aus einer Vielzahl etablierter Tracking-Algorithmen.</li> <li>Anpassbare Tracker-Konfigurationen: Anpassen des Tracking-Algorithmus an spezifische Anforderungen durch Einstellung verschiedener Parameter.</li> </ul>"},{"location":"modes/track/#verfugbare-tracker","title":"Verf\u00fcgbare Tracker","text":"<p>Ultralytics YOLO unterst\u00fctzt die folgenden Tracking-Algorithmen. Sie k\u00f6nnen aktiviert werden, indem Sie die entsprechende YAML-Konfigurationsdatei wie <code>tracker=tracker_type.yaml</code> \u00fcbergeben:</p> <ul> <li>BoT-SORT - Verwenden Sie <code>botsort.yaml</code>, um diesen Tracker zu aktivieren.</li> <li>ByteTrack - Verwenden Sie <code>bytetrack.yaml</code>, um diesen Tracker zu aktivieren.</li> </ul> <p>Der Standardtracker ist BoT-SORT.</p>"},{"location":"modes/track/#verfolgung","title":"Verfolgung","text":"<p>Um den Tracker auf Videostreams auszuf\u00fchren, verwenden Sie ein trainiertes Erkennungs-, Segmentierungs- oder Posierungsmodell wie YOLOv8n, YOLOv8n-seg und YOLOv8n-pose.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein offizielles oder individuelles Modell\nmodel = YOLO('yolov8n.pt')  # Laden Sie ein offizielles Erkennungsmodell\nmodel = YOLO('yolov8n-seg.pt')  # Laden Sie ein offizielles Segmentierungsmodell\nmodel = YOLO('yolov8n-pose.pt')  # Laden Sie ein offizielles Posierungsmodell\nmodel = YOLO('path/to/best.pt')  # Laden Sie ein individuell trainiertes Modell\n\n# F\u00fchren Sie die Verfolgung mit dem Modell durch\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Verfolgung mit Standardtracker\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Verfolgung mit ByteTrack-Tracker\n</code></pre> <pre><code># F\u00fchren Sie die Verfolgung mit verschiedenen Modellen \u00fcber die Befehlszeilenschnittstelle durch\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Erkennungsmodell\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Segmentierungsmodell\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Offizielles Posierungsmodell\nyolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Individuell trainiertes Modell\n\n# Verfolgung mit ByteTrack-Tracker\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Wie in der obigen Nutzung zu sehen ist, ist die Verfolgung f\u00fcr alle Detect-, Segment- und Pose-Modelle verf\u00fcgbar, die auf Videos oder Streaming-Quellen ausgef\u00fchrt werden.</p>"},{"location":"modes/track/#konfiguration","title":"Konfiguration","text":""},{"location":"modes/track/#tracking-argumente","title":"Tracking-Argumente","text":"<p>Die Tracking-Konfiguration teilt Eigenschaften mit dem Predict-Modus, wie <code>conf</code>, <code>iou</code> und <code>show</code>. F\u00fcr weitere Konfigurationen siehe die Seite des Predict-Modells.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Konfigurieren Sie die Tracking-Parameter und f\u00fchren Sie den Tracker aus\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Konfigurieren Sie die Tracking-Parameter und f\u00fchren Sie den Tracker \u00fcber die Befehlszeilenschnittstelle aus\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#tracker-auswahl","title":"Tracker-Auswahl","text":"<p>Ultralytics erm\u00f6glicht es Ihnen auch, eine modifizierte Tracker-Konfigurationsdatei zu verwenden. Hierf\u00fcr kopieren Sie einfach eine Tracker-Konfigurationsdatei (zum Beispiel <code>custom_tracker.yaml</code>) von ultralytics/cfg/trackers und \u00e4ndern jede Konfiguration (au\u00dfer dem <code>tracker_type</code>), wie es Ihren Bed\u00fcrfnissen entspricht.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie das Modell und f\u00fchren Sie den Tracker mit einer individuellen Konfigurationsdatei aus\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Laden Sie das Modell und f\u00fchren Sie den Tracker mit einer individuellen Konfigurationsdatei \u00fcber die Befehlszeilenschnittstelle aus\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>F\u00fcr eine umfassende Liste der Tracking-Argumente siehe die Seite ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#python-beispiele","title":"Python-Beispiele","text":""},{"location":"modes/track/#persistierende-tracks-schleife","title":"Persistierende Tracks-Schleife","text":"<p>Hier ist ein Python-Skript, das OpenCV (<code>cv2</code>) und YOLOv8 verwendet, um Objektverfolgung in Videoframes durchzuf\u00fchren. Dieses Skript setzt voraus, dass Sie die notwendigen Pakete (<code>opencv-python</code> und <code>ultralytics</code>) bereits installiert haben. Das Argument <code>persist=True</code> teilt dem Tracker mit, dass das aktuelle Bild oder Frame das n\u00e4chste in einer Sequenz ist und Tracks aus dem vorherigen Bild im aktuellen Bild erwartet werden.</p> <p>Streaming-For-Schleife mit Tracking</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Laden Sie das YOLOv8-Modell\nmodel = YOLO('yolov8n.pt')\n\n# \u00d6ffnen Sie die Videodatei\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Schleife durch die Videoframes\nwhile cap.isOpened():\n    # Einen Frame aus dem Video lesen\n    success, frame = cap.read()\n\n    if success:\n        # F\u00fchren Sie YOLOv8-Tracking im Frame aus, wobei Tracks zwischen Frames beibehalten werden\n        results = model.track(frame, persist=True)\n\n        # Visualisieren Sie die Ergebnisse im Frame\n        annotated_frame = results[0].plot()\n\n        # Zeigen Sie den kommentierten Frame an\n        cv2.imshow(\"YOLOv8-Tracking\", annotated_frame)\n\n        # Beenden Sie die Schleife, wenn 'q' gedr\u00fcckt wird\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Beenden Sie die Schleife, wenn das Ende des Videos erreicht ist\n        break\n\n# Geben Sie das Videoaufnahmeobjekt frei und schlie\u00dfen Sie das Anzeigefenster\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Bitte beachten Sie die \u00c4nderung von <code>model(frame)</code> zu <code>model.track(frame)</code>, welche die Objektverfolgung anstelle der einfachen Erkennung aktiviert. Dieses modifizierte Skript f\u00fchrt den Tracker auf jedem Frame des Videos aus, visualisiert die Ergebnisse und zeigt sie in einem Fenster an. Die Schleife kann durch Dr\u00fccken von 'q' beendet werden.</p>"},{"location":"modes/track/#neue-tracker-beisteuern","title":"Neue Tracker beisteuern","text":"<p>Sind Sie versiert in der Multi-Objektverfolgung und haben erfolgreich einen Tracking-Algorithmus mit Ultralytics YOLO implementiert oder angepasst? Wir laden Sie ein, zu unserem Trackers-Bereich in ultralytics/cfg/trackers beizutragen! Ihre realen Anwendungen und L\u00f6sungen k\u00f6nnten f\u00fcr Benutzer, die an Tracking-Aufgaben arbeiten, von unsch\u00e4tzbarem Wert sein.</p> <p>Indem Sie zu diesem Bereich beitragen, helfen Sie, das Spektrum verf\u00fcgbarer Tracking-L\u00f6sungen innerhalb des Ultralytics YOLO-Frameworks zu erweitern und f\u00fcgen eine weitere Funktionsschicht f\u00fcr die Gemeinschaft hinzu.</p> <p>Um Ihren Beitrag einzuleiten, sehen Sie bitte in unserem Contributing Guide f\u00fcr umfassende Anweisungen zur Einreichung eines Pull Requests (PR) \ud83d\udee0\ufe0f. Wir sind gespannt darauf, was Sie beitragen!</p> <p>Gemeinsam verbessern wir die Tracking-F\u00e4higkeiten des Ultralytics YOLO-\u00d6kosystems \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Modelltraining mit Ultralytics YOLO","text":""},{"location":"modes/train/#einleitung","title":"Einleitung","text":"<p>Das Training eines Deep-Learning-Modells beinhaltet das Einspeisen von Daten und die Anpassung seiner Parameter, so dass es genaue Vorhersagen treffen kann. Der Trainingsmodus in Ultralytics YOLOv8 ist f\u00fcr das effektive und effiziente Training von Objekterkennungsmodellen konzipiert und nutzt dabei die F\u00e4higkeiten moderner Hardware voll aus. Dieser Leitfaden zielt darauf ab, alle Details zu vermitteln, die Sie ben\u00f6tigen, um mit dem Training Ihrer eigenen Modelle unter Verwendung des robusten Funktionssatzes von YOLOv8 zu beginnen.</p> <p> Video anschauen: Wie man ein YOLOv8-Modell auf Ihrem benutzerdefinierten Datensatz in Google Colab trainiert. </p>"},{"location":"modes/train/#warum-ultralytics-yolo-fur-das-training-wahlen","title":"Warum Ultralytics YOLO f\u00fcr das Training w\u00e4hlen?","text":"<p>Hier einige \u00fcberzeugende Gr\u00fcnde, sich f\u00fcr den Trainingsmodus von YOLOv8 zu entscheiden:</p> <ul> <li>Effizienz: Machen Sie das Beste aus Ihrer Hardware, egal ob Sie auf einem Single-GPU-Setup sind oder \u00fcber mehrere GPUs skalieren.</li> <li>Vielseitigkeit: Training auf benutzerdefinierten Datens\u00e4tzen zus\u00e4tzlich zu den bereits verf\u00fcgbaren Datens\u00e4tzen wie COCO, VOC und ImageNet.</li> <li>Benutzerfreundlich: Einfache, aber leistungsstarke CLI- und Python-Schnittstellen f\u00fcr ein unkompliziertes Trainingserlebnis.</li> <li>Flexibilit\u00e4t der Hyperparameter: Eine breite Palette von anpassbaren Hyperparametern, um die Modellleistung zu optimieren.</li> </ul>"},{"location":"modes/train/#schlusselfunktionen-des-trainingsmodus","title":"Schl\u00fcsselfunktionen des Trainingsmodus","text":"<p>Die folgenden sind einige bemerkenswerte Funktionen von YOLOv8s Trainingsmodus:</p> <ul> <li>Automatischer Datensatz-Download: Standarddatens\u00e4tze wie COCO, VOC und ImageNet werden bei der ersten Verwendung automatisch heruntergeladen.</li> <li>Multi-GPU-Unterst\u00fctzung: Skalieren Sie Ihr Training nahtlos \u00fcber mehrere GPUs, um den Prozess zu beschleunigen.</li> <li>Konfiguration der Hyperparameter: Die M\u00f6glichkeit zur Modifikation der Hyperparameter \u00fcber YAML-Konfigurationsdateien oder CLI-Argumente.</li> <li>Visualisierung und \u00dcberwachung: Echtzeit-Tracking von Trainingsmetriken und Visualisierung des Lernprozesses f\u00fcr bessere Einsichten.</li> </ul> <p>Tipp</p> <ul> <li>YOLOv8-Datens\u00e4tze wie COCO, VOC, ImageNet und viele andere werden automatisch bei der ersten Verwendung heruntergeladen, d.h. <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#nutzungsbeispiele","title":"Nutzungsbeispiele","text":"<p>Trainieren Sie YOLOv8n auf dem COCO128-Datensatz f\u00fcr 100 Epochen bei einer Bildgr\u00f6\u00dfe von 640. Das Trainingsger\u00e4t kann mit dem Argument <code>device</code> spezifiziert werden. Wenn kein Argument \u00fcbergeben wird, wird GPU <code>device=0</code> verwendet, wenn verf\u00fcgbar, sonst wird <code>device=cpu</code> verwendet. Siehe den Abschnitt Argumente unten f\u00fcr eine vollst\u00e4ndige Liste der Trainingsargumente.</p> <p>Beispiel f\u00fcr Single-GPU- und CPU-Training</p> <p>Das Ger\u00e4t wird automatisch ermittelt. Wenn eine GPU verf\u00fcgbar ist, dann wird diese verwendet, sonst beginnt das Training auf der CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.yaml')  # bauen Sie ein neues Modell aus YAML\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # bauen Sie aus YAML und \u00fcbertragen Sie Gewichte\n\n# Trainieren Sie das Modell\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Bauen Sie ein neues Modell aus YAML und beginnen Sie das Training von Grund auf\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Beginnen Sie das Training von einem vortrainierten *.pt Modell\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Bauen Sie ein neues Modell aus YAML, \u00fcbertragen Sie vortrainierte Gewichte darauf und beginnen Sie das Training\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#multi-gpu-training","title":"Multi-GPU-Training","text":"<p>Multi-GPU-Training erm\u00f6glicht eine effizientere Nutzung von verf\u00fcgbaren Hardware-Ressourcen, indem die Trainingslast \u00fcber mehrere GPUs verteilt wird. Diese Funktion ist \u00fcber sowohl die Python-API als auch die Befehlszeilenschnittstelle verf\u00fcgbar. Um das Multi-GPU-Training zu aktivieren, geben Sie die GPU-Ger\u00e4te-IDs an, die Sie verwenden m\u00f6chten.</p> <p>Beispiel f\u00fcr Multi-GPU-Training</p> <p>Um mit 2 GPUs zu trainieren, verwenden Sie die folgenden Befehle f\u00fcr CUDA-Ger\u00e4te 0 und 1. Erweitern Sie dies bei Bedarf auf zus\u00e4tzliche GPUs.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\n\n# Trainieren Sie das Modell mit 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Beginnen Sie das Training von einem vortrainierten *.pt Modell unter Verwendung der GPUs 0 und 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#apple-m1-und-m2-mps-training","title":"Apple M1- und M2-MPS-Training","text":"<p>Mit der Unterst\u00fctzung f\u00fcr Apple M1- und M2-Chips, die in den Ultralytics YOLO-Modellen integriert ist, ist es jetzt m\u00f6glich, Ihre Modelle auf Ger\u00e4ten zu trainieren, die das leistungsstarke Metal Performance Shaders (MPS)-Framework nutzen. MPS bietet eine leistungsstarke Methode zur Ausf\u00fchrung von Berechnungs- und Bildverarbeitungsaufgaben auf Apples benutzerdefinierten Siliziumchips.</p> <p>Um das Training auf Apple M1- und M2-Chips zu erm\u00f6glichen, sollten Sie 'mps' als Ihr Ger\u00e4t angeben, wenn Sie den Trainingsprozess starten. Unten ist ein Beispiel, wie Sie dies in Python und \u00fcber die Befehlszeile tun k\u00f6nnten:</p> <p>MPS-Training Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Laden Sie ein Modell\nmodel = YOLO('yolov8n.pt')  # laden Sie ein vortrainiertes Modell (empfohlen f\u00fcr das Training)\n\n# Trainieren Sie das Modell mit 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Beginnen Sie das Training von einem vortrainierten *.pt Modell unter Verwendung der GPUs 0 und 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Indem sie die Rechenleistung der M1/M2-Chips nutzen, erm\u00f6glicht dies eine effizientere Verarbeitung der Trainingsaufgaben. F\u00fcr detailliertere Anleitungen und fortgeschrittene Konfigurationsoptionen beziehen Sie sich bitte auf die PyTorch MPS-Dokumentation.</p>"},{"location":"modes/train/#protokollierung","title":"Protokollierung","text":"<p>Beim Training eines YOLOv8-Modells kann es wertvoll sein, die Leistung des Modells im Laufe der Zeit zu verfolgen. Hier kommt die Protokollierung ins Spiel. Ultralytics' YOLO unterst\u00fctzt drei Typen von Loggern - Comet, ClearML und TensorBoard.</p> <p>Um einen Logger zu verwenden, w\u00e4hlen Sie ihn aus dem Dropdown-Men\u00fc im obigen Codeausschnitt aus und f\u00fchren ihn aus. Der ausgew\u00e4hlte Logger wird installiert und initialisiert.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet ist eine Plattform, die Datenwissenschaftlern und Entwicklern erlaubt, Experimente und Modelle zu verfolgen, zu vergleichen, zu erkl\u00e4ren und zu optimieren. Es bietet Funktionen wie Echtzeitmetriken, Code-Diffs und das Verfolgen von Hyperparametern.</p> <p>Um Comet zu verwenden:</p> <p>Beispiel</p> Python <pre><code># pip installieren comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Vergessen Sie nicht, sich auf der Comet-Website anzumelden und Ihren API-Schl\u00fcssel zu erhalten. Sie m\u00fcssen diesen zu Ihren Umgebungsvariablen oder Ihrem Skript hinzuf\u00fcgen, um Ihre Experimente zu protokollieren.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML ist eine Open-Source-Plattform, die das Verfolgen von Experimenten automatisiert und hilft, Ressourcen effizient zu teilen. Sie ist darauf ausgelegt, Teams bei der Verwaltung, Ausf\u00fchrung und Reproduktion ihrer ML-Arbeiten effizienter zu unterst\u00fctzen.</p> <p>Um ClearML zu verwenden:</p> <p>Beispiel</p> Python <pre><code># pip installieren clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Nach dem Ausf\u00fchren dieses Skripts m\u00fcssen Sie sich auf dem Browser bei Ihrem ClearML-Konto anmelden und Ihre Sitzung authentifizieren.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard ist ein Visualisierungstoolset f\u00fcr TensorFlow. Es erm\u00f6glicht Ihnen, Ihren TensorFlow-Graphen zu visualisieren, quantitative Metriken \u00fcber die Ausf\u00fchrung Ihres Graphen zu plotten und zus\u00e4tzliche Daten wie Bilder zu zeigen, die durch ihn hindurchgehen.</p> <p>Um TensorBoard in Google Colab zu verwenden:</p> <p>Beispiel</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # ersetzen Sie mit Ihrem 'runs' Verzeichnis\n</code></pre> <p>Um TensorBoard lokal auszuf\u00fchren, f\u00fchren Sie den folgenden Befehl aus und betrachten Sie die Ergebnisse unter http://localhost:6006/.</p> <p>Beispiel</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # ersetzen Sie mit Ihrem 'runs' Verzeichnis\n</code></pre> <p>Dies l\u00e4dt TensorBoard und weist es an, das Verzeichnis zu verwenden, in dem Ihre Trainingsprotokolle gespeichert sind.</p> <p>Nachdem Sie Ihren Logger eingerichtet haben, k\u00f6nnen Sie mit Ihrem Modelltraining fortfahren. Alle Trainingsmetriken werden automatisch in Ihrer gew\u00e4hlten Plattform protokolliert, und Sie k\u00f6nnen auf diese Protokolle zugreifen, um die Leistung Ihres Modells im Laufe der Zeit zu \u00fcberwachen, verschiedene Modelle zu vergleichen und Bereiche f\u00fcr Verbesserungen zu identifizieren.</p>"},{"location":"modes/val/","title":"Modellvalidierung mit Ultralytics YOLO","text":""},{"location":"modes/val/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Die Validierung ist ein kritischer Schritt im Machine-Learning-Prozess, der es Ihnen erm\u00f6glicht, die Qualit\u00e4t Ihrer trainierten Modelle zu bewerten. Der Val-Modus in Ultralytics YOLOv8 bietet eine robuste Suite von Tools und Metriken zur Bewertung der Leistung Ihrer Objekterkennungsmodelle. Dieser Leitfaden dient als umfassende Ressource, um zu verstehen, wie Sie den Val-Modus effektiv nutzen k\u00f6nnen, um sicherzustellen, dass Ihre Modelle sowohl genau als auch zuverl\u00e4ssig sind.</p>"},{"location":"modes/val/#warum-mit-ultralytics-yolo-validieren","title":"Warum mit Ultralytics YOLO validieren?","text":"<p>Hier sind die Vorteile der Verwendung des Val-Modus von YOLOv8:</p> <ul> <li>Pr\u00e4zision: Erhalten Sie genaue Metriken wie mAP50, mAP75 und mAP50-95, um Ihr Modell umfassend zu bewerten.</li> <li>Bequemlichkeit: Nutzen Sie integrierte Funktionen, die Trainingseinstellungen speichern und so den Validierungsprozess vereinfachen.</li> <li>Flexibilit\u00e4t: Validieren Sie Ihr Modell mit den gleichen oder verschiedenen Datens\u00e4tzen und Bildgr\u00f6\u00dfen.</li> <li>Hyperparameter-Tuning: Verwenden Sie Validierungsmetriken, um Ihr Modell f\u00fcr eine bessere Leistung zu optimieren.</li> </ul>"},{"location":"modes/val/#schlusselfunktionen-des-val-modus","title":"Schl\u00fcsselfunktionen des Val-Modus","text":"<p>Dies sind die bemerkenswerten Funktionen, die der Val-Modus von YOLOv8 bietet:</p> <ul> <li>Automatisierte Einstellungen: Modelle erinnern sich an ihre Trainingskonfigurationen f\u00fcr eine unkomplizierte Validierung.</li> <li>Unterst\u00fctzung mehrerer Metriken: Bewerten Sie Ihr Modell anhand einer Reihe von Genauigkeitsmetriken.</li> <li>CLI- und Python-API: W\u00e4hlen Sie zwischen Befehlszeilenschnittstelle oder Python-API basierend auf Ihrer Pr\u00e4ferenz f\u00fcr die Validierung.</li> <li>Datenkompatibilit\u00e4t: Funktioniert nahtlos mit Datens\u00e4tzen, die w\u00e4hrend der Trainingsphase sowie mit benutzerdefinierten Datens\u00e4tzen verwendet wurden.</li> </ul> <p>Tipp</p> <ul> <li>YOLOv8-Modelle speichern automatisch ihre Trainingseinstellungen, sodass Sie ein Modell mit der gleichen Bildgr\u00f6\u00dfe und dem urspr\u00fcnglichen Datensatz leicht validieren k\u00f6nnen, indem Sie einfach <code>yolo val model=yolov8n.pt</code> oder <code>model('yolov8n.pt').val()</code> ausf\u00fchren</li> </ul>"},{"location":"modes/val/#beispielverwendung","title":"Beispielverwendung","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-Modells auf dem COCO128-Datensatz. Es muss kein Argument \u00fcbergeben werden, da das <code>model</code> seine Trainings-<code>data</code> und Argumente als Modellattribute speichert. Siehe Abschnitt \u201eArgumente\u201c unten f\u00fcr eine vollst\u00e4ndige Liste der Exportargumente.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('path/to/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell validieren\nmetrics = model.val()  # keine Argumente ben\u00f6tigt, Datensatz und Einstellungen gespeichert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # eine Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # offizielles Modell validieren\nyolo detect val model=path/to/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"modes/val/#argumente","title":"Argumente","text":"<p>Validierungseinstellungen f\u00fcr YOLO-Modelle beziehen sich auf verschiedene Hyperparameter und Konfigurationen, die verwendet werden, um die Leistung des Modells an einem Validierungsdatensatz zu bewerten. Diese Einstellungen k\u00f6nnen die Leistung, Geschwindigkeit und Genauigkeit des Modells beeinflussen. Einige g\u00e4ngige YOLO-Validierungseinstellungen umfassen die Batch-Gr\u00f6\u00dfe, die H\u00e4ufigkeit der Validierung w\u00e4hrend des Trainings und die Metriken zur Bewertung der Modellleistung. Andere Faktoren, die den Validierungsprozess beeinflussen k\u00f6nnen, sind die Gr\u00f6\u00dfe und Zusammensetzung des Validierungsdatensatzes und die spezifische Aufgabe, f\u00fcr die das Modell verwendet wird. Es ist wichtig, diese Einstellungen sorgf\u00e4ltig abzustimmen und zu experimentieren, um sicherzustellen, dass das Modell auf dem Validierungsdatensatz gut funktioniert sowie \u00dcberanpassung zu erkennen und zu verhindern.</p> Key Value Beschreibung <code>data</code> <code>None</code> Pfad zur Datendatei, z.B. coco128.yaml <code>imgsz</code> <code>640</code> Gr\u00f6\u00dfe der Eingabebilder als ganzzahlige Zahl <code>batch</code> <code>16</code> Anzahl der Bilder pro Batch (-1 f\u00fcr AutoBatch) <code>save_json</code> <code>False</code> Ergebnisse in JSON-Datei speichern <code>save_hybrid</code> <code>False</code> hybride Version der Labels speichern (Labels + zus\u00e4tzliche Vorhersagen) <code>conf</code> <code>0.001</code> Objekterkennungsschwelle f\u00fcr Zuversichtlichkeit <code>iou</code> <code>0.6</code> Schwellenwert f\u00fcr IoU (Intersection over Union) f\u00fcr NMS <code>max_det</code> <code>300</code> maximale Anzahl an Vorhersagen pro Bild <code>half</code> <code>True</code> Halbpr\u00e4zision verwenden (FP16) <code>device</code> <code>None</code> Ger\u00e4t zur Ausf\u00fchrung, z.B. CUDA device=0/1/2/3 oder device=cpu <code>dnn</code> <code>False</code> OpenCV DNN f\u00fcr ONNX-Inf erenz nutzen <code>plots</code> <code>False</code> Diagramme w\u00e4hrend des Trainings anzeigen <code>rect</code> <code>False</code> rechteckige Validierung mit jeder Batch-Charge f\u00fcr minimale Polsterung <code>split</code> <code>val</code> Zu verwendende Daten-Teilmenge f\u00fcr Validierung, z.B. 'val', 'test' oder 'train'"},{"location":"tasks/","title":"Ultralytics YOLOv8 Aufgaben","text":"<p>YOLOv8 ist ein KI-Framework, das mehrere Aufgaben im Bereich der Computer Vision unterst\u00fctzt. Das Framework kann f\u00fcr die Erkennung, Segmentierung, Klassifizierung und die Pose-Sch\u00e4tzung verwendet werden. Jede dieser Aufgaben hat ein unterschiedliches Ziel und Anwendungsgebiete.</p> <p>Hinweis</p> <p>\ud83d\udea7 Unsere mehrsprachigen Dokumentation befindet sich derzeit im Aufbau und wir arbeiten hart daran, sie zu verbessern. Danke f\u00fcr Ihre Geduld! \ud83d\ude4f</p> <p> Schauen Sie zu: Entdecken Sie Ultralytics YOLO Aufgaben: Objekterkennung, Segmentierung, Verfolgung und Pose-Sch\u00e4tzung. </p>"},{"location":"tasks/#erkennung","title":"Erkennung","text":"<p>Erkennung ist die prim\u00e4re von YOLOv8 unterst\u00fctzte Aufgabe. Sie beinhaltet das Erkennen von Objekten in einem Bild oder Videobild und das Zeichnen von Rahmen um sie herum. Die erkannten Objekte werden anhand ihrer Merkmale in verschiedene Kategorien klassifiziert. YOLOv8 kann mehrere Objekte in einem einzelnen Bild oder Videobild mit hoher Genauigkeit und Geschwindigkeit erkennen.</p> <p>Beispiele f\u00fcr Erkennung</p>"},{"location":"tasks/#segmentierung","title":"Segmentierung","text":"<p>Segmentierung ist eine Aufgabe, die das Aufteilen eines Bildes in unterschiedliche Regionen anhand des Bildinhalts beinhaltet. Jeder Region wird basierend auf ihrem Inhalt eine Markierung zugewiesen. Diese Aufgabe ist n\u00fctzlich in Anwendungen wie der Bildsegmentierung und medizinischen Bildgebung. YOLOv8 verwendet eine Variante der U-Net-Architektur, um die Segmentierung durchzuf\u00fchren.</p> <p>Beispiele f\u00fcr Segmentierung</p>"},{"location":"tasks/#klassifizierung","title":"Klassifizierung","text":"<p>Klassifizierung ist eine Aufgabe, die das Einordnen eines Bildes in verschiedene Kategorien umfasst. YOLOv8 kann genutzt werden, um Bilder anhand ihres Inhalts zu klassifizieren. Es verwendet eine Variante der EfficientNet-Architektur, um die Klassifizierung durchzuf\u00fchren.</p> <p>Beispiele f\u00fcr Klassifizierung</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>Die Pose-/Keypoint-Erkennung ist eine Aufgabe, die das Erkennen von spezifischen Punkten in einem Bild oder Videobild beinhaltet. Diese Punkte werden als Keypoints bezeichnet und werden zur Bewegungsverfolgung oder Pose-Sch\u00e4tzung verwendet. YOLOv8 kann Keypoints in einem Bild oder Videobild mit hoher Genauigkeit und Geschwindigkeit erkennen.</p> <p>Beispiele f\u00fcr Posen</p>"},{"location":"tasks/#fazit","title":"Fazit","text":"<p>YOLOv8 unterst\u00fctzt mehrere Aufgaben, einschlie\u00dflich Erkennung, Segmentierung, Klassifizierung und Keypoint-Erkennung. Jede dieser Aufgaben hat unterschiedliche Ziele und Anwendungsgebiete. Durch das Verst\u00e4ndnis der Unterschiede zwischen diesen Aufgaben k\u00f6nnen Sie die geeignete Aufgabe f\u00fcr Ihre Anwendung im Bereich der Computer Vision ausw\u00e4hlen.</p>"},{"location":"tasks/classify/","title":"Bildklassifizierung","text":"<p>Bildklassifizierung ist die einfachste der drei Aufgaben und besteht darin, ein ganzes Bild in eine von einem Satz vordefinierter Klassen zu klassifizieren.</p> <p>Die Ausgabe eines Bildklassifizierers ist ein einzelnes Klassenlabel und eine Vertrauenspunktzahl. Bildklassifizierung ist n\u00fctzlich, wenn Sie nur wissen m\u00fcssen, zu welcher Klasse ein Bild geh\u00f6rt, und nicht wissen m\u00fcssen, wo sich Objekte dieser Klasse befinden oder wie ihre genaue Form ist.</p> <p>Tipp</p> <p>YOLOv8 Classify-Modelle verwenden den Suffix <code>-cls</code>, z.B. <code>yolov8n-cls.pt</code> und sind auf ImageNet vortrainiert.</p>"},{"location":"tasks/classify/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Classify-Modelle gezeigt. Detect-, Segment- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Classify-Modelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle werden automatisch vom neuesten Ultralytics-Release beim ersten Gebrauch heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) Genauigkeit<sup>top1 Genauigkeit<sup>top5 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) bei 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Genauigkeit-Werte sind Modellgenauigkeiten auf dem ImageNet-Datensatz Validierungsset.   Zur Reproduktion <code>yolo val classify data=pfad/zu/ImageNet device=0 verwenden</code></li> <li>Geschwindigkeit Durchschnitt \u00fcber ImageNet-Validierungsbilder mit einer Amazon EC2 P4d-Instanz.   Zur Reproduktion <code>yolo val classify data=pfad/zu/ImageNet batch=1 device=0|cpu verwenden</code></li> </ul>"},{"location":"tasks/classify/#trainieren","title":"Trainieren","text":"<p>Trainieren Sie das YOLOv8n-cls-Modell auf dem MNIST160-Datensatz f\u00fcr 100 Epochen bei Bildgr\u00f6\u00dfe 64. Eine vollst\u00e4ndige Liste der verf\u00fcgbaren Argumente finden Sie auf der Seite Konfiguration.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.yaml')  # ein neues Modell aus YAML erstellen\nmodel = YOLO('yolov8n-cls.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # aus YAML erstellen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Ein neues Modell aus YAML erstellen und das Training von Grund auf starten\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Das Training von einem vortrainierten *.pt Modell starten\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Ein neues Modell aus YAML erstellen, vortrainierte Gewichte \u00fcbertragen und das Training starten\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#datenformat","title":"Datenformat","text":"<p>Das Datenformat f\u00fcr YOLO-Klassifizierungsdatens\u00e4tze finden Sie im Detail im Datenleitfaden.</p>"},{"location":"tasks/classify/#validieren","title":"Validieren","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-cls-Modells auf dem MNIST160-Datensatz. Kein Argument muss \u00fcbergeben werden, da das <code>modell</code> seine Trainings<code>daten</code> und Argumente als Modellattribute beh\u00e4lt.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # keine Argumente ben\u00f6tigt, Datensatz und Einstellungen gespeichert\nmetrics.top1   # top1 Genauigkeit\nmetrics.top5   # top5 Genauigkeit\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # ein offizielles Modell validieren\nyolo classify val model=pfad/zu/best.pt  # ein benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/classify/#vorhersagen","title":"Vorhersagen","text":"<p>Verwenden Sie ein trainiertes YOLOv8n-cls-Modell, um Vorhersagen auf Bildern durchzuf\u00fchren.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell vorhersagen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # mit offiziellem Modell vorhersagen\nyolo classify predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # mit benutzerdefiniertem Modell vorhersagen\n</code></pre> <p>Vollst\u00e4ndige Details zum <code>predict</code>-Modus finden Sie auf der Seite Vorhersage.</p>"},{"location":"tasks/classify/#exportieren","title":"Exportieren","text":"<p>Exportieren Sie ein YOLOv8n-cls-Modell in ein anderes Format wie ONNX, CoreML usw.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Ein Modell laden\nmodel = YOLO('yolov8n-cls.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8-cls Exportformate stehen in der folgenden Tabelle. Sie k\u00f6nnen direkt auf exportierten Modellen vorhersagen oder validieren, d.h. <code>yolo predict model=yolov8n-cls.onnx</code>. Nutzungsexempel werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vollst\u00e4ndige Details zum <code>export</code> finden Sie auf der Seite Export.</p>"},{"location":"tasks/detect/","title":"Objekterkennung","text":"<p>Objekterkennung ist eine Aufgabe, die das Identifizieren der Position und Klasse von Objekten in einem Bild oder Videostream umfasst.</p> <p>Die Ausgabe eines Objekterkenners ist eine Menge von Begrenzungsrahmen, die die Objekte im Bild umschlie\u00dfen, zusammen mit Klassenbezeichnungen und Vertrauenswerten f\u00fcr jedes Feld. Objekterkennung ist eine gute Wahl, wenn Sie Objekte von Interesse in einer Szene identifizieren m\u00fcssen, aber nicht genau wissen m\u00fcssen, wo das Objekt ist oder wie seine genaue Form ist.</p> <p> Sehen Sie: Objekterkennung mit vortrainiertem Ultralytics YOLOv8 Modell. </p> <p>Tipp</p> <p>YOLOv8 Detect Modelle sind die Standard YOLOv8 Modelle, zum Beispiel <code>yolov8n.pt</code>, und sind vortrainiert auf dem COCO-Datensatz.</p>"},{"location":"tasks/detect/#modelle","title":"Modelle","text":"<p>Hier werden die vortrainierten YOLOv8 Detect Modelle gezeigt. Detect, Segment und Pose Modelle sind vortrainiert auf dem COCO-Datensatz, w\u00e4hrend die Classify Modelle vortrainiert sind auf dem ImageNet-Datensatz.</p> <p>Modelle werden automatisch von der neuesten Ultralytics Ver\u00f6ffentlichung bei Erstbenutzung heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>val50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>mAP<sup>val</sup> Werte sind f\u00fcr Single-Modell Single-Scale auf dem COCO val2017 Datensatz.   Reproduzieren mit <code>yolo val detect data=coco.yaml device=0</code></li> <li>Geschwindigkeit gemittelt \u00fcber COCO Val Bilder mit einer Amazon EC2 P4d-Instanz.   Reproduzieren mit <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#training","title":"Training","text":"<p>YOLOv8n auf dem COCO128-Datensatz f\u00fcr 100 Epochen bei Bildgr\u00f6\u00dfe 640 trainieren. F\u00fcr eine vollst\u00e4ndige Liste verf\u00fcgbarer Argumente siehe die Konfigurationsseite.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.yaml')  # ein neues Modell aus YAML aufbauen\nmodel = YOLO('yolov8n.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr Training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # aus YAML aufbauen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML aufbauen und Training von Grund auf starten\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Training von einem vortrainierten *.pt Modell starten\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML aufbauen, vortrainierte Gewichte \u00fcbertragen und Training starten\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#datenformat","title":"Datenformat","text":"<p>Das Datenformat f\u00fcr YOLO-Erkennungsdatens\u00e4tze finden Sie detailliert im Dataset Guide. Um Ihren vorhandenen Datensatz von anderen Formaten (wie COCO etc.) in das YOLO-Format zu konvertieren, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/detect/#validierung","title":"Validierung","text":"<p>Genauigkeit des trainierten YOLOv8n-Modells auf dem COCO128-Datensatz validieren. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>modell</code> seine Trainingsdaten und Argumente als Modellattribute beibeh\u00e4lt.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # keine Argumente n\u00f6tig, Datensatz und Einstellungen erinnert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # eine Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # offizielles Modell validieren\nyolo detect val model=pfad/zum/besten.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/detect/#vorhersage","title":"Vorhersage","text":"<p>Ein trainiertes YOLOv8n-Modell verwenden, um Vorhersagen auf Bildern durchzuf\u00fchren.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell vorhersagen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit offiziellem Modell\nyolo detect predict model=pfad/zum/besten.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit benutzerdefiniertem Modell\n</code></pre> <p>Volle Details \u00fcber den <code>predict</code>-Modus finden Sie auf der Predict-Seite.</p>"},{"location":"tasks/detect/#export","title":"Export","text":"<p>Ein YOLOv8n-Modell in ein anderes Format wie ONNX, CoreML usw. exportieren.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zum/besten.pt')  # ein benutzerdefiniert trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zum/besten.pt format=onnx  # benutzerdefiniert trainiertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8 Exportformate sind in der untenstehenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf den exportierten Modellen Vorhersagen treffen oder diese validieren, zum Beispiel <code>yolo predict model=yolov8n.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Volle Details zum <code>export</code> finden Sie auf der Export-Seite.</p>"},{"location":"tasks/pose/","title":"Pose-Sch\u00e4tzung","text":"<p>Die Pose-Sch\u00e4tzung ist eine Aufgabe, die das Identifizieren der Lage spezifischer Punkte in einem Bild beinhaltet, die normalerweise als Schl\u00fcsselpunkte bezeichnet werden. Die Schl\u00fcsselpunkte k\u00f6nnen verschiedene Teile des Objekts wie Gelenke, Landmarken oder andere charakteristische Merkmale repr\u00e4sentieren. Die Positionen der Schl\u00fcsselpunkte sind \u00fcblicherweise als eine Gruppe von 2D <code>[x, y]</code> oder 3D <code>[x, y, sichtbar]</code> Koordinaten dargestellt.</p> <p>Das Ergebnis eines Pose-Sch\u00e4tzungsmodells ist eine Gruppe von Punkten, die die Schl\u00fcsselpunkte auf einem Objekt im Bild darstellen, normalerweise zusammen mit den Konfidenzwerten f\u00fcr jeden Punkt. Die Pose-Sch\u00e4tzung eignet sich gut, wenn Sie spezifische Teile eines Objekts in einer Szene identifizieren m\u00fcssen und deren Lage zueinander.</p> <p> Ansehen: Pose-Sch\u00e4tzung mit Ultralytics YOLOv8. </p> <p>Tipp</p> <p>YOLOv8 pose-Modelle verwenden den Suffix <code>-pose</code>, z. B. <code>yolov8n-pose.pt</code>. Diese Modelle sind auf dem COCO-Schl\u00fcsselpunkte-Datensatz trainiert und f\u00fcr eine Vielzahl von Pose-Sch\u00e4tzungsaufgaben geeignet.</p>"},{"location":"tasks/pose/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Pose-Modelle gezeigt. Erkennungs-, Segmentierungs- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Klassifizierungsmodelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle werden automatisch aus der neuesten Ultralytics-Ver\u00f6ffentlichung bei erstmaliger Verwendung heruntergeladen.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>pose50-95 mAP<sup>pose50 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50,4 80,1 131,8 1,18 3,3 9,2 YOLOv8s-pose 640 60,0 86,2 233,2 1,42 11,6 30,2 YOLOv8m-pose 640 65,0 88,8 456,3 2,00 26,4 81,0 YOLOv8l-pose 640 67,6 90,0 784,5 2,59 44,4 168,6 YOLOv8x-pose 640 69,2 90,2 1607,1 3,73 69,4 263,2 YOLOv8x-pose-p6 1280 71,6 91,2 4088,7 10,04 99,1 1066,4 <ul> <li>mAP<sup>val</sup> Werte gelten f\u00fcr ein einzelnes Modell mit einfacher Skala auf dem COCO Keypoints val2017-Datensatz.   Zu reproduzieren mit <code>yolo val pose data=coco-pose.yaml device=0</code>.</li> <li>Geschwindigkeit gemittelt \u00fcber COCO-Validierungsbilder mit einer Amazon EC2 P4d-Instanz.   Zu reproduzieren mit <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code>.</li> </ul>"},{"location":"tasks/pose/#trainieren","title":"Trainieren","text":"<p>Trainieren Sie ein YOLOv8-Pose-Modell auf dem COCO128-Pose-Datensatz.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.yaml')  # ein neues Modell aus YAML bauen\nmodel = YOLO('yolov8n-pose.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # aus YAML bauen und Gewichte \u00fcbertragen\n\n# Modell trainieren\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML bauen und das Training von Grund auf starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Training von einem vortrainierten *.pt Modell starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML bauen, vortrainierte Gewichte \u00fcbertragen und das Training starten\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#datensatzformat","title":"Datensatzformat","text":"<p>Das YOLO-Pose-Datensatzformat finden Sie detailliert im Datensatz-Leitfaden. Um Ihren bestehenden Datensatz aus anderen Formaten (wie COCO usw.) in das YOLO-Format zu konvertieren, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/pose/#validieren","title":"Validieren","text":"<p>Die Genauigkeit des trainierten YOLOv8n-Pose-Modells auf dem COCO128-Pose-Datensatz validieren. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>Modell</code> seine Trainings<code>daten</code> und Argumente als Modellattribute beibeh\u00e4lt.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell validieren\nmetrics = model.val()  # keine Argumente n\u00f6tig, Datensatz und Einstellungen sind gespeichert\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # Liste enth\u00e4lt map50-95 jeder Kategorie\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # offizielles Modell validieren\nyolo pose val model=pfad/zu/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/pose/#vorhersagen","title":"Vorhersagen","text":"<p>Ein trainiertes YOLOv8n-Pose-Modell verwenden, um Vorhersagen auf Bildern zu machen.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Mit dem Modell Vorhersagen machen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild machen\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit dem offiziellen Modell machen\nyolo pose predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit dem benutzerdefinierten Modell machen\n</code></pre> <p>Vollst\u00e4ndige <code>predict</code>-Modusdetails finden Sie auf der Vorhersage-Seite.</p>"},{"location":"tasks/pose/#exportieren","title":"Exportieren","text":"<p>Ein YOLOv8n-Pose-Modell in ein anderes Format wie ONNX, CoreML usw. exportieren.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-pose.pt')  # ein offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # ein benutzerdefiniertes Modell laden\n\n# Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes Modell exportieren\n</code></pre> <p>Verf\u00fcgbare YOLOv8-Pose-Exportformate sind in der folgenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf exportierten Modellen vorhersagen oder validieren, z. B. <code>yolo predict model=yolov8n-pose.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach Abschluss des Exports angezeigt.</p> Format <code>format</code> Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimieren</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> <p>Vollst\u00e4ndige <code>export</code>-Details finden Sie auf der Export-Seite.</p>"},{"location":"tasks/segment/","title":"Instanzsegmentierung","text":"<p>Instanzsegmentierung geht einen Schritt weiter als die Objekterkennung und beinhaltet die Identifizierung einzelner Objekte in einem Bild und deren Abtrennung vom Rest des Bildes.</p> <p>Das Ergebnis eines Instanzsegmentierungsmodells ist eine Reihe von Masken oder Konturen, die jedes Objekt im Bild umrei\u00dfen, zusammen mit Klassenbezeichnungen und Vertrauensscores f\u00fcr jedes Objekt. Instanzsegmentierung ist n\u00fctzlich, wenn man nicht nur wissen muss, wo sich Objekte in einem Bild befinden, sondern auch, welche genaue Form sie haben.</p> <p> Schauen Sie: F\u00fchren Sie Segmentierung mit dem vortrainierten Ultralytics YOLOv8 Modell in Python aus. </p> <p>Tipp</p> <p>YOLOv8 Segment-Modelle verwenden das Suffix <code>-seg</code>, d.h. <code>yolov8n-seg.pt</code> und sind auf dem COCO-Datensatz vortrainiert.</p>"},{"location":"tasks/segment/#modelle","title":"Modelle","text":"<p>Hier werden vortrainierte YOLOv8 Segment-Modelle gezeigt. Detect-, Segment- und Pose-Modelle sind auf dem COCO-Datensatz vortrainiert, w\u00e4hrend Classify-Modelle auf dem ImageNet-Datensatz vortrainiert sind.</p> <p>Modelle laden sich automatisch von der neuesten Ultralytics Ver\u00f6ffentlichung beim ersten Gebrauch herunter.</p> Modell Gr\u00f6\u00dfe<sup>(Pixel) mAP<sup>Kasten50-95 mAP<sup>Masken50-95 Geschwindigkeit<sup>CPU ONNX(ms) Geschwindigkeit<sup>A100 TensorRT(ms) Parameter<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Die mAP<sup>val</sup>-Werte sind f\u00fcr ein einzelnes Modell, einzelne Skala auf dem COCO val2017-Datensatz.   Zum Reproduzieren nutzen Sie <code>yolo val segment data=coco.yaml device=0</code></li> <li>Die Geschwindigkeit ist \u00fcber die COCO-Validierungsbilder gemittelt und verwendet eine Amazon EC2 P4d-Instanz.   Zum Reproduzieren <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#training","title":"Training","text":"<p>Trainieren Sie YOLOv8n-seg auf dem COCO128-seg-Datensatz f\u00fcr 100 Epochen mit einer Bildgr\u00f6\u00dfe von 640. Eine vollst\u00e4ndige Liste der verf\u00fcgbaren Argumente finden Sie auf der Seite Konfiguration.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.yaml')  # ein neues Modell aus YAML erstellen\nmodel = YOLO('yolov8n-seg.pt')  # ein vortrainiertes Modell laden (empfohlen f\u00fcr das Training)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # aus YAML erstellen und Gewichte \u00fcbertragen\n\n# Das Modell trainieren\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Ein neues Modell aus YAML erstellen und das Training von vorne beginnen\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Das Training von einem vortrainierten *.pt Modell aus starten\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Ein neues Modell aus YAML erstellen, vortrainierte Gewichte darauf \u00fcbertragen und das Training beginnen\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#datenformat","title":"Datenformat","text":"<p>Das YOLO Segmentierungsdatenformat finden Sie detailliert im Dataset Guide. Um Ihre vorhandenen Daten aus anderen Formaten (wie COCO usw.) in das YOLO-Format umzuwandeln, verwenden Sie bitte das JSON2YOLO-Tool von Ultralytics.</p>"},{"location":"tasks/segment/#val","title":"Val","text":"<p>Validieren Sie die Genauigkeit des trainierten YOLOv8n-seg-Modells auf dem COCO128-seg-Datensatz. Es m\u00fcssen keine Argumente \u00fcbergeben werden, da das <code>Modell</code> seine Trainingsdaten und -argumente als Modellattribute beh\u00e4lt.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes Modell laden\n\n# Das Modell validieren\nmetrics = model.val()  # Keine Argumente erforderlich, Datensatz und Einstellungen werden behalten\nmetrics.box.map    # mAP50-95(B)\nmetrics.box.map50  # mAP50(B)\nmetrics.box.map75  # mAP75(B)\nmetrics.box.maps   # eine Liste enth\u00e4lt mAP50-95(B) f\u00fcr jede Kategorie\nmetrics.seg.map    # mAP50-95(M)\nmetrics.seg.map50  # mAP50(M)\nmetrics.seg.map75  # mAP75(M)\nmetrics.seg.maps   # eine Liste enth\u00e4lt mAP50-95(M) f\u00fcr jede Kategorie\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # offizielles Modell validieren\nyolo segment val model=pfad/zu/best.pt  # benutzerdefiniertes Modell validieren\n</code></pre>"},{"location":"tasks/segment/#predict","title":"Predict","text":"<p>Verwenden Sie ein trainiertes YOLOv8n-seg-Modell f\u00fcr Vorhersagen auf Bildern.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes Modell laden\n\n# Mit dem Modell Vorhersagen treffen\nresults = model('https://ultralytics.com/images/bus.jpg')  # Vorhersage auf einem Bild\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit offiziellem Modell treffen\nyolo segment predict model=pfad/zu/best.pt source='https://ultralytics.com/images/bus.jpg'  # Vorhersage mit benutzerdefiniertem Modell treffen\n</code></pre> <p>Die vollst\u00e4ndigen Details zum <code>predict</code>-Modus finden Sie auf der Seite Predict.</p>"},{"location":"tasks/segment/#export","title":"Export","text":"<p>Exportieren Sie ein YOLOv8n-seg-Modell in ein anderes Format wie ONNX, CoreML usw.</p> <p>Beispiel</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Modell laden\nmodel = YOLO('yolov8n-seg.pt')  # offizielles Modell laden\nmodel = YOLO('pfad/zu/best.pt')  # benutzerdefiniertes trainiertes Modell laden\n\n# Das Modell exportieren\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # offizielles Modell exportieren\nyolo export model=pfad/zu/best.pt format=onnx  # benutzerdefiniertes trainiertes Modell exportieren\n</code></pre> <p>Die verf\u00fcgbaren YOLOv8-seg-Exportformate sind in der folgenden Tabelle aufgef\u00fchrt. Sie k\u00f6nnen direkt auf exportierten Modellen Vorhersagen treffen oder sie validieren, z.B. <code>yolo predict model=yolov8n-seg.onnx</code>. Verwendungsbeispiele werden f\u00fcr Ihr Modell nach dem Export angezeigt.</p> Format <code>format</code>-Argument Modell Metadaten Argumente PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimieren</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>dynamisch</code>, <code>vereinfachen</code>, <code>Arbeitsspeicher</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>halb</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>halb</code> <p>Die vollst\u00e4ndigen Details zum <code>export</code> finden Sie auf der Seite Export.</p>"}]}