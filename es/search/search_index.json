{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Inicio","text":"<p>Presentamos Ultralytics YOLOv8, la \u00faltima versi\u00f3n del aclamado modelo para detecci\u00f3n de objetos y segmentaci\u00f3n de im\u00e1genes en tiempo real. YOLOv8 est\u00e1 construido sobre avances de vanguardia en aprendizaje profundo y visi\u00f3n por computadora, ofreciendo un rendimiento sin paralelo en t\u00e9rminos de velocidad y precisi\u00f3n. Su dise\u00f1o simplificado lo hace adecuado para varias aplicaciones y f\u00e1cilmente adaptable a diferentes plataformas de hardware, desde dispositivos de borde hasta API en la nube.</p> <p>Explore los documentos de YOLOv8, un recurso integral dise\u00f1ado para ayudarle a comprender y utilizar sus caracter\u00edsticas y capacidades. Independientemente de que sea un practicante experimentado en aprendizaje autom\u00e1tico o nuevo en el campo, este centro tiene como objetivo maximizar el potencial de YOLOv8 en sus proyectos.</p> <p>Note</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n en varios idiomas est\u00e1 actualmente en construcci\u00f3n y estamos trabajando duro para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p>"},{"location":"#donde-empezar","title":"D\u00f3nde empezar","text":"<ul> <li>Instalar <code>ultralytics</code> con pip y comenzar a funcionar en minutos \u00a0  Comenzar</li> <li>Predecir nuevas im\u00e1genes y videos con YOLOv8 \u00a0  Predecir en Im\u00e1genes</li> <li>Entrenar un nuevo modelo YOLOv8 en su propio conjunto de datos personalizado \u00a0  Entrenar un Modelo</li> <li>Explorar tareas de YOLOv8 como segmentar, clasificar, posar y seguir \u00a0  Explorar Tareas</li> </ul> <p> Ver: C\u00f3mo entrenar un modelo YOLOv8 en Su Conjunto de Datos Personalizado en Google Colab. </p>"},{"location":"#yolo-una-breve-historia","title":"YOLO: Una Breve Historia","text":"<p>YOLO (You Only Look Once), un modelo popular de detecci\u00f3n de objetos y segmentaci\u00f3n de im\u00e1genes, fue desarrollado por Joseph Redmon y Ali Farhadi en la Universidad de Washington. Lanzado en 2015, YOLO r\u00e1pidamente gan\u00f3 popularidad por su alta velocidad y precisi\u00f3n.</p> <ul> <li>YOLOv2, lanzado en 2016, mejor\u00f3 el modelo original incorporando normalizaci\u00f3n por lotes, cajas ancla y cl\u00fasteres de dimensiones.</li> <li>YOLOv3, lanzado en 2018, mejor\u00f3 a\u00fan m\u00e1s el rendimiento del modelo usando una red dorsal m\u00e1s eficiente, m\u00faltiples anclas y agrupaci\u00f3n piramidal espacial.</li> <li>YOLOv4 fue lanzado en 2020, introduciendo innovaciones como la ampliaci\u00f3n de datos del mosaico, un nuevo cabezal de detecci\u00f3n sin ancla y una nueva funci\u00f3n de p\u00e9rdida.</li> <li>YOLOv5 mejor\u00f3 a\u00fan m\u00e1s el rendimiento del modelo y agreg\u00f3 nuevas caracter\u00edsticas como la optimizaci\u00f3n de hiperpar\u00e1metros, seguimiento de experimentos integrados y exportaci\u00f3n autom\u00e1tica a formatos de exportaci\u00f3n populares.</li> <li>YOLOv6 fue publicado en c\u00f3digo abierto por Meituan en 2022 y se utiliza en muchos de los robots de entrega aut\u00f3nomos de la empresa.</li> <li>YOLOv7 a\u00f1adi\u00f3 tareas adicionales como la estimaci\u00f3n de posturas en el conjunto de datos COCO keypoints.</li> <li>YOLOv8 es la \u00faltima versi\u00f3n de YOLO de Ultralytics. Como un modelo de vanguardia y del estado del arte (SOTA), YOLOv8 se basa en el \u00e9xito de las versiones anteriores, introduciendo nuevas caracter\u00edsticas y mejoras para obtener un rendimiento mejorado, flexibilidad y eficiencia. YOLOv8 soporta una gama completa de tareas de IA de visi\u00f3n, incluyendo detecci\u00f3n, segmentaci\u00f3n, estimaci\u00f3n de pose, seguimiento y clasificaci\u00f3n. Esta versatilidad permite a los usuarios aprovechar las capacidades de YOLOv8 en una amplia gama de aplicaciones y dominios.</li> </ul>"},{"location":"#licencias-de-yolo-como-estan-licenciados-los-yolo-de-ultralytics","title":"Licencias de YOLO: \u00bfC\u00f3mo est\u00e1n licenciados los YOLO de Ultralytics?","text":"<p>Ultralytics ofrece dos opciones de licencia para acomodar casos de uso diversos:</p> <ul> <li>Licencia AGPL-3.0: Esta licencia de c\u00f3digo abierto aprobada por OSI es ideal para estudiantes y entusiastas, promoviendo la colaboraci\u00f3n abierta y el intercambio de conocimiento. Consulte el archivo LICENSE para obtener m\u00e1s detalles.</li> <li>Licencia Empresarial: Dise\u00f1ada para uso comercial, esta licencia permite la integraci\u00f3n sin problemas de software de Ultralytics y modelos de IA en bienes y servicios comerciales, eludiendo los requisitos de c\u00f3digo abierto de AGPL-3.0. Si su escenario implica la incorporaci\u00f3n de nuestras soluciones en una oferta comercial, p\u00f3ngase en contacto a trav\u00e9s de Licencias de Ultralytics.</li> </ul> <p>Nuestra estrategia de licenciamiento est\u00e1 dise\u00f1ada para asegurar que cualquier mejora a nuestros proyectos de c\u00f3digo abierto se devuelva a la comunidad. Mantenemos los principios del c\u00f3digo abierto cerca de nuestros corazones \u2764\ufe0f, y nuestra misi\u00f3n es garantizar que nuestras contribuciones puedan ser utilizadas y ampliadas de formas que sean beneficiosas para todos.</p>"},{"location":"quickstart/","title":"Inicio r\u00e1pido","text":""},{"location":"quickstart/#instalar-ultralytics","title":"Instalar Ultralytics","text":"<p>Ultralytics ofrece varios m\u00e9todos de instalaci\u00f3n incluyendo pip, conda y Docker. Instala YOLOv8 a trav\u00e9s del paquete <code>ultralytics</code> de pip para la \u00faltima versi\u00f3n estable o clonando el repositorio de GitHub de Ultralytics para obtener la versi\u00f3n m\u00e1s actualizada. Docker se puede utilizar para ejecutar el paquete en un contenedor aislado, evitando la instalaci\u00f3n local.</p> <p>Instalar</p> Instalaci\u00f3n con Pip (recomendado)Instalaci\u00f3n con CondaClonar con Git <p>Instala el paquete <code>ultralytics</code> usando pip o actualiza una instalaci\u00f3n existente ejecutando <code>pip install -U ultralytics</code>. Visita el \u00cdndice de Paquetes de Python (PyPI) para m\u00e1s detalles sobre el paquete <code>ultralytics</code>: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Instalar el paquete ultralytics desde PyPI\npip install ultralytics\n</code></pre> <p>Tambi\u00e9n puedes instalar el paquete <code>ultralytics</code> directamente del repositorio en GitHub. Esto puede ser \u00fatil si quieres la \u00faltima versi\u00f3n de desarrollo. Aseg\u00farate de tener la herramienta de l\u00ednea de comandos Git instalada en tu sistema. El comando <code>@main</code> instala la rama <code>main</code> y puede modificarse a otra rama, es decir, <code>@my-branch</code>, o eliminarse por completo para volver por defecto a la rama <code>main</code>.</p> <pre><code># Instalar el paquete ultralytics desde GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda es un gestor de paquetes alternativo a pip que tambi\u00e9n puede utilizarse para la instalaci\u00f3n. Visita Anaconda para m\u00e1s detalles en https://anaconda.org/conda-forge/ultralytics. El repositorio de paquetes de alimentaci\u00f3n de Ultralytics para actualizar el paquete de conda est\u00e1 en https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Instalar el paquete ultralytics usando conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Note</p> <p>Si est\u00e1s instalando en un entorno CUDA, la mejor pr\u00e1ctica es instalar <code>ultralytics</code>, <code>pytorch</code> y <code>pytorch-cuda</code> en el mismo comando para permitir que el gestor de paquetes de conda resuelva cualquier conflicto, o en su defecto instalar <code>pytorch-cuda</code> al final para permitir que sobrescriba el paquete espec\u00edfico de CPU <code>pytorch</code> si es necesario. <pre><code># Instalar todos los paquetes juntos usando conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clona el repositorio <code>ultralytics</code> si est\u00e1s interesado en contribuir al desarrollo o deseas experimentar con el c\u00f3digo fuente m\u00e1s reciente. Despu\u00e9s de clonar, navega al directorio e instala el paquete en modo editable <code>-e</code> usando pip. <pre><code># Clonar el repositorio ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navegar al directorio clonado\ncd ultralytics\n\n# Instalar el paquete en modo editable para desarrollo\npip install -e .\n</code></pre></p> <p>Consulta el archivo requirements.txt de <code>ultralytics</code> para ver una lista de dependencias. Ten en cuenta que todos los ejemplos anteriores instalan todas las dependencias requeridas.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Consejo</p> <p>Los requisitos de PyTorch var\u00edan seg\u00fan el sistema operativo y los requisitos de CUDA, por lo que se recomienda instalar primero PyTorch siguiendo las instrucciones en https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#imagen-docker-de-conda","title":"Imagen Docker de Conda","text":"<p>Las im\u00e1genes Docker de Conda de Ultralytics tambi\u00e9n est\u00e1n disponibles en DockerHub. Estas im\u00e1genes est\u00e1n basadas en Miniconda3 y son una manera simple de comenzar a usar <code>ultralytics</code> en un entorno Conda.</p> <pre><code># Establecer el nombre de la imagen como una variable\nt=ultralytics/ultralytics:latest-conda\n\n# Descargar la \u00faltima imagen de ultralytics de Docker Hub\nsudo docker pull $t\n\n# Ejecutar la imagen de ultralytics en un contenedor con soporte para GPU\nsudo docker run -it --ipc=host --gpus all $t  # todas las GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # especificar GPUs\n</code></pre>"},{"location":"quickstart/#usar-ultralytics-con-cli","title":"Usar Ultralytics con CLI","text":"<p>La interfaz de l\u00ednea de comandos (CLI) de Ultralytics permite el uso de comandos simples de una sola l\u00ednea sin la necesidad de un entorno de Python. La CLI no requiere personalizaci\u00f3n ni c\u00f3digo Python. Puedes simplemente ejecutar todas las tareas desde el terminal con el comando <code>yolo</code>. Consulta la Gu\u00eda de CLI para aprender m\u00e1s sobre el uso de YOLOv8 desde la l\u00ednea de comandos.</p> <p>Example</p> SintaxisEntrenarPredecirValidarExportarEspecial <p>Los comandos <code>yolo</code> de Ultralytics usan la siguiente sintaxis: <pre><code>yolo TAREA MODO ARGUMENTOS\n\nDonde   TAREA (opcional) es uno de [detectar, segmentar, clasificar]\n        MODO (requerido) es uno de [train, val, predict, export, track]\n        ARGUMENTOS (opcionales) son cualquier n\u00famero de pares personalizados 'arg=valor' como 'imgsz=320' que sobrescriben los valores por defecto.\n</code></pre> Ver todos los ARGUMENTOS en la gu\u00eda completa Configuration Guide o con <code>yolo cfg</code></p> <p>Entrenar un modelo de detecci\u00f3n durante 10 \u00e9pocas con una tasa de aprendizaje inicial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predecir un video de YouTube usando un modelo de segmentaci\u00f3n preentrenado con un tama\u00f1o de imagen de 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validar un modelo de detecci\u00f3n preentrenado con un tama\u00f1o de lote de 1 y un tama\u00f1o de imagen de 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportar un modelo de clasificaci\u00f3n YOLOv8n a formato ONNX con un tama\u00f1o de imagen de 224 por 128 (no se requiere TAREA) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Ejecutar comandos especiales para ver la versi\u00f3n, ver configuraciones, ejecutar chequeos y m\u00e1s: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Advertencia</p> <p>Los argumentos deben pasarse como pares <code>arg=valor</code>, separados por un signo igual <code>=</code> y delimitados por espacios <code></code> entre pares. No utilices prefijos de argumentos <code>--</code> ni comas <code>,</code> entre los argumentos.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Gu\u00eda de CLI</p>"},{"location":"quickstart/#usar-ultralytics-con-python","title":"Usar Ultralytics con Python","text":"<p>La interfaz de Python de YOLOv8 permite una integraci\u00f3n perfecta en tus proyectos de Python, facilitando la carga, ejecuci\u00f3n y procesamiento de la salida del modelo. Dise\u00f1ada con sencillez y facilidad de uso en mente, la interfaz de Python permite a los usuarios implementar r\u00e1pidamente la detecci\u00f3n de objetos, segmentaci\u00f3n y clasificaci\u00f3n en sus proyectos. Esto hace que la interfaz de Python de YOLOv8 sea una herramienta invaluable para cualquier persona que busque incorporar estas funcionalidades en sus proyectos de Python.</p> <p>Por ejemplo, los usuarios pueden cargar un modelo, entrenarlo, evaluar su rendimiento en un conjunto de validaci\u00f3n e incluso exportarlo al formato ONNX con solo unas pocas l\u00edneas de c\u00f3digo. Consulta la Gu\u00eda de Python para aprender m\u00e1s sobre el uso de YOLOv8 dentro de tus proyectos de Python.</p> <p>Example</p> <pre><code>from ultralytics import YOLO\n\n# Crear un nuevo modelo YOLO desde cero\nmodel = YOLO('yolov8n.yaml')\n\n# Cargar un modelo YOLO preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.pt')\n\n# Entrenar el modelo usando el conjunto de datos 'coco128.yaml' durante 3 \u00e9pocas\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Evaluar el rendimiento del modelo en el conjunto de validaci\u00f3n\nresults = model.val()\n\n# Realizar detecci\u00f3n de objetos en una imagen usando el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportar el modelo al formato ONNX\nsuccess = model.export(format='onnx')\n</code></pre> <p>Gu\u00eda de Python</p>"},{"location":"datasets/","title":"Resumen de Conjuntos de Datos","text":"<p>Ultralytics brinda soporte para varios conjuntos de datos para facilitar tareas de visi\u00f3n por computadora como detecci\u00f3n, segmentaci\u00f3n de instancias, estimaci\u00f3n de poses, clasificaci\u00f3n y seguimiento de m\u00faltiples objetos. A continuaci\u00f3n se presenta una lista de los principales conjuntos de datos de Ultralytics, seguido por un resumen de cada tarea de visi\u00f3n por computadora y los respectivos conjuntos de datos.</p> <p>Note</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n multiling\u00fce est\u00e1 actualmente en construcci\u00f3n y estamos trabajando arduamente para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p>"},{"location":"datasets/#conjuntos-de-datos-de-deteccion","title":"Conjuntos de Datos de Detecci\u00f3n","text":"<p>La detecci\u00f3n de objetos con cuadros delimitadores es una t\u00e9cnica de visi\u00f3n por computadora que implica detectar y localizar objetos en una imagen dibujando un cuadro alrededor de cada objeto.</p> <ul> <li>Argoverse: Un conjunto de datos que contiene datos de seguimiento en 3D y predicci\u00f3n de movimientos en entornos urbanos con anotaciones detalladas.</li> <li>COCO: Un conjunto de datos a gran escala dise\u00f1ado para detecci\u00f3n de objetos, segmentaci\u00f3n y subtitulado con m\u00e1s de 200 mil im\u00e1genes etiquetadas.</li> <li>COCO8: Contiene las primeras 4 im\u00e1genes de COCO train y COCO val, adecuado para pruebas r\u00e1pidas.</li> <li>Global Wheat 2020: Un conjunto de datos de im\u00e1genes de cabezas de trigo recolectadas alrededor del mundo para tareas de detecci\u00f3n y localizaci\u00f3n de objetos.</li> <li>Objects365: Un conjunto de datos a gran escala y de alta calidad para la detecci\u00f3n de objetos con 365 categor\u00edas y m\u00e1s de 600 mil im\u00e1genes anotadas.</li> <li>OpenImagesV7: Un conjunto de datos completo de Google con 1.7 millones de im\u00e1genes de entrenamiento y 42 mil im\u00e1genes de validaci\u00f3n.</li> <li>SKU-110K: Un conjunto de datos que presenta detecci\u00f3n de objetos densa en entornos minoristas con m\u00e1s de 11 mil im\u00e1genes y 1.7 millones de cuadros delimitadores.</li> <li>VisDrone: Un conjunto de datos que contiene datos de detecci\u00f3n de objetos y seguimiento de m\u00faltiples objetos de im\u00e1genes capturadas por drones con m\u00e1s de 10 mil im\u00e1genes y secuencias de video.</li> <li>VOC: El conjunto de datos de Clases de Objetos Visuales de Pascal (VOC) para la detecci\u00f3n de objetos y segmentaci\u00f3n con 20 clases de objetos y m\u00e1s de 11 mil im\u00e1genes.</li> <li>xView: Un conjunto de datos para la detecci\u00f3n de objetos en im\u00e1genes a\u00e9reas con 60 categor\u00edas de objetos y m\u00e1s de un mill\u00f3n de objetos anotados.</li> </ul>"},{"location":"datasets/#conjuntos-de-datos-de-segmentacion-de-instancias","title":"Conjuntos de Datos de Segmentaci\u00f3n de Instancias","text":"<p>La segmentaci\u00f3n de instancias es una t\u00e9cnica de visi\u00f3n por computadora que implica identificar y localizar objetos en una imagen a nivel de p\u00edxel.</p> <ul> <li>COCO: Un conjunto de datos a gran escala dise\u00f1ado para tareas de detecci\u00f3n de objetos, segmentaci\u00f3n y subtitulado con m\u00e1s de 200 mil im\u00e1genes etiquetadas.</li> <li>COCO8-seg: Un conjunto de datos m\u00e1s peque\u00f1o para tareas de segmentaci\u00f3n de instancias, que contiene un subconjunto de 8 im\u00e1genes de COCO con anotaciones de segmentaci\u00f3n.</li> </ul>"},{"location":"datasets/#estimacion-de-poses","title":"Estimaci\u00f3n de Poses","text":"<p>La estimaci\u00f3n de poses es una t\u00e9cnica utilizada para determinar la pose del objeto en relaci\u00f3n con la c\u00e1mara o el sistema de coordenadas del mundo.</p> <ul> <li>COCO: Un conjunto de datos a gran escala con anotaciones de pose humana dise\u00f1ado para tareas de estimaci\u00f3n de poses.</li> <li>COCO8-pose: Un conjunto de datos m\u00e1s peque\u00f1o para tareas de estimaci\u00f3n de poses, que contiene un subconjunto de 8 im\u00e1genes de COCO con anotaciones de pose humana.</li> <li>Tiger-pose: Un conjunto de datos compacto que consiste en 263 im\u00e1genes centradas en tigres, anotadas con 12 puntos clave por tigre para tareas de estimaci\u00f3n de poses.</li> </ul>"},{"location":"datasets/#clasificacion","title":"Clasificaci\u00f3n","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n por computadora que implica categorizar una imagen en una o m\u00e1s clases o categor\u00edas predefinidas basadas en su contenido visual.</p> <ul> <li>Caltech 101: Un conjunto de datos que contiene im\u00e1genes de 101 categor\u00edas de objetos para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>Caltech 256: Una versi\u00f3n extendida de Caltech 101 con 256 categor\u00edas de objetos y im\u00e1genes m\u00e1s desafiantes.</li> <li>CIFAR-10: Un conjunto de datos de 60 mil im\u00e1genes a color de 32x32 en 10 clases, con 6 mil im\u00e1genes por clase.</li> <li>CIFAR-100: Una versi\u00f3n extendida de CIFAR-10 con 100 categor\u00edas de objetos y 600 im\u00e1genes por clase.</li> <li>Fashion-MNIST: Un conjunto de datos compuesto por 70 mil im\u00e1genes en escala de grises de 10 categor\u00edas de moda para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>ImageNet: Un conjunto de datos a gran escala para detecci\u00f3n de objetos y clasificaci\u00f3n de im\u00e1genes con m\u00e1s de 14 millones de im\u00e1genes y 20 mil categor\u00edas.</li> <li>ImageNet-10: Un subconjunto m\u00e1s peque\u00f1o de ImageNet con 10 categor\u00edas para experimentaci\u00f3n y pruebas m\u00e1s r\u00e1pidas.</li> <li>Imagenette: Un subconjunto m\u00e1s peque\u00f1o de ImageNet que contiene 10 clases f\u00e1cilmente distinguibles para entrenamientos y pruebas m\u00e1s r\u00e1pidos.</li> <li>Imagewoof: Un subconjunto m\u00e1s desafiante de ImageNet que contiene 10 categor\u00edas de razas de perros para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>MNIST: Un conjunto de datos de 70 mil im\u00e1genes en escala de grises de d\u00edgitos escritos a mano para tareas de clasificaci\u00f3n de im\u00e1genes.</li> </ul>"},{"location":"datasets/#cuadros-delimitadores-orientados-obb","title":"Cuadros Delimitadores Orientados (OBB)","text":"<p>Los Cuadros Delimitadores Orientados (OBB) es un m\u00e9todo en visi\u00f3n por computadora para detectar objetos angulados en im\u00e1genes utilizando cuadros delimitadores rotados, a menudo aplicado en im\u00e1genes a\u00e9reas y satelitales.</p> <ul> <li>DOTAv2: Un popular conjunto de datos de im\u00e1genes a\u00e9reas de OBB con 1.7 millones de instancias y 11,268 im\u00e1genes.</li> </ul>"},{"location":"datasets/#seguimiento-de-multiples-objetos","title":"Seguimiento de M\u00faltiples Objetos","text":"<p>El seguimiento de m\u00faltiples objetos es una t\u00e9cnica de visi\u00f3n por computadora que implica detectar y seguir m\u00faltiples objetos a lo largo del tiempo en una secuencia de video.</p> <ul> <li>Argoverse: Un conjunto de datos que contiene datos de seguimiento en 3D y predicci\u00f3n de movimientos en entornos urbanos con anotaciones detalladas para tareas de seguimiento de m\u00faltiples objetos.</li> <li>VisDrone: Un conjunto de datos que contiene datos de detecci\u00f3n de objetos y seguimiento de m\u00faltiples objetos de im\u00e1genes capturadas por drones con m\u00e1s de 10 mil im\u00e1genes y secuencias de video.</li> </ul>"},{"location":"datasets/#contribuir-con-nuevos-conjuntos-de-datos","title":"Contribuir con Nuevos Conjuntos de Datos","text":"<p>Contribuir con un nuevo conjunto de datos implica varios pasos para garantizar que se alinee bien con la infraestructura existente. A continuaci\u00f3n, se presentan los pasos necesarios:</p>"},{"location":"datasets/#pasos-para-contribuir-con-un-nuevo-conjunto-de-datos","title":"Pasos para Contribuir con un Nuevo Conjunto de Datos","text":"<ol> <li> <p>Recolectar Im\u00e1genes: Re\u00fane las im\u00e1genes que pertenecen al conjunto de datos. Estas podr\u00edan ser recopiladas de varias fuentes, tales como bases de datos p\u00fablicas o tu propia colecci\u00f3n.</p> </li> <li> <p>Annotar Im\u00e1genes: Anota estas im\u00e1genes con cuadros delimitadores, segmentos o puntos clave, dependiendo de la tarea.</p> </li> <li> <p>Exportar Anotaciones: Convierte estas anotaciones en el formato de archivo *.txt de YOLO que Ultralytics soporta.</p> </li> <li> <p>Organizar Conjunto de Datos: Organiza tu conjunto de datos en la estructura de carpetas correcta. Deber\u00edas tener directorios de nivel superior <code>train/</code> y <code>val/</code>, y dentro de cada uno, un subdirectorio <code>images/</code> y <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Crear un Archivo <code>data.yaml</code>: En el directorio ra\u00edz de tu conjunto de datos, crea un archivo <code>data.yaml</code> que describa el conjunto de datos, clases y otra informaci\u00f3n necesaria.</p> </li> <li> <p>Optimizar Im\u00e1genes (Opcional): Si deseas reducir el tama\u00f1o del conjunto de datos para un procesamiento m\u00e1s eficiente, puedes optimizar las im\u00e1genes usando el c\u00f3digo a continuaci\u00f3n. Esto no es requerido, pero se recomienda para tama\u00f1os de conjuntos de datos m\u00e1s peque\u00f1os y velocidades de descarga m\u00e1s r\u00e1pidas.</p> </li> <li> <p>Comprimir Conjunto de Datos: Comprime toda la carpeta del conjunto de datos en un archivo .zip.</p> </li> <li> <p>Documentar y PR: Crea una p\u00e1gina de documentaci\u00f3n describiendo tu conjunto de datos y c\u00f3mo encaja en el marco existente. Despu\u00e9s de eso, env\u00eda una Solicitud de Extracci\u00f3n (PR). Consulta las Pautas de Contribuci\u00f3n de Ultralytics para obtener m\u00e1s detalles sobre c\u00f3mo enviar una PR.</p> </li> </ol>"},{"location":"datasets/#codigo-de-ejemplo-para-optimizar-y-comprimir-un-conjunto-de-datos","title":"C\u00f3digo de Ejemplo para Optimizar y Comprimir un Conjunto de Datos","text":"<p>Optimizar y Comprimir un Conjunto de Datos</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definir el directorio del conjunto de datos\npath = Path('ruta/al/conjunto-de-datos')\n\n# Optimizar im\u00e1genes en el conjunto de datos (opcional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Comprimir el conjunto de datos en 'ruta/al/conjunto-de-datos.zip'\nzip_directory(path)\n</code></pre> <p>Siguiendo estos pasos, puedes contribuir con un nuevo conjunto de datos que se integre bien con la estructura existente de Ultralytics.</p>"},{"location":"models/","title":"Modelos soportados por Ultralytics","text":"<p>\u00a1Bienvenido a la documentaci\u00f3n de modelos de Ultralytics! Ofrecemos soporte para una amplia gama de modelos, cada uno adaptado a tareas espec\u00edficas como detecci\u00f3n de objetos, segmentaci\u00f3n de instancias, clasificaci\u00f3n de im\u00e1genes, estimaci\u00f3n de postura y seguimiento de m\u00faltiples objetos. Si est\u00e1s interesado en contribuir con tu arquitectura de modelo a Ultralytics, consulta nuestra Gu\u00eda de Contribuci\u00f3n.</p> <p>Note</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n en varios idiomas est\u00e1 actualmente en construcci\u00f3n y estamos trabajando arduamente para mejorarla. \u00a1Gracias por tu paciencia! \ud83d\ude4f</p>"},{"location":"models/#modelos-destacados","title":"Modelos Destacados","text":"<p>Aqu\u00ed tienes algunos de los modelos clave soportados:</p> <ol> <li>YOLOv3: La tercera iteraci\u00f3n de la familia de modelos YOLO, originalmente creada por Joseph Redmon, conocida por su capacidad de detecci\u00f3n de objetos en tiempo real de manera eficiente.</li> <li>YOLOv4: Una actualizaci\u00f3n para la red oscura de YOLOv3, lanzada por Alexey Bochkovskiy en 2020.</li> <li>YOLOv5: Una versi\u00f3n mejorada de la arquitectura YOLO por Ultralytics, que ofrece mejores compensaciones de rendimiento y velocidad en comparaci\u00f3n con versiones anteriores.</li> <li>YOLOv6: Lanzado por Meituan en 2022, y utilizado en muchos de los robots aut\u00f3nomos de entrega de la compa\u00f1\u00eda.</li> <li>YOLOv7: Modelos YOLO actualizados lanzados en 2022 por los autores de YOLOv4.</li> <li>YOLOv8: La \u00faltima versi\u00f3n de la familia YOLO, que presenta capacidades mejoradas como segmentaci\u00f3n de instancias, estimaci\u00f3n de postura/puntos clave y clasificaci\u00f3n.</li> <li>Modelo de Segmentaci\u00f3n de Cualquier Cosa (SAM): El Modelo de Segmentaci\u00f3n de Cualquier Cosa (SAM) de Meta.</li> <li>Modelo de Segmentaci\u00f3n de Cualquier Cosa M\u00f3vil (MobileSAM): MobileSAM para aplicaciones m\u00f3viles, por la Universidad Kyung Hee.</li> <li>Modelo de Segmentaci\u00f3n de Cualquier Cosa R\u00e1pida (FastSAM): FastSAM del Grupo de An\u00e1lisis de Im\u00e1genes y Video, Instituto de Automatizaci\u00f3n, Academia China de Ciencias.</li> <li>YOLO-NAS: Modelos de B\u00fasqueda de Arquitectura Neural de YOLO (NAS).</li> <li>Transformadores de Detecci\u00f3n en Tiempo Real (RT-DETR): Modelos de Transformadores de Detecci\u00f3n en Tiempo Real (RT-DETR) de Baidu PaddlePaddle.</li> </ol> <p> Mira: Ejecuta modelos YOLO de Ultralytics en solo unas pocas l\u00edneas de c\u00f3digo. </p>"},{"location":"models/#comenzando-ejemplos-de-uso","title":"Comenzando: Ejemplos de Uso","text":"<p>Ejemplo</p> PythonCLI <p>Los modelos preentrenados en PyTorch <code>*.pt</code> as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code> pueden pasarse a las clases <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> y <code>RTDETR()</code> para crear una instancia de modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo YOLOv8n preentrenado en COCO\nmodelo = YOLO('yolov8n.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresultados = modelo.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ejecutar inferencia con el modelo YOLOv8n en la imagen 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Comandos CLI est\u00e1n disponibles para ejecutar directamente los modelos:</p> <pre><code># Cargar un modelo YOLOv8n preentrenado en COCO y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Cargar un modelo YOLOv8n preentrenado en COCO y ejecutar inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribuyendo-con-nuevos-modelos","title":"Contribuyendo con Nuevos Modelos","text":"<p>\u00bfInteresado en contribuir con tu modelo a Ultralytics? \u00a1Genial! Siempre estamos abiertos a expandir nuestro portafolio de modelos.</p> <ol> <li> <p>Haz un Fork del Repositorio: Comienza haciendo un fork del repositorio de GitHub de Ultralytics.</p> </li> <li> <p>Clona tu Fork: Clona tu fork en tu m\u00e1quina local y crea una nueva rama para trabajar.</p> </li> <li> <p>Implementa tu Modelo: A\u00f1ade tu modelo siguiendo los est\u00e1ndares y gu\u00edas de codificaci\u00f3n proporcionados en nuestra Gu\u00eda de Contribuci\u00f3n.</p> </li> <li> <p>Prueba a Fondo: Aseg\u00farate de probar tu modelo rigurosamente, tanto de manera aislada como parte del pipeline.</p> </li> <li> <p>Crea un Pull Request: Una vez que est\u00e9s satisfecho con tu modelo, crea un pull request al repositorio principal para su revisi\u00f3n.</p> </li> <li> <p>Revisi\u00f3n de C\u00f3digo y Fusi\u00f3n: Despu\u00e9s de la revisi\u00f3n, si tu modelo cumple con nuestros criterios, se fusionar\u00e1 en el repositorio principal.</p> </li> </ol> <p>Consulta nuestra Gu\u00eda de Contribuci\u00f3n para los pasos detallados.</p>"},{"location":"modes/","title":"Modos de Ultralytics YOLOv8","text":""},{"location":"modes/#introduccion","title":"Introducci\u00f3n","text":"<p>Ultralytics YOLOv8 no es solo otro modelo de detecci\u00f3n de objetos; es un marco de trabajo vers\u00e1til dise\u00f1ado para cubrir todo el ciclo de vida de los modelos de aprendizaje autom\u00e1tico, desde la ingesta de datos y el entrenamiento del modelo hasta la validaci\u00f3n, implementaci\u00f3n y seguimiento en el mundo real. Cada modo sirve para un prop\u00f3sito espec\u00edfico y est\u00e1 dise\u00f1ado para ofrecerte la flexibilidad y eficiencia necesarias para diferentes tareas y casos de uso.</p> <p> Mira: Tutorial de Modos Ultralytics: Entrenar, Validar, Predecir, Exportar y Hacer Benchmarking. </p>"},{"location":"modes/#modos-a-primera-vista","title":"Modos a Primera Vista","text":"<p>Comprender los diferentes modos que soporta Ultralytics YOLOv8 es cr\u00edtico para sacar el m\u00e1ximo provecho a tus modelos:</p> <ul> <li>Modo Entrenar (Train): Afina tu modelo en conjuntos de datos personalizados o pre-cargados.</li> <li>Modo Validar (Val): Un punto de control post-entrenamiento para validar el rendimiento del modelo.</li> <li>Modo Predecir (Predict): Libera el poder predictivo de tu modelo en datos del mundo real.</li> <li>Modo Exportar (Export): Prepara tu modelo para la implementaci\u00f3n en varios formatos.</li> <li>Modo Seguir (Track): Extiende tu modelo de detecci\u00f3n de objetos a aplicaciones de seguimiento en tiempo real.</li> <li>Modo Benchmark (Benchmark): Analiza la velocidad y precisi\u00f3n de tu modelo en diversos entornos de implementaci\u00f3n.</li> </ul> <p>Esta gu\u00eda completa tiene como objetivo proporcionarte una visi\u00f3n general y conocimientos pr\u00e1cticos de cada modo, ayud\u00e1ndote a aprovechar todo el potencial de YOLOv8.</p>"},{"location":"modes/#entrenar-train","title":"Entrenar (Train)","text":"<p>El modo Entrenar se utiliza para entrenar un modelo YOLOv8 en un conjunto de datos personalizado. En este modo, el modelo se entrena utilizando el conjunto de datos y los hiperpar\u00e1metros especificados. El proceso de entrenamiento implica optimizar los par\u00e1metros del modelo para que pueda predecir con precisi\u00f3n las clases y ubicaciones de los objetos en una imagen.</p> <p>Ejemplos de Entrenamiento</p>"},{"location":"modes/#validar-val","title":"Validar (Val)","text":"<p>El modo Validar se usa para validar un modelo YOLOv8 despu\u00e9s de haber sido entrenado. En este modo, el modelo se eval\u00faa en un conjunto de validaci\u00f3n para medir su precisi\u00f3n y rendimiento de generalizaci\u00f3n. Este modo se puede usar para ajustar los hiperpar\u00e1metros del modelo y mejorar su rendimiento.</p> <p>Ejemplos de Validaci\u00f3n</p>"},{"location":"modes/#predecir-predict","title":"Predecir (Predict)","text":"<p>El modo Predecir se utiliza para realizar predicciones usando un modelo YOLOv8 entrenado en im\u00e1genes o videos nuevos. En este modo, el modelo se carga desde un archivo de punto de control, y el usuario puede proporcionar im\u00e1genes o videos para realizar inferencias. El modelo predice las clases y ubicaciones de los objetos en las im\u00e1genes o videos de entrada.</p> <p>Ejemplos de Predicci\u00f3n</p>"},{"location":"modes/#exportar-export","title":"Exportar (Export)","text":"<p>El modo Exportar se utiliza para exportar un modelo YOLOv8 a un formato que se pueda usar para la implementaci\u00f3n. En este modo, el modelo se convierte a un formato que puede ser utilizado por otras aplicaciones de software o dispositivos de hardware. Este modo es \u00fatil al implementar el modelo en entornos de producci\u00f3n.</p> <p>Ejemplos de Exportaci\u00f3n</p>"},{"location":"modes/#seguir-track","title":"Seguir (Track)","text":"<p>El modo Seguir se usa para rastrear objetos en tiempo real utilizando un modelo YOLOv8. En este modo, el modelo se carga desde un archivo de punto de control, y el usuario puede proporcionar un flujo de video en vivo para realizar seguimiento de objetos en tiempo real. Este modo es \u00fatil para aplicaciones como sistemas de vigilancia o coches aut\u00f3nomos.</p> <p>Ejemplos de Seguimiento</p>"},{"location":"modes/#benchmark-benchmark","title":"Benchmark (Benchmark)","text":"<p>El modo Benchmark se utiliza para perfilar la velocidad y precisi\u00f3n de varios formatos de exportaci\u00f3n de YOLOv8. Los benchmarks proporcionan informaci\u00f3n sobre el tama\u00f1o del formato de exportaci\u00f3n, sus m\u00e9tricas de <code>mAP50-95</code> (para detecci\u00f3n de objetos, segmentaci\u00f3n y pose) o m\u00e9tricas de <code>accuracy_top5</code> (para clasificaci\u00f3n), y el tiempo de inferencia en milisegundos por imagen a trav\u00e9s de varios formatos de exportaci\u00f3n como ONNX, OpenVINO, TensorRT y otros. Esta informaci\u00f3n puede ayudar a los usuarios a elegir el formato de exportaci\u00f3n \u00f3ptimo para su caso de uso espec\u00edfico, basado en sus requerimientos de velocidad y precisi\u00f3n.</p> <p>Ejemplos de Benchmarking</p>"},{"location":"modes/benchmark/","title":"Model Benchmarking con Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introduccion","title":"Introducci\u00f3n","text":"<p>Una vez que su modelo est\u00e1 entrenado y validado, el siguiente paso l\u00f3gico es evaluar su rendimiento en varios escenarios del mundo real. El modo benchmark en Ultralytics YOLOv8 cumple con este prop\u00f3sito proporcionando un marco s\u00f3lido para valorar la velocidad y exactitud de su modelo a trav\u00e9s de una gama de formatos de exportaci\u00f3n.</p>"},{"location":"modes/benchmark/#por-que-es-crucial-el-benchmarking","title":"\u00bfPor Qu\u00e9 Es Crucial el Benchmarking?","text":"<ul> <li>Decisiones Informadas: Obtenga perspectivas sobre el equilibrio entre velocidad y precisi\u00f3n.</li> <li>Asignaci\u00f3n de Recursos: Entienda c\u00f3mo diferentes formatos de exportaci\u00f3n se desempe\u00f1an en diferentes hardware.</li> <li>Optimizaci\u00f3n: Aprenda cu\u00e1l formato de exportaci\u00f3n ofrece el mejor rendimiento para su caso de uso espec\u00edfico.</li> <li>Eficiencia de Costo: Haga un uso m\u00e1s eficiente de los recursos de hardware basado en los resultados del benchmark.</li> </ul>"},{"location":"modes/benchmark/#metricas-clave-en-el-modo-benchmark","title":"M\u00e9tricas Clave en el Modo Benchmark","text":"<ul> <li>mAP50-95: Para detecci\u00f3n de objetos, segmentaci\u00f3n y estimaci\u00f3n de pose.</li> <li>accuracy_top5: Para clasificaci\u00f3n de im\u00e1genes.</li> <li>Tiempo de Inferencia: Tiempo tomado para cada imagen en milisegundos.</li> </ul>"},{"location":"modes/benchmark/#formatos-de-exportacion-soportados","title":"Formatos de Exportaci\u00f3n Soportados","text":"<ul> <li>ONNX: Para un rendimiento \u00f3ptimo de CPU</li> <li>TensorRT: Para la m\u00e1xima eficiencia de GPU</li> <li>OpenVINO: Para la optimizaci\u00f3n en hardware de Intel</li> <li>CoreML, TensorFlow SavedModel y M\u00e1s: Para necesidades de despliegue diversas.</li> </ul> <p>Consejo</p> <ul> <li>Exporte a ONNX o OpenVINO para acelerar la velocidad de CPU hasta 3 veces.</li> <li>Exporte a TensorRT para acelerar la velocidad de GPU hasta 5 veces.</li> </ul>"},{"location":"modes/benchmark/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Ejecute benchmarks de YOLOv8n en todos los formatos de exportaci\u00f3n soportados incluyendo ONNX, TensorRT, etc. Vea la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark en GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumentos","title":"Argumentos","text":"<p>Argumentos como <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code>, y <code>verbose</code> proporcionan a los usuarios la flexibilidad de ajustar los benchmarks a sus necesidades espec\u00edficas y comparar el rendimiento de diferentes formatos de exportaci\u00f3n con facilidad.</p> Clave Valor Descripci\u00f3n <code>model</code> <code>None</code> ruta al archivo del modelo, es decir, yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> ruta a YAML que referencia el conjunto de datos de benchmarking (bajo la etiqueta <code>val</code>) <code>imgsz</code> <code>640</code> tama\u00f1o de imagen como escalar o lista (h, w), es decir, (640, 480) <code>half</code> <code>False</code> cuantificaci\u00f3n FP16 <code>int8</code> <code>False</code> cuantificaci\u00f3n INT8 <code>device</code> <code>None</code> dispositivo en el que se ejecutar\u00e1, es decir, dispositivo cuda=0 o dispositivo=0,1,2,3 o dispositivo=cpu <code>verbose</code> <code>False</code> no continuar en caso de error (bool), o umbral de piso de valor (float)"},{"location":"modes/benchmark/#formatos-de-exportacion","title":"Formatos de Exportaci\u00f3n","text":"<p>Los benchmarks intentar\u00e1n ejecutarse autom\u00e1ticamente en todos los posibles formatos de exportaci\u00f3n a continuaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vea los detalles completos de <code>export</code> en la p\u00e1gina Export.</p>"},{"location":"modes/export/","title":"Exportaci\u00f3n de Modelos con Ultralytics YOLO","text":""},{"location":"modes/export/#introduccion","title":"Introducci\u00f3n","text":"<p>El objetivo final de entrenar un modelo es desplegarlo para aplicaciones en el mundo real. El modo exportaci\u00f3n en Ultralytics YOLOv8 ofrece una gama vers\u00e1til de opciones para exportar tu modelo entrenado a diferentes formatos, haci\u00e9ndolo desplegable en varias plataformas y dispositivos. Esta gu\u00eda integral pretende guiarte a trav\u00e9s de los matices de la exportaci\u00f3n de modelos, mostrando c\u00f3mo lograr la m\u00e1xima compatibilidad y rendimiento.</p> <p> Ver: C\u00f3mo Exportar un Modelo Entrenado Personalizado de Ultralytics YOLOv8 y Ejecutar Inferencia en Vivo en la Webcam. </p>"},{"location":"modes/export/#por-que-elegir-el-modo-exportacion-de-yolov8","title":"\u00bfPor Qu\u00e9 Elegir el Modo Exportaci\u00f3n de YOLOv8?","text":"<ul> <li>Versatilidad: Exporta a m\u00faltiples formatos incluyendo ONNX, TensorRT, CoreML y m\u00e1s.</li> <li>Rendimiento: Acelera hasta 5 veces la velocidad en GPU con TensorRT y 3 veces en CPU con ONNX o OpenVINO.</li> <li>Compatibilidad: Hacer que tu modelo sea universalmente desplegable en numerosos entornos de hardware y software.</li> <li>Facilidad de Uso: Interfaz de l\u00ednea de comandos simple y API de Python para una exportaci\u00f3n de modelos r\u00e1pida y sencilla.</li> </ul>"},{"location":"modes/export/#caracteristicas-clave-del-modo-de-exportacion","title":"Caracter\u00edsticas Clave del Modo de Exportaci\u00f3n","text":"<p>Aqu\u00ed tienes algunas de las funcionalidades destacadas:</p> <ul> <li>Exportaci\u00f3n con Un Solo Clic: Comandos simples para exportar a diferentes formatos.</li> <li>Exportaci\u00f3n por Lotes: Exporta modelos capaces de inferencia por lotes.</li> <li>Inferencia Optimizada: Los modelos exportados est\u00e1n optimizados para tiempos de inferencia m\u00e1s r\u00e1pidos.</li> <li>V\u00eddeos Tutoriales: Gu\u00edas y tutoriales en profundidad para una experiencia de exportaci\u00f3n fluida.</li> </ul> <p>Consejo</p> <ul> <li>Exporta a ONNX u OpenVINO para acelerar la CPU hasta 3 veces.</li> <li>Exporta a TensorRT para acelerar la GPU hasta 5 veces.</li> </ul>"},{"location":"modes/export/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Exporta un modelo YOLOv8n a un formato diferente como ONNX o TensorRT. Consulta la secci\u00f3n Argumentos m\u00e1s abajo para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carga un modelo\nmodel = YOLO('yolov8n.pt')  # carga un modelo oficial\nmodel = YOLO('path/to/best.pt')  # carga un modelo entrenado personalizado\n\n# Exporta el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporta modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exporta modelo entrenado personalizado\n</code></pre>"},{"location":"modes/export/#argumentos","title":"Argumentos","text":"<p>Los ajustes de exportaci\u00f3n para modelos YOLO se refieren a las diversas configuraciones y opciones utilizadas para guardar o exportar el modelo para su uso en otros entornos o plataformas. Estos ajustes pueden afectar el rendimiento del modelo, su tama\u00f1o y su compatibilidad con diferentes sistemas. Algunos ajustes comunes de exportaci\u00f3n de YOLO incluyen el formato del archivo del modelo exportado (p. ej., ONNX, TensorFlow SavedModel), el dispositivo en el que se ejecutar\u00e1 el modelo (p. ej., CPU, GPU) y la presencia de caracter\u00edsticas adicionales como m\u00e1scaras o m\u00faltiples etiquetas por caja. Otros factores que pueden afectar el proceso de exportaci\u00f3n incluyen la tarea espec\u00edfica para la que se est\u00e1 utilizando el modelo y los requisitos o limitaciones del entorno o plataforma objetivo. Es importante considerar y configurar cuidadosamente estos ajustes para asegurar que el modelo exportado est\u00e1 optimizado para el caso de uso previsto y se pueda utilizar eficazmente en el entorno objetivo.</p> Llave Valor Descripci\u00f3n <code>format</code> <code>'torchscript'</code> formato al que exportar <code>imgsz</code> <code>640</code> tama\u00f1o de imagen como escalar o lista (h, w), p. ej. (640, 480) <code>keras</code> <code>False</code> usu Keras para la exportaci\u00f3n de TF SavedModel <code>optimize</code> <code>False</code> TorchScript: optimizar para m\u00f3vil <code>half</code> <code>False</code> cuantificaci\u00f3n FP16 <code>int8</code> <code>False</code> cuantificaci\u00f3n INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT: ejes din\u00e1micos <code>simplify</code> <code>False</code> ONNX/TensorRT: simplificar modelo <code>opset</code> <code>None</code> ONNX: versi\u00f3n de opset (opcional, por defecto la m\u00e1s reciente) <code>workspace</code> <code>4</code> TensorRT: tama\u00f1o del espacio de trabajo (GB) <code>nms</code> <code>False</code> CoreML: a\u00f1adir NMS"},{"location":"modes/export/#formatos-de-exportacion","title":"Formatos de Exportaci\u00f3n","text":"<p>Los formatos de exportaci\u00f3n disponibles de YOLOv8 est\u00e1n en la tabla a continuaci\u00f3n. Puedes exportar a cualquier formato usando el argumento <code>format</code>, por ejemplo, <code>format='onnx'</code> o <code>format='engine'</code>.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Predicci\u00f3n del Modelo con YOLO de Ultralytics","text":""},{"location":"modes/predict/#introduccion","title":"Introducci\u00f3n","text":"<p>En el mundo del aprendizaje autom\u00e1tico y la visi\u00f3n por computadora, el proceso de dar sentido a los datos visuales se denomina 'inferencia' o 'predicci\u00f3n'. YOLOv8 de Ultralytics ofrece una caracter\u00edstica poderosa conocida como modo predictivo que est\u00e1 dise\u00f1ada para inferencias de alto rendimiento y en tiempo real en una amplia gama de fuentes de datos.</p> <p> Ver: C\u00f3mo Extraer las Salidas del Modelo YOLOv8 de Ultralytics para Proyectos Personalizados. </p>"},{"location":"modes/predict/#aplicaciones-en-el-mundo-real","title":"Aplicaciones en el Mundo Real","text":"Manufactura Deportes Seguridad Detecci\u00f3n de Repuestos de Veh\u00edculos Detecci\u00f3n de Jugadores de F\u00fatbol Detecci\u00f3n de Ca\u00eddas de Personas"},{"location":"modes/predict/#por-que-utilizar-yolo-de-ultralytics-para-la-inferencia","title":"\u00bfPor Qu\u00e9 Utilizar YOLO de Ultralytics para la Inferencia?","text":"<p>Estas son algunas razones para considerar el modo predictivo de YOLOv8 para sus necesidades de inferencia:</p> <ul> <li>Versatilidad: Capaz de realizar inferencias en im\u00e1genes, videos e incluso transmisiones en vivo.</li> <li>Rendimiento: Dise\u00f1ado para procesamiento en tiempo real y de alta velocidad sin sacrificar precisi\u00f3n.</li> <li>Facilidad de Uso: Interfaces de Python y CLI intuitivas para una r\u00e1pida implementaci\u00f3n y pruebas.</li> <li>Alta Personalizaci\u00f3n: Diversos ajustes y par\u00e1metros para afinar el comportamiento de inferencia del modelo seg\u00fan sus requisitos espec\u00edficos.</li> </ul>"},{"location":"modes/predict/#caracteristicas-principales-del-modo-predictivo","title":"Caracter\u00edsticas Principales del Modo Predictivo","text":"<p>El modo predictivo de YOLOv8 est\u00e1 dise\u00f1ado para ser robusto y vers\u00e1til, y cuenta con:</p> <ul> <li>Compatibilidad con M\u00faltiples Fuentes de Datos: Ya sea que sus datos est\u00e9n en forma de im\u00e1genes individuales, una colecci\u00f3n de im\u00e1genes, archivos de video o transmisiones de video en tiempo real, el modo predictivo le tiene cubierto.</li> <li>Modo de Transmisi\u00f3n: Utilice la funci\u00f3n de transmisi\u00f3n para generar un generador eficiente de memoria de objetos <code>Results</code>. Active esto configurando <code>stream=True</code> en el m\u00e9todo de llamada del predictor.</li> <li>Procesamiento por Lotes: La capacidad de procesar m\u00faltiples im\u00e1genes o fotogramas de video en un solo lote, acelerando a\u00fan m\u00e1s el tiempo de inferencia.</li> <li>Amigable para la Integraci\u00f3n: Se integra f\u00e1cilmente con pipelines de datos existentes y otros componentes de software, gracias a su API flexible.</li> </ul> <p>Los modelos YOLO de Ultralytics devuelven ya sea una lista de objetos <code>Results</code> de Python, o un generador de objetos <code>Results</code> de Python eficiente en memoria cuando se pasa <code>stream=True</code> al modelo durante la inferencia:</p> <p>Predict</p> Devolver una lista con <code>stream=False</code>Devolver un generador con <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n preentrenado\n\n# Ejecutar inferencia por lotes en una lista de im\u00e1genes\nresults = model(['im1.jpg', 'im2.jpg'])  # devuelve una lista de objetos Results\n\n# Procesar lista de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para salidas de bbox\n    masks = result.masks  # Objeto Masks para salidas de m\u00e1scaras de segmentaci\u00f3n\n    keypoints = result.keypoints  # Objeto Keypoints para salidas de postura\n    probs = result.probs  # Objeto Probs para salidas de clasificaci\u00f3n\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n preentrenado\n\n# Ejecutar inferencia por lotes en una lista de im\u00e1genes\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # devuelve un generador de objetos Results\n\n# Procesar generador de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para salidas de bbox\n   .masks = result.masks  # Objeto Masks para salidas de m\u00e1scaras de segmentaci\u00f3n\n    keypoints = result.keypoints  # Objeto Keypoints para salidas de postura\n    probs = result.probs  # Objeto Probs para salidas de clasificaci\u00f3n\n</code></pre>"},{"location":"modes/predict/#fuentes-de-inferencia","title":"Fuentes de Inferencia","text":"<p>YOLOv8 puede procesar diferentes tipos de fuentes de entrada para la inferencia, como se muestra en la tabla a continuaci\u00f3n. Las fuentes incluyen im\u00e1genes est\u00e1ticas, transmisiones de video y varios formatos de datos. La tabla tambi\u00e9n indica si cada fuente se puede utilizar en modo de transmisi\u00f3n con el argumento <code>stream=True</code> \u2705. El modo de transmisi\u00f3n es beneficioso para procesar videos o transmisiones en vivo ya que crea un generador de resultados en lugar de cargar todos los fotogramas en la memoria.</p> <p>Consejo</p> <p>Utilice <code>stream=True</code> para procesar videos largos o conjuntos de datos grandes para gestionar eficientemente la memoria. Cuando <code>stream=False</code>, los resultados de todos los fotogramas o puntos de datos se almacenan en la memoria, lo que puede aumentar r\u00e1pidamente y causar errores de memoria insuficiente para entradas grandes. En contraste, <code>stream=True</code> utiliza un generador, que solo mantiene los resultados del fotograma o punto de datos actual en la memoria, reduciendo significativamente el consumo de memoria y previniendo problemas de falta de memoria.</p> Fuente Argumento Tipo Notas imagen <code>'image.jpg'</code> <code>str</code> o <code>Path</code> Archivo \u00fanico de imagen. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL a una imagen. captura de pantalla <code>'screen'</code> <code>str</code> Captura una captura de pantalla. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Formato HWC con canales RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Formato HWC con canales BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Formato HWC con canales BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Formato BCHW con canales RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> o <code>Path</code> Archivo CSV que contiene rutas a im\u00e1genes, videos o directorios. video \u2705 <code>'video.mp4'</code> <code>str</code> o <code>Path</code> Archivo de video en formatos como MP4, AVI, etc. directorio \u2705 <code>'path/'</code> <code>str</code> o <code>Path</code> Ruta a un directorio que contiene im\u00e1genes o videos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Patr\u00f3n glob para coincidir con m\u00faltiples archivos. Utilice el car\u00e1cter <code>*</code> como comod\u00edn. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL a un video de YouTube. transmisi\u00f3n \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL para protocolos de transmisi\u00f3n como RTSP, RTMP, TCP o una direcci\u00f3n IP. multi-transmisi\u00f3n \u2705 <code>'list.streams'</code> <code>str</code> o <code>Path</code> Archivo de texto <code>*.streams</code> con una URL de transmisi\u00f3n por fila, es decir, 8 transmisiones se ejecutar\u00e1n con tama\u00f1o de lote 8. <p>A continuaci\u00f3n se muestran ejemplos de c\u00f3digo para usar cada tipo de fuente:</p> <p>Fuentes de predicci\u00f3n</p> imagencaptura de pantallaURLPILOpenCVnumpytorch <p>Ejecute inferencia en un archivo de imagen. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir la ruta al archivo de imagen\nsource = 'ruta/a/imagen.jpg'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en el contenido actual de la pantalla como captura de pantalla. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir captura de pantalla actual como fuente\nsource = 'screen'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen o video alojados remotamente a trav\u00e9s de URL. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir URL remota de imagen o video\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen abierta con la Biblioteca de Im\u00e1genes de Python (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Abrir una imagen usando PIL\nsource = Image.open('ruta/a/imagen.jpg')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen le\u00edda con OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Leer una imagen usando OpenCV\nsource = cv2.imread('ruta/a/imagen.jpg')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen representada como un array de numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Crear un array aleatorio de numpy con forma HWC (640, 640, 3) con valores en rango [0, 255] y tipo uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen representada como un tensor de PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#cargar-el-modelo-yolov8n-preentrenado","title":"Cargar el modelo YOLOv8n preentrenado","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#crear-un-tensor-aleatorio-de-torch-con-forma-bchw-1-3-640-640-con-valores-en-rango-0-1-y-tipo-float32","title":"Crear un tensor aleatorio de torch con forma BCHW (1, 3, 640, 640) con valores en rango [0, 1] y tipo float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#ejecutar-inferencia-en-la-fuente","title":"Ejecutar inferencia en la fuente","text":"<p>results = model(source)  # lista de objetos Results</p>"},{"location":"modes/track/","title":"Seguimiento de M\u00faltiples Objetos con Ultralytics YOLO","text":"<p>El seguimiento de objetos en el \u00e1mbito del an\u00e1lisis de video es una tarea cr\u00edtica que no solo identifica la ubicaci\u00f3n y clase de objetos dentro del cuadro, sino que tambi\u00e9n mantiene una ID \u00fanica para cada objeto detectado a medida que avanza el video. Las aplicaciones son ilimitadas, desde vigilancia y seguridad hasta an\u00e1lisis deportivos en tiempo real.</p>"},{"location":"modes/track/#por-que-elegir-ultralytics-yolo-para-el-seguimiento-de-objetos","title":"\u00bfPor Qu\u00e9 Elegir Ultralytics YOLO para el Seguimiento de Objetos?","text":"<p>La salida de los rastreadores de Ultralytics es consistente con la detecci\u00f3n de objetos est\u00e1ndar, pero con el valor a\u00f1adido de las IDs de objetos. Esto facilita el seguimiento de objetos en flujos de video y la realizaci\u00f3n de an\u00e1lisis posteriores. Aqu\u00ed tienes algunas razones por las que deber\u00edas considerar usar Ultralytics YOLO para tus necesidades de seguimiento de objetos:</p> <ul> <li>Eficiencia: Procesa flujos de video en tiempo real sin comprometer la precisi\u00f3n.</li> <li>Flexibilidad: Soporta m\u00faltiples algoritmos de seguimiento y configuraciones.</li> <li>Facilidad de Uso: API simple de Python y opciones CLI para una r\u00e1pida integraci\u00f3n y despliegue.</li> <li>Personalizaci\u00f3n: F\u00e1cil de usar con modelos YOLO entrenados a medida, permitiendo la integraci\u00f3n en aplicaciones espec\u00edficas del dominio.</li> </ul> <p> Ver: Detecci\u00f3n de Objetos y Seguimiento con Ultralytics YOLOv8. </p>"},{"location":"modes/track/#aplicaciones-en-el-mundo-real","title":"Aplicaciones en el Mundo Real","text":"Transporte Venta al por Menor Acuicultura Seguimiento de Veh\u00edculos Seguimiento de Personas Seguimiento de Peces"},{"location":"modes/track/#caracteristicas-a-simple-vista","title":"Caracter\u00edsticas a Simple Vista","text":"<p>Ultralytics YOLO extiende sus caracter\u00edsticas de detecci\u00f3n de objetos para proporcionar un seguimiento de objetos robusto y vers\u00e1til:</p> <ul> <li>Seguimiento en Tiempo Real: Rastrea sin problemas los objetos en videos de alta frecuencia de cuadros.</li> <li>Soporte de M\u00faltiples Rastreadores: Elige entre una variedad de algoritmos de seguimiento establecidos.</li> <li>Configuraciones de Rastreador Personalizables: Adapta el algoritmo de seguimiento para satisfacer requisitos espec\u00edficos ajustando diversos par\u00e1metros.</li> </ul>"},{"location":"modes/track/#rastreadores-disponibles","title":"Rastreadores Disponibles","text":"<p>Ultralytics YOLO soporta los siguientes algoritmos de seguimiento. Pueden ser habilitados pasando el archivo de configuraci\u00f3n YAML relevante como <code>tracker=tracker_type.yaml</code>:</p> <ul> <li>BoT-SORT - Usa <code>botsort.yaml</code> para habilitar este rastreador.</li> <li>ByteTrack - Usa <code>bytetrack.yaml</code> para habilitar este rastreador.</li> </ul> <p>El rastreador predeterminado es BoT-SORT.</p>"},{"location":"modes/track/#seguimiento","title":"Seguimiento","text":"<p>Para ejecutar el rastreador en flujos de video, usa un modelo Detect, Segment o Pose entrenado tales como YOLOv8n, YOLOv8n-seg y YOLOv8n-pose.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo oficial o personalizado\nmodel = YOLO('yolov8n.pt')  # Cargar un modelo oficial Detect\nmodel = YOLO('yolov8n-seg.pt')  # Cargar un modelo oficial Segment\nmodel = YOLO('yolov8n-pose.pt')  # Cargar un modelo oficial Pose\nmodel = YOLO('path/to/best.pt')  # Cargar un modelo entrenado a medida\n\n# Realizar el seguimiento con el modelo\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Seguimiento con el rastreador predeterminado\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Seguimiento con el rastreador ByteTrack\n</code></pre> <pre><code># Realizar seguimiento con varios modelos usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Detect\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Segment\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Pose\nyolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo entrenado a medida\n\n# Realizar seguimiento usando el rastreador ByteTrack\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Como se puede ver en el uso anterior, el seguimiento est\u00e1 disponible para todos los modelos Detect, Segment y Pose ejecutados en videos o fuentes de transmisi\u00f3n.</p>"},{"location":"modes/track/#configuracion","title":"Configuraci\u00f3n","text":""},{"location":"modes/track/#argumentos-de-seguimiento","title":"Argumentos de Seguimiento","text":"<p>La configuraci\u00f3n de seguimiento comparte propiedades con el modo Predict, como <code>conf</code>, <code>iou</code> y <code>show</code>. Para configuraciones adicionales, consulta la p\u00e1gina del modelo Predict.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurar los par\u00e1metros de seguimiento y ejecutar el rastreador\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurar par\u00e1metros de seguimiento y ejecutar el rastreador usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#seleccion-de-rastreador","title":"Selecci\u00f3n de Rastreador","text":"<p>Ultralytics tambi\u00e9n te permite usar un archivo de configuraci\u00f3n de rastreador modificado. Para hacerlo, simplemente haz una copia de un archivo de configuraci\u00f3n de rastreador (por ejemplo, <code>custom_tracker.yaml</code>) de ultralytics/cfg/trackers y modifica cualquier configuraci\u00f3n (excepto el <code>tracker_type</code>) seg\u00fan tus necesidades.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo y ejecutar el rastreador con un archivo de configuraci\u00f3n personalizado\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Cargar el modelo y ejecutar el rastreador con un archivo de configuraci\u00f3n personalizado usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Para obtener una lista completa de los argumentos de seguimiento, consulta la p\u00e1gina ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#ejemplos-en-python","title":"Ejemplos en Python","text":""},{"location":"modes/track/#bucle-de-seguimiento-persistente","title":"Bucle de Seguimiento Persistente","text":"<p>Aqu\u00ed hay un script en Python que utiliza OpenCV (<code>cv2</code>) y YOLOv8 para ejecutar el seguimiento de objetos en fotogramas de video. Este script a\u00fan asume que ya has instalado los paquetes necesarios (<code>opencv-python</code> y <code>ultralytics</code>). El argumento <code>persist=True</code> le indica al rastreador que la imagen o fotograma actual es el siguiente en una secuencia y que espera rastros de la imagen anterior en la imagen actual.</p> <p>Bucle de transmisi\u00f3n en vivo con seguimiento</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Abrir el archivo de video\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Bucle a trav\u00e9s de los fotogramas del video\nwhile cap.isOpened():\n    # Leer un fotograma del video\n    success, frame = cap.read()\n\n    if success:\n        # Ejecutar seguimiento YOLOv8 en el fotograma, persistiendo los rastreos entre fotogramas\n        results = model.track(frame, persist=True)\n\n        # Visualizar los resultados en el fotograma\n        annotated_frame = results[0].plot()\n\n        # Mostrar el fotograma anotado\n        cv2.imshow(\"Seguimiento YOLOv8\", annotated_frame)\n\n        # Romper el bucle si se presiona 'q'\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Romper el bucle si se alcanza el final del video\n        break\n\n# Liberar el objeto de captura de video y cerrar la ventana de visualizaci\u00f3n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Toma en cuenta el cambio de <code>model(frame)</code> a <code>model.track(frame)</code>, que habilita el seguimiento de objetos en lugar de simplemente la detecci\u00f3n. Este script modificado ejecutar\u00e1 el rastreador en cada fotograma del video, visualizar\u00e1 los resultados y los mostrar\u00e1 en una ventana. El bucle puede ser terminado presionando 'q'.</p>"},{"location":"modes/track/#contribuir-con-nuevos-rastreadores","title":"Contribuir con Nuevos Rastreadores","text":"<p>\u00bfEres experto en seguimiento de m\u00faltiples objetos y has implementado o adaptado exitosamente un algoritmo de seguimiento con Ultralytics YOLO? Te invitamos a contribuir en nuestra secci\u00f3n de Rastreadores en ultralytics/cfg/trackers! Tus aplicaciones en el mundo real y soluciones podr\u00edan ser invaluables para los usuarios que trabajan en tareas de seguimiento.</p> <p>Al contribuir en esta secci\u00f3n, ayudar\u00e1s a ampliar el alcance de las soluciones de seguimiento disponibles dentro del marco de trabajo de Ultralytics YOLO, a\u00f1adiendo otra capa de funcionalidad y utilidad para la comunidad.</p> <p>Para iniciar tu contribuci\u00f3n, por favor consulta nuestra Gu\u00eda de Contribuci\u00f3n para obtener instrucciones completas sobre c\u00f3mo enviar una Solicitud de Extracci\u00f3n (PR) \ud83d\udee0\ufe0f. \u00a1Estamos emocionados de ver lo que traes a la mesa!</p> <p>Juntos, vamos a mejorar las capacidades de seguimiento del ecosistema Ultralytics YOLO \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Entrenamiento de Modelos con Ultralytics YOLO","text":""},{"location":"modes/train/#introduccion","title":"Introducci\u00f3n","text":"<p>Entrenar un modelo de aprendizaje profundo implica alimentarlo con datos y ajustar sus par\u00e1metros para que pueda hacer predicciones precisas. El modo de entrenamiento en Ultralytics YOLOv8 est\u00e1 dise\u00f1ado para un entrenamiento efectivo y eficiente de modelos de detecci\u00f3n de objetos, aprovechando al m\u00e1ximo las capacidades del hardware moderno. Esta gu\u00eda tiene como objetivo cubrir todos los detalles que necesita para comenzar a entrenar sus propios modelos utilizando el robusto conjunto de caracter\u00edsticas de YOLOv8.</p> <p> Ver: C\u00f3mo Entrenar un modelo YOLOv8 en Tu Conjunto de Datos Personalizado en Google Colab. </p>"},{"location":"modes/train/#por-que-elegir-ultralytics-yolo-para-entrenamiento","title":"\u00bfPor Qu\u00e9 Elegir Ultralytics YOLO para Entrenamiento?","text":"<p>Aqu\u00ed hay algunas razones convincentes para optar por el modo Entrenamiento de YOLOv8:</p> <ul> <li>Eficiencia: Aprovecha al m\u00e1ximo tu hardware, ya sea en una configuraci\u00f3n de una sola GPU o escalando entre m\u00faltiples GPUs.</li> <li>Versatilidad: Entrena con conjuntos de datos personalizados adem\u00e1s de los ya disponibles como COCO, VOC e ImageNet.</li> <li>Amigable al Usuario: Interfaces CLI y Python simples pero potentes para una experiencia de entrenamiento sencilla.</li> <li>Flexibilidad de Hiperpar\u00e1metros: Una amplia gama de hiperpar\u00e1metros personalizables para ajustar el rendimiento del modelo.</li> </ul>"},{"location":"modes/train/#caracteristicas-clave-del-modo-entrenamiento","title":"Caracter\u00edsticas Clave del Modo Entrenamiento","text":"<p>Las siguientes son algunas caracter\u00edsticas notables del modo Entrenamiento de YOLOv8:</p> <ul> <li>Descarga Autom\u00e1tica de Conjuntos de Datos: Conjuntos de datos est\u00e1ndar como COCO, VOC e ImageNet se descargan autom\u00e1ticamente en el primer uso.</li> <li>Soporte Multi-GPU: Escala tus esfuerzos de entrenamiento sin problemas en m\u00faltiples GPUs para acelerar el proceso.</li> <li>Configuraci\u00f3n de Hiperpar\u00e1metros: La opci\u00f3n de modificar hiperpar\u00e1metros a trav\u00e9s de archivos de configuraci\u00f3n YAML o argumentos CLI.</li> <li>Visualizaci\u00f3n y Monitoreo: Seguimiento en tiempo real de m\u00e9tricas de entrenamiento y visualizaci\u00f3n del proceso de aprendizaje para una mejor comprensi\u00f3n.</li> </ul> <p>Consejo</p> <ul> <li>Los conjuntos de datos de YOLOv8 como COCO, VOC, ImageNet y muchos otros se descargan autom\u00e1ticamente en el primer uso, es decir, <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Entrena YOLOv8n en el conjunto de datos COCO128 durante 100 \u00e9pocas con un tama\u00f1o de imagen de 640. El dispositivo de entrenamiento se puede especificar usando el argumento <code>device</code>. Si no se pasa ning\u00fan argumento, se usar\u00e1 la GPU <code>device=0</code> si est\u00e1 disponible; de lo contrario, se usar\u00e1 <code>device=cpu</code>. Consulta la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de entrenamiento.</p> <p>Ejemplo de Entrenamiento con una sola GPU y CPU</p> <p>El dispositivo se determina autom\u00e1ticamente. Si hay una GPU disponible, se usar\u00e1; de lo contrario, el entrenamiento comenzar\u00e1 en la CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.yaml')  # construir un modelo nuevo desde YAML\nmodel = YOLO('yolov8n.pt')    # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un modelo nuevo desde YAML y comenzar el entrenamiento desde cero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Comenzar el entrenamiento desde un modelo preentrenado *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir un modelo nuevo desde YAML, transferir pesos preentrenados a \u00e9l y comenzar el entrenamiento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#entrenamiento-multi-gpu","title":"Entrenamiento Multi-GPU","text":"<p>El entrenamiento Multi-GPU permite una utilizaci\u00f3n m\u00e1s eficiente de los recursos de hardware disponibles, distribuyendo la carga de entrenamiento en varias GPUs. Esta caracter\u00edstica est\u00e1 disponible tanto a trav\u00e9s de la API de Python como de la interfaz de l\u00ednea de comandos. Para habilitar el entrenamiento Multi-GPU, especifica los IDs de los dispositivos GPU que deseas usar.</p> <p>Ejemplo de Entrenamiento Multi-GPU</p> <p>Para entrenar con 2 GPUs, dispositivos CUDA 0 y 1, usa los siguientes comandos. Ampl\u00eda a GPUs adicionales seg\u00fan sea necesario.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\n\n# Entrenar el modelo con 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Comenzar el entrenamiento desde un modelo preentrenado *.pt usando las GPUs 0 y 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#entrenamiento-con-apple-m1-y-m2-mps","title":"Entrenamiento con Apple M1 y M2 MPS","text":"<p>Con el soporte para los chips Apple M1 y M2 integrados en los modelos Ultralytics YOLO, ahora es posible entrenar tus modelos en dispositivos que utilizan el potente marco de Metal Performance Shaders (MPS). El MPS ofrece una forma de alto rendimiento para ejecutar tareas de c\u00e1lculo y procesamiento de im\u00e1genes en el silicio personalizado de Apple.</p> <p>Para habilitar el entrenamiento en chips Apple M1 y M2, debes especificar 'mps' como tu dispositivo al iniciar el proceso de entrenamiento. A continuaci\u00f3n se muestra un ejemplo de c\u00f3mo podr\u00edas hacer esto en Python y a trav\u00e9s de la l\u00ednea de comandos:</p> <p>Ejemplo de Entrenamiento MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\n\n# Entrenar el modelo con 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Comenzar el entrenamiento desde un modelo preentrenado *.pt usando las GPUs 0 y 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Al aprovechar el poder computacional de los chips M1/M2, esto permite un procesamiento m\u00e1s eficiente de las tareas de entrenamiento. Para obtener una gu\u00eda m\u00e1s detallada y opciones de configuraci\u00f3n avanzadas, consulta la documentaci\u00f3n de PyTorch MPS.</p>"},{"location":"modes/train/#registros-logging","title":"Registros (Logging)","text":"<p>Al entrenar un modelo YOLOv8, puedes encontrar valioso llevar un registro del rendimiento del modelo con el tiempo. Aqu\u00ed es donde entra en juego el registro. Ultralytics' YOLO ofrece soporte para tres tipos de registradores: Comet, ClearML y TensorBoard.</p> <p>Para usar un registrador, selecci\u00f3nalo en el men\u00fa desplegable en el fragmento de c\u00f3digo anterior y ejec\u00fatalo. El registrador elegido se instalar\u00e1 e inicializar\u00e1.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet es una plataforma que permite a los cient\u00edficos de datos y desarrolladores rastrear, comparar, explicar y optimizar experimentos y modelos. Ofrece funcionalidades como m\u00e9tricas en tiempo real, diferencias de c\u00f3digo y seguimiento de hiperpar\u00e1metros.</p> <p>Para usar Comet:</p> <p>Ejemplo</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Recuerda iniciar sesi\u00f3n en tu cuenta de Comet en su sitio web y obtener tu clave API. Necesitar\u00e1s agregar esto a tus variables de entorno o tu script para registrar tus experimentos.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML es una plataforma de c\u00f3digo abierto que automatiza el seguimiento de experimentos y ayuda con la compartici\u00f3n eficiente de recursos. Est\u00e1 dise\u00f1ado para ayudar a los equipos a gestionar, ejecutar y reproducir su trabajo de ML de manera m\u00e1s eficiente.</p> <p>Para usar ClearML:</p> <p>Ejemplo</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Despu\u00e9s de ejecutar este script, necesitar\u00e1s iniciar sesi\u00f3n en tu cuenta de ClearML en el navegador y autenticar tu sesi\u00f3n.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard es una herramienta de visualizaci\u00f3n para TensorFlow. Te permite visualizar tu grafo TensorFlow, trazar m\u00e9tricas cuantitativas sobre la ejecuci\u00f3n de tu grafo y mostrar datos adicionales como im\u00e1genes que lo atraviesan.</p> <p>Para usar TensorBoard en Google Colab:</p> <p>Ejemplo</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # reemplazar con el directorio 'runs'\n</code></pre> <p>Para usar TensorBoard localmente, ejecuta el siguiente comando y visualiza los resultados en http://localhost:6006/.</p> <p>Ejemplo</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # reemplazar con el directorio 'runs'\n</code></pre> <p>Esto cargar\u00e1 TensorBoard y lo dirigir\u00e1 al directorio donde se guardan tus registros de entrenamiento.</p> <p>Despu\u00e9s de configurar tu registrador, puedes proceder con tu entrenamiento de modelo. Todas las m\u00e9tricas de entrenamiento se registrar\u00e1n autom\u00e1ticamente en la plataforma elegida y podr\u00e1s acceder a estos registros para monitorear el rendimiento de tu modelo con el tiempo, comparar diferentes modelos e identificar \u00e1reas de mejora.</p>"},{"location":"modes/val/","title":"Validaci\u00f3n de modelos con Ultralytics YOLO","text":""},{"location":"modes/val/#introduccion","title":"Introducci\u00f3n","text":"<p>La validaci\u00f3n es un paso cr\u00edtico en el flujo de trabajo de aprendizaje autom\u00e1tico, permiti\u00e9ndole evaluar la calidad de sus modelos entrenados. El modo Val en Ultralytics YOLOv8 proporciona un robusto conjunto de herramientas y m\u00e9tricas para evaluar el rendimiento de sus modelos de detecci\u00f3n de objetos. Esta gu\u00eda sirve como un recurso completo para comprender c\u00f3mo utilizar efectivamente el modo Val para asegurar que sus modelos sean precisos y confiables.</p>"},{"location":"modes/val/#por-que-validar-con-ultralytics-yolo","title":"\u00bfPor qu\u00e9 validar con Ultralytics YOLO?","text":"<p>Estas son las ventajas de usar el modo Val de YOLOv8:</p> <ul> <li>Precisi\u00f3n: Obtenga m\u00e9tricas precisas como mAP50, mAP75 y mAP50-95 para evaluar de manera integral su modelo.</li> <li>Comodidad: Utilice funciones integradas que recuerdan los ajustes de entrenamiento, simplificando el proceso de validaci\u00f3n.</li> <li>Flexibilidad: Valide su modelo con el mismo conjunto de datos o diferentes conjuntos de datos y tama\u00f1os de imagen.</li> <li>Ajuste de Hiperpar\u00e1metros: Use las m\u00e9tricas de validaci\u00f3n para ajustar su modelo y mejorar el rendimiento.</li> </ul>"},{"location":"modes/val/#caracteristicas-principales-del-modo-val","title":"Caracter\u00edsticas principales del modo Val","text":"<p>Estas son las funcionalidades notables ofrecidas por el modo Val de YOLOv8:</p> <ul> <li>Configuraciones Automatizadas: Los modelos recuerdan sus configuraciones de entrenamiento para una validaci\u00f3n sencilla.</li> <li>Soporte de M\u00faltiples M\u00e9tricas: Eval\u00fae su modelo basado en una gama de m\u00e9tricas de precisi\u00f3n.</li> <li>CLI y API de Python: Elija entre la interfaz de l\u00ednea de comandos o API de Python basada en su preferencia para validaci\u00f3n.</li> <li>Compatibilidad de Datos: Funciona sin problemas con conjuntos de datos utilizados durante la fase de entrenamiento as\u00ed como con conjuntos de datos personalizados.</li> </ul> <p>Consejo</p> <ul> <li>Los modelos YOLOv8 recuerdan autom\u00e1ticamente sus ajustes de entrenamiento, as\u00ed que puede validar un modelo en el mismo tama\u00f1o de imagen y en el conjunto de datos original f\u00e1cilmente con solo <code>yolo val model=yolov8n.pt</code> o <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Valide la precisi\u00f3n del modelo YOLOv8n entrenado en el conjunto de datos COCO128. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo. Vea la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y ajustes se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista que contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val model oficial\nyolo detect val model=ruta/a/best.pt  # val model personalizado\n</code></pre>"},{"location":"modes/val/#argumentos","title":"Argumentos","text":"<p>Los ajustes de validaci\u00f3n para modelos YOLO se refieren a los diversos hiperpar\u00e1metros y configuraciones utilizados para evaluar el rendimiento del modelo en un conjunto de datos de validaci\u00f3n. Estos ajustes pueden afectar el rendimiento, la velocidad y la precisi\u00f3n del modelo. Algunos ajustes comunes de validaci\u00f3n YOLO incluyen el tama\u00f1o del lote, la frecuencia con la que se realiza la validaci\u00f3n durante el entrenamiento y las m\u00e9tricas utilizadas para evaluar el rendimiento del modelo. Otros factores que pueden afectar el proceso de validaci\u00f3n incluyen el tama\u00f1o y la composici\u00f3n del conjunto de datos de validaci\u00f3n y la tarea espec\u00edfica para la que se utiliza el modelo. Es importante ajustar y experimentar cuidadosamente con estos ajustes para asegurarse de que el modelo est\u00e9 funcionando bien en el conjunto de datos de validaci\u00f3n y para detectar y prevenir el sobreajuste.</p> Clave Valor Descripci\u00f3n <code>data</code> <code>None</code> ruta al archivo de datos, por ejemplo coco128.yaml <code>imgsz</code> <code>640</code> tama\u00f1o de las im\u00e1genes de entrada como entero <code>batch</code> <code>16</code> n\u00famero de im\u00e1genes por lote (-1 para AutoBatch) <code>save_json</code> <code>False</code> guardar resultados en archivo JSON <code>save_hybrid</code> <code>False</code> guardar versi\u00f3n h\u00edbrida de las etiquetas (etiquetas + predicciones adicionales) <code>conf</code> <code>0.001</code> umbral de confianza del objeto para detecci\u00f3n <code>iou</code> <code>0.6</code> umbral de Intersecci\u00f3n sobre Uni\u00f3n (IoU) para NMS <code>max_det</code> <code>300</code> n\u00famero m\u00e1ximo de detecciones por imagen <code>half</code> <code>True</code> usar precisi\u00f3n de punto flotante de media preci\u00f3n (FP16) <code>device</code> <code>None</code> dispositivo en el que se ejecuta, por ejemplo dispositivo cuda=0/1/2/3 o dispositivo=cpu <code>dnn</code> <code>False</code> utilizar OpenCV DNN para inferencia ONNX <code>plots</code> <code>False</code> mostrar gr\u00e1ficos durante el entrenamiento <code>rect</code> <code>False</code> val rectangular con cada lote compilado para el m\u00ednimo relleno <code>split</code> <code>val</code> divisi\u00f3n del conjunto de datos a utilizar para la validaci\u00f3n, por ejemplo 'val', 'test' o 'train'"},{"location":"tasks/","title":"Tareas de Ultralytics YOLOv8","text":"<p>YOLOv8 es un marco de trabajo de IA que soporta m\u00faltiples tareas de visi\u00f3n por computadora. El marco puede usarse para realizar detecci\u00f3n, segmentaci\u00f3n, clasificaci\u00f3n y estimaci\u00f3n de pose. Cada una de estas tareas tiene un objetivo y caso de uso diferente.</p> <p>Note</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n multilenguaje est\u00e1 actualmente en construcci\u00f3n y estamos trabajando arduamente para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p> <p> Mire: Explore las Tareas de Ultralytics YOLO: Detecci\u00f3n de Objetos, Segmentaci\u00f3n, Seguimiento y Estimaci\u00f3n de Pose. </p>"},{"location":"tasks/#deteccion","title":"Detecci\u00f3n","text":"<p>La detecci\u00f3n es la tarea principal soportada por YOLOv8. Implica detectar objetos en una imagen o cuadro de video y dibujar cuadros delimitadores alrededor de ellos. Los objetos detectados se clasifican en diferentes categor\u00edas basadas en sus caracter\u00edsticas. YOLOv8 puede detectar m\u00faltiples objetos en una sola imagen o cuadro de video con alta precisi\u00f3n y velocidad.</p> <p>Ejemplos de Detecci\u00f3n</p>"},{"location":"tasks/#segmentacion","title":"Segmentaci\u00f3n","text":"<p>La segmentaci\u00f3n es una tarea que implica segmentar una imagen en diferentes regiones basadas en el contenido de la imagen. A cada regi\u00f3n se le asigna una etiqueta basada en su contenido. Esta tarea es \u00fatil en aplicaciones tales como segmentaci\u00f3n de im\u00e1genes y im\u00e1genes m\u00e9dicas. YOLOv8 utiliza una variante de la arquitectura U-Net para realizar la segmentaci\u00f3n.</p> <p>Ejemplos de Segmentaci\u00f3n</p>"},{"location":"tasks/#clasificacion","title":"Clasificaci\u00f3n","text":"<p>La clasificaci\u00f3n es una tarea que implica clasificar una imagen en diferentes categor\u00edas. YOLOv8 puede usarse para clasificar im\u00e1genes basadas en su contenido. Utiliza una variante de la arquitectura EfficientNet para realizar la clasificaci\u00f3n.</p> <p>Ejemplos de Clasificaci\u00f3n</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>La detecci\u00f3n de pose/puntos clave es una tarea que implica detectar puntos espec\u00edficos en una imagen o cuadro de video. Estos puntos se conocen como puntos clave y se utilizan para rastrear el movimiento o la estimaci\u00f3n de la pose. YOLOv8 puede detectar puntos clave en una imagen o cuadro de video con alta precisi\u00f3n y velocidad.</p> <p>Ejemplos de Pose</p>"},{"location":"tasks/#conclusion","title":"Conclusi\u00f3n","text":"<p>YOLOv8 soporta m\u00faltiples tareas, incluyendo detecci\u00f3n, segmentaci\u00f3n, clasificaci\u00f3n y detecci\u00f3n de puntos clave. Cada una de estas tareas tiene diferentes objetivos y casos de uso. Al entender las diferencias entre estas tareas, puede elegir la tarea adecuada para su aplicaci\u00f3n de visi\u00f3n por computadora.</p>"},{"location":"tasks/classify/","title":"Clasificaci\u00f3n de Im\u00e1genes","text":"<p>La clasificaci\u00f3n de im\u00e1genes es la tarea m\u00e1s sencilla de las tres y consiste en clasificar una imagen completa en una de un conjunto de clases predefinidas.</p> <p>La salida de un clasificador de im\u00e1genes es una \u00fanica etiqueta de clase y una puntuaci\u00f3n de confianza. La clasificaci\u00f3n de im\u00e1genes es \u00fatil cuando solo necesita saber a qu\u00e9 clase pertenece una imagen y no necesita conocer d\u00f3nde est\u00e1n ubicados los objetos de esa clase o cu\u00e1l es su forma exacta.</p> <p>Consejo</p> <p>Los modelos YOLOv8 Classify utilizan el sufijo <code>-cls</code>, por ejemplo, <code>yolov8n-cls.pt</code> y est\u00e1n preentrenados en ImageNet.</p>"},{"location":"tasks/classify/#modelos","title":"Modelos","text":"<p>Los modelos Classify preentrenados YOLOv8 se muestran aqu\u00ed. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics en el primer uso.</p> Modelo Tama\u00f1o<sup>(p\u00edxeles) Exactitud<sup>top1 Exactitud<sup>top5 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) Par\u00e1metros<sup>(M) FLOPs<sup>(B) en 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Los valores de Exactitud son las precisiones de los modelos en el conjunto de datos de validaci\u00f3n de ImageNet.   Para reproducir usar <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Velocidad promediada sobre im\u00e1genes de validaci\u00f3n de ImageNet usando una instancia de Amazon EC2 P4d Para reproducir usar <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena el modelo YOLOv8n-cls en el conjunto de datos MNIST160 durante 100 \u00e9pocas con un tama\u00f1o de imagen de 64. Para obtener una lista completa de argumentos disponibles, consulte la p\u00e1gina de Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y empezar entrenamiento desde cero\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Empezar entrenamiento desde un modelo *.pt preentrenado\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados e iniciar entrenamiento\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de clasificaci\u00f3n YOLO puede encontrarse en detalle en la Gu\u00eda de Conjuntos de Datos.</p>"},{"location":"tasks/classify/#validacion","title":"Validaci\u00f3n","text":"<p>Validar la exactitud del modelo YOLOv8n-cls entrenado en el conjunto de datos MNIST160. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene su <code>data</code> y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.top1   # precisi\u00f3n top1\nmetrics.top5   # precisi\u00f3n top5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # validar modelo oficial\nyolo classify val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/classify/#prediccion","title":"Predicci\u00f3n","text":"<p>Usar un modelo YOLOv8n-cls entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Ver detalles completos del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/classify/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exportar un modelo YOLOv8n-cls a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n disponibles para YOLOv8-cls se encuentran en la tabla a continuaci\u00f3n. Puede predecir o validar directamente en modelos exportados, por ejemplo, <code>yolo predict model=yolov8n-cls.onnx</code>. Ejemplos de uso se muestran para su modelo despu\u00e9s de que se completa la exportaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vea detalles completos de <code>exportaci\u00f3n</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"},{"location":"tasks/detect/","title":"Detecci\u00f3n de Objetos","text":"<p>La detecci\u00f3n de objetos es una tarea que implica identificar la ubicaci\u00f3n y clase de objetos en una imagen o flujo de video.</p> <p>La salida de un detector de objetos es un conjunto de cajas delimitadoras que encierran a los objetos en la imagen, junto con etiquetas de clase y puntajes de confianza para cada caja. La detecci\u00f3n de objetos es una buena opci\u00f3n cuando necesitas identificar objetos de inter\u00e9s en una escena, pero no necesitas saber exactamente d\u00f3nde se encuentra el objeto o su forma exacta.</p> <p> Ver: Detecci\u00f3n de Objetos con Modelo Preentrenado YOLOv8 de Ultralytics. </p> <p>Consejo</p> <p>Los modelos YOLOv8 Detect son los modelos predeterminados de YOLOv8, es decir, <code>yolov8n.pt</code> y est\u00e1n preentrenados en COCO.</p>"},{"location":"tasks/detect/#modelos","title":"Modelos","text":"<p>Los modelos preentrenados de YOLOv8 Detect se muestran aqu\u00ed. Los modelos de Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos de Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en el primer uso.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>val50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Los valores de mAP<sup>val</sup> son para un solo modelo a una sola escala en el conjunto de datos COCO val2017.   Reproduce utilizando <code>yolo val detect data=coco.yaml device=0</code></li> <li>La Velocidad es el promedio sobre las im\u00e1genes de COCO val utilizando una instancia Amazon EC2 P4d.   Reproduce utilizando <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena a YOLOv8n en el conjunto de datos COCO128 durante 100 \u00e9pocas a tama\u00f1o de imagen 640. Para una lista completa de argumentos disponibles, consulta la p\u00e1gina Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.yaml')  # construye un nuevo modelo desde YAML\nmodel = YOLO('yolov8n.pt')  # carga un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construye desde YAML y transfiere los pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar entrenamiento desde cero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Comenzar entrenamiento desde un modelo *.pt preentrenado\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar entrenamiento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de detecci\u00f3n de YOLO se puede encontrar en detalle en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente desde otros formatos (como COCO, etc.) al formato YOLO, por favor usa la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/detect/#validacion","title":"Validaci\u00f3n","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n entrenado en el conjunto de datos COCO128. No es necesario pasar ning\u00fan argumento, ya que el <code>modelo</code> retiene sus datos de <code>entrenamiento</code> y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # sin argumentos necesarios, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # validar modelo oficial\nyolo detect val model=ruta/a/mejor.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/detect/#prediccion","title":"Predicci\u00f3n","text":"<p>Utiliza un modelo YOLOv8n entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo detect predict model=ruta/a/mejor.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Consulta los detalles completos del modo <code>predict</code> en la p\u00e1gina Predicci\u00f3n.</p>"},{"location":"tasks/detect/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exporta un modelo YOLOv8n a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exportar modelo oficial\nyolo export model=ruta/a/mejor.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n de YOLOv8 disponibles se encuentran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, es decir, <code>yolo predict model=yolov8n.onnx</code>. Ejemplos de uso se muestran para tu modelo despu\u00e9s de que la exportaci\u00f3n se completa.</p> Formato Argumento <code>format</code> Modelo Metadata Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimizar</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>din\u00e1mico</code>, <code>simplificar</code>, <code>conjunto de operaciones</code> OpenVINO <code>openvino</code> <code>modelo_yolov8n_openvino/</code> \u2705 <code>imgsz</code>, <code>mitad</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>din\u00e1mico</code>, <code>simplificar</code>, <code>espacio de trabajo</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>modelo_guardado_yolov8n/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>modelo_web_yolov8n/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>modelo_yolov8n_paddle/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>modelo_ncnn_yolov8n/</code> \u2705 <code>imgsz</code>, <code>mitad</code> <p>Consulta los detalles completos de la <code>exportaci\u00f3n</code> en la p\u00e1gina Exportar.</p>"},{"location":"tasks/pose/","title":"Estimaci\u00f3n de Pose","text":"<p>La estimaci\u00f3n de pose es una tarea que implica identificar la ubicaci\u00f3n de puntos espec\u00edficos en una imagen, com\u00fanmente referidos como puntos clave. Estos puntos clave pueden representar varias partes del objeto, como articulaciones, puntos de referencia u otras caracter\u00edsticas distintivas. La ubicaci\u00f3n de los puntos clave generalmente se representa como un conjunto de coordenadas 2D <code>[x, y]</code> o 3D <code>[x, y, visible]</code>.</p> <p>La salida de un modelo de estimaci\u00f3n de pose es un conjunto de puntos que representan los puntos clave en un objeto de la imagen, generalmente junto con las puntuaciones de confianza para cada punto. La estimaci\u00f3n de pose es una buena opci\u00f3n cuando se necesita identificar partes espec\u00edficas de un objeto en una escena y su ubicaci\u00f3n relativa entre ellas.</p> <p> Ver: Estimaci\u00f3n de Pose con Ultralytics YOLOv8. </p> <p>Consejo</p> <p>Los modelos pose YOLOv8 utilizan el sufijo <code>-pose</code>, por ejemplo, <code>yolov8n-pose.pt</code>. Estos modelos est\u00e1n entrenados en el conjunto de datos COCO keypoints y son adecuados para una variedad de tareas de estimaci\u00f3n de pose.</p>"},{"location":"tasks/pose/#modelos","title":"Modelos","text":"<p>Aqu\u00ed se muestran los modelos preentrenados de YOLOv8 Pose. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en el primer uso.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>pose50-95 mAP<sup>pose50 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>Los valores de mAP<sup>val</sup> son para un solo modelo a una sola escala en el conjunto de datos COCO Keypoints val2017.   Reproducir con <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>Velocidad promediada sobre im\u00e1genes COCO val usando una instancia Amazon EC2 P4d.   Reproducir con <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#entrenar","title":"Entrenar","text":"<p>Entrena un modelo YOLOv8-pose en el conjunto de datos COCO128-pose.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo preentrenado (recomendado para entrenar)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construir desde YAML y transferir los pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar entrenamiento desde cero\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Empezar entrenamiento desde un modelo *.pt preentrenado\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar entrenamiento\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de pose de YOLO se puede encontrar en detalle en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente de otros formatos (como COCO, etc.) al formato de YOLO, usa la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/pose/#validar","title":"Validar","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n-pose entrenado en el conjunto de datos COCO128-pose. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> mantiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # modelo oficial de val\nyolo pose val model=path/to/best.pt  # modelo personalizado de val\n</code></pre>"},{"location":"tasks/pose/#predecir","title":"Predecir","text":"<p>Usa un modelo YOLOv8n-pose entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo pose predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Consulta los detalles completos del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/pose/#exportar","title":"Exportar","text":"<p>Exporta un modelo YOLOv8n Pose a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n de YOLOv8-pose disponibles se muestran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, por ejemplo, <code>yolo predict model=yolov8n-pose.onnx</code>. Los ejemplos de uso se muestran para tu modelo despu\u00e9s de que la exportaci\u00f3n se completa.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>din\u00e1mico</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>din\u00e1mico</code>, <code>simplify</code>, <code>espacio de trabajo</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consulta los detalles completos del modo <code>export</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"},{"location":"tasks/segment/","title":"Segmentaci\u00f3n de Instancias","text":"<p>La segmentaci\u00f3n de instancias va un paso m\u00e1s all\u00e1 de la detecci\u00f3n de objetos e implica identificar objetos individuales en una imagen y segmentarlos del resto de la imagen.</p> <p>La salida de un modelo de segmentaci\u00f3n de instancias es un conjunto de m\u00e1scaras o contornos que delimitan cada objeto en la imagen, junto con etiquetas de clase y puntajes de confianza para cada objeto. La segmentaci\u00f3n de instancias es \u00fatil cuando necesitas saber no solo d\u00f3nde est\u00e1n los objetos en una imagen, sino tambi\u00e9n cu\u00e1l es su forma exacta.</p> <p> Mira: Ejecuta la Segmentaci\u00f3n con el Modelo Ultralytics YOLOv8 Preentrenado en Python. </p> <p>Consejo</p> <p>Los modelos YOLOv8 Segment utilizan el sufijo <code>-seg</code>, es decir, <code>yolov8n-seg.pt</code> y est\u00e1n preentrenados en el COCO.</p>"},{"location":"tasks/segment/#modelos","title":"Modelos","text":"<p>Aqu\u00ed se muestran los modelos Segment preentrenados YOLOv8. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los Modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en su primer uso.</p> Modelo Tama\u00f1o<sup>(p\u00edxeles) mAP<sup>caja50-95 mAP<sup>m\u00e1scara50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) Par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Los valores mAP<sup>val</sup> son para un \u00fanico modelo a una \u00fanica escala en el conjunto de datos COCO val2017.   Reproducir utilizando <code>yolo val segment data=coco.yaml device=0</code></li> <li>La Velocidad promediada sobre im\u00e1genes de COCO val utilizando una instancia de Amazon EC2 P4d.   Reproducir utilizando <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena el modelo YOLOv8n-seg en el conjunto de datos COCO128-seg durante 100 \u00e9pocas con tama\u00f1o de imagen de 640. Para una lista completa de argumentos disponibles, consulta la p\u00e1gina de Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar a entrenar desde cero\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Comenzar a entrenar desde un modelo *.pt preentrenado\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar a entrenar\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de segmentaci\u00f3n YOLO puede encontrarse detallado en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente de otros formatos (como COCO, etc.) al formato YOLO, utiliza la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/segment/#validacion","title":"Validaci\u00f3n","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n-seg entrenado en el conjunto de datos COCO128-seg. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # una lista contiene map50-95(B) de cada categor\u00eda\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # una lista contiene map50-95(M) de cada categor\u00eda\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # validar el modelo oficial\nyolo segment val model=ruta/a/mejor.pt  # validar el modelo personalizado\n</code></pre>"},{"location":"tasks/segment/#prediccion","title":"Predicci\u00f3n","text":"<p>Usa un modelo YOLOv8n-seg entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con el modelo oficial\nyolo segment predict model=ruta/a/mejor.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con el modelo personalizado\n</code></pre> <p>Consulta todos los detalles del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/segment/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exporta un modelo YOLOv8n-seg a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exportar el modelo oficial\nyolo export model=ruta/a/mejor.pt format=onnx  # exportar el modelo entrenado personalizado\n</code></pre> <p>Los formatos disponibles para exportar YOLOv8-seg se muestran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, es decir, <code>yolo predict model=yolov8n-seg.onnx</code>. Se muestran ejemplos de uso para tu modelo despu\u00e9s de que se completa la exportaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consulta todos los detalles del modo <code>export</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"}]}