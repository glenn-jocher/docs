{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Inicio","text":"<p>Presentamos Ultralytics YOLOv8, la \u00faltima versi\u00f3n del aclamado modelo para detecci\u00f3n de objetos y segmentaci\u00f3n de im\u00e1genes en tiempo real. YOLOv8 est\u00e1 construido sobre avances de vanguardia en aprendizaje profundo y visi\u00f3n por computadora, ofreciendo un rendimiento sin paralelo en t\u00e9rminos de velocidad y precisi\u00f3n. Su dise\u00f1o simplificado lo hace adecuado para varias aplicaciones y f\u00e1cilmente adaptable a diferentes plataformas de hardware, desde dispositivos de borde hasta API en la nube.</p> <p>Explore los documentos de YOLOv8, un recurso integral dise\u00f1ado para ayudarle a comprender y utilizar sus caracter\u00edsticas y capacidades. Independientemente de que sea un practicante experimentado en aprendizaje autom\u00e1tico o nuevo en el campo, este centro tiene como objetivo maximizar el potencial de YOLOv8 en sus proyectos.</p> <p>Nota</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n en varios idiomas est\u00e1 actualmente en construcci\u00f3n y estamos trabajando duro para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p>"},{"location":"#donde-empezar","title":"D\u00f3nde empezar","text":"<ul> <li>Instalar <code>ultralytics</code> con pip y comenzar a funcionar en minutos \u00a0  Comenzar</li> <li>Predecir nuevas im\u00e1genes y videos con YOLOv8 \u00a0  Predecir en Im\u00e1genes</li> <li>Entrenar un nuevo modelo YOLOv8 en su propio conjunto de datos personalizado \u00a0  Entrenar un Modelo</li> <li>Explorar tareas de YOLOv8 como segmentar, clasificar, posar y seguir \u00a0  Explorar Tareas</li> </ul> <p> Ver: C\u00f3mo entrenar un modelo YOLOv8 en Su Conjunto de Datos Personalizado en Google Colab. </p>"},{"location":"#yolo-una-breve-historia","title":"YOLO: Una Breve Historia","text":"<p>YOLO (You Only Look Once), un modelo popular de detecci\u00f3n de objetos y segmentaci\u00f3n de im\u00e1genes, fue desarrollado por Joseph Redmon y Ali Farhadi en la Universidad de Washington. Lanzado en 2015, YOLO r\u00e1pidamente gan\u00f3 popularidad por su alta velocidad y precisi\u00f3n.</p> <ul> <li>YOLOv2, lanzado en 2016, mejor\u00f3 el modelo original incorporando normalizaci\u00f3n por lotes, cajas ancla y cl\u00fasteres de dimensiones.</li> <li>YOLOv3, lanzado en 2018, mejor\u00f3 a\u00fan m\u00e1s el rendimiento del modelo usando una red dorsal m\u00e1s eficiente, m\u00faltiples anclas y agrupaci\u00f3n piramidal espacial.</li> <li>YOLOv4 fue lanzado en 2020, introduciendo innovaciones como la ampliaci\u00f3n de datos del mosaico, un nuevo cabezal de detecci\u00f3n sin ancla y una nueva funci\u00f3n de p\u00e9rdida.</li> <li>YOLOv5 mejor\u00f3 a\u00fan m\u00e1s el rendimiento del modelo y agreg\u00f3 nuevas caracter\u00edsticas como la optimizaci\u00f3n de hiperpar\u00e1metros, seguimiento de experimentos integrados y exportaci\u00f3n autom\u00e1tica a formatos de exportaci\u00f3n populares.</li> <li>YOLOv6 fue publicado en c\u00f3digo abierto por Meituan en 2022 y se utiliza en muchos de los robots de entrega aut\u00f3nomos de la empresa.</li> <li>YOLOv7 a\u00f1adi\u00f3 tareas adicionales como la estimaci\u00f3n de posturas en el conjunto de datos COCO keypoints.</li> <li>YOLOv8 es la \u00faltima versi\u00f3n de YOLO de Ultralytics. Como un modelo de vanguardia y del estado del arte (SOTA), YOLOv8 se basa en el \u00e9xito de las versiones anteriores, introduciendo nuevas caracter\u00edsticas y mejoras para obtener un rendimiento mejorado, flexibilidad y eficiencia. YOLOv8 soporta una gama completa de tareas de IA de visi\u00f3n, incluyendo detecci\u00f3n, segmentaci\u00f3n, estimaci\u00f3n de pose, seguimiento y clasificaci\u00f3n. Esta versatilidad permite a los usuarios aprovechar las capacidades de YOLOv8 en una amplia gama de aplicaciones y dominios.</li> </ul>"},{"location":"#licencias-de-yolo-como-estan-licenciados-los-yolo-de-ultralytics","title":"Licencias de YOLO: \u00bfC\u00f3mo est\u00e1n licenciados los YOLO de Ultralytics?","text":"<p>Ultralytics ofrece dos opciones de licencia para acomodar casos de uso diversos:</p> <ul> <li>Licencia AGPL-3.0: Esta licencia de c\u00f3digo abierto aprobada por OSI es ideal para estudiantes y entusiastas, promoviendo la colaboraci\u00f3n abierta y el intercambio de conocimiento. Consulte el archivo LICENSE para obtener m\u00e1s detalles.</li> <li>Licencia Empresarial: Dise\u00f1ada para uso comercial, esta licencia permite la integraci\u00f3n sin problemas de software de Ultralytics y modelos de IA en bienes y servicios comerciales, eludiendo los requisitos de c\u00f3digo abierto de AGPL-3.0. Si su escenario implica la incorporaci\u00f3n de nuestras soluciones en una oferta comercial, p\u00f3ngase en contacto a trav\u00e9s de Licencias de Ultralytics.</li> </ul> <p>Nuestra estrategia de licenciamiento est\u00e1 dise\u00f1ada para asegurar que cualquier mejora a nuestros proyectos de c\u00f3digo abierto se devuelva a la comunidad. Mantenemos los principios del c\u00f3digo abierto cerca de nuestros corazones \u2764\ufe0f, y nuestra misi\u00f3n es garantizar que nuestras contribuciones puedan ser utilizadas y ampliadas de formas que sean beneficiosas para todos.</p>"},{"location":"quickstart/","title":"Inicio r\u00e1pido","text":""},{"location":"quickstart/#instalar-ultralytics","title":"Instalar Ultralytics","text":"<p>Ultralytics ofrece varios m\u00e9todos de instalaci\u00f3n incluyendo pip, conda y Docker. Instala YOLOv8 a trav\u00e9s del paquete <code>ultralytics</code> de pip para la \u00faltima versi\u00f3n estable o clonando el repositorio de GitHub de Ultralytics para obtener la versi\u00f3n m\u00e1s actualizada. Docker se puede utilizar para ejecutar el paquete en un contenedor aislado, evitando la instalaci\u00f3n local.</p> <p>Instalar</p> Instalaci\u00f3n con Pip (recomendado)Instalaci\u00f3n con CondaClonar con Git <p>Instala el paquete <code>ultralytics</code> usando pip o actualiza una instalaci\u00f3n existente ejecutando <code>pip install -U ultralytics</code>. Visita el \u00cdndice de Paquetes de Python (PyPI) para m\u00e1s detalles sobre el paquete <code>ultralytics</code>: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Instalar el paquete ultralytics desde PyPI\npip install ultralytics\n</code></pre> <p>Tambi\u00e9n puedes instalar el paquete <code>ultralytics</code> directamente del repositorio en GitHub. Esto puede ser \u00fatil si quieres la \u00faltima versi\u00f3n de desarrollo. Aseg\u00farate de tener la herramienta de l\u00ednea de comandos Git instalada en tu sistema. El comando <code>@main</code> instala la rama <code>main</code> y puede modificarse a otra rama, es decir, <code>@my-branch</code>, o eliminarse por completo para volver por defecto a la rama <code>main</code>.</p> <pre><code># Instalar el paquete ultralytics desde GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda es un gestor de paquetes alternativo a pip que tambi\u00e9n puede utilizarse para la instalaci\u00f3n. Visita Anaconda para m\u00e1s detalles en https://anaconda.org/conda-forge/ultralytics. El repositorio de paquetes de alimentaci\u00f3n de Ultralytics para actualizar el paquete de conda est\u00e1 en https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Instalar el paquete ultralytics usando conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Nota</p> <p>Si est\u00e1s instalando en un entorno CUDA, la mejor pr\u00e1ctica es instalar <code>ultralytics</code>, <code>pytorch</code> y <code>pytorch-cuda</code> en el mismo comando para permitir que el gestor de paquetes de conda resuelva cualquier conflicto, o en su defecto instalar <code>pytorch-cuda</code> al final para permitir que sobrescriba el paquete espec\u00edfico de CPU <code>pytorch</code> si es necesario. <pre><code># Instalar todos los paquetes juntos usando conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clona el repositorio <code>ultralytics</code> si est\u00e1s interesado en contribuir al desarrollo o deseas experimentar con el c\u00f3digo fuente m\u00e1s reciente. Despu\u00e9s de clonar, navega al directorio e instala el paquete en modo editable <code>-e</code> usando pip. <pre><code># Clonar el repositorio ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navegar al directorio clonado\ncd ultralytics\n\n# Instalar el paquete en modo editable para desarrollo\npip install -e .\n</code></pre></p> <p>Consulta el archivo requirements.txt de <code>ultralytics</code> para ver una lista de dependencias. Ten en cuenta que todos los ejemplos anteriores instalan todas las dependencias requeridas.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Consejo</p> <p>Los requisitos de PyTorch var\u00edan seg\u00fan el sistema operativo y los requisitos de CUDA, por lo que se recomienda instalar primero PyTorch siguiendo las instrucciones en https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#imagen-docker-de-conda","title":"Imagen Docker de Conda","text":"<p>Las im\u00e1genes Docker de Conda de Ultralytics tambi\u00e9n est\u00e1n disponibles en DockerHub. Estas im\u00e1genes est\u00e1n basadas en Miniconda3 y son una manera simple de comenzar a usar <code>ultralytics</code> en un entorno Conda.</p> <pre><code># Establecer el nombre de la imagen como una variable\nt=ultralytics/ultralytics:latest-conda\n\n# Descargar la \u00faltima imagen de ultralytics de Docker Hub\nsudo docker pull $t\n\n# Ejecutar la imagen de ultralytics en un contenedor con soporte para GPU\nsudo docker run -it --ipc=host --gpus all $t  # todas las GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # especificar GPUs\n</code></pre>"},{"location":"quickstart/#usar-ultralytics-con-cli","title":"Usar Ultralytics con CLI","text":"<p>La interfaz de l\u00ednea de comandos (CLI) de Ultralytics permite el uso de comandos simples de una sola l\u00ednea sin la necesidad de un entorno de Python. La CLI no requiere personalizaci\u00f3n ni c\u00f3digo Python. Puedes simplemente ejecutar todas las tareas desde el terminal con el comando <code>yolo</code>. Consulta la Gu\u00eda de CLI para aprender m\u00e1s sobre el uso de YOLOv8 desde la l\u00ednea de comandos.</p> <p>Ejemplo</p> SintaxisEntrenarPredecirValidarExportarEspecial <p>Los comandos <code>yolo</code> de Ultralytics usan la siguiente sintaxis: <pre><code>yolo TAREA MODO ARGUMENTOS\n\nDonde   TAREA (opcional) es uno de [detectar, segmentar, clasificar]\n        MODO (requerido) es uno de [train, val, predict, export, track]\n        ARGUMENTOS (opcionales) son cualquier n\u00famero de pares personalizados 'arg=valor' como 'imgsz=320' que sobrescriben los valores por defecto.\n</code></pre> Ver todos los ARGUMENTOS en la gu\u00eda completa Configuration Guide o con <code>yolo cfg</code></p> <p>Entrenar un modelo de detecci\u00f3n durante 10 \u00e9pocas con una tasa de aprendizaje inicial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predecir un video de YouTube usando un modelo de segmentaci\u00f3n preentrenado con un tama\u00f1o de imagen de 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validar un modelo de detecci\u00f3n preentrenado con un tama\u00f1o de lote de 1 y un tama\u00f1o de imagen de 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportar un modelo de clasificaci\u00f3n YOLOv8n a formato ONNX con un tama\u00f1o de imagen de 224 por 128 (no se requiere TAREA) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Ejecutar comandos especiales para ver la versi\u00f3n, ver configuraciones, ejecutar chequeos y m\u00e1s: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Advertencia</p> <p>Los argumentos deben pasarse como pares <code>arg=valor</code>, separados por un signo igual <code>=</code> y delimitados por espacios <code></code> entre pares. No utilices prefijos de argumentos <code>--</code> ni comas <code>,</code> entre los argumentos.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Gu\u00eda de CLI</p>"},{"location":"quickstart/#usar-ultralytics-con-python","title":"Usar Ultralytics con Python","text":"<p>La interfaz de Python de YOLOv8 permite una integraci\u00f3n perfecta en tus proyectos de Python, facilitando la carga, ejecuci\u00f3n y procesamiento de la salida del modelo. Dise\u00f1ada con sencillez y facilidad de uso en mente, la interfaz de Python permite a los usuarios implementar r\u00e1pidamente la detecci\u00f3n de objetos, segmentaci\u00f3n y clasificaci\u00f3n en sus proyectos. Esto hace que la interfaz de Python de YOLOv8 sea una herramienta invaluable para cualquier persona que busque incorporar estas funcionalidades en sus proyectos de Python.</p> <p>Por ejemplo, los usuarios pueden cargar un modelo, entrenarlo, evaluar su rendimiento en un conjunto de validaci\u00f3n e incluso exportarlo al formato ONNX con solo unas pocas l\u00edneas de c\u00f3digo. Consulta la Gu\u00eda de Python para aprender m\u00e1s sobre el uso de YOLOv8 dentro de tus proyectos de Python.</p> <p>Ejemplo</p> <pre><code>from ultralytics import YOLO\n\n# Crear un nuevo modelo YOLO desde cero\nmodel = YOLO('yolov8n.yaml')\n\n# Cargar un modelo YOLO preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.pt')\n\n# Entrenar el modelo usando el conjunto de datos 'coco128.yaml' durante 3 \u00e9pocas\nresults = model.train(data='coco128.yaml', epochs=3)\n\n# Evaluar el rendimiento del modelo en el conjunto de validaci\u00f3n\nresults = model.val()\n\n# Realizar detecci\u00f3n de objetos en una imagen usando el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')\n\n# Exportar el modelo al formato ONNX\nsuccess = model.export(format='onnx')\n</code></pre> <p>Gu\u00eda de Python</p>"},{"location":"datasets/","title":"Resumen de Conjuntos de Datos","text":"<p>Ultralytics brinda soporte para varios conjuntos de datos para facilitar tareas de visi\u00f3n por computadora como detecci\u00f3n, segmentaci\u00f3n de instancias, estimaci\u00f3n de poses, clasificaci\u00f3n y seguimiento de m\u00faltiples objetos. A continuaci\u00f3n se presenta una lista de los principales conjuntos de datos de Ultralytics, seguido por un resumen de cada tarea de visi\u00f3n por computadora y los respectivos conjuntos de datos.</p> <p>Nota</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n multiling\u00fce est\u00e1 actualmente en construcci\u00f3n y estamos trabajando arduamente para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p>"},{"location":"datasets/#conjuntos-de-datos-de-deteccion","title":"Conjuntos de Datos de Detecci\u00f3n","text":"<p>La detecci\u00f3n de objetos con cuadros delimitadores es una t\u00e9cnica de visi\u00f3n por computadora que implica detectar y localizar objetos en una imagen dibujando un cuadro alrededor de cada objeto.</p> <ul> <li>Argoverse: Un conjunto de datos que contiene datos de seguimiento en 3D y predicci\u00f3n de movimientos en entornos urbanos con anotaciones detalladas.</li> <li>COCO: Un conjunto de datos a gran escala dise\u00f1ado para detecci\u00f3n de objetos, segmentaci\u00f3n y subtitulado con m\u00e1s de 200 mil im\u00e1genes etiquetadas.</li> <li>COCO8: Contiene las primeras 4 im\u00e1genes de COCO train y COCO val, adecuado para pruebas r\u00e1pidas.</li> <li>Global Wheat 2020: Un conjunto de datos de im\u00e1genes de cabezas de trigo recolectadas alrededor del mundo para tareas de detecci\u00f3n y localizaci\u00f3n de objetos.</li> <li>Objects365: Un conjunto de datos a gran escala y de alta calidad para la detecci\u00f3n de objetos con 365 categor\u00edas y m\u00e1s de 600 mil im\u00e1genes anotadas.</li> <li>OpenImagesV7: Un conjunto de datos completo de Google con 1.7 millones de im\u00e1genes de entrenamiento y 42 mil im\u00e1genes de validaci\u00f3n.</li> <li>SKU-110K: Un conjunto de datos que presenta detecci\u00f3n de objetos densa en entornos minoristas con m\u00e1s de 11 mil im\u00e1genes y 1.7 millones de cuadros delimitadores.</li> <li>VisDrone: Un conjunto de datos que contiene datos de detecci\u00f3n de objetos y seguimiento de m\u00faltiples objetos de im\u00e1genes capturadas por drones con m\u00e1s de 10 mil im\u00e1genes y secuencias de video.</li> <li>VOC: El conjunto de datos de Clases de Objetos Visuales de Pascal (VOC) para la detecci\u00f3n de objetos y segmentaci\u00f3n con 20 clases de objetos y m\u00e1s de 11 mil im\u00e1genes.</li> <li>xView: Un conjunto de datos para la detecci\u00f3n de objetos en im\u00e1genes a\u00e9reas con 60 categor\u00edas de objetos y m\u00e1s de un mill\u00f3n de objetos anotados.</li> </ul>"},{"location":"datasets/#conjuntos-de-datos-de-segmentacion-de-instancias","title":"Conjuntos de Datos de Segmentaci\u00f3n de Instancias","text":"<p>La segmentaci\u00f3n de instancias es una t\u00e9cnica de visi\u00f3n por computadora que implica identificar y localizar objetos en una imagen a nivel de p\u00edxel.</p> <ul> <li>COCO: Un conjunto de datos a gran escala dise\u00f1ado para tareas de detecci\u00f3n de objetos, segmentaci\u00f3n y subtitulado con m\u00e1s de 200 mil im\u00e1genes etiquetadas.</li> <li>COCO8-seg: Un conjunto de datos m\u00e1s peque\u00f1o para tareas de segmentaci\u00f3n de instancias, que contiene un subconjunto de 8 im\u00e1genes de COCO con anotaciones de segmentaci\u00f3n.</li> </ul>"},{"location":"datasets/#estimacion-de-poses","title":"Estimaci\u00f3n de Poses","text":"<p>La estimaci\u00f3n de poses es una t\u00e9cnica utilizada para determinar la pose del objeto en relaci\u00f3n con la c\u00e1mara o el sistema de coordenadas del mundo.</p> <ul> <li>COCO: Un conjunto de datos a gran escala con anotaciones de pose humana dise\u00f1ado para tareas de estimaci\u00f3n de poses.</li> <li>COCO8-pose: Un conjunto de datos m\u00e1s peque\u00f1o para tareas de estimaci\u00f3n de poses, que contiene un subconjunto de 8 im\u00e1genes de COCO con anotaciones de pose humana.</li> <li>Tiger-pose: Un conjunto de datos compacto que consiste en 263 im\u00e1genes centradas en tigres, anotadas con 12 puntos clave por tigre para tareas de estimaci\u00f3n de poses.</li> </ul>"},{"location":"datasets/#clasificacion","title":"Clasificaci\u00f3n","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n por computadora que implica categorizar una imagen en una o m\u00e1s clases o categor\u00edas predefinidas basadas en su contenido visual.</p> <ul> <li>Caltech 101: Un conjunto de datos que contiene im\u00e1genes de 101 categor\u00edas de objetos para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>Caltech 256: Una versi\u00f3n extendida de Caltech 101 con 256 categor\u00edas de objetos y im\u00e1genes m\u00e1s desafiantes.</li> <li>CIFAR-10: Un conjunto de datos de 60 mil im\u00e1genes a color de 32x32 en 10 clases, con 6 mil im\u00e1genes por clase.</li> <li>CIFAR-100: Una versi\u00f3n extendida de CIFAR-10 con 100 categor\u00edas de objetos y 600 im\u00e1genes por clase.</li> <li>Fashion-MNIST: Un conjunto de datos compuesto por 70 mil im\u00e1genes en escala de grises de 10 categor\u00edas de moda para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>ImageNet: Un conjunto de datos a gran escala para detecci\u00f3n de objetos y clasificaci\u00f3n de im\u00e1genes con m\u00e1s de 14 millones de im\u00e1genes y 20 mil categor\u00edas.</li> <li>ImageNet-10: Un subconjunto m\u00e1s peque\u00f1o de ImageNet con 10 categor\u00edas para experimentaci\u00f3n y pruebas m\u00e1s r\u00e1pidas.</li> <li>Imagenette: Un subconjunto m\u00e1s peque\u00f1o de ImageNet que contiene 10 clases f\u00e1cilmente distinguibles para entrenamientos y pruebas m\u00e1s r\u00e1pidos.</li> <li>Imagewoof: Un subconjunto m\u00e1s desafiante de ImageNet que contiene 10 categor\u00edas de razas de perros para tareas de clasificaci\u00f3n de im\u00e1genes.</li> <li>MNIST: Un conjunto de datos de 70 mil im\u00e1genes en escala de grises de d\u00edgitos escritos a mano para tareas de clasificaci\u00f3n de im\u00e1genes.</li> </ul>"},{"location":"datasets/#cuadros-delimitadores-orientados-obb","title":"Cuadros Delimitadores Orientados (OBB)","text":"<p>Los Cuadros Delimitadores Orientados (OBB) es un m\u00e9todo en visi\u00f3n por computadora para detectar objetos angulados en im\u00e1genes utilizando cuadros delimitadores rotados, a menudo aplicado en im\u00e1genes a\u00e9reas y satelitales.</p> <ul> <li>DOTAv2: Un popular conjunto de datos de im\u00e1genes a\u00e9reas de OBB con 1.7 millones de instancias y 11,268 im\u00e1genes.</li> </ul>"},{"location":"datasets/#seguimiento-de-multiples-objetos","title":"Seguimiento de M\u00faltiples Objetos","text":"<p>El seguimiento de m\u00faltiples objetos es una t\u00e9cnica de visi\u00f3n por computadora que implica detectar y seguir m\u00faltiples objetos a lo largo del tiempo en una secuencia de video.</p> <ul> <li>Argoverse: Un conjunto de datos que contiene datos de seguimiento en 3D y predicci\u00f3n de movimientos en entornos urbanos con anotaciones detalladas para tareas de seguimiento de m\u00faltiples objetos.</li> <li>VisDrone: Un conjunto de datos que contiene datos de detecci\u00f3n de objetos y seguimiento de m\u00faltiples objetos de im\u00e1genes capturadas por drones con m\u00e1s de 10 mil im\u00e1genes y secuencias de video.</li> </ul>"},{"location":"datasets/#contribuir-con-nuevos-conjuntos-de-datos","title":"Contribuir con Nuevos Conjuntos de Datos","text":"<p>Contribuir con un nuevo conjunto de datos implica varios pasos para garantizar que se alinee bien con la infraestructura existente. A continuaci\u00f3n, se presentan los pasos necesarios:</p>"},{"location":"datasets/#pasos-para-contribuir-con-un-nuevo-conjunto-de-datos","title":"Pasos para Contribuir con un Nuevo Conjunto de Datos","text":"<ol> <li> <p>Recolectar Im\u00e1genes: Re\u00fane las im\u00e1genes que pertenecen al conjunto de datos. Estas podr\u00edan ser recopiladas de varias fuentes, tales como bases de datos p\u00fablicas o tu propia colecci\u00f3n.</p> </li> <li> <p>Annotar Im\u00e1genes: Anota estas im\u00e1genes con cuadros delimitadores, segmentos o puntos clave, dependiendo de la tarea.</p> </li> <li> <p>Exportar Anotaciones: Convierte estas anotaciones en el formato de archivo *.txt de YOLO que Ultralytics soporta.</p> </li> <li> <p>Organizar Conjunto de Datos: Organiza tu conjunto de datos en la estructura de carpetas correcta. Deber\u00edas tener directorios de nivel superior <code>train/</code> y <code>val/</code>, y dentro de cada uno, un subdirectorio <code>images/</code> y <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Crear un Archivo <code>data.yaml</code>: En el directorio ra\u00edz de tu conjunto de datos, crea un archivo <code>data.yaml</code> que describa el conjunto de datos, clases y otra informaci\u00f3n necesaria.</p> </li> <li> <p>Optimizar Im\u00e1genes (Opcional): Si deseas reducir el tama\u00f1o del conjunto de datos para un procesamiento m\u00e1s eficiente, puedes optimizar las im\u00e1genes usando el c\u00f3digo a continuaci\u00f3n. Esto no es requerido, pero se recomienda para tama\u00f1os de conjuntos de datos m\u00e1s peque\u00f1os y velocidades de descarga m\u00e1s r\u00e1pidas.</p> </li> <li> <p>Comprimir Conjunto de Datos: Comprime toda la carpeta del conjunto de datos en un archivo .zip.</p> </li> <li> <p>Documentar y PR: Crea una p\u00e1gina de documentaci\u00f3n describiendo tu conjunto de datos y c\u00f3mo encaja en el marco existente. Despu\u00e9s de eso, env\u00eda una Solicitud de Extracci\u00f3n (PR). Consulta las Pautas de Contribuci\u00f3n de Ultralytics para obtener m\u00e1s detalles sobre c\u00f3mo enviar una PR.</p> </li> </ol>"},{"location":"datasets/#codigo-de-ejemplo-para-optimizar-y-comprimir-un-conjunto-de-datos","title":"C\u00f3digo de Ejemplo para Optimizar y Comprimir un Conjunto de Datos","text":"<p>Optimizar y Comprimir un Conjunto de Datos</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# Definir el directorio del conjunto de datos\npath = Path('ruta/al/conjunto-de-datos')\n\n# Optimizar im\u00e1genes en el conjunto de datos (opcional)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Comprimir el conjunto de datos en 'ruta/al/conjunto-de-datos.zip'\nzip_directory(path)\n</code></pre> <p>Siguiendo estos pasos, puedes contribuir con un nuevo conjunto de datos que se integre bien con la estructura existente de Ultralytics.</p>"},{"location":"models/","title":"Modelos soportados por Ultralytics","text":"<p>\u00a1Bienvenido a la documentaci\u00f3n de modelos de Ultralytics! Ofrecemos soporte para una amplia gama de modelos, cada uno adaptado a tareas espec\u00edficas como detecci\u00f3n de objetos, segmentaci\u00f3n de instancias, clasificaci\u00f3n de im\u00e1genes, estimaci\u00f3n de posturas, y seguimiento de m\u00faltiples objetos. Si est\u00e1s interesado en contribuir con tu arquitectura de modelo a Ultralytics, consulta nuestra Gu\u00eda de Contribuci\u00f3n.</p> <p>Nota</p> <p>\ud83d\udea7 Estamos trabajando arduamente para mejorar nuestra documentaci\u00f3n en varios idiomas actualmente en construcci\u00f3n. \u00a1Gracias por tu paciencia! \ud83d\ude4f</p>"},{"location":"models/#modelos-destacados","title":"Modelos destacados","text":"<p>Aqu\u00ed est\u00e1n algunos de los modelos clave soportados:</p> <ol> <li>YOLOv3: La tercera iteraci\u00f3n de la familia de modelos YOLO, original de Joseph Redmon, conocida por su capacidad de detecci\u00f3n de objetos en tiempo real eficientemente.</li> <li>YOLOv4: Una actualizaci\u00f3n nativa de darknet para YOLOv3, lanzada por Alexey Bochkovskiy en 2020.</li> <li>YOLOv5: Una versi\u00f3n mejorada de la arquitectura YOLO por Ultralytics, ofreciendo un mejor rendimiento y compromiso de velocidad comparado con versiones anteriores.</li> <li>YOLOv6: Lanzado por Meituan en 2022, y utilizado en muchos de los robots de entrega aut\u00f3nomos de la compa\u00f1\u00eda.</li> <li>YOLOv7: Modelos YOLO actualizados lanzados en 2022 por los autores de YOLOv4.</li> <li>YOLOv8 NUEVO \ud83d\ude80: La \u00faltima versi\u00f3n de la familia YOLO, con capacidades mejoradas como segmentaci\u00f3n de instancias, estimaci\u00f3n de posturas/puntos clave y clasificaci\u00f3n.</li> <li>Modelo Segment Anything (SAM): Modelo Segment Anything (SAM) de Meta.</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM para aplicaciones m\u00f3viles, por la Universidad de Kyung Hee.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM por el Grupo de An\u00e1lisis de Imagen y Video, Instituto de Automatizaci\u00f3n, Academia China de Ciencias.</li> <li>YOLO-NAS: Modelos YOLO de B\u00fasqueda de Arquitectura Neural (NAS).</li> <li>Transformadores de Detecci\u00f3n en Tiempo Real (RT-DETR): Modelos de Transformador de Detecci\u00f3n en Tiempo Real (RT-DETR) de Baidu's PaddlePaddle.</li> </ol> <p> Mira: Ejecuta modelos YOLO de Ultralytics en solo unas pocas l\u00edneas de c\u00f3digo. </p>"},{"location":"models/#empezando-ejemplos-de-uso","title":"Empezando: Ejemplos de Uso","text":"<p>Este ejemplo proporciona ejemplos simples de entrenamiento e inferencia YOLO. Para la documentaci\u00f3n completa de estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict, Train, Val y Export.</p> <p>Nota que el siguiente ejemplo es para los modelos YOLOv8 Detect para detecci\u00f3n de objetos. Para tareas adicionales soportadas, consulta la documentaci\u00f3n de Segment, Classify y Pose.</p> <p>Ejemplo</p> PythonCLI <p>Los modelos pre-entrenados <code>*.pt</code> de PyTorch as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code> se pueden pasar a las clases <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> y <code>RTDETR()</code> para crear una instancia de modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo YOLOv8n preentrenado en COCO\nmodel = YOLO('yolov8n.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodel.info()\n\n# Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ejecutar inferencia con el modelo YOLOv8n en la imagen 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Los comandos CLI est\u00e1n disponibles para ejecutar directamente los modelos:</p> <pre><code># Cargar un modelo YOLOv8n preentrenado en COCO y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Cargar un modelo YOLOv8n preentrenado en COCO y ejecutar inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribuir-con-nuevos-modelos","title":"Contribuir con Nuevos Modelos","text":"<p>\u00bfInteresado en contribuir con tu modelo a Ultralytics? \u00a1Genial! Siempre estamos abiertos a expandir nuestro portafolio de modelos.</p> <ol> <li> <p>Haz un Fork del Repositorio: Comienza haciendo un fork del repositorio de GitHub de Ultralytics.</p> </li> <li> <p>Clona tu Fork: Clona tu fork a tu m\u00e1quina local y crea una nueva rama para trabajar.</p> </li> <li> <p>Implementa tu Modelo: A\u00f1ade tu modelo siguiendo los est\u00e1ndares de codificaci\u00f3n y directrices proporcionadas en nuestra Gu\u00eda de Contribuci\u00f3n.</p> </li> <li> <p>Prueba Rigurosamente: Aseg\u00farate de probar tu modelo rigurosamente, tanto de forma aislada como parte del proceso.</p> </li> <li> <p>Crea un Pull Request: Una vez que est\u00e9s satisfecho con tu modelo, crea un pull request al repositorio principal para revisi\u00f3n.</p> </li> <li> <p>Revisi\u00f3n de C\u00f3digo y Fusi\u00f3n: Despu\u00e9s de la revisi\u00f3n, si tu modelo cumple con nuestros criterios, ser\u00e1 fusionado al repositorio principal.</p> </li> </ol> <p>Para pasos detallados, consulta nuestra Gu\u00eda de Contribuci\u00f3n.</p>"},{"location":"models/fast-sam/","title":"Modelo para Segmentar Cualquier Cosa R\u00e1pidamente (FastSAM)","text":"<p>El Modelo para Segmentar Cualquier Cosa R\u00e1pidamente (FastSAM) es una soluci\u00f3n novedosa basada en CNN que funciona en tiempo real para la tarea de Segmentar Cualquier Cosa. Esta tarea est\u00e1 dise\u00f1ada para segmentar cualquier objeto dentro de una imagen bas\u00e1ndose en diversas indicaciones posibles de interacci\u00f3n del usuario. FastSAM reduce significativamente las demandas computacionales a la vez que mantiene un rendimiento competitivo, lo que lo convierte en una opci\u00f3n pr\u00e1ctica para una variedad de tareas de visi\u00f3n.</p> <p></p>"},{"location":"models/fast-sam/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>FastSAM est\u00e1 dise\u00f1ado para abordar las limitaciones del Modelo para Segmentar Cualquier Cosa (SAM), un modelo Transformer pesado con requerimientos sustanciales de recursos computacionales. FastSAM divide la tarea de segmentar cualquier cosa en dos etapas secuenciales: segmentaci\u00f3n de todas las instancias y selecci\u00f3n basada en indicaciones. La primera etapa utiliza YOLOv8-seg para producir las m\u00e1scaras de segmentaci\u00f3n de todas las instancias en la imagen. En la segunda etapa, produce la regi\u00f3n de inter\u00e9s correspondiente a la indicaci\u00f3n.</p>"},{"location":"models/fast-sam/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ol> <li> <p>Soluci\u00f3n en tiempo real: Al aprovechar la eficiencia computacional de las CNN, FastSAM proporciona una soluci\u00f3n en tiempo real para la tarea de segmentar cualquier cosa, lo que lo hace valioso para aplicaciones industriales que requieren resultados r\u00e1pidos.</p> </li> <li> <p>Eficiencia y rendimiento: FastSAM ofrece una reducci\u00f3n significativa en las demandas computacionales y de recursos sin comprometer la calidad del rendimiento. Alcanza un rendimiento comparable al de SAM, pero con recursos computacionales dr\u00e1sticamente reducidos, lo que permite su aplicaci\u00f3n en tiempo real.</p> </li> <li> <p>Segmentaci\u00f3n guiada por indicaciones: FastSAM puede segmentar cualquier objeto dentro de una imagen guiado por diversas indicaciones posibles de interacci\u00f3n del usuario, lo que proporciona flexibilidad y adaptabilidad en diferentes escenarios.</p> </li> <li> <p>Basado en YOLOv8-seg: FastSAM se basa en YOLOv8-seg, un detector de objetos equipado con una rama de segmentaci\u00f3n de instancias. Esto le permite producir de manera efectiva las m\u00e1scaras de segmentaci\u00f3n de todas las instancias en una imagen.</p> </li> <li> <p>Resultados competitivos en pruebas de referencia: En la tarea de propuesta de objetos de MS COCO, FastSAM alcanza puntuaciones altas a una velocidad significativamente m\u00e1s r\u00e1pida que SAM en una sola tarjeta NVIDIA RTX 3090, lo que demuestra su eficiencia y capacidad.</p> </li> <li> <p>Aplicaciones pr\u00e1cticas: El enfoque propuesto proporciona una soluci\u00f3n nueva y pr\u00e1ctica para un gran n\u00famero de tareas de visi\u00f3n a una velocidad muy alta, varias veces m\u00e1s r\u00e1pida que los m\u00e9todos actuales.</p> </li> <li> <p>Factibilidad de compresi\u00f3n del modelo: FastSAM demuestra la factibilidad de un camino que puede reducir significativamente el esfuerzo computacional al introducir una prioridad artificial en la estructura, abriendo as\u00ed nuevas posibilidades para la arquitectura de modelos grandes en tareas generales de visi\u00f3n.</p> </li> </ol>"},{"location":"models/fast-sam/#modelos-disponibles-tareas-admitidas-y-modos-de-funcionamiento","title":"Modelos disponibles, tareas admitidas y modos de funcionamiento","text":"<p>Esta tabla presenta los modelos disponibles con sus pesos pre-entrenados espec\u00edficos, las tareas que admiten y su compatibilidad con diferentes modos de funcionamiento, como Inference (inferencia), Validation (validaci\u00f3n), Training (entrenamiento) y Export (exportaci\u00f3n), indicados mediante emojis \u2705 para los modos admitidos y emojis \u274c para los modos no admitidos.</p> Tipo de modelo Pesos pre-entrenados Tareas admitidas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n FastSAM-s <code>FastSAM-s.pt</code> Segmentaci\u00f3n de Instancias \u2705 \u274c \u274c \u2705 FastSAM-x <code>FastSAM-x.pt</code> Segmentaci\u00f3n de Instancias \u2705 \u274c \u274c \u2705"},{"location":"models/fast-sam/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Los modelos FastSAM son f\u00e1ciles de integrar en tus aplicaciones Python. Ultralytics proporciona una API y comandos de l\u00ednea de comandos (CLI) f\u00e1ciles de usar para agilizar el desarrollo.</p>"},{"location":"models/fast-sam/#uso-de-prediccion","title":"Uso de predicci\u00f3n","text":"<p>Para realizar la detecci\u00f3n de objetos en una imagen, utiliza el m\u00e9todo <code>predict</code> de la siguiente manera:</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import FastSAM\nfrom ultralytics.models.fastsam import FastSAMPrompt\n\n# Define una fuente de inferencia\nsource = 'ruta/hacia/bus.jpg'\n\n# Crea un modelo FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # o FastSAM-x.pt\n\n# Ejecuta la inferencia en una imagen\neverything_results = model(source, device='cpu', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n\n# Prepara un objeto de procesamiento de indicaciones\nprompt_process = FastSAMPrompt(source, everything_results, device='cpu')\n\n# Indicaci\u00f3n Everything\nann = prompt_process.everything_prompt()\n\n# Caja predeterminada [0,0,0,0] -&gt; [x1,y1,x2,y2]\nann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n\n# Indicaci\u00f3n de texto\nann = prompt_process.text_prompt(text='una foto de un perro')\n\n# Indicaci\u00f3n de punto\n# puntos predeterminados [[0,0]] [[x1,y1],[x2,y2]]\n# etiqueta_predeterminada [0] [1,0] 0:fondo, 1:primer plano\nann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\nprompt_process.plot(annotations=ann, output='./')\n</code></pre> <pre><code># Carga un modelo FastSAM y segmenta todo con \u00e9l\nyolo segment predict model=FastSAM-s.pt source=ruta/hacia/bus.jpg imgsz=640\n</code></pre> <p>Este fragmento de c\u00f3digo demuestra la simplicidad de cargar un modelo pre-entrenado y realizar una predicci\u00f3n en una imagen.</p>"},{"location":"models/fast-sam/#uso-de-validacion","title":"Uso de validaci\u00f3n","text":"<p>La validaci\u00f3n del modelo en un conjunto de datos se puede realizar de la siguiente manera:</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import FastSAM\n\n# Crea un modelo FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # o FastSAM-x.pt\n\n# Valida el modelo\nresults = model.val(data='coco8-seg.yaml')\n</code></pre> <pre><code># Carga un modelo FastSAM y valida en el conjunto de datos de ejemplo COCO8 con un tama\u00f1o de imagen de 640\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n</code></pre> <p>Ten en cuenta que FastSAM solo admite la detecci\u00f3n y segmentaci\u00f3n de una sola clase de objeto. Esto significa que reconocer\u00e1 y segmentar\u00e1 todos los objetos como si fueran de la misma clase. Por lo tanto, al preparar el conjunto de datos, debes convertir todos los IDs de categor\u00eda de objetos a 0.</p>"},{"location":"models/fast-sam/#uso-oficial-de-fastsam","title":"Uso oficial de FastSAM","text":"<p>FastSAM tambi\u00e9n est\u00e1 disponible directamente en el repositorio https://github.com/CASIA-IVA-Lab/FastSAM. Aqu\u00ed hay una descripci\u00f3n general breve de los pasos t\u00edpicos que podr\u00edas seguir para usar FastSAM:</p>"},{"location":"models/fast-sam/#instalacion","title":"Instalaci\u00f3n","text":"<ol> <li> <p>Clona el repositorio de FastSAM:    <pre><code>git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n</code></pre></p> </li> <li> <p>Crea y activa un entorno Conda con Python 3.9:    <pre><code>conda create -n FastSAM python=3.9\nconda activate FastSAM\n</code></pre></p> </li> <li> <p>Navega hasta el repositorio clonado e instala los paquetes requeridos:    <pre><code>cd FastSAM\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Instala el modelo CLIP:    <pre><code>pip install git+https://github.com/openai/CLIP.git\n</code></pre></p> </li> </ol>"},{"location":"models/fast-sam/#ejemplo-de-uso","title":"Ejemplo de uso","text":"<ol> <li> <p>Descarga un punto de control del modelo.</p> </li> <li> <p>Utiliza FastSAM para inferencia. Ejemplos de comandos:</p> <ul> <li> <p>Segmentar todo en una imagen:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n</code></pre></p> </li> <li> <p>Segmentar objetos espec\u00edficos utilizando una indicaci\u00f3n de texto:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"el perro amarillo\"\n</code></pre></p> </li> <li> <p>Segmentar objetos dentro de una caja delimitadora (proporciona las coordenadas de la caja en formato xywh):   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n</code></pre></p> </li> <li> <p>Segmentar objetos cerca de puntos espec\u00edficos:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n</code></pre></p> </li> </ul> </li> </ol> <p>Adem\u00e1s, puedes probar FastSAM a trav\u00e9s de una demostraci\u00f3n en Colab o en la demostraci\u00f3n web de HuggingFace para tener una experiencia visual.</p>"},{"location":"models/fast-sam/#citas-y-agradecimientos","title":"Citas y agradecimientos","text":"<p>Nos gustar\u00eda agradecer a los autores de FastSAM por sus importantes contribuciones en el campo de la segmentaci\u00f3n de instancias en tiempo real:</p> BibTeX <pre><code>@misc{zhao2023fast,\n      title={Fast Segment Anything},\n      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\n      year={2023},\n      eprint={2306.12156},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>El art\u00edculo original de FastSAM se puede encontrar en arXiv. Los autores han puesto su trabajo a disposici\u00f3n del p\u00fablico, y el c\u00f3digo base se puede acceder en GitHub. Agradecemos sus esfuerzos para avanzar en el campo y hacer que su trabajo sea accesible a la comunidad en general.</p>"},{"location":"models/mobile-sam/","title":"MobileSAM (Mobile Segment Anything Model)","text":""},{"location":"models/mobile-sam/#segmentacion-movil-de-cualquier-cosa-mobilesam","title":"Segmentaci\u00f3n M\u00f3vil de Cualquier Cosa (MobileSAM)","text":"<p>El art\u00edculo de MobileSAM ahora est\u00e1 disponible en arXiv.</p> <p>Una demostraci\u00f3n de MobileSAM funcionando en una CPU se puede acceder en este enlace de demostraci\u00f3n. El rendimiento en una CPU Mac i5 tarda aproximadamente 3 segundos. En la demostraci\u00f3n de Hugging Face, la interfaz y las CPUs de menor rendimiento contribuyen a una respuesta m\u00e1s lenta, pero sigue funcionando de manera efectiva.</p> <p>MobileSAM se implementa en varios proyectos, incluyendo Grounding-SAM, AnyLabeling y Segment Anything in 3D.</p> <p>MobileSAM se entrena en una sola GPU con un conjunto de datos de 100k (1% de las im\u00e1genes originales) en menos de un d\u00eda. El c\u00f3digo para este entrenamiento estar\u00e1 disponible en el futuro.</p>"},{"location":"models/mobile-sam/#modelos-disponibles-tareas-admitidas-y-modos-de-operacion","title":"Modelos Disponibles, Tareas Admitidas y Modos de Operaci\u00f3n","text":"<p>Esta tabla presenta los modelos disponibles con sus pesos pre-entrenados espec\u00edficos, las tareas que admiten y su compatibilidad con diferentes modos de operaci\u00f3n como Inference (Inferencia), Validation (Validaci\u00f3n), Training (Entrenamiento) y Export (Exportaci\u00f3n), indicados por emojis \u2705 para los modos admitidos y emojis \u274c para los modos no admitidos.</p> Tipo de Modelo Pesos Pre-entrenados Tareas Admitidas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n MobileSAM <code>mobile_sam.pt</code> Segmentaci\u00f3n de Instancias \u2705 \u274c \u274c \u2705"},{"location":"models/mobile-sam/#adaptacion-de-sam-a-mobilesam","title":"Adaptaci\u00f3n de SAM a MobileSAM","text":"<p>Dado que MobileSAM mantiene el mismo pipeline que SAM original, hemos incorporado el pre-procesamiento, post-procesamiento y todas las dem\u00e1s interfaces del original. En consecuencia, aquellos que actualmente utilizan SAM original pueden hacer la transici\u00f3n a MobileSAM con un esfuerzo m\u00ednimo.</p> <p>MobileSAM tiene un rendimiento comparable a SAM original y mantiene el mismo pipeline excepto por un cambio en el codificador de im\u00e1genes. Espec\u00edficamente, reemplazamos el codificador de im\u00e1genes original ViT-H pesado (632M) por uno m\u00e1s peque\u00f1o, Tiny-ViT (5M). En una sola GPU, MobileSAM funciona a aproximadamente 12ms por imagen: 8ms en el codificador de im\u00e1genes y 4ms en el decodificador de m\u00e1scaras.</p> <p>La siguiente tabla proporciona una comparaci\u00f3n de los codificadores de im\u00e1genes basados en ViT:</p> Codificador de Im\u00e1genes SAM Original MobileSAM Par\u00e1metros 611M 5M Velocidad 452ms 8ms <p>Tanto SAM original como MobileSAM utilizan el mismo decodificador de m\u00e1scaras guiado por instrucciones:</p> Decodificador de M\u00e1scaras SAM Original MobileSAM Par\u00e1metros 3.876M 3.876M Velocidad 4ms 4ms <p>Aqu\u00ed est\u00e1 la comparaci\u00f3n de todo el pipeline:</p> Pipeline Completo (Enc+Dec) SAM Original MobileSAM Par\u00e1metros 615M 9.66M Velocidad 456ms 12ms <p>El rendimiento de MobileSAM y SAM original se demuestra utilizando tanto un punto como una caja como instrucciones.</p> <p></p> <p></p> <p>Con su rendimiento superior, MobileSAM es aproximadamente 5 veces m\u00e1s peque\u00f1o y 7 veces m\u00e1s r\u00e1pido que el actual FastSAM. M\u00e1s detalles est\u00e1n disponibles en la p\u00e1gina del proyecto de MobileSAM.</p>"},{"location":"models/mobile-sam/#probando-mobilesam-en-ultralytics","title":"Probando MobileSAM en Ultralytics","text":"<p>Al igual que SAM original, ofrecemos un m\u00e9todo sencillo de prueba en Ultralytics, que incluye modos tanto para instrucciones de Punto como para Caja.</p>"},{"location":"models/mobile-sam/#descarga-del-modelo","title":"Descarga del Modelo","text":"<p>Puedes descargar el modelo aqu\u00ed.</p>"},{"location":"models/mobile-sam/#instruccion-de-punto","title":"Instrucci\u00f3n de Punto","text":"<p>Ejemplo</p> Python <pre><code>from ultralytics import SAM\n\n# Carga el modelo\nmodel = SAM('mobile_sam.pt')\n\n# Predice un segmento basado en una instrucci\u00f3n de punto\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre>"},{"location":"models/mobile-sam/#instruccion-de-caja","title":"Instrucci\u00f3n de Caja","text":"<p>Ejemplo</p> Python <pre><code>from ultralytics import SAM\n\n# Carga el modelo\nmodel = SAM('mobile_sam.pt')\n\n# Predice un segmento basado en una instrucci\u00f3n de caja\nmodel.predict('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n</code></pre> <p>Hemos implementado <code>MobileSAM</code> y <code>SAM</code> utilizando la misma API. Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo usarlo, consulta la p\u00e1gina de SAM.</p>"},{"location":"models/mobile-sam/#citaciones-y-reconocimientos","title":"Citaciones y Reconocimientos","text":"<p>Si encuentras \u00fatil MobileSAM en tu investigaci\u00f3n o trabajo de desarrollo, considera citar nuestro art\u00edculo:</p> BibTeX <p>```bibtex @article{mobile_sam,   title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},   author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},   journal={arXiv preprint arXiv:2306.14289},   year={2023} }</p>"},{"location":"models/rtdetr/","title":"RT-DETR de Baidu: Un Detector de Objetos en Tiempo Real Basado en Vision Transformers","text":""},{"location":"models/rtdetr/#resumen","title":"Resumen","text":"<p>Real-Time Detection Transformer (RT-DETR), desarrollado por Baidu, es un avanzado detector de objetos de extremo a extremo que proporciona un rendimiento en tiempo real manteniendo una alta precisi\u00f3n. Utiliza la potencia de Vision Transformers (ViT) para procesar de manera eficiente caracter\u00edsticas de m\u00faltiples escalas mediante la descomposici\u00f3n de la interacci\u00f3n intra-escala y la fusi\u00f3n inter-escala. RT-DETR es altamente adaptable y permite ajustar de manera flexible la velocidad de inferencia utilizando diferentes capas de decodificador sin necesidad de volver a entrenar el modelo. El modelo se destaca en plataformas aceleradas como CUDA con TensorRT, superando a muchos otros detectores de objetos en tiempo real.</p> <p> Resumen de RT-DETR de Baidu. El diagrama de la arquitectura del modelo RT-DETR muestra las \u00faltimas tres etapas del canal (S3, S4, S5) como entrada al codificador. El eficiente codificador h\u00edbrido transforma caracter\u00edsticas de m\u00faltiples escalas en una secuencia de caracter\u00edsticas de imagen a trav\u00e9s del m\u00f3dulo de interacci\u00f3n de caracter\u00edsticas intra-escala (AIFI) y el m\u00f3dulo de fusi\u00f3n de caracter\u00edsticas inter-escala (CCFM). Se utiliza la selecci\u00f3n de consultas IoU-aware para seleccionar un n\u00famero fijo de caracter\u00edsticas de imagen que servir\u00e1n como consultas iniciales de objetos para el decodificador. Finalmente, el decodificador con cabeceras de predicci\u00f3n auxiliares optimiza iterativamente las consultas de objetos para generar cajas y puntuaciones de confianza (fuente).</p>"},{"location":"models/rtdetr/#caracteristicas-clave","title":"Caracter\u00edsticas Clave","text":"<ul> <li>Codificador H\u00edbrido Eficiente: RT-DETR de Baidu utiliza un codificador h\u00edbrido eficiente que procesa caracter\u00edsticas de m\u00faltiples escalas mediante la descomposici\u00f3n de la interacci\u00f3n intra-escala y la fusi\u00f3n inter-escala. Este dise\u00f1o \u00fanico basado en Vision Transformers reduce los costos computacionales y permite la detecci\u00f3n de objetos en tiempo real.</li> <li>Selecci\u00f3n de Consultas IoU-aware: RT-DETR de Baidu mejora la inicializaci\u00f3n de las consultas de objetos utilizando la selecci\u00f3n de consultas IoU-aware. Esto permite que el modelo se enfoque en los objetos m\u00e1s relevantes de la escena, mejorando la precisi\u00f3n en la detecci\u00f3n.</li> <li>Velocidad de Inferencia Adaptable: RT-DETR de Baidu admite ajustes flexibles de la velocidad de inferencia utilizando diferentes capas de decodificador sin necesidad de volver a entrenar el modelo. Esta adaptabilidad facilita la aplicaci\u00f3n pr\u00e1ctica en diversos escenarios de detecci\u00f3n de objetos en tiempo real.</li> </ul>"},{"location":"models/rtdetr/#modelos-pre-entrenados","title":"Modelos Pre-entrenados","text":"<p>La API de Python de Ultralytics proporciona modelos pre-entrenados de RT-DETR de PaddlePaddle en diferentes escalas:</p> <ul> <li>RT-DETR-L: 53.0% AP en COCO val2017, 114 FPS en GPU T4</li> <li>RT-DETR-X: 54.8% AP en COCO val2017, 74 FPS en GPU T4</li> </ul>"},{"location":"models/rtdetr/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Este ejemplo proporciona ejemplos sencillos de entrenamiento e inferencia de RT-DETRR. Para obtener una documentaci\u00f3n completa sobre estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict,  Train, Val y Export.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import RTDETR\n\n# Cargar un modelo RT-DETR-l pre-entrenado en COCO\nmodel = RTDETR('rtdetr-l.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodel.info()\n\n# Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Realizar inferencia con el modelo RT-DETR-l en la imagen 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <pre><code># Cargar un modelo RT-DETR-l pre-entrenado en COCO y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Cargar un modelo RT-DETR-l pre-entrenado en COCO y realizar inferencia en la imagen 'bus.jpg'\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/rtdetr/#tareas-y-modos-admitidos","title":"Tareas y Modos Admitidos","text":"<p>Esta tabla presenta los tipos de modelos, los pesos pre-entrenados espec\u00edficos, las tareas admitidas por cada modelo y los diversos modos (Train , Val, Predict, Export) admitidos, indicados por los emojis \u2705.</p> Tipo de Modelo Pesos Pre-entrenados Tareas Admitidas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n RT-DETR Large <code>rtdetr-l.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 RT-DETR Extra-Large <code>rtdetr-x.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705"},{"location":"models/rtdetr/#citaciones-y-agradecimientos","title":"Citaciones y Agradecimientos","text":"<p>Si utilizas RT-DETR de Baidu en tu investigaci\u00f3n o trabajo de desarrollo, por favor cita el art\u00edculo original:</p> BibTeX <pre><code>@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Wenyu Lv and Shangliang Xu and Yian Zhao and Guanzhong Wang and Jinman Wei and Cheng Cui and Yuning Du and Qingqing Dang and Yi Liu},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Nos gustar\u00eda agradecer a Baidu y al equipo de PaddlePaddle por crear y mantener este valioso recurso para la comunidad de visi\u00f3n por computadora. Apreciamos enormemente su contribuci\u00f3n al campo con el desarrollo del detector de objetos en tiempo real basado en Vision Transformers, RT-DETR.</p> <p>keywords: RT-DETR, Transformer, ViT, Vision Transformers, Baidu RT-DETR, PaddlePaddle, Paddle Paddle RT-DETR, detecci\u00f3n de objetos en tiempo real, detecci\u00f3n de objetos basada en Vision Transformers, modelos pre-entrenados PaddlePaddle RT-DETR, uso de RT-DETR de Baidu, API de Python de Ultralytics</p>"},{"location":"models/sam/","title":"Segment Anything Model (SAM)","text":"<p>Bienvenido al frontera de la segmentaci\u00f3n de im\u00e1genes con el Segment Anything Model, o SAM. Este modelo revolucionario ha cambiado el juego al introducir la segmentaci\u00f3n de im\u00e1genes por indicaci\u00f3n con rendimiento en tiempo real, estableciendo nuevos est\u00e1ndares en el campo.</p>"},{"location":"models/sam/#introduccion-a-sam-segment-anything-model","title":"Introducci\u00f3n a SAM: Segment Anything Model","text":"<p>El Segment Anything Model, o SAM, es un modelo de segmentaci\u00f3n de im\u00e1genes de vanguardia que permite la segmentaci\u00f3n por indicaci\u00f3n, ofreciendo una versatilidad sin igual en las tareas de an\u00e1lisis de im\u00e1genes. SAM forma el coraz\u00f3n de la iniciativa Segment Anything, un proyecto innovador que presenta un modelo, una tarea y un conjunto de datos nuevos para la segmentaci\u00f3n de im\u00e1genes.</p> <p>El dise\u00f1o avanzado de SAM le permite adaptarse a nuevas distribuciones y tareas de im\u00e1genes sin conocimientos previos, una caracter\u00edstica conocida como transferencia sin entrenamiento. Entrenado en el extenso conjunto de datos SA-1B, que contiene m\u00e1s de mil millones de m\u00e1scaras distribuidas en once millones de im\u00e1genes seleccionadas cuidadosamente, SAM ha demostrado un impresionante rendimiento en la transferencia sin entrenamiento, superando en muchos casos los resultados de supervisi\u00f3n completa anteriores.</p> <p> Im\u00e1genes de ejemplo con m\u00e1scaras superpuestas de nuestro nuevo conjunto de datos, SA-1B. SA-1B contiene 11 millones de im\u00e1genes diversas de alta resoluci\u00f3n, con licencia y protecci\u00f3n de la privacidad, y 1.1 mil millones de m\u00e1scaras de segmentaci\u00f3n de alta calidad. Estas m\u00e1scaras fueron anotadas completamente autom\u00e1ticamente por SAM y, seg\u00fan las calificaciones humanas y numerosos experimentos, tienen una alta calidad y diversidad. Las im\u00e1genes se agrupan por n\u00famero de m\u00e1scaras por imagen para su visualizaci\u00f3n (hay aproximadamente 100 m\u00e1scaras por imagen en promedio).</p>"},{"location":"models/sam/#caracteristicas-clave-del-segment-anything-model-sam","title":"Caracter\u00edsticas clave del Segment Anything Model (SAM)","text":"<ul> <li>Tarea de segmentaci\u00f3n por indicaci\u00f3n: SAM fue dise\u00f1ado teniendo en cuenta una tarea de segmentaci\u00f3n por indicaci\u00f3n, lo que le permite generar m\u00e1scaras de segmentaci\u00f3n v\u00e1lidas a partir de cualquier indicaci\u00f3n dada, como pistas espaciales o de texto que identifican un objeto.</li> <li>Arquitectura avanzada: El Segment Anything Model utiliza un potente codificador de im\u00e1genes, un codificador de indicaciones y un decodificador de m\u00e1scaras ligero. Esta arquitectura \u00fanica permite la indicaci\u00f3n flexible, el c\u00e1lculo de m\u00e1scaras en tiempo real y la conciencia de ambig\u00fcedades en las tareas de segmentaci\u00f3n.</li> <li>El conjunto de datos SA-1B: Introducido por el proyecto Segment Anything, el conjunto de datos SA-1B cuenta con m\u00e1s de mil millones de m\u00e1scaras en once millones de im\u00e1genes. Como el conjunto de datos de segmentaci\u00f3n m\u00e1s grande hasta la fecha, proporciona a SAM una fuente de datos de entrenamiento diversa y a gran escala.</li> <li>Rendimiento en la transferencia sin entrenamiento: SAM muestra un destacado rendimiento en la transferencia sin entrenamiento en diversas tareas de segmentaci\u00f3n, lo que lo convierte en una herramienta lista para usar en diversas aplicaciones con una necesidad m\u00ednima de ingenier\u00eda de indicaci\u00f3n.</li> </ul> <p>Para obtener una visi\u00f3n m\u00e1s detallada del Segment Anything Model y el conjunto de datos SA-1B, visita el sitio web de Segment Anything y consulta el art\u00edculo de investigaci\u00f3n Segment Anything.</p>"},{"location":"models/sam/#modelos-disponibles-tareas-admitidas-y-modos-de-funcionamiento","title":"Modelos disponibles, tareas admitidas y modos de funcionamiento","text":"<p>Esta tabla muestra los modelos disponibles con sus pesos pre-entrenados espec\u00edficos, las tareas que admiten y su compatibilidad con diferentes modos de funcionamiento como Inference, Validation, Training y Export, indicados con emojis \u2705 para los modos admitidos y emojis \u274c para los modos no admitidos.</p> Tipo de modelo Pesos pre-entrenados Tareas admitidas Inference Validation Training Export SAM base <code>sam_b.pt</code> Segmentaci\u00f3n de instancias \u2705 \u274c \u274c \u2705 SAM large <code>sam_l.pt</code> Segmentaci\u00f3n de instancias \u2705 \u274c \u274c \u2705"},{"location":"models/sam/#como-usar-sam-versatilidad-y-potencia-en-la-segmentacion-de-imagenes","title":"C\u00f3mo usar SAM: Versatilidad y potencia en la segmentaci\u00f3n de im\u00e1genes","text":"<p>El Segment Anything Model se puede utilizar para una multitud de tareas posteriores que van m\u00e1s all\u00e1 de sus datos de entrenamiento. Esto incluye detecci\u00f3n de bordes, generaci\u00f3n de propuestas de objetos, segmentaci\u00f3n de instancias y predicci\u00f3n preliminar de texto a m\u00e1scara. Con la ingenier\u00eda de indicaci\u00f3n, SAM puede adaptarse r\u00e1pidamente a nuevas tareas y distribuciones de datos de manera sin entrenamiento, estableci\u00e9ndolo como una herramienta vers\u00e1til y potente para todas tus necesidades de segmentaci\u00f3n de im\u00e1genes.</p>"},{"location":"models/sam/#ejemplo-de-prediccion-con-sam","title":"Ejemplo de predicci\u00f3n con SAM","text":"<p>Segmentar con indicaciones</p> <p>Segmenta la imagen con las indicaciones proporcionadas.</p> Python <pre><code>from ultralytics import SAM\n\n# Cargar un modelo\nmodelo = SAM('sam_b.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Ejecutar inferencia con indicaciones de bboxes\nmodelo('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n\n# Ejecutar inferencia con indicaciones de puntos\nmodelo('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre> <p>Segmentar todo</p> <p>Segmenta toda la imagen.</p> PythonCLI <pre><code>from ultralytics import SAM\n\n# Cargar un modelo\nmodelo = SAM('sam_b.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Ejecutar inferencia\nmodelo('ruta/hacia/imagen.jpg')\n</code></pre> <pre><code># Ejecutar inferencia con un modelo SAM\nyolo predict model=sam_b.pt source=ruta/hacia/imagen.jpg\n</code></pre> <ul> <li>La l\u00f3gica aqu\u00ed es segmentar toda la imagen si no se proporcionan indicaciones (bboxes/puntos/m\u00e1scaras).</li> </ul> <p>Ejemplo de SAMPredictor</p> <p>De esta manera, puedes configurar una imagen una vez y ejecutar inferencia con indicaciones m\u00faltiples sin ejecutar el codificador de im\u00e1genes m\u00faltiples veces.</p> Inferencia con indicaciones <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Crear SAMPredictor\nopciones = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(opciones=opciones)\n\n# Establecer imagen\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # establecer con archivo de imagen\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # establecer con np.ndarray\nresultados = predictor(bboxes=[439, 437, 524, 709])\nresultados = predictor(points=[900, 370], labels=[1])\n\n# Restablecer imagen\npredictor.reset_image()\n</code></pre> <p>Segmentar todo con argumentos adicionales.</p> Segmentar todo <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Crear SAMPredictor\nopciones = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(opciones=opciones)\n\n# Segmentar con argumentos adicionales\nresultados = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n</code></pre> <ul> <li>M\u00e1s argumentos adicionales para <code>Segmentar todo</code> en <code>Referencia de Predictor/generate</code>.</li> </ul>"},{"location":"models/sam/#sam-comparado-con-yolov8","title":"SAM comparado con YOLOv8","text":"<p>Aqu\u00ed comparamos el modelo SAM m\u00e1s peque\u00f1o de Meta, SAM-b, con el modelo de segmentaci\u00f3n m\u00e1s peque\u00f1o de Ultralytics, YOLOv8n-seg:</p> Modelo Tama\u00f1o Par\u00e1metros Velocidad (CPU) SAM-b de Meta 358 MB 94.7 M 51096 ms/im MobileSAM 40.7 MB 10.1 M 46122 ms/im FastSAM-s con respaldo de YOLOv8 23.7 MB 11.8 M 115 ms/im YOLOv8n-seg de Ultralytics 6.7 MB (53.4 veces m\u00e1s peque\u00f1o) 3.4 M (27.9 veces menos) 59 ms/im (866 veces m\u00e1s r\u00e1pido) <p>Esta comparaci\u00f3n muestra las diferencias de \u00f3rdenes de magnitud en los tama\u00f1os y velocidades de los modelos. Si bien SAM presenta capacidades \u00fanicas para la segmentaci\u00f3n autom\u00e1tica, no es un competidor directo de los modelos de segmentaci\u00f3n YOLOv8, que son m\u00e1s peque\u00f1os, m\u00e1s r\u00e1pidos y m\u00e1s eficientes.</p> <p>Las pruebas se realizaron en una MacBook Apple M2 de 2023 con 16 GB de RAM. Para reproducir esta prueba:</p> <p>Ejemplo</p> Python <pre><code>from ultralytics import FastSAM, SAM, YOLO\n\n# Perfil del modelo SAM-b\nmodelo = SAM('sam_b.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil de MobileSAM\nmodelo = SAM('mobile_sam.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil de FastSAM-s\nmodelo = FastSAM('FastSAM-s.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n\n# Perfil de YOLOv8n-seg\nmodelo = YOLO('yolov8n-seg.pt')\nmodelo.info()\nmodelo('ultralytics/assets')\n</code></pre>"},{"location":"models/sam/#auto-anotacion-un-camino-rapido-hacia-conjuntos-de-datos-de-segmentacion","title":"Auto-anotaci\u00f3n: un camino r\u00e1pido hacia conjuntos de datos de segmentaci\u00f3n","text":"<p>La auto-anotaci\u00f3n es una caracter\u00edstica clave de SAM que permite a los usuarios generar un conjunto de datos de segmentaci\u00f3n utilizando un modelo de detecci\u00f3n pre-entrenado. Esta funci\u00f3n permite una anotaci\u00f3n r\u00e1pida y precisa de un gran n\u00famero de im\u00e1genes, evitando la necesidad de una etiquetaci\u00f3n manual que consume mucho tiempo.</p>"},{"location":"models/sam/#generar-tu-conjunto-de-datos-de-segmentacion-utilizando-un-modelo-de-deteccion","title":"Generar tu conjunto de datos de segmentaci\u00f3n utilizando un modelo de detecci\u00f3n","text":"<p>Para auto-anotar tu conjunto de datos con el marco de trabajo de Ultralytics, utiliza la funci\u00f3n <code>auto_annotate</code> como se muestra a continuaci\u00f3n:</p> <p>Ejemplo</p> Python <pre><code>from ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"ruta/a/las/imagenes\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argumento Tipo Descripci\u00f3n Predeterminado data str Ruta a una carpeta que contiene las im\u00e1genes a anotar. det_model str, opcional Modelo de detecci\u00f3n YOLO pre-entrenado. Por defecto, 'yolov8x.pt'. 'yolov8x.pt' sam_model str, opcional Modelo de segmentaci\u00f3n SAM pre-entrenado. Por defecto, 'sam_b.pt'. 'sam_b.pt' device str, opcional Dispositivo en el que ejecutar los modelos. Por defecto, una cadena vac\u00eda (CPU o GPU, si est\u00e1 disponible). output_dir str, None, opcional Directorio para guardar los resultados anotados. Por defecto, una carpeta 'labels' en el mismo directorio que 'data'. None <p>La funci\u00f3n <code>auto_annotate</code> toma la ruta de tus im\u00e1genes, con argumentos opcionales para especificar los modelos de detecci\u00f3n y segmentaci\u00f3n SAM pre-entrenados, el dispositivo en el que ejecutar los modelos, y el directorio de salida para guardar los resultados anotados.</p> <p>La auto-anotaci\u00f3n con modelos pre-entrenados puede reducir dr\u00e1sticamente el tiempo y el esfuerzo requeridos para crear conjuntos de datos de segmentaci\u00f3n de alta calidad. Esta caracter\u00edstica es especialmente beneficiosa para investigadores y desarrolladores que trabajan con grandes colecciones de im\u00e1genes, ya que les permite centrarse en el desarrollo y la evaluaci\u00f3n de modelos en lugar de en la anotaci\u00f3n manual.</p>"},{"location":"models/sam/#citas-y-agradecimientos","title":"Citas y agradecimientos","text":"<p>Si encuentras \u00fatil SAM en tu trabajo de investigaci\u00f3n o desarrollo, considera citar nuestro art\u00edculo:</p> BibTeX <pre><code>@misc{kirillov2023segment,\n      title={Segment Anything},\n      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross Girshick},\n      year={2023},\n      eprint={2304.02643},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Nos gustar\u00eda expresar nuestro agradecimiento a Meta AI por crear y mantener este valioso recurso para la comunidad de visi\u00f3n por computadora.</p> <p>keywords: Segment Anything, Segment Anything Model, SAM, Meta SAM, segmentaci\u00f3n de im\u00e1genes, segmentaci\u00f3n por indicaci\u00f3n, rendimiento en la transferencia sin entrenamiento, conjunto de datos SA-1B, arquitectura avanzada, auto-anotaci\u00f3n, Ultralytics, modelos pre-entrenados, SAM base, SAM large, segmentaci\u00f3n de instancias, visi\u00f3n por computadora, IA, inteligencia artificial, aprendizaje autom\u00e1tico, anotaci\u00f3n de datos, m\u00e1scaras de segmentaci\u00f3n, modelo de detecci\u00f3n, modelo de detecci\u00f3n YOLO, bibtex, Meta AI.</p>"},{"location":"models/yolo-nas/","title":"YOLO-NAS","text":""},{"location":"models/yolo-nas/#vision-general","title":"Visi\u00f3n general","text":"<p>Desarrollado por Deci AI, YOLO-NAS es un modelo revolucionario de detecci\u00f3n de objetos. Es el producto de una tecnolog\u00eda avanzada de B\u00fasqueda de Arquitectura Neural, meticulosamente dise\u00f1ada para abordar las limitaciones de los modelos YOLO anteriores. Con mejoras significativas en el soporte de cuantizaci\u00f3n y el equilibrio entre precisi\u00f3n y latencia, YOLO-NAS representa un gran avance en la detecci\u00f3n de objetos.</p> <p> Visi\u00f3n general de YOLO-NAS. YOLO-NAS utiliza bloques conscientes de cuantizaci\u00f3n y cuantizaci\u00f3n selectiva para un rendimiento \u00f3ptimo. El modelo, cuando se convierte en su versi\u00f3n cuantizada INT8, experimenta una ca\u00edda m\u00ednima de precisi\u00f3n, una mejora significativa en comparaci\u00f3n con otros modelos. Estos avances culminan en una arquitectura superior con capacidades de detecci\u00f3n de objetos sin precedentes y un rendimiento sobresaliente.</p>"},{"location":"models/yolo-nas/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<ul> <li>Bloque b\u00e1sico compatible con cuantizaci\u00f3n: YOLO-NAS introduce un nuevo bloque b\u00e1sico que es compatible con la cuantizaci\u00f3n, abordando una de las limitaciones significativas de los modelos YOLO anteriores.</li> <li>Entrenamiento sofisticado y cuantizaci\u00f3n: YOLO-NAS utiliza esquemas avanzados de entrenamiento y cuantizaci\u00f3n posterior para mejorar el rendimiento.</li> <li>Optimizaci\u00f3n AutoNAC y pre-entrenamiento: YOLO-NAS utiliza la optimizaci\u00f3n AutoNAC y se pre-entrena en conjuntos de datos prominentes como COCO, Objects365 y Roboflow 100. Este pre-entrenamiento lo hace extremadamente adecuado para tareas de detecci\u00f3n de objetos en entornos de producci\u00f3n.</li> </ul>"},{"location":"models/yolo-nas/#modelos-pre-entrenados","title":"Modelos pre-entrenados","text":"<p>Experimenta el poder de la detecci\u00f3n de objetos de pr\u00f3xima generaci\u00f3n con los modelos pre-entrenados de YOLO-NAS proporcionados por Ultralytics. Estos modelos est\u00e1n dise\u00f1ados para ofrecer un rendimiento de primera clase tanto en velocidad como en precisi\u00f3n. Elige entre una variedad de opciones adaptadas a tus necesidades espec\u00edficas:</p> Modelo mAP Latencia (ms) YOLO-NAS S 47.5 3.21 YOLO-NAS M 51.55 5.85 YOLO-NAS L 52.22 7.87 YOLO-NAS S INT-8 47.03 2.36 YOLO-NAS M INT-8 51.0 3.78 YOLO-NAS L INT-8 52.1 4.78 <p>Cada variante del modelo est\u00e1 dise\u00f1ada para ofrecer un equilibrio entre la Precisi\u00f3n Promedio de las Areas (mAP, por sus siglas en ingl\u00e9s) y la latencia, ayud\u00e1ndote a optimizar tus tareas de detecci\u00f3n de objetos en t\u00e9rminos de rendimiento y velocidad.</p>"},{"location":"models/yolo-nas/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Ultralytics ha facilitado la integraci\u00f3n de los modelos YOLO-NAS en tus aplicaciones de Python a trav\u00e9s de nuestro paquete <code>ultralytics</code>. El paquete proporciona una API de Python f\u00e1cil de usar para agilizar el proceso.</p> <p>Los siguientes ejemplos muestran c\u00f3mo usar los modelos YOLO-NAS con el paquete <code>ultralytics</code> para inferencia y validaci\u00f3n:</p>"},{"location":"models/yolo-nas/#ejemplos-de-inferencia-y-validacion","title":"Ejemplos de inferencia y validaci\u00f3n","text":"<p>En este ejemplo validamos YOLO-NAS-s en el conjunto de datos COCO8.</p> <p>Ejemplo</p> <p>Este ejemplo proporciona un c\u00f3digo simple de inferencia y validaci\u00f3n para YOLO-NAS. Para manejar los resultados de la inferencia, consulta el modo Predict. Para usar YOLO-NAS con modos adicionales, consulta Val y Export. El paquete <code>ultralytics</code> para YOLO-NAS no admite entrenamiento.</p> PythonCLI <p>Los archivos de modelos pre-entrenados <code>*.pt</code> de PyTorch se pueden pasar a la clase <code>NAS()</code> para crear una instancia del modelo en Python:</p> <pre><code>from ultralytics import NAS\n\n# Carga un modelo YOLO-NAS-s pre-entrenado en COCO\nmodelo = NAS('yolo_nas_s.pt')\n\n# Muestra informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Valida el modelo en el conjunto de datos de ejemplo COCO8\nresultados = modelo.val(data='coco8.yaml')\n\n# Ejecuta inferencia con el modelo YOLO-NAS-s en la imagen 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Los comandos CLI est\u00e1n disponibles para ejecutar directamente los modelos:</p> <pre><code># Carga un modelo YOLO-NAS-s pre-entrenado en COCO y valida su rendimiento en el conjunto de datos de ejemplo COCO8\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Carga un modelo YOLO-NAS-s pre-entrenado en COCO y ejecuta inferencia en la imagen 'bus.jpg'\nyolo predict model=yolo_nas_s.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolo-nas/#tareas-y-modos-compatibles","title":"Tareas y modos compatibles","text":"<p>Ofrecemos tres variantes de los modelos YOLO-NAS: Small (s), Medium (m) y Large (l). Cada variante est\u00e1 dise\u00f1ada para satisfacer diferentes necesidades computacionales y de rendimiento:</p> <ul> <li>YOLO-NAS-s: Optimizado para entornos donde los recursos computacionales son limitados pero la eficiencia es clave.</li> <li>YOLO-NAS-m: Ofrece un enfoque equilibrado, adecuado para la detecci\u00f3n de objetos de prop\u00f3sito general con mayor precisi\u00f3n.</li> <li>YOLO-NAS-l: Adaptados para escenarios que requieren la mayor precisi\u00f3n, donde los recursos computacionales son menos restrictivos.</li> </ul> <p>A continuaci\u00f3n se muestra una descripci\u00f3n detallada de cada modelo, incluyendo enlaces a sus pesos pre-entrenados, las tareas que admiten y su compatibilidad con diferentes modos de funcionamiento.</p> Tipo de modelo Pesos pre-entrenados Tareas admitidas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n YOLO-NAS-s yolo_nas_s.pt Detecci\u00f3n de objetos \u2705 \u2705 \u274c \u2705 YOLO-NAS-m yolo_nas_m.pt Detecci\u00f3n de objetos \u2705 \u2705 \u274c \u2705 YOLO-NAS-l yolo_nas_l.pt Detecci\u00f3n de objetos \u2705 \u2705 \u274c \u2705"},{"location":"models/yolo-nas/#citaciones-y-agradecimientos","title":"Citaciones y agradecimientos","text":"<p>Si utilizas YOLO-NAS en tu investigaci\u00f3n o trabajo de desarrollo, por favor cita SuperGradients:</p> BibTeX <pre><code>@misc{supergradients,\n      doi = {10.5281/ZENODO.7789328},\n      url = {https://zenodo.org/record/7789328},\n      author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\n      title = {Super-Gradients},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      year = {2021},\n}\n</code></pre> <p>Agradecemos al equipo de SuperGradients de Deci AI por sus esfuerzos en la creaci\u00f3n y mantenimiento de este valioso recurso para la comunidad de visi\u00f3n por computadora. Creemos que YOLO-NAS, con su arquitectura innovadora y sus capacidades de detecci\u00f3n de objetos superiores, se convertir\u00e1 en una herramienta fundamental tanto para desarrolladores como para investigadores.</p> <p>keywords: YOLO-NAS, Deci AI, detecci\u00f3n de objetos, aprendizaje profundo, b\u00fasqueda de arquitectura neural, API de Ultralytics Python, modelo YOLO, SuperGradients, modelos pre-entrenados, bloque b\u00e1sico compatible con cuantizaci\u00f3n, esquemas avanzados de entrenamiento, cuantizaci\u00f3n posterior, optimizaci\u00f3n AutoNAC, COCO, Objects365, Roboflow 100</p>"},{"location":"models/yolov3/","title":"YOLOv3, YOLOv3-Ultralytics y YOLOv3u","text":""},{"location":"models/yolov3/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>Este documento presenta una descripci\u00f3n general de tres modelos de detecci\u00f3n de objetos estrechamente relacionados, conocidos como YOLOv3, YOLOv3-Ultralytics y YOLOv3u.</p> <ol> <li> <p>YOLOv3: Esta es la tercera versi\u00f3n del algoritmo de detecci\u00f3n de objetos You Only Look Once (YOLO). Originalmente desarrollado por Joseph Redmon, YOLOv3 mejor\u00f3 a sus predecesores al introducir caracter\u00edsticas como predicciones multiescala y tres tama\u00f1os diferentes de n\u00facleos de detecci\u00f3n.</p> </li> <li> <p>YOLOv3-Ultralytics: Esta es la implementaci\u00f3n de YOLOv3 realizada por Ultralytics. Reproduce la arquitectura original de YOLOv3 y ofrece funcionalidades adicionales, como soporte para m\u00e1s modelos pre-entrenados y opciones de personalizaci\u00f3n m\u00e1s f\u00e1ciles.</p> </li> <li> <p>YOLOv3u: Esta es una versi\u00f3n actualizada de YOLOv3-Ultralytics que incorpora la cabeza dividida sin anclaje y sin objeto utilizada en los modelos YOLOv8. YOLOv3u mantiene la misma arquitectura de columna vertebral y cuello que YOLOv3, pero con la cabeza de detecci\u00f3n actualizada de YOLOv8.</p> </li> </ol> <p></p>"},{"location":"models/yolov3/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<ul> <li> <p>YOLOv3: Introdujo el uso de tres escalas diferentes para la detecci\u00f3n, aprovechando tres tama\u00f1os diferentes de n\u00facleos de detecci\u00f3n: 13x13, 26x26 y 52x52. Esto mejor\u00f3 significativamente la precisi\u00f3n de detecci\u00f3n para objetos de diferentes tama\u00f1os. Adem\u00e1s, YOLOv3 a\u00f1adi\u00f3 caracter\u00edsticas como predicciones con m\u00faltiples etiquetas para cada cuadro delimitador y una mejor red extractora de caracter\u00edsticas.</p> </li> <li> <p>YOLOv3-Ultralytics: La implementaci\u00f3n de Ultralytics de YOLOv3 proporciona el mismo rendimiento que el modelo original, pero cuenta con soporte adicional para m\u00e1s modelos pre-entrenados, m\u00e9todos de entrenamiento adicionales y opciones de personalizaci\u00f3n m\u00e1s f\u00e1ciles. Esto lo hace m\u00e1s vers\u00e1til y f\u00e1cil de usar para aplicaciones pr\u00e1cticas.</p> </li> <li> <p>YOLOv3u: Este modelo actualizado incorpora la cabeza dividida sin anclaje y sin objeto de YOLOv8. Al eliminar la necesidad de cajas de anclaje predefinidas y puntuaciones de objeto, este dise\u00f1o de cabeza de detecci\u00f3n puede mejorar la capacidad del modelo para detectar objetos de diferentes tama\u00f1os y formas. Esto hace que YOLOv3u sea m\u00e1s robusto y preciso para tareas de detecci\u00f3n de objetos.</p> </li> </ul>"},{"location":"models/yolov3/#tareas-y-modos-admitidos","title":"Tareas y modos admitidos","text":"<p>La serie YOLOv3, que incluye YOLOv3, YOLOv3-Ultralytics y YOLOv3u, est\u00e1 dise\u00f1ada espec\u00edficamente para tareas de detecci\u00f3n de objetos. Estos modelos son reconocidos por su eficacia en diversos escenarios del mundo real, equilibrando precisi\u00f3n y velocidad. Cada variante ofrece caracter\u00edsticas y optimizaciones \u00fanicas, lo que los hace adecuados para una variedad de aplicaciones.</p> <p>Los tres modelos admiten un conjunto completo de modos, asegurando versatilidad en diversas etapas del despliegue y desarrollo del modelo. Estos modos incluyen Inferencia, Validaci\u00f3n, Entrenamiento y Exportaci\u00f3n, proporcionando a los usuarios un conjunto completo de herramientas para una detecci\u00f3n de objetos efectiva.</p> Tipo de modelo Tareas admitidas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n YOLOv3 Detecci\u00f3n de objetos \u2705 \u2705 \u2705 \u2705 YOLOv3-Ultralytics Detecci\u00f3n de objetos \u2705 \u2705 \u2705 \u2705 YOLOv3u Detecci\u00f3n de objetos \u2705 \u2705 \u2705 \u2705 <p>Esta tabla proporciona una visi\u00f3n r\u00e1pida de las capacidades de cada variante de YOLOv3, destacando su versatilidad y aptitud para diversas tareas y modos operativos en flujos de trabajo de detecci\u00f3n de objetos.</p>"},{"location":"models/yolov3/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Este ejemplo proporciona ejemplos sencillos de entrenamiento e inferencia de YOLOv3. Para obtener documentaci\u00f3n completa sobre estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict, Train, Val y Export.</p> <p>Ejemplo</p> PythonCLI <p>Los modelos pre-entrenados de PyTorch en archivos <code>*.pt</code>, as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code>, se pueden pasar a la clase <code>YOLO()</code> para crear una instancia del modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo YOLOv3n pre-entrenado en COCO\nmodel = YOLO('yolov3n.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodel.info()\n\n# Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ejecutar inferencia con el modelo YOLOv3n en la imagen 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Hay comandos de CLI disponibles para ejecutar directamente los modelos:</p> <pre><code># Cargar un modelo YOLOv3n pre-entrenado en COCO y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=yolov3n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Cargar un modelo YOLOv3n pre-entrenado en COCO y ejecutar inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov3n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov3/#citaciones-y-agradecimientos","title":"Citaciones y agradecimientos","text":"<p>Si utilizas YOLOv3 en tu investigaci\u00f3n, por favor, cita los art\u00edculos originales de YOLO y el repositorio de YOLOv3 de Ultralytics:</p> BibTeX <pre><code>@article{redmon2018yolov3,\n  title={YOLOv3: An Incremental Improvement},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1804.02767},\n  year={2018}\n}\n</code></pre> <p>Gracias a Joseph Redmon y Ali Farhadi por desarrollar YOLOv3 original.</p>"},{"location":"models/yolov4/","title":"YOLOv4: Detecci\u00f3n de objetos r\u00e1pida y precisa","text":"<p>Bienvenido a la p\u00e1gina de documentaci\u00f3n de Ultralytics para YOLOv4, un detector de objetos en tiempo real de vanguardia lanzado en 2020 por Alexey Bochkovskiy en https://github.com/AlexeyAB/darknet. YOLOv4 est\u00e1 dise\u00f1ado para ofrecer un equilibrio \u00f3ptimo entre velocidad y precisi\u00f3n, lo que lo convierte en una excelente opci\u00f3n para muchas aplicaciones.</p> <p> Diagrama de arquitectura de YOLOv4. Muestra el intrincado dise\u00f1o de red de YOLOv4, incluyendo los componentes backbone, neck y head, y sus capas interconectadas para una detecci\u00f3n de objetos en tiempo real \u00f3ptima.</p>"},{"location":"models/yolov4/#introduccion","title":"Introducci\u00f3n","text":"<p>YOLOv4 significa You Only Look Once versi\u00f3n 4. Es un modelo de detecci\u00f3n de objetos en tiempo real desarrollado para abordar las limitaciones de versiones anteriores de YOLO como YOLOv3 y otros modelos de detecci\u00f3n de objetos. A diferencia de otros detectores de objetos basados en redes neuronales convolucionales (CNN), YOLOv4 no solo es aplicable para sistemas de recomendaci\u00f3n, sino tambi\u00e9n para la gesti\u00f3n de procesos independientes y la reducci\u00f3n de la entrada humana. Su funcionamiento en unidades de procesamiento de gr\u00e1ficos (GPU) convencionales permite su uso masivo a un precio asequible, y est\u00e1 dise\u00f1ado para funcionar en tiempo real en una GPU convencional, siendo necesario solo una GPU para el entrenamiento.</p>"},{"location":"models/yolov4/#arquitectura","title":"Arquitectura","text":"<p>YOLOv4 utiliza varias caracter\u00edsticas innovadoras que trabajan juntas para optimizar su rendimiento. Estas incluyen Conexiones Residuales Ponderadas (WRC), Conexiones Parciales Cruzadas en Etapas (CSP), Normalizaci\u00f3n Cruzada de Mini-Batch (CmBN), Entrenamiento Autoadversarial (SAT), Activaci\u00f3n Mish, Aumento de Datos Mosaico, Regularizaci\u00f3n DropBlock y P\u00e9rdida CIoU. Estas caracter\u00edsticas se combinan para lograr resultados de vanguardia.</p> <p>Un detector de objetos t\u00edpico est\u00e1 compuesto por varias partes, incluyendo la entrada, el backbone (espinazo), el neck (cuello) y el head (cabeza). El backbone de YOLOv4 est\u00e1 pre-entrenado en ImageNet y se utiliza para predecir las clases y las cajas delimitadoras de los objetos. El backbone puede ser de varios modelos, incluyendo VGG, ResNet, ResNeXt o DenseNet. La parte del neck del detector se utiliza para recolectar mapas de caracter\u00edsticas de diferentes etapas y generalmente incluye varias rutas de abajo hacia arriba y varias rutas de arriba hacia abajo. La parte de la cabeza es la que se utiliza para realizar las detecciones y clasificaciones finales de objetos.</p>"},{"location":"models/yolov4/#bolsa-de-regalos","title":"Bolsa de regalos","text":"<p>YOLOv4 tambi\u00e9n utiliza m\u00e9todos conocidos como \"bolsa de regalos\" (bag of freebies), que son t\u00e9cnicas que mejoran la precisi\u00f3n del modelo durante el entrenamiento sin aumentar el costo de la inferencia. La ampliaci\u00f3n de datos es una t\u00e9cnica com\u00fan de la bolsa de regalos utilizada en la detecci\u00f3n de objetos, que aumenta la variabilidad de las im\u00e1genes de entrada para mejorar la robustez del modelo. Algunos ejemplos de ampliaci\u00f3n de datos incluyen distorsiones fotom\u00e9tricas (ajuste del brillo, contraste, matiz, saturaci\u00f3n y ruido de una imagen) y distorsiones geom\u00e9tricas (agregar escalado, recorte, volteo y rotaci\u00f3n aleatorios). Estas t\u00e9cnicas ayudan al modelo a generalizar mejor para diferentes tipos de im\u00e1genes.</p>"},{"location":"models/yolov4/#caracteristicas-y-rendimiento","title":"Caracter\u00edsticas y rendimiento","text":"<p>YOLOv4 est\u00e1 dise\u00f1ado para obtener una velocidad y precisi\u00f3n \u00f3ptimas en la detecci\u00f3n de objetos. La arquitectura de YOLOv4 incluye CSPDarknet53 como backbone, PANet como neck y YOLOv3 como cabeza de detecci\u00f3n. Este dise\u00f1o permite que YOLOv4 realice la detecci\u00f3n de objetos a una velocidad impresionante, lo que lo hace adecuado para aplicaciones en tiempo real. YOLOv4 tambi\u00e9n sobresale en precisi\u00f3n, logrando resultados de vanguardia en los benchmarks de detecci\u00f3n de objetos.</p>"},{"location":"models/yolov4/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Hasta el momento de escribir este documento, Ultralytics actualmente no admite modelos YOLOv4. Por lo tanto, cualquier usuario interesado en usar YOLOv4 deber\u00e1 consultar directamente el repositorio de YOLOv4 en GitHub para obtener instrucciones de instalaci\u00f3n y uso.</p> <p>Aqu\u00ed hay un resumen breve de los pasos t\u00edpicos que podr\u00edas seguir para usar YOLOv4:</p> <ol> <li> <p>Visita el repositorio de YOLOv4 en GitHub: https://github.com/AlexeyAB/darknet.</p> </li> <li> <p>Sigue las instrucciones proporcionadas en el archivo README para la instalaci\u00f3n. Esto generalmente implica clonar el repositorio, instalar las dependencias necesarias y configurar las variables de entorno necesarias.</p> </li> <li> <p>Una vez que la instalaci\u00f3n est\u00e9 completa, puedes entrenar y usar el modelo seg\u00fan las instrucciones de uso proporcionadas en el repositorio. Esto normalmente implica preparar tu conjunto de datos, configurar los par\u00e1metros del modelo, entrenar el modelo y luego usar el modelo entrenado para realizar la detecci\u00f3n de objetos.</p> </li> </ol> <p>Ten en cuenta que los pasos espec\u00edficos pueden variar dependiendo de tu caso de uso espec\u00edfico y del estado actual del repositorio de YOLOv4. Por lo tanto, se recomienda encarecidamente consultar directamente las instrucciones proporcionadas en el repositorio de YOLOv4 en GitHub.</p> <p>Lamentamos cualquier inconveniente que esto pueda causar y nos esforzaremos por actualizar este documento con ejemplos de uso para Ultralytics una vez que se implemente el soporte para YOLOv4.</p>"},{"location":"models/yolov4/#conclusion","title":"Conclusi\u00f3n","text":"<p>YOLOv4 es un modelo de detecci\u00f3n de objetos potente y eficiente que logra un equilibrio entre velocidad y precisi\u00f3n. Su uso de caracter\u00edsticas \u00fanicas y t\u00e9cnicas de bolsa de regalos durante el entrenamiento le permite realizar un excelente desempe\u00f1o en tareas de detecci\u00f3n de objetos en tiempo real. YOLOv4 puede ser entrenado y utilizado por cualquier persona con una GPU convencional, lo que lo hace accesible y pr\u00e1ctico para una amplia gama de aplicaciones.</p>"},{"location":"models/yolov4/#citaciones-y-agradecimientos","title":"Citaciones y agradecimientos","text":"<p>Nos gustar\u00eda reconocer a los autores de YOLOv4 por sus importantes contribuciones en el campo de la detecci\u00f3n de objetos en tiempo real:</p> BibTeX <pre><code>@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>El art\u00edculo original de YOLOv4 se puede encontrar en arXiv. Los autores han puesto su trabajo a disposici\u00f3n del p\u00fablico, y el c\u00f3digo se puede acceder en GitHub. Apreciamos sus esfuerzos en el avance del campo y en hacer que su trabajo sea accesible para la comunidad en general.</p>"},{"location":"models/yolov5/","title":"YOLOv5","text":""},{"location":"models/yolov5/#resumen","title":"Resumen","text":"<p>YOLOv5u representa un avance en las metodolog\u00edas de detecci\u00f3n de objetos. Originado a partir de la arquitectura fundamental del modelo YOLOv5 desarrollado por Ultralytics, YOLOv5u integra la divisi\u00f3n de la cabeza Ultralytics sin anclas y sin atenci\u00f3n al objeto, una caracter\u00edstica introducida previamente en los modelos YOLOv8. Esta adaptaci\u00f3n perfecciona la arquitectura del modelo, resultando en un mejor equilibrio entre precisi\u00f3n y velocidad en tareas de detecci\u00f3n de objetos. Con base en los resultados emp\u00edricos y sus caracter\u00edsticas derivadas, YOLOv5u proporciona una alternativa eficiente para aquellos que buscan soluciones robustas tanto en investigaci\u00f3n como en aplicaciones pr\u00e1cticas.</p> <p></p>"},{"location":"models/yolov5/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<ul> <li> <p>Cabeza dividida Ultralytics sin anclas: Los modelos tradicionales de detecci\u00f3n de objetos dependen de cajas de anclaje predefinidas para predecir la ubicaci\u00f3n de los objetos. Sin embargo, YOLOv5u moderniza este enfoque. Al adoptar una cabeza Ultralytics dividida sin anclas, se garantiza un mecanismo de detecci\u00f3n m\u00e1s flexible y adaptable, lo que en consecuencia mejora el rendimiento en diversos escenarios.</p> </li> <li> <p>Equilibrio \u00f3ptimo entre precisi\u00f3n y velocidad: La velocidad y la precisi\u00f3n suelen ser contrapuestas. Pero YOLOv5u desaf\u00eda este equilibrio. Ofrece un balance calibrado, garantizando detecciones en tiempo real sin comprometer la precisi\u00f3n. Esta caracter\u00edstica es especialmente valiosa para aplicaciones que requieren respuestas r\u00e1pidas, como veh\u00edculos aut\u00f3nomos, rob\u00f3tica y an\u00e1lisis de video en tiempo real.</p> </li> <li> <p>Variedad de modelos pre-entrenados: Entendiendo que diferentes tareas requieren diferentes herramientas, YOLOv5u proporciona una gran cantidad de modelos pre-entrenados. Ya sea que te enfoques en Inferencia, Validaci\u00f3n o Entrenamiento, hay un modelo a la medida esper\u00e1ndote. Esta variedad asegura que no est\u00e9s utilizando una soluci\u00f3n gen\u00e9rica, sino un modelo espec\u00edficamente ajustado para tu desaf\u00edo \u00fanico.</p> </li> </ul>"},{"location":"models/yolov5/#tareas-y-modos-soportados","title":"Tareas y Modos Soportados","text":"<p>Los modelos YOLOv5u, con diferentes pesos pre-entrenados, sobresalen en las tareas de Detecci\u00f3n de Objetos. Soportan una amplia gama de modos que los hacen adecuados para diversas aplicaciones, desde el desarrollo hasta la implementaci\u00f3n.</p> Tipo de Modelo Pesos Pre-entrenados Tarea Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n YOLOv5u <code>yolov5nu</code>, <code>yolov5su</code>, <code>yolov5mu</code>, <code>yolov5lu</code>, <code>yolov5xu</code>, <code>yolov5n6u</code>, <code>yolov5s6u</code>, <code>yolov5m6u</code>, <code>yolov5l6u</code>, <code>yolov5x6u</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 <p>Esta tabla proporciona una descripci\u00f3n detallada de las variantes de modelos YOLOv5u, destacando su aplicabilidad en tareas de detecci\u00f3n de objetos y el soporte para varios modos operativos como Inferencia, Validaci\u00f3n, Entrenamiento y Exportaci\u00f3n. Este soporte integral asegura que los usuarios puedan aprovechar al m\u00e1ximo las capacidades de los modelos YOLOv5u en una amplia gama de escenarios de detecci\u00f3n de objetos.</p>"},{"location":"models/yolov5/#metricas-de-rendimiento","title":"M\u00e9tricas de Rendimiento","text":"<p>Rendimiento</p> Detecci\u00f3n <p>Consulta la Documentaci\u00f3n de Detecci\u00f3n para obtener ejemplos de uso con estos modelos entrenados en COCO, los cuales incluyen 80 clases pre-entrenadas.</p> Modelo YAML tama\u00f1o<sup>(p\u00edxeles) mAP<sup>val50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) yolov5nu.pt yolov5n.yaml 640 34.3 73.6 1.06 2.6 7.7 yolov5su.pt yolov5s.yaml 640 43.0 120.7 1.27 9.1 24.0 yolov5mu.pt yolov5m.yaml 640 49.0 233.9 1.86 25.1 64.2 yolov5lu.pt yolov5l.yaml 640 52.2 408.4 2.50 53.2 135.0 yolov5xu.pt yolov5x.yaml 640 53.2 763.2 3.81 97.2 246.4 yolov5n6u.pt yolov5n6.yaml 1280 42.1 211.0 1.83 4.3 7.8 yolov5s6u.pt yolov5s6.yaml 1280 48.6 422.6 2.34 15.3 24.6 yolov5m6u.pt yolov5m6.yaml 1280 53.6 810.9 4.36 41.2 65.7 yolov5l6u.pt yolov5l6.yaml 1280 55.7 1470.9 5.47 86.1 137.4 yolov5x6u.pt yolov5x6.yaml 1280 56.8 2436.5 8.98 155.4 250.7"},{"location":"models/yolov5/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Este ejemplo proporciona ejemplos sencillos de entrenamiento e inferencia de YOLOv5. Para obtener documentaci\u00f3n completa sobre estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict, Train, Val y Export.</p> <p>Ejemplo</p> PythonCLI <p>Los modelos pre-entrenados <code>*.pt</code> de PyTorch, as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code>, se pueden pasar a la clase <code>YOLO()</code> para crear una instancia de modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo YOLOv5n pre-entrenado en COCO\nmodelo = YOLO('yolov5n.pt')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Entrenar el modelo con el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresultados = modelo.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ejecutar inferencia con el modelo YOLOv5n en la imagen 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Hay comandos de CLI disponibles para ejecutar directamente los modelos:</p> <pre><code># Cargar un modelo YOLOv5n pre-entrenado en COCO y entrenarlo con el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Cargar un modelo YOLOv5n pre-entrenado en COCO y ejecutar inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov5/#citaciones-y-reconocimientos","title":"Citaciones y Reconocimientos","text":"<p>Si utilizas YOLOv5 o YOLOv5u en tu investigaci\u00f3n, por favor cita el repositorio de Ultralytics YOLOv5 de la siguiente manera:</p> BibTeX <pre><code>@software{yolov5,\n  title = {Ultralytics YOLOv5},\n  author = {Glenn Jocher},\n  year = {2020},\n  version = {7.0},\n  license = {AGPL-3.0},\n  url = {https://github.com/ultralytics/yolov5},\n  doi = {10.5281/zenodo.3908559},\n  orcid = {0000-0001-5950-6979}\n}\n</code></pre> <p>Ten en cuenta que los modelos YOLOv5 se proporcionan bajo las licencias AGPL-3.0 y Enterprise.</p>"},{"location":"models/yolov6/","title":"Meituan YOLOv6","text":""},{"location":"models/yolov6/#vision-general","title":"Visi\u00f3n general","text":"<p>Meituan YOLOv6 es un detector de objetos de \u00faltima generaci\u00f3n que ofrece un notable equilibrio entre velocidad y precisi\u00f3n, lo que lo convierte en una opci\u00f3n popular para aplicaciones en tiempo real. Este modelo presenta varias mejoras notables en su arquitectura y esquema de entrenamiento, que incluyen la implementaci\u00f3n de un m\u00f3dulo de Concatenaci\u00f3n Bidireccional (BiC), una estrategia de entrenamiento con anclas (AAT) y un dise\u00f1o de columna vertebral y cuello mejorado para lograr una precisi\u00f3n de \u00faltima generaci\u00f3n en el conjunto de datos COCO.</p> <p> Visi\u00f3n general de YOLOv6. Diagrama de la arquitectura del modelo que muestra los componentes de la red redesdise\u00f1ados y las estrategias de entrenamiento que han llevado a mejoras significativas en el rendimiento. (a) El cuello de YOLOv6 (N y S se muestran). Se\u00f1alar que, en M/L, RepBlocks es reemplazado por CSPStackRep. (b) La estructura de un m\u00f3dulo BiC. (c) Un bloque SimCSPSPPF. (fuente).</p>"},{"location":"models/yolov6/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<ul> <li>M\u00f3dulo de Concatenaci\u00f3n Bidireccional (BiC): YOLOv6 introduce un m\u00f3dulo de BiC en el cuello del detector, mejorando las se\u00f1ales de localizaci\u00f3n y ofreciendo mejoras en el rendimiento con una degradaci\u00f3n de velocidad despreciable.</li> <li>Estrategia de Entrenamiento con Anclas (AAT): Este modelo propone AAT para disfrutar de los beneficios de los paradigmas basados en anclas y sin anclas sin comprometer la eficiencia de inferencia.</li> <li>Dise\u00f1o de Columna Vertebral y Cuello Mejorado: Al profundizar en YOLOv6 para incluir otra etapa en la columna vertebral y el cuello, este modelo logra un rendimiento de \u00faltima generaci\u00f3n en el conjunto de datos COCO con una entrada de alta resoluci\u00f3n.</li> <li>Estrategia de Auto-Destilaci\u00f3n: Se implementa una nueva estrategia de auto-destilaci\u00f3n para mejorar el rendimiento de los modelos m\u00e1s peque\u00f1os de YOLOv6, mejorando la rama de regresi\u00f3n auxiliar durante el entrenamiento y elimin\u00e1ndola durante la inferencia para evitar una marcada disminuci\u00f3n de velocidad.</li> </ul>"},{"location":"models/yolov6/#metricas-de-rendimiento","title":"M\u00e9tricas de rendimiento","text":"<p>YOLOv6 proporciona varios modelos pre-entrenados con diferentes escalas:</p> <ul> <li>YOLOv6-N: 37.5% de precisi\u00f3n promedio (AP) en COCO val2017 a 1187 FPS con la GPU NVIDIA Tesla T4.</li> <li>YOLOv6-S: 45.0% de AP a 484 FPS.</li> <li>YOLOv6-M: 50.0% de AP a 226 FPS.</li> <li>YOLOv6-L: 52.8% de AP a 116 FPS.</li> <li>YOLOv6-L6: Precisi\u00f3n de \u00faltima generaci\u00f3n en tiempo real.</li> </ul> <p>YOLOv6 tambi\u00e9n proporciona modelos cuantizados para diferentes precisiones y modelos optimizados para plataformas m\u00f3viles.</p>"},{"location":"models/yolov6/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Este ejemplo proporciona ejemplos sencillos de entrenamiento e inferencia con YOLOv6. Para obtener documentaci\u00f3n completa sobre estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict, Train, Val y Export.</p> <p>Ejemplo</p> PythonCLI <p>Los modelos pre-entrenados en <code>*.pt</code> de PyTorch, as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code>, se pueden pasar a la clase <code>YOLO()</code> para crear una instancia del modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Construir un modelo YOLOv6n desde cero\nmodelo = YOLO('yolov6n.yaml')\n\n# Mostrar informaci\u00f3n del modelo (opcional)\nmodelo.info()\n\n# Entrenar el modelo en el conjunto de datos de ejemplo COCO8 durante 100 epochs\nresultados = modelo.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ejecutar inferencia con el modelo YOLOv6n en la imagen 'bus.jpg'\nresultados = modelo('path/to/bus.jpg')\n</code></pre> <p>Se dispone de comandos de l\u00ednea de comandos (CLI) para ejecutar directamente los modelos:</p> <pre><code># Construir un modelo YOLOv6n desde cero y entrenarlo en el conjunto de datos de ejemplo COCO8 durante 100 epochs\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Construir un modelo YOLOv6n desde cero y ejecutar inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov6n.yaml source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov6/#tareas-y-modos-soportados","title":"Tareas y Modos Soportados","text":"<p>La serie YOLOv6 ofrece una variedad de modelos, cada uno optimizado para Detecci\u00f3n de Objetos de alto rendimiento. Estos modelos se adaptan a distintas necesidades computacionales y requisitos de precisi\u00f3n, lo que los hace vers\u00e1tiles para una amplia gama de aplicaciones.</p> Tipo de Modelo Pesos Pre-entrenados Tareas Soportadas Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n YOLOv6-N <code>yolov6-n.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-S <code>yolov6-s.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-M <code>yolov6-m.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-L <code>yolov6-l.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 YOLOv6-L6 <code>yolov6-l6.pt</code> Detecci\u00f3n de Objetos \u2705 \u2705 \u2705 \u2705 <p>Esta tabla proporciona una descripci\u00f3n detallada de las variantes del modelo YOLOv6, destacando sus capacidades en tareas de detecci\u00f3n de objetos y su compatibilidad con varios modos operativos como Inferencia, Validaci\u00f3n, Entrenamiento y Exportaci\u00f3n. Este soporte integral garantiza que los usuarios puedan aprovechar al m\u00e1ximo las capacidades de los modelos YOLOv6 en una amplia gama de escenarios de detecci\u00f3n de objetos.</p>"},{"location":"models/yolov6/#citaciones-y-agradecimientos","title":"Citaciones y Agradecimientos","text":"<p>Nos gustar\u00eda agradecer a los autores por sus importantes contribuciones en el campo de la detecci\u00f3n de objetos en tiempo real:</p> BibTeX <pre><code>@misc{li2023yolov6,\n      title={YOLOv6 v3.0: A Full-Scale Reloading},\n      author={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\n      year={2023},\n      eprint={2301.05586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Se puede encontrar el art\u00edculo original de YOLOv6 en arXiv. Los autores han puesto su trabajo a disposici\u00f3n del p\u00fablico y el c\u00f3digo fuente se puede acceder en GitHub. Agradecemos sus esfuerzos en avanzar en el campo y hacer que su trabajo sea accesible para la comunidad en general.</p>"},{"location":"models/yolov7/","title":"YOLOv7: Bolsa de Caracter\u00edsticas Entrenable","text":"<p>YOLOv7 es un detector de objetos en tiempo real de \u00faltima generaci\u00f3n que supera a todos los detectores de objetos conocidos tanto en velocidad como en precisi\u00f3n en el rango de 5 FPS a 160 FPS. Tiene la mayor precisi\u00f3n (56.8% AP) entre todos los detectores de objetos en tiempo real conocidos con una velocidad de 30 FPS o superior en la GPU V100. Adem\u00e1s, YOLOv7 supera a otros detectores de objetos como YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5 y muchos otros en cuanto a velocidad y precisi\u00f3n. El modelo se entrena desde cero utilizando el conjunto de datos MS COCO sin utilizar ning\u00fan otro conjunto de datos o pesos pre-entrenados. El c\u00f3digo fuente de YOLOv7 est\u00e1 disponible en GitHub.</p> <p> **Comparaci\u00f3n de los detectores de objetos de estado del arte. ** Seg\u00fan los resultados en la Tabla 2, sabemos que el m\u00e9todo propuesto tiene el mejor equilibrio entre velocidad y precisi\u00f3n de manera integral. Si comparamos YOLOv7-tiny-SiLU con YOLOv5-N (r6.1), nuestro m\u00e9todo es 127 fps m\u00e1s r\u00e1pido y un 10.7% m\u00e1s preciso en AP. Adem\u00e1s, YOLOv7 tiene un AP del 51.4% a una velocidad de cuadro de 161 fps, mientras que PPYOLOE-L con el mismo AP tiene solo una velocidad de cuadro de 78 fps. En t\u00e9rminos de uso de par\u00e1metros, YOLOv7 utiliza un 41% menos que PPYOLOE-L. Si comparamos YOLOv7-X con una velocidad de inferencia de 114 fps con YOLOv5-L (r6.1) con una velocidad de inferencia de 99 fps, YOLOv7-X puede mejorar el AP en un 3.9%. Si se compara YOLOv7-X con YOLOv5-X (r6.1) de una escala similar, la velocidad de inferencia de YOLOv7-X es 31 fps m\u00e1s r\u00e1pida. Adem\u00e1s, en t\u00e9rminos de cantidad de par\u00e1metros y c\u00e1lculos, YOLOv7-X reduce un 22% de los par\u00e1metros y un 8% de los c\u00e1lculos en comparaci\u00f3n con YOLOv5-X (r6.1), pero mejora el AP en un 2.2% (Fuente).</p>"},{"location":"models/yolov7/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>La detecci\u00f3n de objetos en tiempo real es un componente importante en muchos sistemas de visi\u00f3n por computadora, incluyendo el seguimiento de m\u00faltiples objetos, conducci\u00f3n aut\u00f3noma, rob\u00f3tica y an\u00e1lisis de im\u00e1genes m\u00e9dicas. En los \u00faltimos a\u00f1os, el desarrollo de la detecci\u00f3n de objetos en tiempo real se ha centrado en el dise\u00f1o de arquitecturas eficientes y en la mejora de la velocidad de inferencia de diversas CPUs, GPUs y unidades de procesamiento neural (NPUs). YOLOv7 es compatible tanto con GPU para dispositivos m\u00f3viles como con GPU para dispositivos de escritorio, desde el borde hasta la nube.</p> <p>A diferencia de los detectores de objetos en tiempo real tradicionales que se centran en la optimizaci\u00f3n de la arquitectura, YOLOv7 introduce un enfoque en la optimizaci\u00f3n del proceso de entrenamiento. Esto incluye m\u00f3dulos y m\u00e9todos de optimizaci\u00f3n dise\u00f1ados para mejorar la precisi\u00f3n de la detecci\u00f3n de objetos sin aumentar el costo de inferencia, un concepto conocido como \"bolsas de caracter\u00edsticas entrenables\".</p>"},{"location":"models/yolov7/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<p>YOLOv7 introduce varias caracter\u00edsticas clave:</p> <ol> <li> <p>Re-parametrizaci\u00f3n del modelo: YOLOv7 propone un modelo re-parametrizado planificado, que es una estrategia aplicable a capas en diferentes redes con el concepto de propagaci\u00f3n del gradiente.</p> </li> <li> <p>Asignaci\u00f3n din\u00e1mica de etiquetas: El entrenamiento del modelo con m\u00faltiples capas de salida presenta un nuevo problema: \"\u00bfC\u00f3mo asignar objetivos din\u00e1micos para las salidas de diferentes ramas?\" Para resolver este problema, YOLOv7 introduce un nuevo m\u00e9todo de asignaci\u00f3n de etiquetas llamado asignaci\u00f3n de etiquetas guiadas de manera gruesa a fina.</p> </li> <li> <p>Escalado extendido y compuesto: YOLOv7 propone m\u00e9todos de \"escalado extendido\" y \"escalado compuesto\" para el detector de objetos en tiempo real que pueden utilizar eficazmente los par\u00e1metros y c\u00e1lculos.</p> </li> <li> <p>Eficiencia: El m\u00e9todo propuesto por YOLOv7 puede reducir eficazmente aproximadamente el 40% de los par\u00e1metros y el 50% de los c\u00e1lculos del detector de objetos en tiempo real de \u00faltima generaci\u00f3n y tiene una velocidad de inferencia m\u00e1s r\u00e1pida y una mayor precisi\u00f3n de detecci\u00f3n.</p> </li> </ol>"},{"location":"models/yolov7/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Hasta la fecha de redacci\u00f3n de este documento, Ultralytics no admite actualmente modelos YOLOv7. Por lo tanto, los usuarios interesados en utilizar YOLOv7 deber\u00e1n consultar directamente el repositorio de GitHub de YOLOv7 para obtener instrucciones de instalaci\u00f3n y uso.</p> <p>Aqu\u00ed hay un resumen breve de los pasos t\u00edpicos que podr\u00edas seguir para usar YOLOv7:</p> <ol> <li> <p>Visita el repositorio de GitHub de YOLOv7: https://github.com/WongKinYiu/yolov7.</p> </li> <li> <p>Sigue las instrucciones proporcionadas en el archivo README para la instalaci\u00f3n. Esto generalmente implica clonar el repositorio, instalar las dependencias necesarias y configurar las variables de entorno necesarias.</p> </li> <li> <p>Una vez que la instalaci\u00f3n est\u00e9 completa, puedes entrenar y utilizar el modelo seg\u00fan las instrucciones de uso proporcionadas en el repositorio. Esto generalmente implica preparar tu conjunto de datos, configurar los par\u00e1metros del modelo, entrenar el modelo y luego utilizar el modelo entrenado para realizar la detecci\u00f3n de objetos.</p> </li> </ol> <p>Ten en cuenta que los pasos espec\u00edficos pueden variar seg\u00fan tu caso de uso espec\u00edfico y el estado actual del repositorio YOLOv7. Por lo tanto, se recomienda encarecidamente consultar directamente las instrucciones proporcionadas en el repositorio de GitHub de YOLOv7.</p> <p>Lamentamos cualquier inconveniente que esto pueda causar y nos esforzaremos por actualizar este documento con ejemplos de uso para Ultralytics una vez que se implemente el soporte para YOLOv7.</p>"},{"location":"models/yolov7/#citaciones-y-agradecimientos","title":"Citaciones y Agradecimientos","text":"<p>Nos gustar\u00eda agradecer a los autores de YOLOv7 por sus importantes contribuciones en el campo de la detecci\u00f3n de objetos en tiempo real:</p> BibTeX <pre><code>@article{wang2022yolov7,\n  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  journal={arXiv preprint arXiv:2207.02696},\n  year={2022}\n}\n</code></pre> <p>El art\u00edculo original de YOLOv7 se puede encontrar en arXiv. Los autores han hecho su trabajo p\u00fablicamente disponible y el c\u00f3digo se puede acceder en GitHub. Agradecemos sus esfuerzos en el avance del campo y en hacer su trabajo accesible a la comunidad en general.</p>"},{"location":"models/yolov8/","title":"YOLOv8","text":""},{"location":"models/yolov8/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>YOLOv8 es la \u00faltima versi\u00f3n de la serie YOLO de detectores de objetos en tiempo real, ofreciendo un rendimiento de vanguardia en t\u00e9rminos de precisi\u00f3n y velocidad. Bas\u00e1ndose en los avances de las versiones anteriores de YOLO, YOLOv8 presenta nuevas caracter\u00edsticas y optimizaciones que lo convierten en una opci\u00f3n ideal para diversas tareas de detecci\u00f3n de objetos en una amplia gama de aplicaciones.</p> <p></p>"},{"location":"models/yolov8/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li>Arquitecturas avanzadas de columna vertebral y cuello: YOLOv8 utiliza arquitecturas de columna vertebral y cuello de \u00faltima generaci\u00f3n, lo que resulta en una mejor extracci\u00f3n de caracter\u00edsticas y rendimiento de detecci\u00f3n de objetos.</li> <li>Cabeza Ultralytics dividida sin anclaje: YOLOv8 adopta una cabeza Ultralytics dividida sin anclaje, lo que contribuye a una mejor precisi\u00f3n y a un proceso de detecci\u00f3n m\u00e1s eficiente en comparaci\u00f3n con los enfoques basados en anclaje.</li> <li>Equilibrio optimizado entre precisi\u00f3n y velocidad: Con un enfoque en mantener un equilibrio \u00f3ptimo entre precisi\u00f3n y velocidad, YOLOv8 es adecuado para tareas de detecci\u00f3n de objetos en tiempo real en diversas \u00e1reas de aplicaci\u00f3n.</li> <li>Variedad de modelos preentrenados: YOLOv8 ofrece una variedad de modelos preentrenados para adaptarse a diversas tareas y requisitos de rendimiento, lo que facilita encontrar el modelo adecuado para tu caso de uso espec\u00edfico.</li> </ul>"},{"location":"models/yolov8/#tareas-y-modos-compatibles","title":"Tareas y modos compatibles","text":"<p>La serie YOLOv8 ofrece una amplia gama de modelos, cada uno especializado en tareas espec\u00edficas en visi\u00f3n por computadora. Estos modelos est\u00e1n dise\u00f1ados para adaptarse a diversos requisitos, desde la detecci\u00f3n de objetos hasta tareas m\u00e1s complejas como la segmentaci\u00f3n de instancias, la detecci\u00f3n de poses/puntos clave y la clasificaci\u00f3n.</p> <p>Cada variante de la serie YOLOv8 est\u00e1 optimizada para su respectiva tarea, garantizando un alto rendimiento y precisi\u00f3n. Adem\u00e1s, estos modelos son compatibles con varios modos operativos, incluyendo Inference, Validation, Training y Export, lo que facilita su uso en diferentes etapas de implementaci\u00f3n y desarrollo.</p> Modelo Nombres de archivo Tarea Inferencia Validaci\u00f3n Entrenamiento Exportaci\u00f3n YOLOv8 <code>yolov8n.pt</code> <code>yolov8s.pt</code> <code>yolov8m.pt</code> <code>yolov8l.pt</code> <code>yolov8x.pt</code> Detecci\u00f3n \u2705 \u2705 \u2705 \u2705 YOLOv8-seg <code>yolov8n-seg.pt</code> <code>yolov8s-seg.pt</code> <code>yolov8m-seg.pt</code> <code>yolov8l-seg.pt</code> <code>yolov8x-seg.pt</code> Segmentaci\u00f3n de instancias \u2705 \u2705 \u2705 \u2705 YOLOv8-pose <code>yolov8n-pose.pt</code> <code>yolov8s-pose.pt</code> <code>yolov8m-pose.pt</code> <code>yolov8l-pose.pt</code> <code>yolov8x-pose.pt</code> <code>yolov8x-pose-p6.pt</code> Pose/Puntos clave \u2705 \u2705 \u2705 \u2705 YOLOv8-cls <code>yolov8n-cls.pt</code> <code>yolov8s-cls.pt</code> <code>yolov8m-cls.pt</code> <code>yolov8l-cls.pt</code> <code>yolov8x-cls.pt</code> Clasificaci\u00f3n \u2705 \u2705 \u2705 \u2705 <p>Esta tabla proporciona una descripci\u00f3n general de las variantes de modelos YOLOv8, resaltando su aplicabilidad en tareas espec\u00edficas y su compatibilidad con varios modos operativos como Inferencia, Validaci\u00f3n, Entrenamiento y Exportaci\u00f3n. Muestra la versatilidad y robustez de la serie YOLOv8, haci\u00e9ndolos adecuados para una variedad de aplicaciones en visi\u00f3n por computadora.</p>"},{"location":"models/yolov8/#metricas-de-rendimiento","title":"M\u00e9tricas de rendimiento","text":"<p>Rendimiento</p> Detecci\u00f3n (COCO)Detecci\u00f3n (Open Images V7)Segmentaci\u00f3n (COCO)Clasificaci\u00f3n (ImageNet)Pose (COCO) <p>Consulta la documentaci\u00f3n de Detecci\u00f3n para ejemplos de uso con estos modelos entrenados en COCO, que incluyen 80 clases preentrenadas.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>val50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <p>Consulta la documentaci\u00f3n de Detecci\u00f3n para ejemplos de uso con estos modelos entrenados en Open Image V7, que incluyen 600 clases preentrenadas.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>val50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 18.4 142.4 1.21 3.5 10.5 YOLOv8s 640 27.7 183.1 1.40 11.4 29.7 YOLOv8m 640 33.6 408.5 2.26 26.2 80.6 YOLOv8l 640 34.9 596.9 2.43 44.1 167.4 YOLOv8x 640 36.3 860.6 3.56 68.7 260.6 <p>Consulta la documentaci\u00f3n de Segmentaci\u00f3n para ejemplos de uso con estos modelos entrenados en COCO, que incluyen 80 clases preentrenadas.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>caja50-95 mAP<sup>m\u00e1scara50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <p>Consulta la documentaci\u00f3n de Clasificaci\u00f3n para ejemplos de uso con estos modelos entrenados en ImageNet, que incluyen 1000 clases preentrenadas.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) acc<sup>top1 acc<sup>top5 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) a 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <p>Consulta la documentaci\u00f3n de Estimaci\u00f3n de Poses para ejemplos de uso con estos modelos entrenados en COCO, que incluyen 1 clase preentrenada, 'person'.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>pose50-95 mAP<sup>pose50 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4"},{"location":"models/yolov8/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>Este ejemplo proporciona ejemplos sencillos de entrenamiento e inferencia con YOLOv8. Para obtener documentaci\u00f3n completa sobre estos y otros modos, consulta las p\u00e1ginas de documentaci\u00f3n de Predict, Train, Val y Export.</p> <p>Ten en cuenta que el siguiente ejemplo es para modelos de detecci\u00f3n YOLOv8. Para ver las tareas adicionales compatibles, consulta la documentaci\u00f3n de Segment, Classify y Pose.</p> <p>Ejemplo</p> PythonCLI <p>Los modelos preentrenados en PyTorch <code>*.pt</code>, as\u00ed como los archivos de configuraci\u00f3n <code>*.yaml</code>, se pueden pasar a la clase <code>YOLO()</code> para crear una instancia del modelo en Python:</p> <pre><code>from ultralytics import YOLO\n\n# Carga un modelo YOLOv8n preentrenado en COCO\nmodel = YOLO('yolov8n.pt')\n\n# Muestra informaci\u00f3n del modelo (opcional)\nmodel.info()\n\n# Entrena el modelo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Realiza inferencia con el modelo YOLOv8n en la imagen 'bus.jpg'\nresults = model('ruta/a/bus.jpg')\n</code></pre> <p>Hay comandos de CLI disponibles para ejecutar directamente los modelos:</p> <pre><code># Carga un modelo YOLOv8n preentrenado en COCO y entr\u00e9nalo en el conjunto de datos de ejemplo COCO8 durante 100 \u00e9pocas\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Carga un modelo YOLOv8n preentrenado en COCO y realiza inferencia en la imagen 'bus.jpg'\nyolo predict model=yolov8n.pt source=ruta/a/bus.jpg\n</code></pre>"},{"location":"models/yolov8/#citas-y-reconocimientos","title":"Citas y reconocimientos","text":"<p>Si utilizas el modelo YOLOv8 u otro software de este repositorio en tu trabajo, por favor c\u00edtalo utilizando el siguiente formato:</p> BibTeX <pre><code>@software{yolov8_ultralytics,\n  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n  title = {Ultralytics YOLOv8},\n  version = {8.0.0},\n  year = {2023},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n</code></pre> <p>Ten en cuenta que el DOI est\u00e1 pendiente y se agregar\u00e1 a la cita una vez que est\u00e9 disponible. Los modelos de YOLOv8 se proporcionan bajo las licencias AGPL-3.0 y Enterprise.</p>"},{"location":"modes/","title":"Modos de Ultralytics YOLOv8","text":""},{"location":"modes/#introduccion","title":"Introducci\u00f3n","text":"<p>Ultralytics YOLOv8 no es solo otro modelo de detecci\u00f3n de objetos; es un marco de trabajo vers\u00e1til dise\u00f1ado para cubrir todo el ciclo de vida de los modelos de aprendizaje autom\u00e1tico, desde la ingesta de datos y el entrenamiento del modelo hasta la validaci\u00f3n, implementaci\u00f3n y seguimiento en el mundo real. Cada modo sirve para un prop\u00f3sito espec\u00edfico y est\u00e1 dise\u00f1ado para ofrecerte la flexibilidad y eficiencia necesarias para diferentes tareas y casos de uso.</p> <p> Mira: Tutorial de Modos Ultralytics: Entrenar, Validar, Predecir, Exportar y Hacer Benchmarking. </p>"},{"location":"modes/#modos-a-primera-vista","title":"Modos a Primera Vista","text":"<p>Comprender los diferentes modos que soporta Ultralytics YOLOv8 es cr\u00edtico para sacar el m\u00e1ximo provecho a tus modelos:</p> <ul> <li>Modo Entrenar (Train): Afina tu modelo en conjuntos de datos personalizados o pre-cargados.</li> <li>Modo Validar (Val): Un punto de control post-entrenamiento para validar el rendimiento del modelo.</li> <li>Modo Predecir (Predict): Libera el poder predictivo de tu modelo en datos del mundo real.</li> <li>Modo Exportar (Export): Prepara tu modelo para la implementaci\u00f3n en varios formatos.</li> <li>Modo Seguir (Track): Extiende tu modelo de detecci\u00f3n de objetos a aplicaciones de seguimiento en tiempo real.</li> <li>Modo Benchmark (Benchmark): Analiza la velocidad y precisi\u00f3n de tu modelo en diversos entornos de implementaci\u00f3n.</li> </ul> <p>Esta gu\u00eda completa tiene como objetivo proporcionarte una visi\u00f3n general y conocimientos pr\u00e1cticos de cada modo, ayud\u00e1ndote a aprovechar todo el potencial de YOLOv8.</p>"},{"location":"modes/#entrenar-train","title":"Entrenar (Train)","text":"<p>El modo Entrenar se utiliza para entrenar un modelo YOLOv8 en un conjunto de datos personalizado. En este modo, el modelo se entrena utilizando el conjunto de datos y los hiperpar\u00e1metros especificados. El proceso de entrenamiento implica optimizar los par\u00e1metros del modelo para que pueda predecir con precisi\u00f3n las clases y ubicaciones de los objetos en una imagen.</p> <p>Ejemplos de Entrenamiento</p>"},{"location":"modes/#validar-val","title":"Validar (Val)","text":"<p>El modo Validar se usa para validar un modelo YOLOv8 despu\u00e9s de haber sido entrenado. En este modo, el modelo se eval\u00faa en un conjunto de validaci\u00f3n para medir su precisi\u00f3n y rendimiento de generalizaci\u00f3n. Este modo se puede usar para ajustar los hiperpar\u00e1metros del modelo y mejorar su rendimiento.</p> <p>Ejemplos de Validaci\u00f3n</p>"},{"location":"modes/#predecir-predict","title":"Predecir (Predict)","text":"<p>El modo Predecir se utiliza para realizar predicciones usando un modelo YOLOv8 entrenado en im\u00e1genes o videos nuevos. En este modo, el modelo se carga desde un archivo de punto de control, y el usuario puede proporcionar im\u00e1genes o videos para realizar inferencias. El modelo predice las clases y ubicaciones de los objetos en las im\u00e1genes o videos de entrada.</p> <p>Ejemplos de Predicci\u00f3n</p>"},{"location":"modes/#exportar-export","title":"Exportar (Export)","text":"<p>El modo Exportar se utiliza para exportar un modelo YOLOv8 a un formato que se pueda usar para la implementaci\u00f3n. En este modo, el modelo se convierte a un formato que puede ser utilizado por otras aplicaciones de software o dispositivos de hardware. Este modo es \u00fatil al implementar el modelo en entornos de producci\u00f3n.</p> <p>Ejemplos de Exportaci\u00f3n</p>"},{"location":"modes/#seguir-track","title":"Seguir (Track)","text":"<p>El modo Seguir se usa para rastrear objetos en tiempo real utilizando un modelo YOLOv8. En este modo, el modelo se carga desde un archivo de punto de control, y el usuario puede proporcionar un flujo de video en vivo para realizar seguimiento de objetos en tiempo real. Este modo es \u00fatil para aplicaciones como sistemas de vigilancia o coches aut\u00f3nomos.</p> <p>Ejemplos de Seguimiento</p>"},{"location":"modes/#benchmark-benchmark","title":"Benchmark (Benchmark)","text":"<p>El modo Benchmark se utiliza para perfilar la velocidad y precisi\u00f3n de varios formatos de exportaci\u00f3n de YOLOv8. Los benchmarks proporcionan informaci\u00f3n sobre el tama\u00f1o del formato de exportaci\u00f3n, sus m\u00e9tricas de <code>mAP50-95</code> (para detecci\u00f3n de objetos, segmentaci\u00f3n y pose) o m\u00e9tricas de <code>accuracy_top5</code> (para clasificaci\u00f3n), y el tiempo de inferencia en milisegundos por imagen a trav\u00e9s de varios formatos de exportaci\u00f3n como ONNX, OpenVINO, TensorRT y otros. Esta informaci\u00f3n puede ayudar a los usuarios a elegir el formato de exportaci\u00f3n \u00f3ptimo para su caso de uso espec\u00edfico, basado en sus requerimientos de velocidad y precisi\u00f3n.</p> <p>Ejemplos de Benchmarking</p>"},{"location":"modes/benchmark/","title":"Model Benchmarking con Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introduccion","title":"Introducci\u00f3n","text":"<p>Una vez que su modelo est\u00e1 entrenado y validado, el siguiente paso l\u00f3gico es evaluar su rendimiento en varios escenarios del mundo real. El modo benchmark en Ultralytics YOLOv8 cumple con este prop\u00f3sito proporcionando un marco s\u00f3lido para valorar la velocidad y exactitud de su modelo a trav\u00e9s de una gama de formatos de exportaci\u00f3n.</p>"},{"location":"modes/benchmark/#por-que-es-crucial-el-benchmarking","title":"\u00bfPor Qu\u00e9 Es Crucial el Benchmarking?","text":"<ul> <li>Decisiones Informadas: Obtenga perspectivas sobre el equilibrio entre velocidad y precisi\u00f3n.</li> <li>Asignaci\u00f3n de Recursos: Entienda c\u00f3mo diferentes formatos de exportaci\u00f3n se desempe\u00f1an en diferentes hardware.</li> <li>Optimizaci\u00f3n: Aprenda cu\u00e1l formato de exportaci\u00f3n ofrece el mejor rendimiento para su caso de uso espec\u00edfico.</li> <li>Eficiencia de Costo: Haga un uso m\u00e1s eficiente de los recursos de hardware basado en los resultados del benchmark.</li> </ul>"},{"location":"modes/benchmark/#metricas-clave-en-el-modo-benchmark","title":"M\u00e9tricas Clave en el Modo Benchmark","text":"<ul> <li>mAP50-95: Para detecci\u00f3n de objetos, segmentaci\u00f3n y estimaci\u00f3n de pose.</li> <li>accuracy_top5: Para clasificaci\u00f3n de im\u00e1genes.</li> <li>Tiempo de Inferencia: Tiempo tomado para cada imagen en milisegundos.</li> </ul>"},{"location":"modes/benchmark/#formatos-de-exportacion-soportados","title":"Formatos de Exportaci\u00f3n Soportados","text":"<ul> <li>ONNX: Para un rendimiento \u00f3ptimo de CPU</li> <li>TensorRT: Para la m\u00e1xima eficiencia de GPU</li> <li>OpenVINO: Para la optimizaci\u00f3n en hardware de Intel</li> <li>CoreML, TensorFlow SavedModel y M\u00e1s: Para necesidades de despliegue diversas.</li> </ul> <p>Consejo</p> <ul> <li>Exporte a ONNX o OpenVINO para acelerar la velocidad de CPU hasta 3 veces.</li> <li>Exporte a TensorRT para acelerar la velocidad de GPU hasta 5 veces.</li> </ul>"},{"location":"modes/benchmark/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Ejecute benchmarks de YOLOv8n en todos los formatos de exportaci\u00f3n soportados incluyendo ONNX, TensorRT, etc. Vea la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark en GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#argumentos","title":"Argumentos","text":"<p>Argumentos como <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code>, y <code>verbose</code> proporcionan a los usuarios la flexibilidad de ajustar los benchmarks a sus necesidades espec\u00edficas y comparar el rendimiento de diferentes formatos de exportaci\u00f3n con facilidad.</p> Clave Valor Descripci\u00f3n <code>model</code> <code>None</code> ruta al archivo del modelo, es decir, yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> ruta a YAML que referencia el conjunto de datos de benchmarking (bajo la etiqueta <code>val</code>) <code>imgsz</code> <code>640</code> tama\u00f1o de imagen como escalar o lista (h, w), es decir, (640, 480) <code>half</code> <code>False</code> cuantificaci\u00f3n FP16 <code>int8</code> <code>False</code> cuantificaci\u00f3n INT8 <code>device</code> <code>None</code> dispositivo en el que se ejecutar\u00e1, es decir, dispositivo cuda=0 o dispositivo=0,1,2,3 o dispositivo=cpu <code>verbose</code> <code>False</code> no continuar en caso de error (bool), o umbral de piso de valor (float)"},{"location":"modes/benchmark/#formatos-de-exportacion","title":"Formatos de Exportaci\u00f3n","text":"<p>Los benchmarks intentar\u00e1n ejecutarse autom\u00e1ticamente en todos los posibles formatos de exportaci\u00f3n a continuaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vea los detalles completos de <code>export</code> en la p\u00e1gina Export.</p>"},{"location":"modes/export/","title":"Exportaci\u00f3n de Modelos con Ultralytics YOLO","text":""},{"location":"modes/export/#introduccion","title":"Introducci\u00f3n","text":"<p>El objetivo final de entrenar un modelo es desplegarlo para aplicaciones en el mundo real. El modo exportaci\u00f3n en Ultralytics YOLOv8 ofrece una gama vers\u00e1til de opciones para exportar tu modelo entrenado a diferentes formatos, haci\u00e9ndolo desplegable en varias plataformas y dispositivos. Esta gu\u00eda integral pretende guiarte a trav\u00e9s de los matices de la exportaci\u00f3n de modelos, mostrando c\u00f3mo lograr la m\u00e1xima compatibilidad y rendimiento.</p> <p> Ver: C\u00f3mo Exportar un Modelo Entrenado Personalizado de Ultralytics YOLOv8 y Ejecutar Inferencia en Vivo en la Webcam. </p>"},{"location":"modes/export/#por-que-elegir-el-modo-exportacion-de-yolov8","title":"\u00bfPor Qu\u00e9 Elegir el Modo Exportaci\u00f3n de YOLOv8?","text":"<ul> <li>Versatilidad: Exporta a m\u00faltiples formatos incluyendo ONNX, TensorRT, CoreML y m\u00e1s.</li> <li>Rendimiento: Acelera hasta 5 veces la velocidad en GPU con TensorRT y 3 veces en CPU con ONNX o OpenVINO.</li> <li>Compatibilidad: Hacer que tu modelo sea universalmente desplegable en numerosos entornos de hardware y software.</li> <li>Facilidad de Uso: Interfaz de l\u00ednea de comandos simple y API de Python para una exportaci\u00f3n de modelos r\u00e1pida y sencilla.</li> </ul>"},{"location":"modes/export/#caracteristicas-clave-del-modo-de-exportacion","title":"Caracter\u00edsticas Clave del Modo de Exportaci\u00f3n","text":"<p>Aqu\u00ed tienes algunas de las funcionalidades destacadas:</p> <ul> <li>Exportaci\u00f3n con Un Solo Clic: Comandos simples para exportar a diferentes formatos.</li> <li>Exportaci\u00f3n por Lotes: Exporta modelos capaces de inferencia por lotes.</li> <li>Inferencia Optimizada: Los modelos exportados est\u00e1n optimizados para tiempos de inferencia m\u00e1s r\u00e1pidos.</li> <li>V\u00eddeos Tutoriales: Gu\u00edas y tutoriales en profundidad para una experiencia de exportaci\u00f3n fluida.</li> </ul> <p>Consejo</p> <ul> <li>Exporta a ONNX u OpenVINO para acelerar la CPU hasta 3 veces.</li> <li>Exporta a TensorRT para acelerar la GPU hasta 5 veces.</li> </ul>"},{"location":"modes/export/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Exporta un modelo YOLOv8n a un formato diferente como ONNX o TensorRT. Consulta la secci\u00f3n Argumentos m\u00e1s abajo para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Carga un modelo\nmodel = YOLO('yolov8n.pt')  # carga un modelo oficial\nmodel = YOLO('path/to/best.pt')  # carga un modelo entrenado personalizado\n\n# Exporta el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporta modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exporta modelo entrenado personalizado\n</code></pre>"},{"location":"modes/export/#argumentos","title":"Argumentos","text":"<p>Los ajustes de exportaci\u00f3n para modelos YOLO se refieren a las diversas configuraciones y opciones utilizadas para guardar o exportar el modelo para su uso en otros entornos o plataformas. Estos ajustes pueden afectar el rendimiento del modelo, su tama\u00f1o y su compatibilidad con diferentes sistemas. Algunos ajustes comunes de exportaci\u00f3n de YOLO incluyen el formato del archivo del modelo exportado (p. ej., ONNX, TensorFlow SavedModel), el dispositivo en el que se ejecutar\u00e1 el modelo (p. ej., CPU, GPU) y la presencia de caracter\u00edsticas adicionales como m\u00e1scaras o m\u00faltiples etiquetas por caja. Otros factores que pueden afectar el proceso de exportaci\u00f3n incluyen la tarea espec\u00edfica para la que se est\u00e1 utilizando el modelo y los requisitos o limitaciones del entorno o plataforma objetivo. Es importante considerar y configurar cuidadosamente estos ajustes para asegurar que el modelo exportado est\u00e1 optimizado para el caso de uso previsto y se pueda utilizar eficazmente en el entorno objetivo.</p> Llave Valor Descripci\u00f3n <code>format</code> <code>'torchscript'</code> formato al que exportar <code>imgsz</code> <code>640</code> tama\u00f1o de imagen como escalar o lista (h, w), p. ej. (640, 480) <code>keras</code> <code>False</code> usu Keras para la exportaci\u00f3n de TF SavedModel <code>optimize</code> <code>False</code> TorchScript: optimizar para m\u00f3vil <code>half</code> <code>False</code> cuantificaci\u00f3n FP16 <code>int8</code> <code>False</code> cuantificaci\u00f3n INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT: ejes din\u00e1micos <code>simplify</code> <code>False</code> ONNX/TensorRT: simplificar modelo <code>opset</code> <code>None</code> ONNX: versi\u00f3n de opset (opcional, por defecto la m\u00e1s reciente) <code>workspace</code> <code>4</code> TensorRT: tama\u00f1o del espacio de trabajo (GB) <code>nms</code> <code>False</code> CoreML: a\u00f1adir NMS"},{"location":"modes/export/#formatos-de-exportacion","title":"Formatos de Exportaci\u00f3n","text":"<p>Los formatos de exportaci\u00f3n disponibles de YOLOv8 est\u00e1n en la tabla a continuaci\u00f3n. Puedes exportar a cualquier formato usando el argumento <code>format</code>, por ejemplo, <code>format='onnx'</code> o <code>format='engine'</code>.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Predicci\u00f3n del Modelo con YOLO de Ultralytics","text":""},{"location":"modes/predict/#introduccion","title":"Introducci\u00f3n","text":"<p>En el mundo del aprendizaje autom\u00e1tico y la visi\u00f3n por computadora, el proceso de dar sentido a los datos visuales se denomina 'inferencia' o 'predicci\u00f3n'. YOLOv8 de Ultralytics ofrece una caracter\u00edstica poderosa conocida como modo predictivo que est\u00e1 dise\u00f1ada para inferencias de alto rendimiento y en tiempo real en una amplia gama de fuentes de datos.</p> <p> Ver: C\u00f3mo Extraer las Salidas del Modelo YOLOv8 de Ultralytics para Proyectos Personalizados. </p>"},{"location":"modes/predict/#aplicaciones-en-el-mundo-real","title":"Aplicaciones en el Mundo Real","text":"Manufactura Deportes Seguridad Detecci\u00f3n de Repuestos de Veh\u00edculos Detecci\u00f3n de Jugadores de F\u00fatbol Detecci\u00f3n de Ca\u00eddas de Personas"},{"location":"modes/predict/#por-que-utilizar-yolo-de-ultralytics-para-la-inferencia","title":"\u00bfPor Qu\u00e9 Utilizar YOLO de Ultralytics para la Inferencia?","text":"<p>Estas son algunas razones para considerar el modo predictivo de YOLOv8 para sus necesidades de inferencia:</p> <ul> <li>Versatilidad: Capaz de realizar inferencias en im\u00e1genes, videos e incluso transmisiones en vivo.</li> <li>Rendimiento: Dise\u00f1ado para procesamiento en tiempo real y de alta velocidad sin sacrificar precisi\u00f3n.</li> <li>Facilidad de Uso: Interfaces de Python y CLI intuitivas para una r\u00e1pida implementaci\u00f3n y pruebas.</li> <li>Alta Personalizaci\u00f3n: Diversos ajustes y par\u00e1metros para afinar el comportamiento de inferencia del modelo seg\u00fan sus requisitos espec\u00edficos.</li> </ul>"},{"location":"modes/predict/#caracteristicas-principales-del-modo-predictivo","title":"Caracter\u00edsticas Principales del Modo Predictivo","text":"<p>El modo predictivo de YOLOv8 est\u00e1 dise\u00f1ado para ser robusto y vers\u00e1til, y cuenta con:</p> <ul> <li>Compatibilidad con M\u00faltiples Fuentes de Datos: Ya sea que sus datos est\u00e9n en forma de im\u00e1genes individuales, una colecci\u00f3n de im\u00e1genes, archivos de video o transmisiones de video en tiempo real, el modo predictivo le tiene cubierto.</li> <li>Modo de Transmisi\u00f3n: Utilice la funci\u00f3n de transmisi\u00f3n para generar un generador eficiente de memoria de objetos <code>Results</code>. Active esto configurando <code>stream=True</code> en el m\u00e9todo de llamada del predictor.</li> <li>Procesamiento por Lotes: La capacidad de procesar m\u00faltiples im\u00e1genes o fotogramas de video en un solo lote, acelerando a\u00fan m\u00e1s el tiempo de inferencia.</li> <li>Amigable para la Integraci\u00f3n: Se integra f\u00e1cilmente con pipelines de datos existentes y otros componentes de software, gracias a su API flexible.</li> </ul> <p>Los modelos YOLO de Ultralytics devuelven ya sea una lista de objetos <code>Results</code> de Python, o un generador de objetos <code>Results</code> de Python eficiente en memoria cuando se pasa <code>stream=True</code> al modelo durante la inferencia:</p> <p>Predict</p> Devolver una lista con <code>stream=False</code>Devolver un generador con <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n preentrenado\n\n# Ejecutar inferencia por lotes en una lista de im\u00e1genes\nresults = model(['im1.jpg', 'im2.jpg'])  # devuelve una lista de objetos Results\n\n# Procesar lista de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para salidas de bbox\n    masks = result.masks  # Objeto Masks para salidas de m\u00e1scaras de segmentaci\u00f3n\n    keypoints = result.keypoints  # Objeto Keypoints para salidas de postura\n    probs = result.probs  # Objeto Probs para salidas de clasificaci\u00f3n\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # modelo YOLOv8n preentrenado\n\n# Ejecutar inferencia por lotes en una lista de im\u00e1genes\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # devuelve un generador de objetos Results\n\n# Procesar generador de resultados\nfor result in results:\n    boxes = result.boxes  # Objeto Boxes para salidas de bbox\n   .masks = result.masks  # Objeto Masks para salidas de m\u00e1scaras de segmentaci\u00f3n\n    keypoints = result.keypoints  # Objeto Keypoints para salidas de postura\n    probs = result.probs  # Objeto Probs para salidas de clasificaci\u00f3n\n</code></pre>"},{"location":"modes/predict/#fuentes-de-inferencia","title":"Fuentes de Inferencia","text":"<p>YOLOv8 puede procesar diferentes tipos de fuentes de entrada para la inferencia, como se muestra en la tabla a continuaci\u00f3n. Las fuentes incluyen im\u00e1genes est\u00e1ticas, transmisiones de video y varios formatos de datos. La tabla tambi\u00e9n indica si cada fuente se puede utilizar en modo de transmisi\u00f3n con el argumento <code>stream=True</code> \u2705. El modo de transmisi\u00f3n es beneficioso para procesar videos o transmisiones en vivo ya que crea un generador de resultados en lugar de cargar todos los fotogramas en la memoria.</p> <p>Consejo</p> <p>Utilice <code>stream=True</code> para procesar videos largos o conjuntos de datos grandes para gestionar eficientemente la memoria. Cuando <code>stream=False</code>, los resultados de todos los fotogramas o puntos de datos se almacenan en la memoria, lo que puede aumentar r\u00e1pidamente y causar errores de memoria insuficiente para entradas grandes. En contraste, <code>stream=True</code> utiliza un generador, que solo mantiene los resultados del fotograma o punto de datos actual en la memoria, reduciendo significativamente el consumo de memoria y previniendo problemas de falta de memoria.</p> Fuente Argumento Tipo Notas imagen <code>'image.jpg'</code> <code>str</code> o <code>Path</code> Archivo \u00fanico de imagen. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL a una imagen. captura de pantalla <code>'screen'</code> <code>str</code> Captura una captura de pantalla. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Formato HWC con canales RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Formato HWC con canales BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Formato HWC con canales BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Formato BCHW con canales RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> o <code>Path</code> Archivo CSV que contiene rutas a im\u00e1genes, videos o directorios. video \u2705 <code>'video.mp4'</code> <code>str</code> o <code>Path</code> Archivo de video en formatos como MP4, AVI, etc. directorio \u2705 <code>'path/'</code> <code>str</code> o <code>Path</code> Ruta a un directorio que contiene im\u00e1genes o videos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Patr\u00f3n glob para coincidir con m\u00faltiples archivos. Utilice el car\u00e1cter <code>*</code> como comod\u00edn. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL a un video de YouTube. transmisi\u00f3n \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL para protocolos de transmisi\u00f3n como RTSP, RTMP, TCP o una direcci\u00f3n IP. multi-transmisi\u00f3n \u2705 <code>'list.streams'</code> <code>str</code> o <code>Path</code> Archivo de texto <code>*.streams</code> con una URL de transmisi\u00f3n por fila, es decir, 8 transmisiones se ejecutar\u00e1n con tama\u00f1o de lote 8. <p>A continuaci\u00f3n se muestran ejemplos de c\u00f3digo para usar cada tipo de fuente:</p> <p>Fuentes de predicci\u00f3n</p> imagencaptura de pantallaURLPILOpenCVnumpytorch <p>Ejecute inferencia en un archivo de imagen. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir la ruta al archivo de imagen\nsource = 'ruta/a/imagen.jpg'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en el contenido actual de la pantalla como captura de pantalla. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir captura de pantalla actual como fuente\nsource = 'screen'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen o video alojados remotamente a trav\u00e9s de URL. <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Definir URL remota de imagen o video\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen abierta con la Biblioteca de Im\u00e1genes de Python (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Abrir una imagen usando PIL\nsource = Image.open('ruta/a/imagen.jpg')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen le\u00edda con OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Leer una imagen usando OpenCV\nsource = cv2.imread('ruta/a/imagen.jpg')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen representada como un array de numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8n preentrenado\nmodel = YOLO('yolov8n.pt')\n\n# Crear un array aleatorio de numpy con forma HWC (640, 640, 3) con valores en rango [0, 255] y tipo uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Ejecutar inferencia en la fuente\nresults = model(source)  # lista de objetos Results\n</code></pre></p> <p>Ejecute inferencia en una imagen representada como un tensor de PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#cargar-el-modelo-yolov8n-preentrenado","title":"Cargar el modelo YOLOv8n preentrenado","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#crear-un-tensor-aleatorio-de-torch-con-forma-bchw-1-3-640-640-con-valores-en-rango-0-1-y-tipo-float32","title":"Crear un tensor aleatorio de torch con forma BCHW (1, 3, 640, 640) con valores en rango [0, 1] y tipo float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#ejecutar-inferencia-en-la-fuente","title":"Ejecutar inferencia en la fuente","text":"<p>results = model(source)  # lista de objetos Results</p>"},{"location":"modes/track/","title":"Seguimiento de M\u00faltiples Objetos con Ultralytics YOLO","text":"<p>El seguimiento de objetos en el \u00e1mbito del an\u00e1lisis de video es una tarea cr\u00edtica que no solo identifica la ubicaci\u00f3n y clase de objetos dentro del cuadro, sino que tambi\u00e9n mantiene una ID \u00fanica para cada objeto detectado a medida que avanza el video. Las aplicaciones son ilimitadas, desde vigilancia y seguridad hasta an\u00e1lisis deportivos en tiempo real.</p>"},{"location":"modes/track/#por-que-elegir-ultralytics-yolo-para-el-seguimiento-de-objetos","title":"\u00bfPor Qu\u00e9 Elegir Ultralytics YOLO para el Seguimiento de Objetos?","text":"<p>La salida de los rastreadores de Ultralytics es consistente con la detecci\u00f3n de objetos est\u00e1ndar, pero con el valor a\u00f1adido de las IDs de objetos. Esto facilita el seguimiento de objetos en flujos de video y la realizaci\u00f3n de an\u00e1lisis posteriores. Aqu\u00ed tienes algunas razones por las que deber\u00edas considerar usar Ultralytics YOLO para tus necesidades de seguimiento de objetos:</p> <ul> <li>Eficiencia: Procesa flujos de video en tiempo real sin comprometer la precisi\u00f3n.</li> <li>Flexibilidad: Soporta m\u00faltiples algoritmos de seguimiento y configuraciones.</li> <li>Facilidad de Uso: API simple de Python y opciones CLI para una r\u00e1pida integraci\u00f3n y despliegue.</li> <li>Personalizaci\u00f3n: F\u00e1cil de usar con modelos YOLO entrenados a medida, permitiendo la integraci\u00f3n en aplicaciones espec\u00edficas del dominio.</li> </ul> <p> Ver: Detecci\u00f3n de Objetos y Seguimiento con Ultralytics YOLOv8. </p>"},{"location":"modes/track/#aplicaciones-en-el-mundo-real","title":"Aplicaciones en el Mundo Real","text":"Transporte Venta al por Menor Acuicultura Seguimiento de Veh\u00edculos Seguimiento de Personas Seguimiento de Peces"},{"location":"modes/track/#caracteristicas-a-simple-vista","title":"Caracter\u00edsticas a Simple Vista","text":"<p>Ultralytics YOLO extiende sus caracter\u00edsticas de detecci\u00f3n de objetos para proporcionar un seguimiento de objetos robusto y vers\u00e1til:</p> <ul> <li>Seguimiento en Tiempo Real: Rastrea sin problemas los objetos en videos de alta frecuencia de cuadros.</li> <li>Soporte de M\u00faltiples Rastreadores: Elige entre una variedad de algoritmos de seguimiento establecidos.</li> <li>Configuraciones de Rastreador Personalizables: Adapta el algoritmo de seguimiento para satisfacer requisitos espec\u00edficos ajustando diversos par\u00e1metros.</li> </ul>"},{"location":"modes/track/#rastreadores-disponibles","title":"Rastreadores Disponibles","text":"<p>Ultralytics YOLO soporta los siguientes algoritmos de seguimiento. Pueden ser habilitados pasando el archivo de configuraci\u00f3n YAML relevante como <code>tracker=tracker_type.yaml</code>:</p> <ul> <li>BoT-SORT - Usa <code>botsort.yaml</code> para habilitar este rastreador.</li> <li>ByteTrack - Usa <code>bytetrack.yaml</code> para habilitar este rastreador.</li> </ul> <p>El rastreador predeterminado es BoT-SORT.</p>"},{"location":"modes/track/#seguimiento","title":"Seguimiento","text":"<p>Para ejecutar el rastreador en flujos de video, usa un modelo Detect, Segment o Pose entrenado tales como YOLOv8n, YOLOv8n-seg y YOLOv8n-pose.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo oficial o personalizado\nmodel = YOLO('yolov8n.pt')  # Cargar un modelo oficial Detect\nmodel = YOLO('yolov8n-seg.pt')  # Cargar un modelo oficial Segment\nmodel = YOLO('yolov8n-pose.pt')  # Cargar un modelo oficial Pose\nmodel = YOLO('path/to/best.pt')  # Cargar un modelo entrenado a medida\n\n# Realizar el seguimiento con el modelo\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Seguimiento con el rastreador predeterminado\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Seguimiento con el rastreador ByteTrack\n</code></pre> <pre><code># Realizar seguimiento con varios modelos usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Detect\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Segment\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo oficial Pose\nyolo track model=path/to/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Modelo entrenado a medida\n\n# Realizar seguimiento usando el rastreador ByteTrack\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Como se puede ver en el uso anterior, el seguimiento est\u00e1 disponible para todos los modelos Detect, Segment y Pose ejecutados en videos o fuentes de transmisi\u00f3n.</p>"},{"location":"modes/track/#configuracion","title":"Configuraci\u00f3n","text":""},{"location":"modes/track/#argumentos-de-seguimiento","title":"Argumentos de Seguimiento","text":"<p>La configuraci\u00f3n de seguimiento comparte propiedades con el modo Predict, como <code>conf</code>, <code>iou</code> y <code>show</code>. Para configuraciones adicionales, consulta la p\u00e1gina del modelo Predict.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurar los par\u00e1metros de seguimiento y ejecutar el rastreador\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurar par\u00e1metros de seguimiento y ejecutar el rastreador usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#seleccion-de-rastreador","title":"Selecci\u00f3n de Rastreador","text":"<p>Ultralytics tambi\u00e9n te permite usar un archivo de configuraci\u00f3n de rastreador modificado. Para hacerlo, simplemente haz una copia de un archivo de configuraci\u00f3n de rastreador (por ejemplo, <code>custom_tracker.yaml</code>) de ultralytics/cfg/trackers y modifica cualquier configuraci\u00f3n (excepto el <code>tracker_type</code>) seg\u00fan tus necesidades.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar el modelo y ejecutar el rastreador con un archivo de configuraci\u00f3n personalizado\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Cargar el modelo y ejecutar el rastreador con un archivo de configuraci\u00f3n personalizado usando la interfaz de l\u00ednea de comandos\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Para obtener una lista completa de los argumentos de seguimiento, consulta la p\u00e1gina ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#ejemplos-en-python","title":"Ejemplos en Python","text":""},{"location":"modes/track/#bucle-de-seguimiento-persistente","title":"Bucle de Seguimiento Persistente","text":"<p>Aqu\u00ed hay un script en Python que utiliza OpenCV (<code>cv2</code>) y YOLOv8 para ejecutar el seguimiento de objetos en fotogramas de video. Este script a\u00fan asume que ya has instalado los paquetes necesarios (<code>opencv-python</code> y <code>ultralytics</code>). El argumento <code>persist=True</code> le indica al rastreador que la imagen o fotograma actual es el siguiente en una secuencia y que espera rastros de la imagen anterior en la imagen actual.</p> <p>Bucle de transmisi\u00f3n en vivo con seguimiento</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Cargar el modelo YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Abrir el archivo de video\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Bucle a trav\u00e9s de los fotogramas del video\nwhile cap.isOpened():\n    # Leer un fotograma del video\n    success, frame = cap.read()\n\n    if success:\n        # Ejecutar seguimiento YOLOv8 en el fotograma, persistiendo los rastreos entre fotogramas\n        results = model.track(frame, persist=True)\n\n        # Visualizar los resultados en el fotograma\n        annotated_frame = results[0].plot()\n\n        # Mostrar el fotograma anotado\n        cv2.imshow(\"Seguimiento YOLOv8\", annotated_frame)\n\n        # Romper el bucle si se presiona 'q'\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Romper el bucle si se alcanza el final del video\n        break\n\n# Liberar el objeto de captura de video y cerrar la ventana de visualizaci\u00f3n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Toma en cuenta el cambio de <code>model(frame)</code> a <code>model.track(frame)</code>, que habilita el seguimiento de objetos en lugar de simplemente la detecci\u00f3n. Este script modificado ejecutar\u00e1 el rastreador en cada fotograma del video, visualizar\u00e1 los resultados y los mostrar\u00e1 en una ventana. El bucle puede ser terminado presionando 'q'.</p>"},{"location":"modes/track/#contribuir-con-nuevos-rastreadores","title":"Contribuir con Nuevos Rastreadores","text":"<p>\u00bfEres experto en seguimiento de m\u00faltiples objetos y has implementado o adaptado exitosamente un algoritmo de seguimiento con Ultralytics YOLO? Te invitamos a contribuir en nuestra secci\u00f3n de Rastreadores en ultralytics/cfg/trackers! Tus aplicaciones en el mundo real y soluciones podr\u00edan ser invaluables para los usuarios que trabajan en tareas de seguimiento.</p> <p>Al contribuir en esta secci\u00f3n, ayudar\u00e1s a ampliar el alcance de las soluciones de seguimiento disponibles dentro del marco de trabajo de Ultralytics YOLO, a\u00f1adiendo otra capa de funcionalidad y utilidad para la comunidad.</p> <p>Para iniciar tu contribuci\u00f3n, por favor consulta nuestra Gu\u00eda de Contribuci\u00f3n para obtener instrucciones completas sobre c\u00f3mo enviar una Solicitud de Extracci\u00f3n (PR) \ud83d\udee0\ufe0f. \u00a1Estamos emocionados de ver lo que traes a la mesa!</p> <p>Juntos, vamos a mejorar las capacidades de seguimiento del ecosistema Ultralytics YOLO \ud83d\ude4f!</p>"},{"location":"modes/train/","title":"Entrenamiento de Modelos con Ultralytics YOLO","text":""},{"location":"modes/train/#introduccion","title":"Introducci\u00f3n","text":"<p>Entrenar un modelo de aprendizaje profundo implica alimentarlo con datos y ajustar sus par\u00e1metros para que pueda hacer predicciones precisas. El modo de entrenamiento en Ultralytics YOLOv8 est\u00e1 dise\u00f1ado para un entrenamiento efectivo y eficiente de modelos de detecci\u00f3n de objetos, aprovechando al m\u00e1ximo las capacidades del hardware moderno. Esta gu\u00eda tiene como objetivo cubrir todos los detalles que necesita para comenzar a entrenar sus propios modelos utilizando el robusto conjunto de caracter\u00edsticas de YOLOv8.</p> <p> Ver: C\u00f3mo Entrenar un modelo YOLOv8 en Tu Conjunto de Datos Personalizado en Google Colab. </p>"},{"location":"modes/train/#por-que-elegir-ultralytics-yolo-para-entrenamiento","title":"\u00bfPor Qu\u00e9 Elegir Ultralytics YOLO para Entrenamiento?","text":"<p>Aqu\u00ed hay algunas razones convincentes para optar por el modo Entrenamiento de YOLOv8:</p> <ul> <li>Eficiencia: Aprovecha al m\u00e1ximo tu hardware, ya sea en una configuraci\u00f3n de una sola GPU o escalando entre m\u00faltiples GPUs.</li> <li>Versatilidad: Entrena con conjuntos de datos personalizados adem\u00e1s de los ya disponibles como COCO, VOC e ImageNet.</li> <li>Amigable al Usuario: Interfaces CLI y Python simples pero potentes para una experiencia de entrenamiento sencilla.</li> <li>Flexibilidad de Hiperpar\u00e1metros: Una amplia gama de hiperpar\u00e1metros personalizables para ajustar el rendimiento del modelo.</li> </ul>"},{"location":"modes/train/#caracteristicas-clave-del-modo-entrenamiento","title":"Caracter\u00edsticas Clave del Modo Entrenamiento","text":"<p>Las siguientes son algunas caracter\u00edsticas notables del modo Entrenamiento de YOLOv8:</p> <ul> <li>Descarga Autom\u00e1tica de Conjuntos de Datos: Conjuntos de datos est\u00e1ndar como COCO, VOC e ImageNet se descargan autom\u00e1ticamente en el primer uso.</li> <li>Soporte Multi-GPU: Escala tus esfuerzos de entrenamiento sin problemas en m\u00faltiples GPUs para acelerar el proceso.</li> <li>Configuraci\u00f3n de Hiperpar\u00e1metros: La opci\u00f3n de modificar hiperpar\u00e1metros a trav\u00e9s de archivos de configuraci\u00f3n YAML o argumentos CLI.</li> <li>Visualizaci\u00f3n y Monitoreo: Seguimiento en tiempo real de m\u00e9tricas de entrenamiento y visualizaci\u00f3n del proceso de aprendizaje para una mejor comprensi\u00f3n.</li> </ul> <p>Consejo</p> <ul> <li>Los conjuntos de datos de YOLOv8 como COCO, VOC, ImageNet y muchos otros se descargan autom\u00e1ticamente en el primer uso, es decir, <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Entrena YOLOv8n en el conjunto de datos COCO128 durante 100 \u00e9pocas con un tama\u00f1o de imagen de 640. El dispositivo de entrenamiento se puede especificar usando el argumento <code>device</code>. Si no se pasa ning\u00fan argumento, se usar\u00e1 la GPU <code>device=0</code> si est\u00e1 disponible; de lo contrario, se usar\u00e1 <code>device=cpu</code>. Consulta la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de entrenamiento.</p> <p>Ejemplo de Entrenamiento con una sola GPU y CPU</p> <p>El dispositivo se determina autom\u00e1ticamente. Si hay una GPU disponible, se usar\u00e1; de lo contrario, el entrenamiento comenzar\u00e1 en la CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.yaml')  # construir un modelo nuevo desde YAML\nmodel = YOLO('yolov8n.pt')    # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un modelo nuevo desde YAML y comenzar el entrenamiento desde cero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Comenzar el entrenamiento desde un modelo preentrenado *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir un modelo nuevo desde YAML, transferir pesos preentrenados a \u00e9l y comenzar el entrenamiento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#entrenamiento-multi-gpu","title":"Entrenamiento Multi-GPU","text":"<p>El entrenamiento Multi-GPU permite una utilizaci\u00f3n m\u00e1s eficiente de los recursos de hardware disponibles, distribuyendo la carga de entrenamiento en varias GPUs. Esta caracter\u00edstica est\u00e1 disponible tanto a trav\u00e9s de la API de Python como de la interfaz de l\u00ednea de comandos. Para habilitar el entrenamiento Multi-GPU, especifica los IDs de los dispositivos GPU que deseas usar.</p> <p>Ejemplo de Entrenamiento Multi-GPU</p> <p>Para entrenar con 2 GPUs, dispositivos CUDA 0 y 1, usa los siguientes comandos. Ampl\u00eda a GPUs adicionales seg\u00fan sea necesario.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\n\n# Entrenar el modelo con 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Comenzar el entrenamiento desde un modelo preentrenado *.pt usando las GPUs 0 y 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#entrenamiento-con-apple-m1-y-m2-mps","title":"Entrenamiento con Apple M1 y M2 MPS","text":"<p>Con el soporte para los chips Apple M1 y M2 integrados en los modelos Ultralytics YOLO, ahora es posible entrenar tus modelos en dispositivos que utilizan el potente marco de Metal Performance Shaders (MPS). El MPS ofrece una forma de alto rendimiento para ejecutar tareas de c\u00e1lculo y procesamiento de im\u00e1genes en el silicio personalizado de Apple.</p> <p>Para habilitar el entrenamiento en chips Apple M1 y M2, debes especificar 'mps' como tu dispositivo al iniciar el proceso de entrenamiento. A continuaci\u00f3n se muestra un ejemplo de c\u00f3mo podr\u00edas hacer esto en Python y a trav\u00e9s de la l\u00ednea de comandos:</p> <p>Ejemplo de Entrenamiento MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\n\n# Entrenar el modelo con 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Comenzar el entrenamiento desde un modelo preentrenado *.pt usando las GPUs 0 y 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Al aprovechar el poder computacional de los chips M1/M2, esto permite un procesamiento m\u00e1s eficiente de las tareas de entrenamiento. Para obtener una gu\u00eda m\u00e1s detallada y opciones de configuraci\u00f3n avanzadas, consulta la documentaci\u00f3n de PyTorch MPS.</p>"},{"location":"modes/train/#registros-logging","title":"Registros (Logging)","text":"<p>Al entrenar un modelo YOLOv8, puedes encontrar valioso llevar un registro del rendimiento del modelo con el tiempo. Aqu\u00ed es donde entra en juego el registro. Ultralytics' YOLO ofrece soporte para tres tipos de registradores: Comet, ClearML y TensorBoard.</p> <p>Para usar un registrador, selecci\u00f3nalo en el men\u00fa desplegable en el fragmento de c\u00f3digo anterior y ejec\u00fatalo. El registrador elegido se instalar\u00e1 e inicializar\u00e1.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet es una plataforma que permite a los cient\u00edficos de datos y desarrolladores rastrear, comparar, explicar y optimizar experimentos y modelos. Ofrece funcionalidades como m\u00e9tricas en tiempo real, diferencias de c\u00f3digo y seguimiento de hiperpar\u00e1metros.</p> <p>Para usar Comet:</p> <p>Ejemplo</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>Recuerda iniciar sesi\u00f3n en tu cuenta de Comet en su sitio web y obtener tu clave API. Necesitar\u00e1s agregar esto a tus variables de entorno o tu script para registrar tus experimentos.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML es una plataforma de c\u00f3digo abierto que automatiza el seguimiento de experimentos y ayuda con la compartici\u00f3n eficiente de recursos. Est\u00e1 dise\u00f1ado para ayudar a los equipos a gestionar, ejecutar y reproducir su trabajo de ML de manera m\u00e1s eficiente.</p> <p>Para usar ClearML:</p> <p>Ejemplo</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Despu\u00e9s de ejecutar este script, necesitar\u00e1s iniciar sesi\u00f3n en tu cuenta de ClearML en el navegador y autenticar tu sesi\u00f3n.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard es una herramienta de visualizaci\u00f3n para TensorFlow. Te permite visualizar tu grafo TensorFlow, trazar m\u00e9tricas cuantitativas sobre la ejecuci\u00f3n de tu grafo y mostrar datos adicionales como im\u00e1genes que lo atraviesan.</p> <p>Para usar TensorBoard en Google Colab:</p> <p>Ejemplo</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # reemplazar con el directorio 'runs'\n</code></pre> <p>Para usar TensorBoard localmente, ejecuta el siguiente comando y visualiza los resultados en http://localhost:6006/.</p> <p>Ejemplo</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # reemplazar con el directorio 'runs'\n</code></pre> <p>Esto cargar\u00e1 TensorBoard y lo dirigir\u00e1 al directorio donde se guardan tus registros de entrenamiento.</p> <p>Despu\u00e9s de configurar tu registrador, puedes proceder con tu entrenamiento de modelo. Todas las m\u00e9tricas de entrenamiento se registrar\u00e1n autom\u00e1ticamente en la plataforma elegida y podr\u00e1s acceder a estos registros para monitorear el rendimiento de tu modelo con el tiempo, comparar diferentes modelos e identificar \u00e1reas de mejora.</p>"},{"location":"modes/val/","title":"Validaci\u00f3n de modelos con Ultralytics YOLO","text":""},{"location":"modes/val/#introduccion","title":"Introducci\u00f3n","text":"<p>La validaci\u00f3n es un paso cr\u00edtico en el flujo de trabajo de aprendizaje autom\u00e1tico, permiti\u00e9ndole evaluar la calidad de sus modelos entrenados. El modo Val en Ultralytics YOLOv8 proporciona un robusto conjunto de herramientas y m\u00e9tricas para evaluar el rendimiento de sus modelos de detecci\u00f3n de objetos. Esta gu\u00eda sirve como un recurso completo para comprender c\u00f3mo utilizar efectivamente el modo Val para asegurar que sus modelos sean precisos y confiables.</p>"},{"location":"modes/val/#por-que-validar-con-ultralytics-yolo","title":"\u00bfPor qu\u00e9 validar con Ultralytics YOLO?","text":"<p>Estas son las ventajas de usar el modo Val de YOLOv8:</p> <ul> <li>Precisi\u00f3n: Obtenga m\u00e9tricas precisas como mAP50, mAP75 y mAP50-95 para evaluar de manera integral su modelo.</li> <li>Comodidad: Utilice funciones integradas que recuerdan los ajustes de entrenamiento, simplificando el proceso de validaci\u00f3n.</li> <li>Flexibilidad: Valide su modelo con el mismo conjunto de datos o diferentes conjuntos de datos y tama\u00f1os de imagen.</li> <li>Ajuste de Hiperpar\u00e1metros: Use las m\u00e9tricas de validaci\u00f3n para ajustar su modelo y mejorar el rendimiento.</li> </ul>"},{"location":"modes/val/#caracteristicas-principales-del-modo-val","title":"Caracter\u00edsticas principales del modo Val","text":"<p>Estas son las funcionalidades notables ofrecidas por el modo Val de YOLOv8:</p> <ul> <li>Configuraciones Automatizadas: Los modelos recuerdan sus configuraciones de entrenamiento para una validaci\u00f3n sencilla.</li> <li>Soporte de M\u00faltiples M\u00e9tricas: Eval\u00fae su modelo basado en una gama de m\u00e9tricas de precisi\u00f3n.</li> <li>CLI y API de Python: Elija entre la interfaz de l\u00ednea de comandos o API de Python basada en su preferencia para validaci\u00f3n.</li> <li>Compatibilidad de Datos: Funciona sin problemas con conjuntos de datos utilizados durante la fase de entrenamiento as\u00ed como con conjuntos de datos personalizados.</li> </ul> <p>Consejo</p> <ul> <li>Los modelos YOLOv8 recuerdan autom\u00e1ticamente sus ajustes de entrenamiento, as\u00ed que puede validar un modelo en el mismo tama\u00f1o de imagen y en el conjunto de datos original f\u00e1cilmente con solo <code>yolo val model=yolov8n.pt</code> o <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#ejemplos-de-uso","title":"Ejemplos de Uso","text":"<p>Valide la precisi\u00f3n del modelo YOLOv8n entrenado en el conjunto de datos COCO128. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo. Vea la secci\u00f3n de Argumentos a continuaci\u00f3n para una lista completa de argumentos de exportaci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y ajustes se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista que contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val model oficial\nyolo detect val model=ruta/a/best.pt  # val model personalizado\n</code></pre>"},{"location":"modes/val/#argumentos","title":"Argumentos","text":"<p>Los ajustes de validaci\u00f3n para modelos YOLO se refieren a los diversos hiperpar\u00e1metros y configuraciones utilizados para evaluar el rendimiento del modelo en un conjunto de datos de validaci\u00f3n. Estos ajustes pueden afectar el rendimiento, la velocidad y la precisi\u00f3n del modelo. Algunos ajustes comunes de validaci\u00f3n YOLO incluyen el tama\u00f1o del lote, la frecuencia con la que se realiza la validaci\u00f3n durante el entrenamiento y las m\u00e9tricas utilizadas para evaluar el rendimiento del modelo. Otros factores que pueden afectar el proceso de validaci\u00f3n incluyen el tama\u00f1o y la composici\u00f3n del conjunto de datos de validaci\u00f3n y la tarea espec\u00edfica para la que se utiliza el modelo. Es importante ajustar y experimentar cuidadosamente con estos ajustes para asegurarse de que el modelo est\u00e9 funcionando bien en el conjunto de datos de validaci\u00f3n y para detectar y prevenir el sobreajuste.</p> Clave Valor Descripci\u00f3n <code>data</code> <code>None</code> ruta al archivo de datos, por ejemplo coco128.yaml <code>imgsz</code> <code>640</code> tama\u00f1o de las im\u00e1genes de entrada como entero <code>batch</code> <code>16</code> n\u00famero de im\u00e1genes por lote (-1 para AutoBatch) <code>save_json</code> <code>False</code> guardar resultados en archivo JSON <code>save_hybrid</code> <code>False</code> guardar versi\u00f3n h\u00edbrida de las etiquetas (etiquetas + predicciones adicionales) <code>conf</code> <code>0.001</code> umbral de confianza del objeto para detecci\u00f3n <code>iou</code> <code>0.6</code> umbral de Intersecci\u00f3n sobre Uni\u00f3n (IoU) para NMS <code>max_det</code> <code>300</code> n\u00famero m\u00e1ximo de detecciones por imagen <code>half</code> <code>True</code> usar precisi\u00f3n de punto flotante de media preci\u00f3n (FP16) <code>device</code> <code>None</code> dispositivo en el que se ejecuta, por ejemplo dispositivo cuda=0/1/2/3 o dispositivo=cpu <code>dnn</code> <code>False</code> utilizar OpenCV DNN para inferencia ONNX <code>plots</code> <code>False</code> mostrar gr\u00e1ficos durante el entrenamiento <code>rect</code> <code>False</code> val rectangular con cada lote compilado para el m\u00ednimo relleno <code>split</code> <code>val</code> divisi\u00f3n del conjunto de datos a utilizar para la validaci\u00f3n, por ejemplo 'val', 'test' o 'train'"},{"location":"tasks/","title":"Tareas de Ultralytics YOLOv8","text":"<p>YOLOv8 es un marco de trabajo de IA que soporta m\u00faltiples tareas de visi\u00f3n por computadora. El marco puede usarse para realizar detecci\u00f3n, segmentaci\u00f3n, clasificaci\u00f3n y estimaci\u00f3n de pose. Cada una de estas tareas tiene un objetivo y caso de uso diferente.</p> <p>Nota</p> <p>\ud83d\udea7 Nuestra documentaci\u00f3n multilenguaje est\u00e1 actualmente en construcci\u00f3n y estamos trabajando arduamente para mejorarla. \u00a1Gracias por su paciencia! \ud83d\ude4f</p> <p> Mire: Explore las Tareas de Ultralytics YOLO: Detecci\u00f3n de Objetos, Segmentaci\u00f3n, Seguimiento y Estimaci\u00f3n de Pose. </p>"},{"location":"tasks/#deteccion","title":"Detecci\u00f3n","text":"<p>La detecci\u00f3n es la tarea principal soportada por YOLOv8. Implica detectar objetos en una imagen o cuadro de video y dibujar cuadros delimitadores alrededor de ellos. Los objetos detectados se clasifican en diferentes categor\u00edas basadas en sus caracter\u00edsticas. YOLOv8 puede detectar m\u00faltiples objetos en una sola imagen o cuadro de video con alta precisi\u00f3n y velocidad.</p> <p>Ejemplos de Detecci\u00f3n</p>"},{"location":"tasks/#segmentacion","title":"Segmentaci\u00f3n","text":"<p>La segmentaci\u00f3n es una tarea que implica segmentar una imagen en diferentes regiones basadas en el contenido de la imagen. A cada regi\u00f3n se le asigna una etiqueta basada en su contenido. Esta tarea es \u00fatil en aplicaciones tales como segmentaci\u00f3n de im\u00e1genes y im\u00e1genes m\u00e9dicas. YOLOv8 utiliza una variante de la arquitectura U-Net para realizar la segmentaci\u00f3n.</p> <p>Ejemplos de Segmentaci\u00f3n</p>"},{"location":"tasks/#clasificacion","title":"Clasificaci\u00f3n","text":"<p>La clasificaci\u00f3n es una tarea que implica clasificar una imagen en diferentes categor\u00edas. YOLOv8 puede usarse para clasificar im\u00e1genes basadas en su contenido. Utiliza una variante de la arquitectura EfficientNet para realizar la clasificaci\u00f3n.</p> <p>Ejemplos de Clasificaci\u00f3n</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>La detecci\u00f3n de pose/puntos clave es una tarea que implica detectar puntos espec\u00edficos en una imagen o cuadro de video. Estos puntos se conocen como puntos clave y se utilizan para rastrear el movimiento o la estimaci\u00f3n de la pose. YOLOv8 puede detectar puntos clave en una imagen o cuadro de video con alta precisi\u00f3n y velocidad.</p> <p>Ejemplos de Pose</p>"},{"location":"tasks/#conclusion","title":"Conclusi\u00f3n","text":"<p>YOLOv8 soporta m\u00faltiples tareas, incluyendo detecci\u00f3n, segmentaci\u00f3n, clasificaci\u00f3n y detecci\u00f3n de puntos clave. Cada una de estas tareas tiene diferentes objetivos y casos de uso. Al entender las diferencias entre estas tareas, puede elegir la tarea adecuada para su aplicaci\u00f3n de visi\u00f3n por computadora.</p>"},{"location":"tasks/classify/","title":"Clasificaci\u00f3n de Im\u00e1genes","text":"<p>La clasificaci\u00f3n de im\u00e1genes es la tarea m\u00e1s sencilla de las tres y consiste en clasificar una imagen completa en una de un conjunto de clases predefinidas.</p> <p>La salida de un clasificador de im\u00e1genes es una \u00fanica etiqueta de clase y una puntuaci\u00f3n de confianza. La clasificaci\u00f3n de im\u00e1genes es \u00fatil cuando solo necesita saber a qu\u00e9 clase pertenece una imagen y no necesita conocer d\u00f3nde est\u00e1n ubicados los objetos de esa clase o cu\u00e1l es su forma exacta.</p> <p>Consejo</p> <p>Los modelos YOLOv8 Classify utilizan el sufijo <code>-cls</code>, por ejemplo, <code>yolov8n-cls.pt</code> y est\u00e1n preentrenados en ImageNet.</p>"},{"location":"tasks/classify/#modelos","title":"Modelos","text":"<p>Los modelos Classify preentrenados YOLOv8 se muestran aqu\u00ed. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics en el primer uso.</p> Modelo Tama\u00f1o<sup>(p\u00edxeles) Exactitud<sup>top1 Exactitud<sup>top5 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) Par\u00e1metros<sup>(M) FLOPs<sup>(B) en 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Los valores de Exactitud son las precisiones de los modelos en el conjunto de datos de validaci\u00f3n de ImageNet.   Para reproducir usar <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Velocidad promediada sobre im\u00e1genes de validaci\u00f3n de ImageNet usando una instancia de Amazon EC2 P4d Para reproducir usar <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena el modelo YOLOv8n-cls en el conjunto de datos MNIST160 durante 100 \u00e9pocas con un tama\u00f1o de imagen de 64. Para obtener una lista completa de argumentos disponibles, consulte la p\u00e1gina de Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y empezar entrenamiento desde cero\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Empezar entrenamiento desde un modelo *.pt preentrenado\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados e iniciar entrenamiento\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de clasificaci\u00f3n YOLO puede encontrarse en detalle en la Gu\u00eda de Conjuntos de Datos.</p>"},{"location":"tasks/classify/#validacion","title":"Validaci\u00f3n","text":"<p>Validar la exactitud del modelo YOLOv8n-cls entrenado en el conjunto de datos MNIST160. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene su <code>data</code> y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.top1   # precisi\u00f3n top1\nmetrics.top5   # precisi\u00f3n top5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # validar modelo oficial\nyolo classify val model=path/to/best.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/classify/#prediccion","title":"Predicci\u00f3n","text":"<p>Usar un modelo YOLOv8n-cls entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Ver detalles completos del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/classify/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exportar un modelo YOLOv8n-cls a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-cls.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n disponibles para YOLOv8-cls se encuentran en la tabla a continuaci\u00f3n. Puede predecir o validar directamente en modelos exportados, por ejemplo, <code>yolo predict model=yolov8n-cls.onnx</code>. Ejemplos de uso se muestran para su modelo despu\u00e9s de que se completa la exportaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Vea detalles completos de <code>exportaci\u00f3n</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"},{"location":"tasks/detect/","title":"Detecci\u00f3n de Objetos","text":"<p>La detecci\u00f3n de objetos es una tarea que implica identificar la ubicaci\u00f3n y clase de objetos en una imagen o flujo de video.</p> <p>La salida de un detector de objetos es un conjunto de cajas delimitadoras que encierran a los objetos en la imagen, junto con etiquetas de clase y puntajes de confianza para cada caja. La detecci\u00f3n de objetos es una buena opci\u00f3n cuando necesitas identificar objetos de inter\u00e9s en una escena, pero no necesitas saber exactamente d\u00f3nde se encuentra el objeto o su forma exacta.</p> <p> Ver: Detecci\u00f3n de Objetos con Modelo Preentrenado YOLOv8 de Ultralytics. </p> <p>Consejo</p> <p>Los modelos YOLOv8 Detect son los modelos predeterminados de YOLOv8, es decir, <code>yolov8n.pt</code> y est\u00e1n preentrenados en COCO.</p>"},{"location":"tasks/detect/#modelos","title":"Modelos","text":"<p>Los modelos preentrenados de YOLOv8 Detect se muestran aqu\u00ed. Los modelos de Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos de Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en el primer uso.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>val50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Los valores de mAP<sup>val</sup> son para un solo modelo a una sola escala en el conjunto de datos COCO val2017.   Reproduce utilizando <code>yolo val detect data=coco.yaml device=0</code></li> <li>La Velocidad es el promedio sobre las im\u00e1genes de COCO val utilizando una instancia Amazon EC2 P4d.   Reproduce utilizando <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena a YOLOv8n en el conjunto de datos COCO128 durante 100 \u00e9pocas a tama\u00f1o de imagen 640. Para una lista completa de argumentos disponibles, consulta la p\u00e1gina Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.yaml')  # construye un nuevo modelo desde YAML\nmodel = YOLO('yolov8n.pt')  # carga un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construye desde YAML y transfiere los pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar entrenamiento desde cero\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Comenzar entrenamiento desde un modelo *.pt preentrenado\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar entrenamiento\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de detecci\u00f3n de YOLO se puede encontrar en detalle en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente desde otros formatos (como COCO, etc.) al formato YOLO, por favor usa la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/detect/#validacion","title":"Validaci\u00f3n","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n entrenado en el conjunto de datos COCO128. No es necesario pasar ning\u00fan argumento, ya que el <code>modelo</code> retiene sus datos de <code>entrenamiento</code> y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # sin argumentos necesarios, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # validar modelo oficial\nyolo detect val model=ruta/a/mejor.pt  # validar modelo personalizado\n</code></pre>"},{"location":"tasks/detect/#prediccion","title":"Predicci\u00f3n","text":"<p>Utiliza un modelo YOLOv8n entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo detect predict model=ruta/a/mejor.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Consulta los detalles completos del modo <code>predict</code> en la p\u00e1gina Predicci\u00f3n.</p>"},{"location":"tasks/detect/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exporta un modelo YOLOv8n a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exportar modelo oficial\nyolo export model=ruta/a/mejor.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n de YOLOv8 disponibles se encuentran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, es decir, <code>yolo predict model=yolov8n.onnx</code>. Ejemplos de uso se muestran para tu modelo despu\u00e9s de que la exportaci\u00f3n se completa.</p> Formato Argumento <code>format</code> Modelo Metadata Argumentos PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimizar</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>din\u00e1mico</code>, <code>simplificar</code>, <code>conjunto de operaciones</code> OpenVINO <code>openvino</code> <code>modelo_yolov8n_openvino/</code> \u2705 <code>imgsz</code>, <code>mitad</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>din\u00e1mico</code>, <code>simplificar</code>, <code>espacio de trabajo</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>modelo_guardado_yolov8n/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>mitad</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>modelo_web_yolov8n/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>modelo_yolov8n_paddle/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>modelo_ncnn_yolov8n/</code> \u2705 <code>imgsz</code>, <code>mitad</code> <p>Consulta los detalles completos de la <code>exportaci\u00f3n</code> en la p\u00e1gina Exportar.</p>"},{"location":"tasks/pose/","title":"Estimaci\u00f3n de Pose","text":"<p>La estimaci\u00f3n de pose es una tarea que implica identificar la ubicaci\u00f3n de puntos espec\u00edficos en una imagen, com\u00fanmente referidos como puntos clave. Estos puntos clave pueden representar varias partes del objeto, como articulaciones, puntos de referencia u otras caracter\u00edsticas distintivas. La ubicaci\u00f3n de los puntos clave generalmente se representa como un conjunto de coordenadas 2D <code>[x, y]</code> o 3D <code>[x, y, visible]</code>.</p> <p>La salida de un modelo de estimaci\u00f3n de pose es un conjunto de puntos que representan los puntos clave en un objeto de la imagen, generalmente junto con las puntuaciones de confianza para cada punto. La estimaci\u00f3n de pose es una buena opci\u00f3n cuando se necesita identificar partes espec\u00edficas de un objeto en una escena y su ubicaci\u00f3n relativa entre ellas.</p> <p> Ver: Estimaci\u00f3n de Pose con Ultralytics YOLOv8. </p> <p>Consejo</p> <p>Los modelos pose YOLOv8 utilizan el sufijo <code>-pose</code>, por ejemplo, <code>yolov8n-pose.pt</code>. Estos modelos est\u00e1n entrenados en el conjunto de datos COCO keypoints y son adecuados para una variedad de tareas de estimaci\u00f3n de pose.</p>"},{"location":"tasks/pose/#modelos","title":"Modelos","text":"<p>Aqu\u00ed se muestran los modelos preentrenados de YOLOv8 Pose. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en el primer uso.</p> Modelo tama\u00f1o<sup>(p\u00edxeles) mAP<sup>pose50-95 mAP<sup>pose50 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>Los valores de mAP<sup>val</sup> son para un solo modelo a una sola escala en el conjunto de datos COCO Keypoints val2017.   Reproducir con <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>Velocidad promediada sobre im\u00e1genes COCO val usando una instancia Amazon EC2 P4d.   Reproducir con <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#entrenar","title":"Entrenar","text":"<p>Entrena un modelo YOLOv8-pose en el conjunto de datos COCO128-pose.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo preentrenado (recomendado para entrenar)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construir desde YAML y transferir los pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar entrenamiento desde cero\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Empezar entrenamiento desde un modelo *.pt preentrenado\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar entrenamiento\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de pose de YOLO se puede encontrar en detalle en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente de otros formatos (como COCO, etc.) al formato de YOLO, usa la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/pose/#validar","title":"Validar","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n-pose entrenado en el conjunto de datos COCO128-pose. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> mantiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # una lista contiene map50-95 de cada categor\u00eda\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # modelo oficial de val\nyolo pose val model=path/to/best.pt  # modelo personalizado de val\n</code></pre>"},{"location":"tasks/pose/#predecir","title":"Predecir","text":"<p>Usa un modelo YOLOv8n-pose entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo oficial\nyolo pose predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con modelo personalizado\n</code></pre> <p>Consulta los detalles completos del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/pose/#exportar","title":"Exportar","text":"<p>Exporta un modelo YOLOv8n Pose a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-pose.pt')  # cargar un modelo oficial\nmodel = YOLO('path/to/best.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exportar modelo oficial\nyolo export model=path/to/best.pt format=onnx  # exportar modelo entrenado personalizado\n</code></pre> <p>Los formatos de exportaci\u00f3n de YOLOv8-pose disponibles se muestran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, por ejemplo, <code>yolo predict model=yolov8n-pose.onnx</code>. Los ejemplos de uso se muestran para tu modelo despu\u00e9s de que la exportaci\u00f3n se completa.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>din\u00e1mico</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>din\u00e1mico</code>, <code>simplify</code>, <code>espacio de trabajo</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consulta los detalles completos del modo <code>export</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"},{"location":"tasks/segment/","title":"Segmentaci\u00f3n de Instancias","text":"<p>La segmentaci\u00f3n de instancias va un paso m\u00e1s all\u00e1 de la detecci\u00f3n de objetos e implica identificar objetos individuales en una imagen y segmentarlos del resto de la imagen.</p> <p>La salida de un modelo de segmentaci\u00f3n de instancias es un conjunto de m\u00e1scaras o contornos que delimitan cada objeto en la imagen, junto con etiquetas de clase y puntajes de confianza para cada objeto. La segmentaci\u00f3n de instancias es \u00fatil cuando necesitas saber no solo d\u00f3nde est\u00e1n los objetos en una imagen, sino tambi\u00e9n cu\u00e1l es su forma exacta.</p> <p> Mira: Ejecuta la Segmentaci\u00f3n con el Modelo Ultralytics YOLOv8 Preentrenado en Python. </p> <p>Consejo</p> <p>Los modelos YOLOv8 Segment utilizan el sufijo <code>-seg</code>, es decir, <code>yolov8n-seg.pt</code> y est\u00e1n preentrenados en el COCO.</p>"},{"location":"tasks/segment/#modelos","title":"Modelos","text":"<p>Aqu\u00ed se muestran los modelos Segment preentrenados YOLOv8. Los modelos Detect, Segment y Pose est\u00e1n preentrenados en el conjunto de datos COCO, mientras que los modelos Classify est\u00e1n preentrenados en el conjunto de datos ImageNet.</p> <p>Los Modelos se descargan autom\u00e1ticamente desde el \u00faltimo lanzamiento de Ultralytics release en su primer uso.</p> Modelo Tama\u00f1o<sup>(p\u00edxeles) mAP<sup>caja50-95 mAP<sup>m\u00e1scara50-95 Velocidad<sup>CPU ONNX(ms) Velocidad<sup>A100 TensorRT(ms) Par\u00e1metros<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Los valores mAP<sup>val</sup> son para un \u00fanico modelo a una \u00fanica escala en el conjunto de datos COCO val2017.   Reproducir utilizando <code>yolo val segment data=coco.yaml device=0</code></li> <li>La Velocidad promediada sobre im\u00e1genes de COCO val utilizando una instancia de Amazon EC2 P4d.   Reproducir utilizando <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#entrenamiento","title":"Entrenamiento","text":"<p>Entrena el modelo YOLOv8n-seg en el conjunto de datos COCO128-seg durante 100 \u00e9pocas con tama\u00f1o de imagen de 640. Para una lista completa de argumentos disponibles, consulta la p\u00e1gina de Configuraci\u00f3n.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.yaml')  # construir un nuevo modelo desde YAML\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo preentrenado (recomendado para entrenamiento)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construir desde YAML y transferir pesos\n\n# Entrenar el modelo\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construir un nuevo modelo desde YAML y comenzar a entrenar desde cero\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Comenzar a entrenar desde un modelo *.pt preentrenado\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construir un nuevo modelo desde YAML, transferir pesos preentrenados y comenzar a entrenar\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#formato-del-conjunto-de-datos","title":"Formato del conjunto de datos","text":"<p>El formato del conjunto de datos de segmentaci\u00f3n YOLO puede encontrarse detallado en la Gu\u00eda de Conjuntos de Datos. Para convertir tu conjunto de datos existente de otros formatos (como COCO, etc.) al formato YOLO, utiliza la herramienta JSON2YOLO de Ultralytics.</p>"},{"location":"tasks/segment/#validacion","title":"Validaci\u00f3n","text":"<p>Valida la precisi\u00f3n del modelo YOLOv8n-seg entrenado en el conjunto de datos COCO128-seg. No es necesario pasar ning\u00fan argumento ya que el <code>modelo</code> retiene sus <code>datos</code> de entrenamiento y argumentos como atributos del modelo.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Validar el modelo\nmetrics = model.val()  # no se necesitan argumentos, el conjunto de datos y configuraciones se recuerdan\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # una lista contiene map50-95(B) de cada categor\u00eda\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # una lista contiene map50-95(M) de cada categor\u00eda\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # validar el modelo oficial\nyolo segment val model=ruta/a/mejor.pt  # validar el modelo personalizado\n</code></pre>"},{"location":"tasks/segment/#prediccion","title":"Predicci\u00f3n","text":"<p>Usa un modelo YOLOv8n-seg entrenado para realizar predicciones en im\u00e1genes.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo personalizado\n\n# Predecir con el modelo\nresults = model('https://ultralytics.com/images/bus.jpg')  # predecir en una imagen\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con el modelo oficial\nyolo segment predict model=ruta/a/mejor.pt source='https://ultralytics.com/images/bus.jpg'  # predecir con el modelo personalizado\n</code></pre> <p>Consulta todos los detalles del modo <code>predict</code> en la p\u00e1gina de Predicci\u00f3n.</p>"},{"location":"tasks/segment/#exportacion","title":"Exportaci\u00f3n","text":"<p>Exporta un modelo YOLOv8n-seg a un formato diferente como ONNX, CoreML, etc.</p> <p>Ejemplo</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Cargar un modelo\nmodel = YOLO('yolov8n-seg.pt')  # cargar un modelo oficial\nmodel = YOLO('ruta/a/mejor.pt')  # cargar un modelo entrenado personalizado\n\n# Exportar el modelo\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exportar el modelo oficial\nyolo export model=ruta/a/mejor.pt format=onnx  # exportar el modelo entrenado personalizado\n</code></pre> <p>Los formatos disponibles para exportar YOLOv8-seg se muestran en la tabla a continuaci\u00f3n. Puedes predecir o validar directamente en modelos exportados, es decir, <code>yolo predict model=yolov8n-seg.onnx</code>. Se muestran ejemplos de uso para tu modelo despu\u00e9s de que se completa la exportaci\u00f3n.</p> Formato Argumento <code>format</code> Modelo Metadatos Argumentos PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consulta todos los detalles del modo <code>export</code> en la p\u00e1gina de Exportaci\u00f3n.</p>"}]}