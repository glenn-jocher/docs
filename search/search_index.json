{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Introducing Ultralytics YOLOv8, the latest version of the acclaimed real-time object detection and image segmentation model. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</p> <p>Explore the YOLOv8 Docs, a comprehensive resource designed to help you understand and utilize its features and capabilities. Whether you are a seasoned machine learning practitioner or new to the field, this hub aims to maximize YOLOv8's potential in your projects</p>"},{"location":"#where-to-start","title":"Where to Start","text":"<ul> <li>Install <code>ultralytics</code> with pip and get up and running in minutes \u00a0  Get Started</li> <li>Predict new images and videos with YOLOv8 \u00a0  Predict on Images</li> <li>Train a new YOLOv8 model on your own custom dataset \u00a0  Train a Model</li> <li>Explore YOLOv8 tasks like segment, classify, pose and track \u00a0  Explore Tasks</li> </ul>"},{"location":"#yolo-a-brief-history","title":"YOLO: A Brief History","text":"<p>YOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity for its high speed and accuracy.</p> <ul> <li>YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters.</li> <li>YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling.</li> <li>YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</li> <li>YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats.</li> <li>YOLOv6 was open-sourced by Meituan in 2022 and is in use in many of the company's autonomous delivery robots.</li> <li>YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset.</li> <li>YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse applications and domains.</li> </ul>"},{"location":"#yolo-licenses-how-is-ultralytics-yolo-licensed","title":"YOLO Licenses: How is Ultralytics YOLO licensed?","text":"<p>Ultralytics offers two licensing options to accommodate diverse use cases:</p> <ul> <li>AGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.</li> <li>Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.</li> </ul> <p>Our licensing strategy is designed to ensure that any improvements to our open-source projects are returned to the community. We hold the principles of open source close to our hearts \u2764\ufe0f, and our mission is to guarantee that our contributions can be utilized and expanded upon in ways that are beneficial to all.</p>"},{"location":"SECURITY/","title":"Security Policy","text":"<p>At Ultralytics, the security of our users' data and systems is of utmost importance. To ensure the safety and security of our open-source projects, we have implemented several measures to detect and prevent security vulnerabilities.</p>"},{"location":"SECURITY/#snyk-scanning","title":"Snyk Scanning","text":"<p>We use Snyk to regularly scan all Ultralytics repositories for vulnerabilities and security issues. Our goal is to identify and remediate any potential threats as soon as possible, to minimize any risks to our users.</p> <p></p>"},{"location":"SECURITY/#github-codeql-scanning","title":"GitHub CodeQL Scanning","text":"<p>In addition to our Snyk scans, we also use GitHub's CodeQL scans to proactively identify and address security vulnerabilities across all Ultralytics repositories.</p> <p></p>"},{"location":"SECURITY/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>If you suspect or discover a security vulnerability in any of our repositories, please let us know immediately. You can reach out to us directly via our contact form or via security@ultralytics.com. Our security team will investigate and respond as soon as possible.</p> <p>We appreciate your help in keeping all Ultralytics open-source projects secure and safe for everyone.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#install-ultralytics","title":"Install Ultralytics","text":"<p>Ultralytics provides various installation methods including pip, conda, and Docker. Install YOLOv8 via the <code>ultralytics</code> pip package for the latest stable release or by cloning the Ultralytics GitHub repository for the most up-to-date version. Docker can be used to execute the package in an isolated container, avoiding local installation.</p> <p>Install</p> Pip install (recommended)Conda installGit cloneDocker <p>Install the <code>ultralytics</code> package using pip, or update an existing installation by running <code>pip install -U ultralytics</code>. Visit the Python Package Index (PyPI) for more details on the <code>ultralytics</code> package: https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Install the ultralytics package using pip\npip install ultralytics\n</code></pre> <p>Conda is an alternative package manager to pip which may also be used for installation. Visit Anaconda for more details at https://anaconda.org/conda-forge/ultralytics. Ultralytics feedstock repository for updating the conda package is at https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Install the ultralytics package using conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Note</p> <p>If you are installing in a CUDA environment best practice is to install <code>ultralytics</code>, <code>pytorch</code> and <code>pytorch-cuda</code> in the same command to allow the conda package manager to resolve any conflicts, or else to install <code>pytorch-cuda</code> last to allow it override the CPU-specific <code>pytorch</code> package if necesary. <pre><code># Install all packages together using conda\nconda install -c conda-forge -c pytorch -c nvidia ultralytics pytorch torchvision pytorch-cuda=11.8 </code></pre></p> <p>Clone the <code>ultralytics</code> repository if you are interested in contributing to the development or wish to experiment with the latest source code. After cloning, navigate into the directory and install the package in editable mode <code>-e</code> using pip. <pre><code># Clone the ultralytics repository\ngit clone https://github.com/ultralytics/ultralytics\n\n# Navigate to the cloned directory\ncd ultralytics\n\n# Install the package in editable mode for development\npip install -e .\n</code></pre></p> <p>Utilize Docker to execute the <code>ultralytics</code> package in an isolated container. By employing the official <code>ultralytics</code> image from Docker Hub, you can avoid local installation. Below are the commands to get the latest image and execute it:</p> <p></p> <pre><code># Set image name as a variable\nt=ultralytics/ultralytics:latest\n\n# Pull the latest ultralytics image from Docker Hub\nsudo docker pull $t\n# Run the ultralytics image in a container with GPU support\nsudo docker run -it --ipc=host --gpus all $t\n</code></pre> <p>The above command initializes a Docker container with the latest <code>ultralytics</code> image. The <code>-it</code> flag assigns a pseudo-TTY and maintains stdin open, enabling you to interact with the container. The <code>--ipc=host</code> flag sets the IPC (Inter-Process Communication) namespace to the host, which is essential for sharing memory between processes. The <code>--gpus all</code> flag enables access to all available GPUs inside the container, which is crucial for tasks that require GPU computation.</p> <p>Note: To work with files on your local machine within the container, use Docker volumes for mounting a local directory into the container:</p> <pre><code># Mount local directory to a directory inside the container\nsudo docker run -it --ipc=host --gpus all -v /path/on/host:/path/in/container $t\n</code></pre> <p>Alter <code>/path/on/host</code> with the directory path on your local machine, and <code>/path/in/container</code> with the desired path inside the Docker container for accessibility.</p> <p>See the <code>ultralytics</code> requirements.txt file for a list of dependencies. Note that all examples above install all required dependencies.</p> <p>Tip</p> <p>PyTorch requirements vary by operating system and CUDA requirements, so it's recommended to install PyTorch first following instructions at https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#use-ultralytics-with-cli","title":"Use Ultralytics with CLI","text":"<p>The Ultralytics command line interface (CLI) allows for simple single-line commands without the need for a Python environment. CLI requires no customization or Python code. You can simply run all tasks from the terminal with the <code>yolo</code> command. Check out the CLI Guide to learn more about using YOLOv8 from the command line.</p> <p>Example</p> SyntaxTrainPredictValExportSpecial <p>Ultralytics <code>yolo</code> commands use the following syntax: <pre><code>yolo TASK MODE ARGS\n\nWhere   TASK (optional) is one of [detect, segment, classify]\nMODE (required) is one of [train, val, predict, export, track]\nARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n</code></pre> See all ARGS in the full Configuration Guide or with <code>yolo cfg</code></p> <p>Train a detection model for 10 epochs with an initial learning_rate of 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predict a YouTube video using a pretrained segmentation model at image size 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n</code></pre></p> <p>Val a pretrained detection model at batch-size 1 and image size 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Run special commands to see version, view settings, run checks and more: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Warning</p> <p>Arguments must be passed as <code>arg=val</code> pairs, split by an equals <code>=</code> sign and delimited by spaces <code></code> between pairs. Do not use <code>--</code> argument prefixes or commas <code>,</code> between arguments.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>CLI Guide</p>"},{"location":"quickstart/#use-ultralytics-with-python","title":"Use Ultralytics with Python","text":"<p>YOLOv8's Python interface allows for seamless integration into your Python projects, making it easy to load, run, and process the model's output. Designed with simplicity and ease of use in mind, the Python interface enables users to quickly implement object detection, segmentation, and classification in their projects. This makes YOLOv8's Python interface an invaluable tool for anyone looking to incorporate these functionalities into their Python projects.</p> <p>For example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX format with just a few lines of code. Check out the Python Guide to learn more about using YOLOv8 within your Python projects.</p> <p>Example</p> <pre><code>from ultralytics import YOLO\n# Create a new YOLO model from scratch\nmodel = YOLO('yolov8n.yaml')\n# Load a pretrained YOLO model (recommended for training)\nmodel = YOLO('yolov8n.pt')\n# Train the model using the 'coco128.yaml' dataset for 3 epochs\nresults = model.train(data='coco128.yaml', epochs=3)\n# Evaluate the model's performance on the validation set\nresults = model.val()\n# Perform object detection on an image using the model\nresults = model('https://ultralytics.com/images/bus.jpg')\n# Export the model to ONNX format\nsuccess = model.export(format='onnx')\n</code></pre> <p>Python Guide</p>"},{"location":"quickstart/#ultralytics-settings","title":"Ultralytics Settings","text":"<p>The Ultralytics library provides a powerful settings management system to enable fine-grained control over your experiments. By making use of the <code>SettingsManager</code> housed within the <code>ultralytics.utils</code> module, users can readily access and alter their settings. These are stored in a YAML file and can be viewed or modified either directly within the Python environment or via the Command-Line Interface (CLI).</p>"},{"location":"quickstart/#inspecting-settings","title":"Inspecting Settings","text":"<p>To gain insight into the current configuration of your settings, you can view them directly:</p> <p>View settings</p> PythonCLI <p>You can use Python to view your settings. Start by importing the <code>settings</code> object from the <code>ultralytics</code> module. Print and return settings using the following commands: <pre><code>from ultralytics import settings\n# View all settings\nprint(settings)\n# Return a specific setting\nvalue = settings['runs_dir']\n</code></pre></p> <p>Alternatively, the command-line interface allows you to check your settings with a simple command: <pre><code>yolo settings\n</code></pre></p>"},{"location":"quickstart/#modifying-settings","title":"Modifying Settings","text":"<p>Ultralytics allows users to easily modify their settings. Changes can be performed in the following ways:</p> <p>Update settings</p> PythonCLI <p>Within the Python environment, call the <code>update</code> method on the <code>settings</code> object to change your settings: <pre><code>from ultralytics import settings\n# Update a setting\nsettings.update({'runs_dir': '/path/to/runs'})\n# Update multiple settings\nsettings.update({'runs_dir': '/path/to/runs', 'tensorboard': False})\n# Reset settings to default values\nsettings.reset()\n</code></pre></p> <p>If you prefer using the command-line interface, the following commands will allow you to modify your settings: <pre><code># Update a setting\nyolo settings runs_dir='/path/to/runs'\n# Update multiple settings\nyolo settings runs_dir='/path/to/runs' tensorboard=False\n\n# Reset settings to default values\nyolo settings reset\n</code></pre></p>"},{"location":"quickstart/#understanding-settings","title":"Understanding Settings","text":"<p>The table below provides an overview of the settings available for adjustment within Ultralytics. Each setting is outlined along with an example value, the data type, and a brief description.</p> Name Example Value Data Type Description <code>settings_version</code> <code>'0.0.4'</code> <code>str</code> Ultralytics settings version (different from Ultralytics pip version) <code>datasets_dir</code> <code>'/path/to/datasets'</code> <code>str</code> The directory where the datasets are stored <code>weights_dir</code> <code>'/path/to/weights'</code> <code>str</code> The directory where the model weights are stored <code>runs_dir</code> <code>'/path/to/runs'</code> <code>str</code> The directory where the experiment runs are stored <code>uuid</code> <code>'a1b2c3d4'</code> <code>str</code> The unique identifier for the current settings <code>sync</code> <code>True</code> <code>bool</code> Whether to sync analytics and crashes to HUB <code>api_key</code> <code>''</code> <code>str</code> Ultralytics HUB API Key <code>clearml</code> <code>True</code> <code>bool</code> Whether to use ClearML logging <code>comet</code> <code>True</code> <code>bool</code> Whether to use Comet ML for experiment tracking and visualization <code>dvc</code> <code>True</code> <code>bool</code> Whether to use DVC for experiment tracking and version control <code>hub</code> <code>True</code> <code>bool</code> Whether to use Ultralytics HUB integration <code>mlflow</code> <code>True</code> <code>bool</code> Whether to use MLFlow for experiment tracking <code>neptune</code> <code>True</code> <code>bool</code> Whether to use Neptune for experiment tracking <code>raytune</code> <code>True</code> <code>bool</code> Whether to use Ray Tune for hyperparameter tuning <code>tensorboard</code> <code>True</code> <code>bool</code> Whether to use TensorBoard for visualization <code>wandb</code> <code>True</code> <code>bool</code> Whether to use Weights &amp; Biases logging <p>As you navigate through your projects or experiments, be sure to revisit these settings to ensure that they are optimally configured for your needs.</p>"},{"location":"datasets/","title":"Datasets Overview","text":"<p>Ultralytics provides support for various datasets to facilitate computer vision tasks such as detection, instance segmentation, pose estimation, classification, and multi-object tracking. Below is a list of the main Ultralytics datasets, followed by a summary of each computer vision task and the respective datasets.</p>"},{"location":"datasets/#detection-datasets","title":"Detection Datasets","text":"<p>Bounding box object detection is a computer vision technique that involves detecting and localizing objects in an image by drawing a bounding box around each object.</p> <ul> <li>Argoverse: A dataset containing 3D tracking and motion forecasting data from urban environments with rich annotations.</li> <li>COCO: A large-scale dataset designed for object detection, segmentation, and captioning with over 200K labeled images.</li> <li>COCO8: Contains the first 4 images from COCO train and COCO val, suitable for quick tests.</li> <li>Global Wheat 2020: A dataset of wheat head images collected from around the world for object detection and localization tasks.</li> <li>Objects365: A high-quality, large-scale dataset for object detection with 365 object categories and over 600K annotated images.</li> <li>OpenImagesV7: A comprehensive dataset by Google with 1.7M train images and 42k validation images.</li> <li>SKU-110K: A dataset featuring dense object detection in retail environments with over 11K images and 1.7 million bounding boxes.</li> <li>VisDrone: A dataset containing object detection and multi-object tracking data from drone-captured imagery with over 10K images and video sequences.</li> <li>VOC: The Pascal Visual Object Classes (VOC) dataset for object detection and segmentation with 20 object classes and over 11K images.</li> <li>xView: A dataset for object detection in overhead imagery with 60 object categories and over 1 million annotated objects.</li> </ul>"},{"location":"datasets/#instance-segmentation-datasets","title":"Instance Segmentation Datasets","text":"<p>Instance segmentation is a computer vision technique that involves identifying and localizing objects in an image at the pixel level.</p> <ul> <li>COCO: A large-scale dataset designed for object detection, segmentation, and captioning tasks with over 200K labeled images.</li> <li>COCO8-seg: A smaller dataset for instance segmentation tasks, containing a subset of 8 COCO images with segmentation annotations.</li> </ul>"},{"location":"datasets/#pose-estimation","title":"Pose Estimation","text":"<p>Pose estimation is a technique used to determine the pose of the object relative to the camera or the world coordinate system.</p> <ul> <li>COCO: A large-scale dataset with human pose annotations designed for pose estimation tasks.</li> <li>COCO8-pose: A smaller dataset for pose estimation tasks, containing a subset of 8 COCO images with human pose annotations.</li> </ul>"},{"location":"datasets/#classification","title":"Classification","text":"<p>Image classification is a computer vision task that involves categorizing an image into one or more predefined classes or categories based on its visual content.</p> <ul> <li>Caltech 101: A dataset containing images of 101 object categories for image classification tasks.</li> <li>Caltech 256: An extended version of Caltech 101 with 256 object categories and more challenging images.</li> <li>CIFAR-10: A dataset of 60K 32x32 color images in 10 classes, with 6K images per class.</li> <li>CIFAR-100: An extended version of CIFAR-10 with 100 object categories and 600 images per class.</li> <li>Fashion-MNIST: A dataset consisting of 70,000 grayscale images of 10 fashion categories for image classification tasks.</li> <li>ImageNet: A large-scale dataset for object detection and image classification with over 14 million images and 20,000 categories.</li> <li>ImageNet-10: A smaller subset of ImageNet with 10 categories for faster experimentation and testing.</li> <li>Imagenette: A smaller subset of ImageNet that contains 10 easily distinguishable classes for quicker training and testing.</li> <li>Imagewoof: A more challenging subset of ImageNet containing 10 dog breed categories for image classification tasks.</li> <li>MNIST: A dataset of 70,000 grayscale images of handwritten digits for image classification tasks.</li> </ul>"},{"location":"datasets/#oriented-bounding-boxes-obb","title":"Oriented Bounding Boxes (OBB)","text":"<p>Oriented Bounding Boxes (OBB) is a method in computer vision for detecting angled objects in images using rotated bounding boxes, often applied to aerial and satellite imagery.</p> <ul> <li>DOTAv2: A popular OBB aerial imagery dataset with 1.7 million instances and 11,268 images.</li> </ul>"},{"location":"datasets/#multi-object-tracking","title":"Multi-Object Tracking","text":"<p>Multi-object tracking is a computer vision technique that involves detecting and tracking multiple objects over time in a video sequence.</p> <ul> <li>Argoverse: A dataset containing 3D tracking and motion forecasting data from urban environments with rich annotations for multi-object tracking tasks.</li> <li>VisDrone: A dataset containing object detection and multi-object tracking data from drone-captured imagery with over 10K images and video sequences.</li> </ul>"},{"location":"datasets/classify/","title":"Image Classification Datasets Overview","text":""},{"location":"datasets/classify/#dataset-format","title":"Dataset format","text":"<p>The folder structure for classification datasets in torchvision typically follows a standard format:</p> <pre><code>root/\n|-- class1/\n|   |-- img1.jpg\n|   |-- img2.jpg\n|   |-- ...\n|\n|-- class2/\n|   |-- img1.jpg\n|   |-- img2.jpg\n|   |-- ...\n|\n|-- class3/\n|   |-- img1.jpg\n|   |-- img2.jpg\n|   |-- ...\n|\n|-- ...\n</code></pre> <p>In this folder structure, the <code>root</code> directory contains one subdirectory for each class in the dataset. Each subdirectory is named after the corresponding class and contains all the images for that class. Each image file is named uniquely and is typically in a common image file format such as JPEG or PNG.</p> <p>** Example **</p> <p>For example, in the CIFAR10 dataset, the folder structure would look like this:</p> <pre><code>cifar-10-/\n|\n|-- train/\n|   |-- airplane/\n|   |   |-- 10008_airplane.png\n|   |   |-- 10009_airplane.png\n|   |   |-- ...\n|   |\n|   |-- automobile/\n|   |   |-- 1000_automobile.png\n|   |   |-- 1001_automobile.png\n|   |   |-- ...\n|   |\n|   |-- bird/\n|   |   |-- 10014_bird.png\n|   |   |-- 10015_bird.png\n|   |   |-- ...\n|   |\n|   |-- ...\n|\n|-- test/\n|   |-- airplane/\n|   |   |-- 10_airplane.png\n|   |   |-- 11_airplane.png\n|   |   |-- ...\n|   |\n|   |-- automobile/\n|   |   |-- 100_automobile.png\n|   |   |-- 101_automobile.png\n|   |   |-- ...\n|   |\n|   |-- bird/\n|   |   |-- 1000_bird.png\n|   |   |-- 1001_bird.png\n|   |   |-- ...\n|   |\n|   |-- ...\n</code></pre> <p>In this example, the <code>train</code> directory contains subdirectories for each class in the dataset, and each class subdirectory contains all the images for that class. The <code>test</code> directory has a similar structure. The <code>root</code> directory also contains other files that are part of the CIFAR10 dataset.</p>"},{"location":"datasets/classify/#usage","title":"Usage","text":"PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='path/to/dataset', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=path/to/data model=yolov8n-cls.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/classify/#supported-datasets","title":"Supported Datasets","text":"<p>Ultralytics supports the following datasets with automatic download:</p> <ul> <li>Caltech 101: A dataset containing images of 101 object categories for image classification tasks.</li> <li>Caltech 256: An extended version of Caltech 101 with 256 object categories and more challenging images.</li> <li>CIFAR-10: A dataset of 60K 32x32 color images in 10 classes, with 6K images per class.</li> <li>CIFAR-100: An extended version of CIFAR-10 with 100 object categories and 600 images per class.</li> <li>Fashion-MNIST: A dataset consisting of 70,000 grayscale images of 10 fashion categories for image classification tasks.</li> <li>ImageNet: A large-scale dataset for object detection and image classification with over 14 million images and 20,000 categories.</li> <li>ImageNet-10: A smaller subset of ImageNet with 10 categories for faster experimentation and testing.</li> <li>Imagenette: A smaller subset of ImageNet that contains 10 easily distinguishable classes for quicker training and testing.</li> <li>Imagewoof: A more challenging subset of ImageNet containing 10 dog breed categories for image classification tasks.</li> <li>MNIST: A dataset of 70,000 grayscale images of handwritten digits for image classification tasks.</li> </ul>"},{"location":"datasets/classify/#adding-your-own-dataset","title":"Adding your own dataset","text":"<p>If you have your own dataset and would like to use it for training classification models with Ultralytics, ensure that it follows the format specified above under \"Dataset format\" and then point your <code>data</code> argument to the dataset directory.</p>"},{"location":"datasets/classify/caltech101/","title":"Caltech-101 Dataset","text":"<p>The Caltech-101 dataset is a widely used dataset for object recognition tasks, containing around 9,000 images from 101 object categories. The categories were chosen to reflect a variety of real-world objects, and the images themselves were carefully selected and annotated to provide a challenging benchmark for object recognition algorithms.</p>"},{"location":"datasets/classify/caltech101/#key-features","title":"Key Features","text":"<ul> <li>The Caltech-101 dataset comprises around 9,000 color images divided into 101 categories.</li> <li>The categories encompass a wide variety of objects, including animals, vehicles, household items, and people.</li> <li>The number of images per category varies, with about 40 to 800 images in each category.</li> <li>Images are of variable sizes, with most images being medium resolution.</li> <li>Caltech-101 is widely used for training and testing in the field of machine learning, particularly for object recognition tasks.</li> </ul>"},{"location":"datasets/classify/caltech101/#dataset-structure","title":"Dataset Structure","text":"<p>Unlike many other datasets, the Caltech-101 dataset is not formally split into training and testing sets. Users typically create their own splits based on their specific needs. However, a common practice is to use a random subset of images for training (e.g., 30 images per category) and the remaining images for testing.</p>"},{"location":"datasets/classify/caltech101/#applications","title":"Applications","text":"<p>The Caltech-101 dataset is extensively used for training and evaluating deep learning models in object recognition tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. Its wide variety of categories and high-quality images make it an excellent dataset for research and development in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/caltech101/#usage","title":"Usage","text":"<p>To train a YOLO model on the Caltech-101 dataset for 100 epochs, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='caltech101', epochs=100, imgsz=416)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=caltech101 model=yolov8n-cls.pt epochs=100 imgsz=416\n</code></pre>"},{"location":"datasets/classify/caltech101/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The Caltech-101 dataset contains high-quality color images of various objects, providing a well-structured dataset for object recognition tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the objects in the Caltech-101 dataset, emphasizing the significance of a diverse dataset for training robust object recognition models.</p>"},{"location":"datasets/classify/caltech101/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the Caltech-101 dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@article{fei2007learning,\ntitle={Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories},\nauthor={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},\njournal={Computer vision and Image understanding},\nvolume={106},\nnumber={1},\npages={59--70},\nyear={2007},\npublisher={Elsevier}\n}\n</code></pre> <p>We would like to acknowledge Li Fei-Fei, Rob Fergus, and Pietro Perona for creating and maintaining the Caltech-101 dataset as a valuable resource for the machine learning and computer vision research community. For more information about the Caltech-101 dataset and its creators, visit the Caltech-101 dataset website.</p>"},{"location":"datasets/classify/caltech256/","title":"Caltech-256 Dataset","text":"<p>The Caltech-256 dataset is an extensive collection of images used for object classification tasks. It contains around 30,000 images divided into 257 categories (256 object categories and 1 background category). The images are carefully curated and annotated to provide a challenging and diverse benchmark for object recognition algorithms.</p>"},{"location":"datasets/classify/caltech256/#key-features","title":"Key Features","text":"<ul> <li>The Caltech-256 dataset comprises around 30,000 color images divided into 257 categories.</li> <li>Each category contains a minimum of 80 images.</li> <li>The categories encompass a wide variety of real-world objects, including animals, vehicles, household items, and people.</li> <li>Images are of variable sizes and resolutions.</li> <li>Caltech-256 is widely used for training and testing in the field of machine learning, particularly for object recognition tasks.</li> </ul>"},{"location":"datasets/classify/caltech256/#dataset-structure","title":"Dataset Structure","text":"<p>Like Caltech-101, the Caltech-256 dataset does not have a formal split between training and testing sets. Users typically create their own splits according to their specific needs. A common practice is to use a random subset of images for training and the remaining images for testing.</p>"},{"location":"datasets/classify/caltech256/#applications","title":"Applications","text":"<p>The Caltech-256 dataset is extensively used for training and evaluating deep learning models in object recognition tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. Its diverse set of categories and high-quality images make it an invaluable dataset for research and development in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/caltech256/#usage","title":"Usage","text":"<p>To train a YOLO model on the Caltech-256 dataset for 100 epochs, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='caltech256', epochs=100, imgsz=416)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=caltech256 model=yolov8n-cls.pt epochs=100 imgsz=416\n</code></pre>"},{"location":"datasets/classify/caltech256/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The Caltech-256 dataset contains high-quality color images of various objects, providing a comprehensive dataset for object recognition tasks. Here are some examples of images from the dataset (credit):</p> <p></p> <p>The example showcases the diversity and complexity of the objects in the Caltech-256 dataset, emphasizing the importance of a varied dataset for training robust object recognition models.</p>"},{"location":"datasets/classify/caltech256/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the Caltech-256 dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@article{griffin2007caltech,\ntitle={Caltech-256 object category dataset},\nauthor={Griffin, Gregory and Holub, Alex and Perona, Pietro},\nyear={2007}\n}\n</code></pre> <p>We would like to acknowledge Gregory Griffin, Alex Holub, and Pietro Perona for creating and maintaining the Caltech-256 dataset as a valuable resource for the machine learning and computer vision research community. For more information about the</p> <p>Caltech-256 dataset and its creators, visit the Caltech-256 dataset website.</p>"},{"location":"datasets/classify/cifar10/","title":"CIFAR-10 Dataset","text":"<p>The CIFAR-10 (Canadian Institute For Advanced Research) dataset is a collection of images used widely for machine learning and computer vision algorithms. It was developed by researchers at the CIFAR institute and consists of 60,000 32x32 color images in 10 different classes.</p>"},{"location":"datasets/classify/cifar10/#key-features","title":"Key Features","text":"<ul> <li>The CIFAR-10 dataset consists of 60,000 images, divided into 10 classes.</li> <li>Each class contains 6,000 images, split into 5,000 for training and 1,000 for testing.</li> <li>The images are colored and of size 32x32 pixels.</li> <li>The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.</li> <li>CIFAR-10 is commonly used for training and testing in the field of machine learning and computer vision.</li> </ul>"},{"location":"datasets/classify/cifar10/#dataset-structure","title":"Dataset Structure","text":"<p>The CIFAR-10 dataset is split into two subsets:</p> <ol> <li>Training Set: This subset contains 50,000 images used for training machine learning models.</li> <li>Testing Set: This subset consists of 10,000 images used for testing and benchmarking the trained models.</li> </ol>"},{"location":"datasets/classify/cifar10/#applications","title":"Applications","text":"<p>The CIFAR-10 dataset is widely used for training and evaluating deep learning models in image classification tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. The diversity of the dataset in terms of classes and the presence of color images make it a well-rounded dataset for research and development in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/cifar10/#usage","title":"Usage","text":"<p>To train a YOLO model on the CIFAR-10 dataset for 100 epochs with an image size of 32x32, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='cifar10', epochs=100, imgsz=32)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=cifar10 model=yolov8n-cls.pt epochs=100 imgsz=32\n</code></pre>"},{"location":"datasets/classify/cifar10/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The CIFAR-10 dataset contains color images of various objects, providing a well-structured dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the objects in the CIFAR-10 dataset, highlighting the importance of a diverse dataset for training robust image classification models.</p>"},{"location":"datasets/classify/cifar10/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the CIFAR-10 dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@TECHREPORT{Krizhevsky09learningmultiple,\nauthor={Alex Krizhevsky},\ntitle={Learning multiple layers of features from tiny images},\ninstitution={},\nyear={2009}\n}\n</code></pre> <p>We would like to acknowledge Alex Krizhevsky for creating and maintaining the CIFAR-10 dataset as a valuable resource for the machine learning and computer vision research community. For more information about the CIFAR-10 dataset and its creator, visit the CIFAR-10 dataset website.</p>"},{"location":"datasets/classify/cifar100/","title":"CIFAR-100 Dataset","text":"<p>The CIFAR-100 (Canadian Institute For Advanced Research) dataset is a significant extension of the CIFAR-10 dataset, composed of 60,000 32x32 color images in 100 different classes. It was developed by researchers at the CIFAR institute, offering a more challenging dataset for more complex machine learning and computer vision tasks.</p>"},{"location":"datasets/classify/cifar100/#key-features","title":"Key Features","text":"<ul> <li>The CIFAR-100 dataset consists of 60,000 images, divided into 100 classes.</li> <li>Each class contains 600 images, split into 500 for training and 100 for testing.</li> <li>The images are colored and of size 32x32 pixels.</li> <li>The 100 different classes are grouped into 20 coarse categories for higher level classification.</li> <li>CIFAR-100 is commonly used for training and testing in the field of machine learning and computer vision.</li> </ul>"},{"location":"datasets/classify/cifar100/#dataset-structure","title":"Dataset Structure","text":"<p>The CIFAR-100 dataset is split into two subsets:</p> <ol> <li>Training Set: This subset contains 50,000 images used for training machine learning models.</li> <li>Testing Set: This subset consists of 10,000 images used for testing and benchmarking the trained models.</li> </ol>"},{"location":"datasets/classify/cifar100/#applications","title":"Applications","text":"<p>The CIFAR-100 dataset is extensively used for training and evaluating deep learning models in image classification tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. The diversity of the dataset in terms of classes and the presence of color images make it a more challenging and comprehensive dataset for research and development in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/cifar100/#usage","title":"Usage","text":"<p>To train a YOLO model on the CIFAR-100 dataset for 100 epochs with an image size of 32x32, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='cifar100', epochs=100, imgsz=32)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=cifar100 model=yolov8n-cls.pt epochs=100 imgsz=32\n</code></pre>"},{"location":"datasets/classify/cifar100/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The CIFAR-100 dataset contains color images of various objects, providing a well-structured dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the objects in the CIFAR-100 dataset, highlighting the importance of a diverse dataset for training robust image classification models.</p>"},{"location":"datasets/classify/cifar100/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the CIFAR-100 dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@TECHREPORT{Krizhevsky09learningmultiple,\nauthor={Alex Krizhevsky},\ntitle={Learning multiple layers of features from tiny images},\ninstitution={},\nyear={2009}\n}\n</code></pre> <p>We would like to acknowledge Alex Krizhevsky for creating and maintaining the CIFAR-100 dataset as a valuable resource for the machine learning and computer vision research community. For more information about the CIFAR-100 dataset and its creator, visit the CIFAR-100 dataset website.</p>"},{"location":"datasets/classify/fashion-mnist/","title":"Fashion-MNIST Dataset","text":"<p>The Fashion-MNIST dataset is a database of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms.</p>"},{"location":"datasets/classify/fashion-mnist/#key-features","title":"Key Features","text":"<ul> <li>Fashion-MNIST contains 60,000 training images and 10,000 testing images of Zalando's article images.</li> <li>The dataset comprises grayscale images of size 28x28 pixels.</li> <li>Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.</li> <li>Fashion-MNIST is widely used for training and testing in the field of machine learning, especially for image classification tasks.</li> </ul>"},{"location":"datasets/classify/fashion-mnist/#dataset-structure","title":"Dataset Structure","text":"<p>The Fashion-MNIST dataset is split into two subsets:</p> <ol> <li>Training Set: This subset contains 60,000 images used for training machine learning models.</li> <li>Testing Set: This subset consists of 10,000 images used for testing and benchmarking the trained models.</li> </ol>"},{"location":"datasets/classify/fashion-mnist/#labels","title":"Labels","text":"<p>Each training and test example is assigned to one of the following labels:</p> <ol> <li>T-shirt/top</li> <li>Trouser</li> <li>Pullover</li> <li>Dress</li> <li>Coat</li> <li>Sandal</li> <li>Shirt</li> <li>Sneaker</li> <li>Bag</li> <li>Ankle boot</li> </ol>"},{"location":"datasets/classify/fashion-mnist/#applications","title":"Applications","text":"<p>The Fashion-MNIST dataset is widely used for training and evaluating deep learning models in image classification tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. The dataset's simple and well-structured format makes it an essential resource for researchers and practitioners in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/fashion-mnist/#usage","title":"Usage","text":"<p>To train a CNN model on the Fashion-MNIST dataset for 100 epochs with an image size of 28x28, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='fashion-mnist', epochs=100, imgsz=28)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=fashion-mnist model=yolov8n-cls.pt epochs=100 imgsz=28\n</code></pre>"},{"location":"datasets/classify/fashion-mnist/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The Fashion-MNIST dataset contains grayscale images of Zalando's article images, providing a well-structured dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the images in the Fashion-MNIST dataset, highlighting the importance of a diverse dataset for training robust image classification models.</p>"},{"location":"datasets/classify/fashion-mnist/#acknowledgments","title":"Acknowledgments","text":"<p>If you use the Fashion-MNIST dataset in your research or development work, please acknowledge the dataset by linking to the GitHub repository. This dataset was made available by Zalando Research.</p>"},{"location":"datasets/classify/imagenet/","title":"ImageNet Dataset","text":"<p>ImageNet is a large-scale database of annotated images designed for use in visual object recognition research. It contains over 14 million images, with each image annotated using WordNet synsets, making it one of the most extensive resources available for training deep learning models in computer vision tasks.</p>"},{"location":"datasets/classify/imagenet/#key-features","title":"Key Features","text":"<ul> <li>ImageNet contains over 14 million high-resolution images spanning thousands of object categories.</li> <li>The dataset is organized according to the WordNet hierarchy, with each synset representing a category.</li> <li>ImageNet is widely used for training and benchmarking in the field of computer vision, particularly for image classification and object detection tasks.</li> <li>The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has been instrumental in advancing computer vision research.</li> </ul>"},{"location":"datasets/classify/imagenet/#dataset-structure","title":"Dataset Structure","text":"<p>The ImageNet dataset is organized using the WordNet hierarchy. Each node in the hierarchy represents a category, and each category is described by a synset (a collection of synonymous terms). The images in ImageNet are annotated with one or more synsets, providing a rich resource for training models to recognize various objects and their relationships.</p>"},{"location":"datasets/classify/imagenet/#imagenet-large-scale-visual-recognition-challenge-ilsvrc","title":"ImageNet Large Scale Visual Recognition Challenge (ILSVRC)","text":"<p>The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has been an important event in the field of computer vision. It has provided a platform for researchers and developers to evaluate their algorithms and models on a large-scale dataset with standardized evaluation metrics. The ILSVRC has led to significant advancements in the development of deep learning models for image classification, object detection, and other computer vision tasks.</p>"},{"location":"datasets/classify/imagenet/#applications","title":"Applications","text":"<p>The ImageNet dataset is widely used for training and evaluating deep learning models in various computer vision tasks, such as image classification, object detection, and object localization. Some popular deep learning architectures, such as AlexNet, VGG, and ResNet, were developed and benchmarked using the ImageNet dataset.</p>"},{"location":"datasets/classify/imagenet/#usage","title":"Usage","text":"<p>To train a deep learning model on the ImageNet dataset for 100 epochs with an image size of 224x224, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='imagenet', epochs=100, imgsz=224)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo train data=imagenet model=yolov8n-cls.pt epochs=100 imgsz=224\n</code></pre>"},{"location":"datasets/classify/imagenet/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The ImageNet dataset contains high-resolution images spanning thousands of object categories, providing a diverse and extensive dataset for training and evaluating computer vision models. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the images in the ImageNet dataset, highlighting the importance of a diverse dataset for training robust computer vision models.</p>"},{"location":"datasets/classify/imagenet/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the ImageNet dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@article{ILSVRC15,\nauthor = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\ntitle={ImageNet Large Scale Visual Recognition Challenge},\nyear={2015},\njournal={International Journal of Computer Vision (IJCV)},\nvolume={115},\nnumber={3},\npages={211-252}\n}\n</code></pre> <p>We would like to acknowledge the ImageNet team, led by Olga Russakovsky, Jia Deng, and Li Fei-Fei, for creating and maintaining the ImageNet dataset as a valuable resource for the machine learning and computer vision research community. For more information about the ImageNet dataset and its creators, visit the ImageNet website.</p>"},{"location":"datasets/classify/imagenet10/","title":"ImageNet10 Dataset","text":"<p>The ImageNet10 dataset is a small-scale subset of the ImageNet database, developed by Ultralytics and designed for CI tests, sanity checks, and fast testing of training pipelines. This dataset is composed of the first image in the training set and the first image from the validation set of the first 10 classes in ImageNet. Although significantly smaller, it retains the structure and diversity of the original ImageNet dataset.</p>"},{"location":"datasets/classify/imagenet10/#key-features","title":"Key Features","text":"<ul> <li>ImageNet10 is a compact version of ImageNet, with 20 images representing the first 10 classes of the original dataset.</li> <li>The dataset is organized according to the WordNet hierarchy, mirroring the structure of the full ImageNet dataset.</li> <li>It is ideally suited for CI tests, sanity checks, and rapid testing of training pipelines in computer vision tasks.</li> <li>Although not designed for model benchmarking, it can provide a quick indication of a model's basic functionality and correctness.</li> </ul>"},{"location":"datasets/classify/imagenet10/#dataset-structure","title":"Dataset Structure","text":"<p>The ImageNet10 dataset, like the original ImageNet, is organized using the WordNet hierarchy. Each of the 10 classes in ImageNet10 is described by a synset (a collection of synonymous terms). The images in ImageNet10 are annotated with one or more synsets, providing a compact resource for testing models to recognize various objects and their relationships.</p>"},{"location":"datasets/classify/imagenet10/#applications","title":"Applications","text":"<p>The ImageNet10 dataset is useful for quickly testing and debugging computer vision models and pipelines. Its small size allows for rapid iteration, making it ideal for continuous integration tests and sanity checks. It can also be used for fast preliminary testing of new models or changes to existing models before moving on to full-scale testing with the complete ImageNet dataset.</p>"},{"location":"datasets/classify/imagenet10/#usage","title":"Usage","text":"<p>To test a deep learning model on the ImageNet10 dataset with an image size of 224x224, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Test Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='imagenet10', epochs=5, imgsz=224)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo train data=imagenet10 model=yolov8n-cls.pt epochs=5 imgsz=224\n</code></pre>"},{"location":"datasets/classify/imagenet10/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The ImageNet10 dataset contains a subset of images from the original ImageNet dataset. These images are chosen to represent the first 10 classes in the dataset, providing a diverse yet compact dataset for quick testing and evaluation.</p> <p> The example showcases the variety and complexity of the images in the ImageNet10 dataset, highlighting its usefulness for sanity checks and quick testing of computer vision models.</p>"},{"location":"datasets/classify/imagenet10/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the ImageNet10 dataset in your research or development work, please cite the original ImageNet paper:</p> BibTeX <pre><code>@article{ILSVRC15,\nauthor = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\ntitle={ImageNet Large Scale Visual Recognition Challenge},\nyear={2015},\njournal={International Journal of Computer Vision (IJCV)},\nvolume={115},\nnumber={3},\npages={211-252}\n}\n</code></pre> <p>We would like to acknowledge the ImageNet team, led by Olga Russakovsky, Jia Deng, and Li Fei-Fei, for creating and maintaining the ImageNet dataset. The ImageNet10 dataset, while a compact subset, is a valuable resource for quick testing and debugging in the machine learning and computer vision research community. For more information about the ImageNet dataset and its creators, visit the ImageNet website.</p>"},{"location":"datasets/classify/imagenette/","title":"ImageNette Dataset","text":"<p>The ImageNette dataset is a subset of the larger Imagenet dataset, but it only includes 10 easily distinguishable classes. It was created to provide a quicker, easier-to-use version of Imagenet for software development and education.</p>"},{"location":"datasets/classify/imagenette/#key-features","title":"Key Features","text":"<ul> <li>ImageNette contains images from 10 different classes such as tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute.</li> <li>The dataset comprises colored images of varying dimensions.</li> <li>ImageNette is widely used for training and testing in the field of machine learning, especially for image classification tasks.</li> </ul>"},{"location":"datasets/classify/imagenette/#dataset-structure","title":"Dataset Structure","text":"<p>The ImageNette dataset is split into two subsets:</p> <ol> <li>Training Set: This subset contains several thousands of images used for training machine learning models. The exact number varies per class.</li> <li>Validation Set: This subset consists of several hundreds of images used for validating and benchmarking the trained models. Again, the exact number varies per class.</li> </ol>"},{"location":"datasets/classify/imagenette/#applications","title":"Applications","text":"<p>The ImageNette dataset is widely used for training and evaluating deep learning models in image classification tasks, such as Convolutional Neural Networks (CNNs), and various other machine learning algorithms. The dataset's straightforward format and well-chosen classes make it a handy resource for both beginner and experienced practitioners in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/imagenette/#usage","title":"Usage","text":"<p>To train a model on the ImageNette dataset for 100 epochs with a standard image size of 224x224, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='imagenette', epochs=100, imgsz=224)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=imagenette model=yolov8n-cls.pt epochs=100 imgsz=224\n</code></pre>"},{"location":"datasets/classify/imagenette/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The ImageNette dataset contains colored images of various objects and scenes, providing a diverse dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the images in the ImageNette dataset, highlighting the importance of a diverse dataset for training robust image classification models.</p>"},{"location":"datasets/classify/imagenette/#imagenette160-and-imagenette320","title":"ImageNette160 and ImageNette320","text":"<p>For faster prototyping and training, the ImageNette dataset is also available in two reduced sizes: ImageNette160 and ImageNette320. These datasets maintain the same classes and structure as the full ImageNette dataset, but the images are resized to a smaller dimension. As such, these versions of the dataset are particularly useful for preliminary model testing, or when computational resources are limited.</p> <p>To use these datasets, simply replace 'imagenette' with 'imagenette160' or 'imagenette320' in the training command. The following code snippets illustrate this:</p> <p>Train Example with ImageNette160</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model with ImageNette160\nresults = model.train(data='imagenette160', epochs=100, imgsz=160)\n</code></pre> <pre><code># Start training from a pretrained *.pt model with ImageNette160\nyolo detect train data=imagenette160 model=yolov8n-cls.pt epochs=100 imgsz=160\n</code></pre> <p>Train Example with ImageNette320</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model with ImageNette320\nresults = model.train(data='imagenette320', epochs=100, imgsz=320)\n</code></pre> <pre><code># Start training from a pretrained *.pt model with ImageNette320\nyolo detect train data=imagenette320 model=yolov8n-cls.pt epochs=100 imgsz=320\n</code></pre> <p>These smaller versions of the dataset allow for rapid iterations during the development process while still providing valuable and realistic image classification tasks.</p>"},{"location":"datasets/classify/imagenette/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the ImageNette dataset in your research or development work, please acknowledge it appropriately. For more information about the ImageNette dataset, visit the ImageNette dataset GitHub page.</p>"},{"location":"datasets/classify/imagewoof/","title":"ImageWoof Dataset","text":"<p>The ImageWoof dataset is a subset of the ImageNet consisting of 10 classes that are challenging to classify, since they're all dog breeds. It was created as a more difficult task for image classification algorithms to solve, aiming at encouraging development of more advanced models.</p>"},{"location":"datasets/classify/imagewoof/#key-features","title":"Key Features","text":"<ul> <li>ImageWoof contains images of 10 different dog breeds: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, and Old English sheepdog.</li> <li>The dataset provides images at various resolutions (full size, 320px, 160px), accommodating for different computational capabilities and research needs.</li> <li>It also includes a version with noisy labels, providing a more realistic scenario where labels might not always be reliable.</li> </ul>"},{"location":"datasets/classify/imagewoof/#dataset-structure","title":"Dataset Structure","text":"<p>The ImageWoof dataset structure is based on the dog breed classes, with each breed having its own directory of images.</p>"},{"location":"datasets/classify/imagewoof/#applications","title":"Applications","text":"<p>The ImageWoof dataset is widely used for training and evaluating deep learning models in image classification tasks, especially when it comes to more complex and similar classes. The dataset's challenge lies in the subtle differences between the dog breeds, pushing the limits of model's performance and generalization.</p>"},{"location":"datasets/classify/imagewoof/#usage","title":"Usage","text":"<p>To train a CNN model on the ImageWoof dataset for 100 epochs with an image size of 224x224, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='imagewoof', epochs=100, imgsz=224)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=imagewoof model=yolov8n-cls.pt epochs=100 imgsz=224\n</code></pre>"},{"location":"datasets/classify/imagewoof/#dataset-variants","title":"Dataset Variants","text":"<p>ImageWoof dataset comes in three different sizes to accommodate various research needs and computational capabilities:</p> <ol> <li> <p>Full Size (imagewoof): This is the original version of the ImageWoof dataset. It contains full-sized images and is ideal for final training and performance benchmarking.</p> </li> <li> <p>Medium Size (imagewoof320): This version contains images resized to have a maximum edge length of 320 pixels. It's suitable for faster training without significantly sacrificing model performance.</p> </li> <li> <p>Small Size (imagewoof160): This version contains images resized to have a maximum edge length of 160 pixels. It's designed for rapid prototyping and experimentation where training speed is a priority.</p> </li> </ol> <p>To use these variants in your training, simply replace 'imagewoof' in the dataset argument with 'imagewoof320' or 'imagewoof160'. For example:</p> <pre><code># For medium-sized dataset\nmodel.train(data='imagewoof320', epochs=100, imgsz=224)\n# For small-sized dataset\nmodel.train(data='imagewoof160', epochs=100, imgsz=224)\n</code></pre> <p>It's important to note that using smaller images will likely yield lower performance in terms of classification accuracy. However, it's an excellent way to iterate quickly in the early stages of model development and prototyping.</p>"},{"location":"datasets/classify/imagewoof/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The ImageWoof dataset contains colorful images of various dog breeds, providing a challenging dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the subtle differences and similarities among the different dog breeds in the ImageWoof dataset, highlighting the complexity and difficulty of the classification task.</p>"},{"location":"datasets/classify/imagewoof/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the ImageWoof dataset in your research or development work, please make sure to acknowledge the creators of the dataset by linking to the official dataset repository.</p> <p>We would like to acknowledge the FastAI team for creating and maintaining the ImageWoof dataset as a valuable resource for the machine learning and computer vision research community. For more information about the ImageWoof dataset, visit the ImageWoof dataset repository.</p>"},{"location":"datasets/classify/mnist/","title":"MNIST Dataset","text":"<p>The MNIST (Modified National Institute of Standards and Technology) dataset is a large database of handwritten digits that is commonly used for training various image processing systems and machine learning models. It was created by \"re-mixing\" the samples from NIST's original datasets and has become a benchmark for evaluating the performance of image classification algorithms.</p>"},{"location":"datasets/classify/mnist/#key-features","title":"Key Features","text":"<ul> <li>MNIST contains 60,000 training images and 10,000 testing images of handwritten digits.</li> <li>The dataset comprises grayscale images of size 28x28 pixels.</li> <li>The images are normalized to fit into a 28x28 pixel bounding box and anti-aliased, introducing grayscale levels.</li> <li>MNIST is widely used for training and testing in the field of machine learning, especially for image classification tasks.</li> </ul>"},{"location":"datasets/classify/mnist/#dataset-structure","title":"Dataset Structure","text":"<p>The MNIST dataset is split into two subsets:</p> <ol> <li>Training Set: This subset contains 60,000 images of handwritten digits used for training machine learning models.</li> <li>Testing Set: This subset consists of 10,000 images used for testing and benchmarking the trained models.</li> </ol>"},{"location":"datasets/classify/mnist/#extended-mnist-emnist","title":"Extended MNIST (EMNIST)","text":"<p>Extended MNIST (EMNIST) is a newer dataset developed and released by NIST to be the successor to MNIST. While MNIST included images only of handwritten digits, EMNIST includes all the images from NIST Special Database 19, which is a large database of handwritten uppercase and lowercase letters as well as digits. The images in EMNIST were converted into the same 28x28 pixel format, by the same process, as were the MNIST images. Accordingly, tools that work with the older, smaller MNIST dataset will likely work unmodified with EMNIST.</p>"},{"location":"datasets/classify/mnist/#applications","title":"Applications","text":"<p>The MNIST dataset is widely used for training and evaluating deep learning models in image classification tasks, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), and various other machine learning algorithms. The dataset's simple and well-structured format makes it an essential resource for researchers and practitioners in the field of machine learning and computer vision.</p>"},{"location":"datasets/classify/mnist/#usage","title":"Usage","text":"<p>To train a CNN model on the MNIST dataset for 100 epochs with an image size of 32x32, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='mnist', epochs=100, imgsz=32)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\ncnn detect train data=mnist model=yolov8n-cls.pt epochs=100 imgsz=28\n</code></pre>"},{"location":"datasets/classify/mnist/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The MNIST dataset contains grayscale images of handwritten digits, providing a well-structured dataset for image classification tasks. Here are some examples of images from the dataset:</p> <p></p> <p>The example showcases the variety and complexity of the handwritten digits in the MNIST dataset, highlighting the importance of a diverse dataset for training robust image classification models.</p>"},{"location":"datasets/classify/mnist/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the MNIST dataset in your</p> <p>research or development work, please cite the following paper:</p> BibTeX <pre><code>@article{lecun2010mnist,\ntitle={MNIST handwritten digit database},\nauthor={LeCun, Yann and Cortes, Corinna and Burges, CJ},\njournal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\nvolume={2},\nyear={2010}\n}\n</code></pre> <p>We would like to acknowledge Yann LeCun, Corinna Cortes, and Christopher J.C. Burges for creating and maintaining the MNIST dataset as a valuable resource for the machine learning and computer vision research community. For more information about the MNIST dataset and its creators, visit the MNIST dataset website.</p>"},{"location":"datasets/detect/","title":"Object Detection Datasets Overview","text":"<p>Training a robust and accurate object detection model requires a comprehensive dataset. This guide introduces various formats of datasets that are compatible with the Ultralytics YOLO model and provides insights into their structure, usage, and how to convert between different formats.</p>"},{"location":"datasets/detect/#supported-dataset-formats","title":"Supported Dataset Formats","text":""},{"location":"datasets/detect/#ultralytics-yolo-format","title":"Ultralytics YOLO format","text":"<p>The Ultralytics YOLO format is a dataset configuration format that allows you to define the dataset root directory, the relative paths to training/validation/testing image directories or *.txt files containing image paths, and a dictionary of class names. Here is an example:</p> <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Classes (80 COCO classes)\nnames:\n0: person\n1: bicycle\n2: car\n...\n77: teddy bear\n78: hair drier\n79: toothbrush\n</code></pre> <p>Labels for this format should be exported to YOLO format with one <code>*.txt</code> file per image. If there are no objects in an image, no <code>*.txt</code> file is required. The <code>*.txt</code> file should be formatted with one row per object in <code>class x_center y_center width height</code> format. Box coordinates must be in normalized xywh format (from 0 to 1). If your boxes are in pixels, you should divide <code>x_center</code> and <code>width</code> by image width, and <code>y_center</code> and <code>height</code> by image height. Class numbers should be zero-indexed (start with 0).</p> <p></p> <p>The label file corresponding to the above image contains 2 persons (class <code>0</code>) and a tie (class <code>27</code>):</p> <p></p> <p>When using the Ultralytics YOLO format, organize your training and validation images and labels as shown in the example below.</p> <p></p>"},{"location":"datasets/detect/#usage","title":"Usage","text":"<p>Here's how you can use these formats to train your model:</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco8.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/#supported-datasets","title":"Supported Datasets","text":"<p>Here is a list of the supported datasets and a brief description for each:</p> <ul> <li>Argoverse: A collection of sensor data collected from autonomous vehicles. It contains 3D tracking annotations for car objects.</li> <li>COCO: Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset with 80 object categories.</li> <li>COCO8: A smaller subset of the COCO dataset, COCO8 is more lightweight and faster to train.</li> <li>GlobalWheat2020: A dataset containing images of wheat heads for the Global Wheat Challenge 2020.</li> <li>Objects365: A large-scale object detection dataset with 365 object categories and 600k images, aimed at advancing object detection research.</li> <li>OpenImagesV7: A comprehensive dataset by Google with 1.7M train images and 42k validation images.</li> <li>SKU-110K: A dataset containing images of densely packed retail products, intended for retail environment object detection.</li> <li>VisDrone: A dataset focusing on drone-based images, containing various object categories like cars, pedestrians, and cyclists.</li> <li>VOC: PASCAL VOC is a popular object detection dataset with 20 object categories including vehicles, animals, and furniture.</li> <li>xView: A dataset containing high-resolution satellite imagery, designed for the detection of various object classes in overhead views.</li> </ul>"},{"location":"datasets/detect/#adding-your-own-dataset","title":"Adding your own dataset","text":"<p>If you have your own dataset and would like to use it for training detection models with Ultralytics YOLO format, ensure that it follows the format specified above under \"Ultralytics YOLO format\". Convert your annotations to the required format and specify the paths, number of classes, and class names in the YAML configuration file.</p>"},{"location":"datasets/detect/#port-or-convert-label-formats","title":"Port or Convert Label Formats","text":""},{"location":"datasets/detect/#coco-dataset-format-to-yolo-format","title":"COCO Dataset Format to YOLO Format","text":"<p>You can easily convert labels from the popular COCO dataset format to the YOLO format using the following code snippet:</p> <pre><code>from ultralytics.data.converter import convert_coco\nconvert_coco(labels_dir='../coco/annotations/')\n</code></pre> <p>This conversion tool can be used to convert the COCO dataset or any dataset in the COCO format to the Ultralytics YOLO format.</p> <p>Remember to double-check if the dataset you want to use is compatible with your model and follows the necessary format conventions. Properly formatted datasets are crucial for training successful object detection models.</p>"},{"location":"datasets/detect/argoverse/","title":"Argoverse Dataset","text":"<p>The Argoverse dataset is a collection of data designed to support research in autonomous driving tasks, such as 3D tracking, motion forecasting, and stereo depth estimation. Developed by Argo AI, the dataset provides a wide range of high-quality sensor data, including high-resolution images, LiDAR point clouds, and map data.</p> <p>Note</p> <p>The Argoverse dataset *.zip file required for training was removed from Amazon S3 after the shutdown of Argo AI by Ford, but we have made it available for manual download on Google Drive.</p>"},{"location":"datasets/detect/argoverse/#key-features","title":"Key Features","text":"<ul> <li>Argoverse contains over 290K labeled 3D object tracks and 5 million object instances across 1,263 distinct scenes.</li> <li>The dataset includes high-resolution camera images, LiDAR point clouds, and richly annotated HD maps.</li> <li>Annotations include 3D bounding boxes for objects, object tracks, and trajectory information.</li> <li>Argoverse provides multiple subsets for different tasks, such as 3D tracking, motion forecasting, and stereo depth estimation.</li> </ul>"},{"location":"datasets/detect/argoverse/#dataset-structure","title":"Dataset Structure","text":"<p>The Argoverse dataset is organized into three main subsets:</p> <ol> <li>Argoverse 3D Tracking: This subset contains 113 scenes with over 290K labeled 3D object tracks, focusing on 3D object tracking tasks. It includes LiDAR point clouds, camera images, and sensor calibration information.</li> <li>Argoverse Motion Forecasting: This subset consists of 324K vehicle trajectories collected from 60 hours of driving data, suitable for motion forecasting tasks.</li> <li>Argoverse Stereo Depth Estimation: This subset is designed for stereo depth estimation tasks and includes over 10K stereo image pairs with corresponding LiDAR point clouds for ground truth depth estimation.</li> </ol>"},{"location":"datasets/detect/argoverse/#applications","title":"Applications","text":"<p>The Argoverse dataset is widely used for training and evaluating deep learning models in autonomous driving tasks such as 3D object tracking, motion forecasting, and stereo depth estimation. The dataset's diverse set of sensor data, object annotations, and map information make it a valuable resource for researchers and practitioners in the field of autonomous driving.</p>"},{"location":"datasets/detect/argoverse/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. For the case of the Argoverse dataset, the <code>Argoverse.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/Argoverse.yaml.</p> <p>ultralytics/cfg/datasets/Argoverse.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# Argoverse-HD dataset (ring-front-center camera) http://www.cs.cmu.edu/~mengtial/proj/streaming/ by Argo AI\n# Example usage: yolo train data=Argoverse.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 Argoverse  \u2190 downloads here (31.5 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/Argoverse  # dataset root dir\ntrain: Argoverse-1.1/images/train/  # train images (relative to 'path') 39384 images\nval: Argoverse-1.1/images/val/  # val images (relative to 'path') 15062 images\ntest: Argoverse-1.1/images/test/  # test images (optional) https://eval.ai/web/challenges/challenge-page/800/overview\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: bus\n5: truck\n6: traffic_light\n7: stop_sign\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nimport json\nfrom tqdm import tqdm\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\ndef argoverse2yolo(set):\nlabels = {}\na = json.load(open(set, \"rb\"))\nfor annot in tqdm(a['annotations'], desc=f\"Converting {set} to YOLOv5 format...\"):\nimg_id = annot['image_id']\nimg_name = a['images'][img_id]['name']\nimg_label_name = f'{img_name[:-3]}txt'\ncls = annot['category_id']  # instance class id\nx_center, y_center, width, height = annot['bbox']\nx_center = (x_center + width / 2) / 1920.0  # offset and scale\ny_center = (y_center + height / 2) / 1200.0  # offset and scale\nwidth /= 1920.0  # scale\nheight /= 1200.0  # scale\nimg_dir = set.parents[2] / 'Argoverse-1.1' / 'labels' / a['seq_dirs'][a['images'][annot['image_id']]['sid']]\nif not img_dir.exists():\nimg_dir.mkdir(parents=True, exist_ok=True)\nk = str(img_dir / img_label_name)\nif k not in labels:\nlabels[k] = []\nlabels[k].append(f\"{cls} {x_center} {y_center} {width} {height}\\n\")\nfor k in labels:\nwith open(k, \"w\") as f:\nf.writelines(labels[k])\n# Download 'https://argoverse-hd.s3.us-east-2.amazonaws.com/Argoverse-HD-Full.zip' (deprecated S3 link)\ndir = Path(yaml['path'])  # dataset root dir\nurls = ['https://drive.google.com/file/d/1st9qW3BeIwQsnR0t8mRpvbsSWIo16ACi/view?usp=drive_link']\ndownload(urls, dir=dir)\n# Convert\nannotations_dir = 'Argoverse-HD/annotations/'\n(dir / 'Argoverse-1.1' / 'tracking').rename(dir / 'Argoverse-1.1' / 'images')  # rename 'tracking' to 'images'\nfor d in \"train.json\", \"val.json\":\nargoverse2yolo(dir / annotations_dir / d)  # convert Argoverse annotations to YOLO labels\n</code></pre>"},{"location":"datasets/detect/argoverse/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the Argoverse dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='Argoverse.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=Argoverse.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/argoverse/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The Argoverse dataset contains a diverse set of sensor data, including camera images, LiDAR point clouds, and HD map information, providing rich context for autonomous driving tasks. Here are some examples of data from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Argoverse 3D Tracking: This image demonstrates an example of 3D object tracking, where objects are annotated with 3D bounding boxes. The dataset provides LiDAR point clouds and camera images to facilitate the development of models for this task.</li> </ul> <p>The example showcases the variety and complexity of the data in the Argoverse dataset and highlights the importance of high-quality sensor data for autonomous driving tasks.</p>"},{"location":"datasets/detect/argoverse/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the Argoverse dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@inproceedings{chang2019argoverse,\ntitle={Argoverse: 3D Tracking and Forecasting with Rich Maps},\nauthor={Chang, Ming-Fang and Lambert, John and Sangkloy, Patsorn and Singh, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang, Dequan and Carr, Peter and Lucey, Simon and Ramanan, Deva and others},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npages={8748--8757},\nyear={2019}\n}\n</code></pre> <p>We would like to acknowledge Argo AI for creating and maintaining the Argoverse dataset as a valuable resource for the autonomous driving research community. For more information about the Argoverse dataset and its creators, visit the Argoverse dataset website.</p>"},{"location":"datasets/detect/coco/","title":"COCO Dataset","text":"<p>The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models. It is an essential dataset for researchers and developers working on object detection, segmentation, and pose estimation tasks.</p>"},{"location":"datasets/detect/coco/#key-features","title":"Key Features","text":"<ul> <li>COCO contains 330K images, with 200K images having annotations for object detection, segmentation, and captioning tasks.</li> <li>The dataset comprises 80 object categories, including common objects like cars, bicycles, and animals, as well as more specific categories such as umbrellas, handbags, and sports equipment.</li> <li>Annotations include object bounding boxes, segmentation masks, and captions for each image.</li> <li>COCO provides standardized evaluation metrics like mean Average Precision (mAP) for object detection, and mean Average Recall (mAR) for segmentation tasks, making it suitable for comparing model performance.</li> </ul>"},{"location":"datasets/detect/coco/#dataset-structure","title":"Dataset Structure","text":"<p>The COCO dataset is split into three subsets:</p> <ol> <li>Train2017: This subset contains 118K images for training object detection, segmentation, and captioning models.</li> <li>Val2017: This subset has 5K images used for validation purposes during model training.</li> <li>Test2017: This subset consists of 20K images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the COCO evaluation server for performance evaluation.</li> </ol>"},{"location":"datasets/detect/coco/#applications","title":"Applications","text":"<p>The COCO dataset is widely used for training and evaluating deep learning models in object detection (such as YOLO, Faster R-CNN, and SSD), instance segmentation (such as Mask R-CNN), and keypoint detection (such as OpenPose). The dataset's diverse set of object categories, large number of annotated images, and standardized evaluation metrics make it an essential resource for computer vision researchers and practitioners.</p>"},{"location":"datasets/detect/coco/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO dataset, the <code>coco.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml.</p> <p>ultralytics/cfg/datasets/coco.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO 2017 dataset http://cocodataset.org by Microsoft\n# Example usage: yolo train data=coco.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco  \u2190 downloads here (20.1 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco  # dataset root dir\ntrain: train2017.txt  # train images (relative to 'path') 118287 images\nval: val2017.txt  # val images (relative to 'path') 5000 images\ntest: test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n# Download script/URL (optional)\ndownload: |\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\n# Download labels\nsegments = True  # segment or box labels\ndir = Path(yaml['path'])  # dataset root dir\nurl = 'https://github.com/ultralytics/yolov5/releases/download/v1.0/'\nurls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\ndownload(urls, dir=dir.parent)\n# Download data\nurls = ['http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n'http://images.cocodataset.org/zips/val2017.zip',  # 1G, 5k images\n'http://images.cocodataset.org/zips/test2017.zip']  # 7G, 41k images (optional)\ndownload(urls, dir=dir / 'images', threads=3)\n</code></pre>"},{"location":"datasets/detect/coco/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the COCO dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/coco/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The COCO dataset contains a diverse set of images with various object categories and complex scenes. Here are some examples of images from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/detect/coco/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/detect/coco8/","title":"COCO8 Dataset","text":""},{"location":"datasets/detect/coco8/#introduction","title":"Introduction","text":"<p>Ultralytics COCO8 is a small, but versatile object detection dataset composed of the first 8 images of the COCO train 2017 set, 4 for training and 4 for validation. This dataset is ideal for testing and debugging object detection models, or for experimenting with new detection approaches. With 8 images, it is small enough to be easily manageable, yet diverse enough to test training pipelines for errors and act as a sanity check before training larger datasets.</p> <p>This dataset is intended for use with Ultralytics HUB and YOLOv8.</p>"},{"location":"datasets/detect/coco8/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO8 dataset, the <code>coco8.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco8.yaml.</p> <p>ultralytics/cfg/datasets/coco8.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO8 dataset (first 8 images from COCO train2017) by Ultralytics\n# Example usage: yolo train data=coco8.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco8  \u2190 downloads here (1 MB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n# Download script/URL (optional)\ndownload: https://ultralytics.com/assets/coco8.zip\n</code></pre>"},{"location":"datasets/detect/coco8/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the COCO8 dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco8.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/coco8/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>Here are some examples of images from the COCO8 dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO8 dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/detect/coco8/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/detect/globalwheat2020/","title":"Global Wheat Head Dataset","text":"<p>The Global Wheat Head Dataset is a collection of images designed to support the development of accurate wheat head detection models for applications in wheat phenotyping and crop management. Wheat heads, also known as spikes, are the grain-bearing parts of the wheat plant. Accurate estimation of wheat head density and size is essential for assessing crop health, maturity, and yield potential. The dataset, created by a collaboration of nine research institutes from seven countries, covers multiple growing regions to ensure models generalize well across different environments.</p>"},{"location":"datasets/detect/globalwheat2020/#key-features","title":"Key Features","text":"<ul> <li>The dataset contains over 3,000 training images from Europe (France, UK, Switzerland) and North America (Canada).</li> <li>It includes approximately 1,000 test images from Australia, Japan, and China.</li> <li>Images are outdoor field images, capturing the natural variability in wheat head appearances.</li> <li>Annotations include wheat head bounding boxes to support object detection tasks.</li> </ul>"},{"location":"datasets/detect/globalwheat2020/#dataset-structure","title":"Dataset Structure","text":"<p>The Global Wheat Head Dataset is organized into two main subsets:</p> <ol> <li>Training Set: This subset contains over 3,000 images from Europe and North America. The images are labeled with wheat head bounding boxes, providing ground truth for training object detection models.</li> <li>Test Set: This subset consists of approximately 1,000 images from Australia, Japan, and China. These images are used for evaluating the performance of trained models on unseen genotypes, environments, and observational conditions.</li> </ol>"},{"location":"datasets/detect/globalwheat2020/#applications","title":"Applications","text":"<p>The Global Wheat Head Dataset is widely used for training and evaluating deep learning models in wheat head detection tasks. The dataset's diverse set of images, capturing a wide range of appearances, environments, and conditions, make it a valuable resource for researchers and practitioners in the field of plant phenotyping and crop management.</p>"},{"location":"datasets/detect/globalwheat2020/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. For the case of the Global Wheat Head Dataset, the <code>GlobalWheat2020.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/GlobalWheat2020.yaml.</p> <p>ultralytics/cfg/datasets/GlobalWheat2020.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# Global Wheat 2020 dataset http://www.global-wheat.com/ by University of Saskatchewan\n# Example usage: yolo train data=GlobalWheat2020.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 GlobalWheat2020  \u2190 downloads here (7.0 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/GlobalWheat2020  # dataset root dir\ntrain: # train images (relative to 'path') 3422 images\n- images/arvalis_1\n- images/arvalis_2\n- images/arvalis_3\n- images/ethz_1\n- images/rres_1\n- images/inrae_1\n- images/usask_1\nval: # val images (relative to 'path') 748 images (WARNING: train set contains ethz_1)\n- images/ethz_1\ntest: # test images (optional) 1276 images\n- images/utokyo_1\n- images/utokyo_2\n- images/nau_1\n- images/uq_1\n# Classes\nnames:\n0: wheat_head\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\n# Download\ndir = Path(yaml['path'])  # dataset root dir\nurls = ['https://zenodo.org/record/4298502/files/global-wheat-codalab-official.zip',\n'https://github.com/ultralytics/yolov5/releases/download/v1.0/GlobalWheat2020_labels.zip']\ndownload(urls, dir=dir)\n# Make Directories\nfor p in 'annotations', 'images', 'labels':\n(dir / p).mkdir(parents=True, exist_ok=True)\n# Move\nfor p in 'arvalis_1', 'arvalis_2', 'arvalis_3', 'ethz_1', 'rres_1', 'inrae_1', 'usask_1', \\\n'utokyo_1', 'utokyo_2', 'nau_1', 'uq_1':\n(dir / 'global-wheat-codalab-official' / p).rename(dir / 'images' / p)  # move to /images\nf = (dir / 'global-wheat-codalab-official' / p).with_suffix('.json')  # json file\nif f.exists():\nf.rename((dir / 'annotations' / p).with_suffix('.json'))  # move to /annotations\n</code></pre>"},{"location":"datasets/detect/globalwheat2020/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the Global Wheat Head Dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='GlobalWheat2020.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=GlobalWheat2020.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/globalwheat2020/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The Global Wheat Head Dataset contains a diverse set of outdoor field images, capturing the natural variability in wheat head appearances, environments, and conditions. Here are some examples of data from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Wheat Head Detection: This image demonstrates an example of wheat head detection, where wheat heads are annotated with bounding boxes. The dataset provides a variety of images to facilitate the development of models for this task.</li> </ul> <p>The example showcases the variety and complexity of the data in the Global Wheat Head Dataset and highlights the importance of accurate wheat head detection for applications in wheat phenotyping and crop management.</p>"},{"location":"datasets/detect/globalwheat2020/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the Global Wheat Head Dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@article{david2020global,\ntitle={Global Wheat Head Detection (GWHD) Dataset: A Large and Diverse Dataset of High-Resolution RGB-Labelled Images to Develop and Benchmark Wheat Head Detection Methods},\nauthor={David, Etienne and Madec, Simon and Sadeghi-Tehran, Pouria and Aasen, Helge and Zheng, Bangyou and Liu, Shouyang and Kirchgessner, Norbert and Ishikawa, Goro and Nagasawa, Koichi and Badhon, Minhajul and others},\njournal={arXiv preprint arXiv:2005.02162},\nyear={2020}\n}\n</code></pre> <p>We would like to acknowledge the researchers and institutions that contributed to the creation and maintenance of the Global Wheat Head Dataset as a valuable resource for the plant phenotyping and crop management research community. For more information about the dataset and its creators, visit the Global Wheat Head Dataset website.</p>"},{"location":"datasets/detect/objects365/","title":"Objects365 Dataset","text":"<p>The Objects365 dataset is a large-scale, high-quality dataset designed to foster object detection research with a focus on diverse objects in the wild. Created by a team of Megvii researchers, the dataset offers a wide range of high-resolution images with a comprehensive set of annotated bounding boxes covering 365 object categories.</p>"},{"location":"datasets/detect/objects365/#key-features","title":"Key Features","text":"<ul> <li>Objects365 contains 365 object categories, with 2 million images and over 30 million bounding boxes.</li> <li>The dataset includes diverse objects in various scenarios, providing a rich and challenging benchmark for object detection tasks.</li> <li>Annotations include bounding boxes for objects, making it suitable for training and evaluating object detection models.</li> <li>Objects365 pre-trained models significantly outperform ImageNet pre-trained models, leading to better generalization on various tasks.</li> </ul>"},{"location":"datasets/detect/objects365/#dataset-structure","title":"Dataset Structure","text":"<p>The Objects365 dataset is organized into a single set of images with corresponding annotations:</p> <ul> <li>Images: The dataset includes 2 million high-resolution images, each containing a variety of objects across 365 categories.</li> <li>Annotations: The images are annotated with over 30 million bounding boxes, providing comprehensive ground truth information for object detection tasks.</li> </ul>"},{"location":"datasets/detect/objects365/#applications","title":"Applications","text":"<p>The Objects365 dataset is widely used for training and evaluating deep learning models in object detection tasks. The dataset's diverse set of object categories and high-quality annotations make it a valuable resource for researchers and practitioners in the field of computer vision.</p>"},{"location":"datasets/detect/objects365/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. For the case of the Objects365 Dataset, the <code>Objects365.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/Objects365.yaml.</p> <p>ultralytics/cfg/datasets/Objects365.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# Objects365 dataset https://www.objects365.org/ by Megvii\n# Example usage: yolo train data=Objects365.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 Objects365  \u2190 downloads here (712 GB = 367G data + 345G zips)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/Objects365  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 1742289 images\nval: images/val # val images (relative to 'path') 80000 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: Person\n1: Sneakers\n2: Chair\n3: Other Shoes\n4: Hat\n5: Car\n6: Lamp\n7: Glasses\n8: Bottle\n9: Desk\n10: Cup\n11: Street Lights\n12: Cabinet/shelf\n13: Handbag/Satchel\n14: Bracelet\n15: Plate\n16: Picture/Frame\n17: Helmet\n18: Book\n19: Gloves\n20: Storage box\n21: Boat\n22: Leather Shoes\n23: Flower\n24: Bench\n25: Potted Plant\n26: Bowl/Basin\n27: Flag\n28: Pillow\n29: Boots\n30: Vase\n31: Microphone\n32: Necklace\n33: Ring\n34: SUV\n35: Wine Glass\n36: Belt\n37: Monitor/TV\n38: Backpack\n39: Umbrella\n40: Traffic Light\n41: Speaker\n42: Watch\n43: Tie\n44: Trash bin Can\n45: Slippers\n46: Bicycle\n47: Stool\n48: Barrel/bucket\n49: Van\n50: Couch\n51: Sandals\n52: Basket\n53: Drum\n54: Pen/Pencil\n55: Bus\n56: Wild Bird\n57: High Heels\n58: Motorcycle\n59: Guitar\n60: Carpet\n61: Cell Phone\n62: Bread\n63: Camera\n64: Canned\n65: Truck\n66: Traffic cone\n67: Cymbal\n68: Lifesaver\n69: Towel\n70: Stuffed Toy\n71: Candle\n72: Sailboat\n73: Laptop\n74: Awning\n75: Bed\n76: Faucet\n77: Tent\n78: Horse\n79: Mirror\n80: Power outlet\n81: Sink\n82: Apple\n83: Air Conditioner\n84: Knife\n85: Hockey Stick\n86: Paddle\n87: Pickup Truck\n88: Fork\n89: Traffic Sign\n90: Balloon\n91: Tripod\n92: Dog\n93: Spoon\n94: Clock\n95: Pot\n96: Cow\n97: Cake\n98: Dinning Table\n99: Sheep\n100: Hanger\n101: Blackboard/Whiteboard\n102: Napkin\n103: Other Fish\n104: Orange/Tangerine\n105: Toiletry\n106: Keyboard\n107: Tomato\n108: Lantern\n109: Machinery Vehicle\n110: Fan\n111: Green Vegetables\n112: Banana\n113: Baseball Glove\n114: Airplane\n115: Mouse\n116: Train\n117: Pumpkin\n118: Soccer\n119: Skiboard\n120: Luggage\n121: Nightstand\n122: Tea pot\n123: Telephone\n124: Trolley\n125: Head Phone\n126: Sports Car\n127: Stop Sign\n128: Dessert\n129: Scooter\n130: Stroller\n131: Crane\n132: Remote\n133: Refrigerator\n134: Oven\n135: Lemon\n136: Duck\n137: Baseball Bat\n138: Surveillance Camera\n139: Cat\n140: Jug\n141: Broccoli\n142: Piano\n143: Pizza\n144: Elephant\n145: Skateboard\n146: Surfboard\n147: Gun\n148: Skating and Skiing shoes\n149: Gas stove\n150: Donut\n151: Bow Tie\n152: Carrot\n153: Toilet\n154: Kite\n155: Strawberry\n156: Other Balls\n157: Shovel\n158: Pepper\n159: Computer Box\n160: Toilet Paper\n161: Cleaning Products\n162: Chopsticks\n163: Microwave\n164: Pigeon\n165: Baseball\n166: Cutting/chopping Board\n167: Coffee Table\n168: Side Table\n169: Scissors\n170: Marker\n171: Pie\n172: Ladder\n173: Snowboard\n174: Cookies\n175: Radiator\n176: Fire Hydrant\n177: Basketball\n178: Zebra\n179: Grape\n180: Giraffe\n181: Potato\n182: Sausage\n183: Tricycle\n184: Violin\n185: Egg\n186: Fire Extinguisher\n187: Candy\n188: Fire Truck\n189: Billiards\n190: Converter\n191: Bathtub\n192: Wheelchair\n193: Golf Club\n194: Briefcase\n195: Cucumber\n196: Cigar/Cigarette\n197: Paint Brush\n198: Pear\n199: Heavy Truck\n200: Hamburger\n201: Extractor\n202: Extension Cord\n203: Tong\n204: Tennis Racket\n205: Folder\n206: American Football\n207: earphone\n208: Mask\n209: Kettle\n210: Tennis\n211: Ship\n212: Swing\n213: Coffee Machine\n214: Slide\n215: Carriage\n216: Onion\n217: Green beans\n218: Projector\n219: Frisbee\n220: Washing Machine/Drying Machine\n221: Chicken\n222: Printer\n223: Watermelon\n224: Saxophone\n225: Tissue\n226: Toothbrush\n227: Ice cream\n228: Hot-air balloon\n229: Cello\n230: French Fries\n231: Scale\n232: Trophy\n233: Cabbage\n234: Hot dog\n235: Blender\n236: Peach\n237: Rice\n238: Wallet/Purse\n239: Volleyball\n240: Deer\n241: Goose\n242: Tape\n243: Tablet\n244: Cosmetics\n245: Trumpet\n246: Pineapple\n247: Golf Ball\n248: Ambulance\n249: Parking meter\n250: Mango\n251: Key\n252: Hurdle\n253: Fishing Rod\n254: Medal\n255: Flute\n256: Brush\n257: Penguin\n258: Megaphone\n259: Corn\n260: Lettuce\n261: Garlic\n262: Swan\n263: Helicopter\n264: Green Onion\n265: Sandwich\n266: Nuts\n267: Speed Limit Sign\n268: Induction Cooker\n269: Broom\n270: Trombone\n271: Plum\n272: Rickshaw\n273: Goldfish\n274: Kiwi fruit\n275: Router/modem\n276: Poker Card\n277: Toaster\n278: Shrimp\n279: Sushi\n280: Cheese\n281: Notepaper\n282: Cherry\n283: Pliers\n284: CD\n285: Pasta\n286: Hammer\n287: Cue\n288: Avocado\n289: Hamimelon\n290: Flask\n291: Mushroom\n292: Screwdriver\n293: Soap\n294: Recorder\n295: Bear\n296: Eggplant\n297: Board Eraser\n298: Coconut\n299: Tape Measure/Ruler\n300: Pig\n301: Showerhead\n302: Globe\n303: Chips\n304: Steak\n305: Crosswalk Sign\n306: Stapler\n307: Camel\n308: Formula 1\n309: Pomegranate\n310: Dishwasher\n311: Crab\n312: Hoverboard\n313: Meat ball\n314: Rice Cooker\n315: Tuba\n316: Calculator\n317: Papaya\n318: Antelope\n319: Parrot\n320: Seal\n321: Butterfly\n322: Dumbbell\n323: Donkey\n324: Lion\n325: Urinal\n326: Dolphin\n327: Electric Drill\n328: Hair Dryer\n329: Egg tart\n330: Jellyfish\n331: Treadmill\n332: Lighter\n333: Grapefruit\n334: Game board\n335: Mop\n336: Radish\n337: Baozi\n338: Target\n339: French\n340: Spring Rolls\n341: Monkey\n342: Rabbit\n343: Pencil Case\n344: Yak\n345: Red Cabbage\n346: Binoculars\n347: Asparagus\n348: Barbell\n349: Scallop\n350: Noddles\n351: Comb\n352: Dumpling\n353: Oyster\n354: Table Tennis paddle\n355: Cosmetics Brush/Eyeliner Pencil\n356: Chainsaw\n357: Eraser\n358: Lobster\n359: Durian\n360: Okra\n361: Lipstick\n362: Cosmetics Mirror\n363: Curling\n364: Table Tennis\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nfrom tqdm import tqdm\nfrom ultralytics.utils.checks import check_requirements\nfrom ultralytics.utils.downloads import download\nfrom ultralytics.utils.ops import xyxy2xywhn\nimport numpy as np\nfrom pathlib import Path\ncheck_requirements(('pycocotools&gt;=2.0',))\nfrom pycocotools.coco import COCO\n# Make Directories\ndir = Path(yaml['path'])  # dataset root dir\nfor p in 'images', 'labels':\n(dir / p).mkdir(parents=True, exist_ok=True)\nfor q in 'train', 'val':\n(dir / p / q).mkdir(parents=True, exist_ok=True)\n# Train, Val Splits\nfor split, patches in [('train', 50 + 1), ('val', 43 + 1)]:\nprint(f\"Processing {split} in {patches} patches ...\")\nimages, labels = dir / 'images' / split, dir / 'labels' / split\n# Download\nurl = f\"https://dorc.ks3-cn-beijing.ksyun.com/data-set/2020Objects365%E6%95%B0%E6%8D%AE%E9%9B%86/{split}/\"\nif split == 'train':\ndownload([f'{url}zhiyuan_objv2_{split}.tar.gz'], dir=dir)  # annotations json\ndownload([f'{url}patch{i}.tar.gz' for i in range(patches)], dir=images, curl=True, threads=8)\nelif split == 'val':\ndownload([f'{url}zhiyuan_objv2_{split}.json'], dir=dir)  # annotations json\ndownload([f'{url}images/v1/patch{i}.tar.gz' for i in range(15 + 1)], dir=images, curl=True, threads=8)\ndownload([f'{url}images/v2/patch{i}.tar.gz' for i in range(16, patches)], dir=images, curl=True, threads=8)\n# Move\nfor f in tqdm(images.rglob('*.jpg'), desc=f'Moving {split} images'):\nf.rename(images / f.name)  # move to /images/{split}\n# Labels\ncoco = COCO(dir / f'zhiyuan_objv2_{split}.json')\nnames = [x[\"name\"] for x in coco.loadCats(coco.getCatIds())]\nfor cid, cat in enumerate(names):\ncatIds = coco.getCatIds(catNms=[cat])\nimgIds = coco.getImgIds(catIds=catIds)\nfor im in tqdm(coco.loadImgs(imgIds), desc=f'Class {cid + 1}/{len(names)} {cat}'):\nwidth, height = im[\"width\"], im[\"height\"]\npath = Path(im[\"file_name\"])  # image filename\ntry:\nwith open(labels / path.with_suffix('.txt').name, 'a') as file:\nannIds = coco.getAnnIds(imgIds=im[\"id\"], catIds=catIds, iscrowd=None)\nfor a in coco.loadAnns(annIds):\nx, y, w, h = a['bbox']  # bounding box in xywh (xy top-left corner)\nxyxy = np.array([x, y, x + w, y + h])[None]  # pixels(1,4)\nx, y, w, h = xyxy2xywhn(xyxy, w=width, h=height, clip=True)[0]  # normalized and clipped\nfile.write(f\"{cid} {x:.5f} {y:.5f} {w:.5f} {h:.5f}\\n\")\nexcept Exception as e:\nprint(e)\n</code></pre>"},{"location":"datasets/detect/objects365/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the Objects365 dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='Objects365.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=Objects365.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/objects365/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The Objects365 dataset contains a diverse set of high-resolution images with objects from 365 categories, providing rich context for object detection tasks. Here are some examples of the images in the dataset:</p> <p></p> <ul> <li>Objects365: This image demonstrates an example of object detection, where objects are annotated with bounding boxes. The dataset provides a wide range of images to facilitate the development of models for this task.</li> </ul> <p>The example showcases the variety and complexity of the data in the Objects365 dataset and highlights the importance of accurate object detection for computer vision applications.</p>"},{"location":"datasets/detect/objects365/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the Objects365 dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@inproceedings{shao2019objects365,\ntitle={Objects365: A Large-scale, High-quality Dataset for Object Detection},\nauthor={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Li, Jing and Zhang, Xiangyu and Sun, Jian},\nbooktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\npages={8425--8434},\nyear={2019}\n}\n</code></pre> <p>We would like to acknowledge the team of researchers who created and maintain the Objects365 dataset as a valuable resource for the computer vision research community. For more information about the Objects365 dataset and its creators, visit the Objects365 dataset website.</p>"},{"location":"datasets/detect/open-images-v7/","title":"Open Images V7 Dataset","text":"<p>Open Images V7 is a versatile and expansive dataset championed by Google. Aimed at propelling research in the realm of computer vision, it boasts a vast collection of images annotated with a plethora of data, including image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives.</p> <p></p>"},{"location":"datasets/detect/open-images-v7/#key-features","title":"Key Features","text":"<ul> <li>Encompasses ~9M images annotated in various ways to suit multiple computer vision tasks.</li> <li>Houses a staggering 16M bounding boxes across 600 object classes in 1.9M images. These boxes are primarily hand-drawn by experts ensuring high precision.</li> <li>Visual relationship annotations totaling 3.3M are available, detailing 1,466 unique relationship triplets, object properties, and human activities.</li> <li>V5 introduced segmentation masks for 2.8M objects across 350 classes.</li> <li>V6 introduced 675k localized narratives that amalgamate voice, text, and mouse traces highlighting described objects.</li> <li>V7 introduced 66.4M point-level labels on 1.4M images, spanning 5,827 classes.</li> <li>Encompasses 61.4M image-level labels across a diverse set of 20,638 classes.</li> <li>Provides a unified platform for image classification, object detection, relationship detection, instance segmentation, and multimodal image descriptions.</li> </ul>"},{"location":"datasets/detect/open-images-v7/#dataset-structure","title":"Dataset Structure","text":"<p>Open Images V7 is structured in multiple components catering to varied computer vision challenges:</p> <ul> <li>Images: About 9 million images, often showcasing intricate scenes with an average of 8.3 objects per image.</li> <li>Bounding Boxes: Over 16 million boxes that demarcate objects across 600 categories.</li> <li>Segmentation Masks: These detail the exact boundary of 2.8M objects across 350 classes.</li> <li>Visual Relationships: 3.3M annotations indicating object relationships, properties, and actions.</li> <li>Localized Narratives: 675k descriptions combining voice, text, and mouse traces.</li> <li>Point-Level Labels: 66.4M labels across 1.4M images, suitable for zero/few-shot semantic segmentation.</li> </ul>"},{"location":"datasets/detect/open-images-v7/#applications","title":"Applications","text":"<p>Open Images V7 is a cornerstone for training and evaluating state-of-the-art models in various computer vision tasks. The dataset's broad scope and high-quality annotations make it indispensable for researchers and developers specializing in computer vision.</p>"},{"location":"datasets/detect/open-images-v7/#dataset-yaml","title":"Dataset YAML","text":"<p>Typically, datasets come with a YAML (Yet Another Markup Language) file that delineates the dataset's configuration. For the case of Open Images V7, a hypothetical <code>OpenImagesV7.yaml</code> might exist. For accurate paths and configurations, one should refer to the dataset's official repository or documentation.</p> <p>OpenImagesV7.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# Open Images v7 dataset https://storage.googleapis.com/openimages/web/index.html by Google\n# Example usage: yolo train data=open-images-v7.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 open-images-v7  \u2190 downloads here (561 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/open-images-v7  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 1743042 images\nval: images/val  # val images (relative to 'path') 41620 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: Accordion\n1: Adhesive tape\n2: Aircraft\n3: Airplane\n4: Alarm clock\n5: Alpaca\n6: Ambulance\n7: Animal\n8: Ant\n9: Antelope\n10: Apple\n11: Armadillo\n12: Artichoke\n13: Auto part\n14: Axe\n15: Backpack\n16: Bagel\n17: Baked goods\n18: Balance beam\n19: Ball\n20: Balloon\n21: Banana\n22: Band-aid\n23: Banjo\n24: Barge\n25: Barrel\n26: Baseball bat\n27: Baseball glove\n28: Bat (Animal)\n29: Bathroom accessory\n30: Bathroom cabinet\n31: Bathtub\n32: Beaker\n33: Bear\n34: Bed\n35: Bee\n36: Beehive\n37: Beer\n38: Beetle\n39: Bell pepper\n40: Belt\n41: Bench\n42: Bicycle\n43: Bicycle helmet\n44: Bicycle wheel\n45: Bidet\n46: Billboard\n47: Billiard table\n48: Binoculars\n49: Bird\n50: Blender\n51: Blue jay\n52: Boat\n53: Bomb\n54: Book\n55: Bookcase\n56: Boot\n57: Bottle\n58: Bottle opener\n59: Bow and arrow\n60: Bowl\n61: Bowling equipment\n62: Box\n63: Boy\n64: Brassiere\n65: Bread\n66: Briefcase\n67: Broccoli\n68: Bronze sculpture\n69: Brown bear\n70: Building\n71: Bull\n72: Burrito\n73: Bus\n74: Bust\n75: Butterfly\n76: Cabbage\n77: Cabinetry\n78: Cake\n79: Cake stand\n80: Calculator\n81: Camel\n82: Camera\n83: Can opener\n84: Canary\n85: Candle\n86: Candy\n87: Cannon\n88: Canoe\n89: Cantaloupe\n90: Car\n91: Carnivore\n92: Carrot\n93: Cart\n94: Cassette deck\n95: Castle\n96: Cat\n97: Cat furniture\n98: Caterpillar\n99: Cattle\n100: Ceiling fan\n101: Cello\n102: Centipede\n103: Chainsaw\n104: Chair\n105: Cheese\n106: Cheetah\n107: Chest of drawers\n108: Chicken\n109: Chime\n110: Chisel\n111: Chopsticks\n112: Christmas tree\n113: Clock\n114: Closet\n115: Clothing\n116: Coat\n117: Cocktail\n118: Cocktail shaker\n119: Coconut\n120: Coffee\n121: Coffee cup\n122: Coffee table\n123: Coffeemaker\n124: Coin\n125: Common fig\n126: Common sunflower\n127: Computer keyboard\n128: Computer monitor\n129: Computer mouse\n130: Container\n131: Convenience store\n132: Cookie\n133: Cooking spray\n134: Corded phone\n135: Cosmetics\n136: Couch\n137: Countertop\n138: Cowboy hat\n139: Crab\n140: Cream\n141: Cricket ball\n142: Crocodile\n143: Croissant\n144: Crown\n145: Crutch\n146: Cucumber\n147: Cupboard\n148: Curtain\n149: Cutting board\n150: Dagger\n151: Dairy Product\n152: Deer\n153: Desk\n154: Dessert\n155: Diaper\n156: Dice\n157: Digital clock\n158: Dinosaur\n159: Dishwasher\n160: Dog\n161: Dog bed\n162: Doll\n163: Dolphin\n164: Door\n165: Door handle\n166: Doughnut\n167: Dragonfly\n168: Drawer\n169: Dress\n170: Drill (Tool)\n171: Drink\n172: Drinking straw\n173: Drum\n174: Duck\n175: Dumbbell\n176: Eagle\n177: Earrings\n178: Egg (Food)\n179: Elephant\n180: Envelope\n181: Eraser\n182: Face powder\n183: Facial tissue holder\n184: Falcon\n185: Fashion accessory\n186: Fast food\n187: Fax\n188: Fedora\n189: Filing cabinet\n190: Fire hydrant\n191: Fireplace\n192: Fish\n193: Flag\n194: Flashlight\n195: Flower\n196: Flowerpot\n197: Flute\n198: Flying disc\n199: Food\n200: Food processor\n201: Football\n202: Football helmet\n203: Footwear\n204: Fork\n205: Fountain\n206: Fox\n207: French fries\n208: French horn\n209: Frog\n210: Fruit\n211: Frying pan\n212: Furniture\n213: Garden Asparagus\n214: Gas stove\n215: Giraffe\n216: Girl\n217: Glasses\n218: Glove\n219: Goat\n220: Goggles\n221: Goldfish\n222: Golf ball\n223: Golf cart\n224: Gondola\n225: Goose\n226: Grape\n227: Grapefruit\n228: Grinder\n229: Guacamole\n230: Guitar\n231: Hair dryer\n232: Hair spray\n233: Hamburger\n234: Hammer\n235: Hamster\n236: Hand dryer\n237: Handbag\n238: Handgun\n239: Harbor seal\n240: Harmonica\n241: Harp\n242: Harpsichord\n243: Hat\n244: Headphones\n245: Heater\n246: Hedgehog\n247: Helicopter\n248: Helmet\n249: High heels\n250: Hiking equipment\n251: Hippopotamus\n252: Home appliance\n253: Honeycomb\n254: Horizontal bar\n255: Horse\n256: Hot dog\n257: House\n258: Houseplant\n259: Human arm\n260: Human beard\n261: Human body\n262: Human ear\n263: Human eye\n264: Human face\n265: Human foot\n266: Human hair\n267: Human hand\n268: Human head\n269: Human leg\n270: Human mouth\n271: Human nose\n272: Humidifier\n273: Ice cream\n274: Indoor rower\n275: Infant bed\n276: Insect\n277: Invertebrate\n278: Ipod\n279: Isopod\n280: Jacket\n281: Jacuzzi\n282: Jaguar (Animal)\n283: Jeans\n284: Jellyfish\n285: Jet ski\n286: Jug\n287: Juice\n288: Kangaroo\n289: Kettle\n290: Kitchen &amp; dining room table\n291: Kitchen appliance\n292: Kitchen knife\n293: Kitchen utensil\n294: Kitchenware\n295: Kite\n296: Knife\n297: Koala\n298: Ladder\n299: Ladle\n300: Ladybug\n301: Lamp\n302: Land vehicle\n303: Lantern\n304: Laptop\n305: Lavender (Plant)\n306: Lemon\n307: Leopard\n308: Light bulb\n309: Light switch\n310: Lighthouse\n311: Lily\n312: Limousine\n313: Lion\n314: Lipstick\n315: Lizard\n316: Lobster\n317: Loveseat\n318: Luggage and bags\n319: Lynx\n320: Magpie\n321: Mammal\n322: Man\n323: Mango\n324: Maple\n325: Maracas\n326: Marine invertebrates\n327: Marine mammal\n328: Measuring cup\n329: Mechanical fan\n330: Medical equipment\n331: Microphone\n332: Microwave oven\n333: Milk\n334: Miniskirt\n335: Mirror\n336: Missile\n337: Mixer\n338: Mixing bowl\n339: Mobile phone\n340: Monkey\n341: Moths and butterflies\n342: Motorcycle\n343: Mouse\n344: Muffin\n345: Mug\n346: Mule\n347: Mushroom\n348: Musical instrument\n349: Musical keyboard\n350: Nail (Construction)\n351: Necklace\n352: Nightstand\n353: Oboe\n354: Office building\n355: Office supplies\n356: Orange\n357: Organ (Musical Instrument)\n358: Ostrich\n359: Otter\n360: Oven\n361: Owl\n362: Oyster\n363: Paddle\n364: Palm tree\n365: Pancake\n366: Panda\n367: Paper cutter\n368: Paper towel\n369: Parachute\n370: Parking meter\n371: Parrot\n372: Pasta\n373: Pastry\n374: Peach\n375: Pear\n376: Pen\n377: Pencil case\n378: Pencil sharpener\n379: Penguin\n380: Perfume\n381: Person\n382: Personal care\n383: Personal flotation device\n384: Piano\n385: Picnic basket\n386: Picture frame\n387: Pig\n388: Pillow\n389: Pineapple\n390: Pitcher (Container)\n391: Pizza\n392: Pizza cutter\n393: Plant\n394: Plastic bag\n395: Plate\n396: Platter\n397: Plumbing fixture\n398: Polar bear\n399: Pomegranate\n400: Popcorn\n401: Porch\n402: Porcupine\n403: Poster\n404: Potato\n405: Power plugs and sockets\n406: Pressure cooker\n407: Pretzel\n408: Printer\n409: Pumpkin\n410: Punching bag\n411: Rabbit\n412: Raccoon\n413: Racket\n414: Radish\n415: Ratchet (Device)\n416: Raven\n417: Rays and skates\n418: Red panda\n419: Refrigerator\n420: Remote control\n421: Reptile\n422: Rhinoceros\n423: Rifle\n424: Ring binder\n425: Rocket\n426: Roller skates\n427: Rose\n428: Rugby ball\n429: Ruler\n430: Salad\n431: Salt and pepper shakers\n432: Sandal\n433: Sandwich\n434: Saucer\n435: Saxophone\n436: Scale\n437: Scarf\n438: Scissors\n439: Scoreboard\n440: Scorpion\n441: Screwdriver\n442: Sculpture\n443: Sea lion\n444: Sea turtle\n445: Seafood\n446: Seahorse\n447: Seat belt\n448: Segway\n449: Serving tray\n450: Sewing machine\n451: Shark\n452: Sheep\n453: Shelf\n454: Shellfish\n455: Shirt\n456: Shorts\n457: Shotgun\n458: Shower\n459: Shrimp\n460: Sink\n461: Skateboard\n462: Ski\n463: Skirt\n464: Skull\n465: Skunk\n466: Skyscraper\n467: Slow cooker\n468: Snack\n469: Snail\n470: Snake\n471: Snowboard\n472: Snowman\n473: Snowmobile\n474: Snowplow\n475: Soap dispenser\n476: Sock\n477: Sofa bed\n478: Sombrero\n479: Sparrow\n480: Spatula\n481: Spice rack\n482: Spider\n483: Spoon\n484: Sports equipment\n485: Sports uniform\n486: Squash (Plant)\n487: Squid\n488: Squirrel\n489: Stairs\n490: Stapler\n491: Starfish\n492: Stationary bicycle\n493: Stethoscope\n494: Stool\n495: Stop sign\n496: Strawberry\n497: Street light\n498: Stretcher\n499: Studio couch\n500: Submarine\n501: Submarine sandwich\n502: Suit\n503: Suitcase\n504: Sun hat\n505: Sunglasses\n506: Surfboard\n507: Sushi\n508: Swan\n509: Swim cap\n510: Swimming pool\n511: Swimwear\n512: Sword\n513: Syringe\n514: Table\n515: Table tennis racket\n516: Tablet computer\n517: Tableware\n518: Taco\n519: Tank\n520: Tap\n521: Tart\n522: Taxi\n523: Tea\n524: Teapot\n525: Teddy bear\n526: Telephone\n527: Television\n528: Tennis ball\n529: Tennis racket\n530: Tent\n531: Tiara\n532: Tick\n533: Tie\n534: Tiger\n535: Tin can\n536: Tire\n537: Toaster\n538: Toilet\n539: Toilet paper\n540: Tomato\n541: Tool\n542: Toothbrush\n543: Torch\n544: Tortoise\n545: Towel\n546: Tower\n547: Toy\n548: Traffic light\n549: Traffic sign\n550: Train\n551: Training bench\n552: Treadmill\n553: Tree\n554: Tree house\n555: Tripod\n556: Trombone\n557: Trousers\n558: Truck\n559: Trumpet\n560: Turkey\n561: Turtle\n562: Umbrella\n563: Unicycle\n564: Van\n565: Vase\n566: Vegetable\n567: Vehicle\n568: Vehicle registration plate\n569: Violin\n570: Volleyball (Ball)\n571: Waffle\n572: Waffle iron\n573: Wall clock\n574: Wardrobe\n575: Washing machine\n576: Waste container\n577: Watch\n578: Watercraft\n579: Watermelon\n580: Weapon\n581: Whale\n582: Wheel\n583: Wheelchair\n584: Whisk\n585: Whiteboard\n586: Willow\n587: Window\n588: Window blind\n589: Wine\n590: Wine glass\n591: Wine rack\n592: Winter melon\n593: Wok\n594: Woman\n595: Wood-burning stove\n596: Woodpecker\n597: Worm\n598: Wrench\n599: Zebra\n600: Zucchini\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nfrom ultralytics.utils import LOGGER, SETTINGS, Path, is_ubuntu, get_ubuntu_version\nfrom ultralytics.utils.checks import check_requirements, check_version\ncheck_requirements('fiftyone')\nif is_ubuntu() and check_version(get_ubuntu_version(), '&gt;=22.04'):\n# Ubuntu&gt;=22.04 patch https://github.com/voxel51/fiftyone/issues/2961#issuecomment-1666519347\ncheck_requirements('fiftyone-db-ubuntu2204')\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport warnings\nname = 'open-images-v7'\nfraction = 1.0  # fraction of full dataset to use\nLOGGER.warning('WARNING \u26a0\ufe0f Open Images V7 dataset requires at least **561 GB of free space. Starting download...')\nfor split in 'train', 'validation':  # 1743042 train, 41620 val images\ntrain = split == 'train'\n# Load Open Images dataset\ndataset = foz.load_zoo_dataset(name,\nsplit=split,\nlabel_types=['detections'],\ndataset_dir=Path(SETTINGS['datasets_dir']) / 'fiftyone' / name,\nmax_samples=round((1743042 if train else 41620) * fraction))\n# Define classes\nif train:\nclasses = dataset.default_classes  # all classes\n# classes = dataset.distinct('ground_truth.detections.label')  # only observed classes\n# Export to YOLO format\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"fiftyone.utils.yolo\")\ndataset.export(export_dir=str(Path(SETTINGS['datasets_dir']) / name),\ndataset_type=fo.types.YOLOv5Dataset,\nlabel_field='ground_truth',\nsplit='val' if split == 'validation' else split,\nclasses=classes,\noverwrite=train)\n</code></pre>"},{"location":"datasets/detect/open-images-v7/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the Open Images V7 dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Warning</p> <p>The complete Open Images V7 dataset comprises 1,743,042 training images and 41,620 validation images, requiring approximately 561 GB of storage space upon download.</p> <p>Executing the commands provided below will trigger an automatic download of the full dataset if it's not already present locally. Before running the below example it's crucial to:</p> <ul> <li>Verify that your device has enough storage capacity.</li> <li>Ensure a robust and speedy internet connection.</li> </ul> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Train the model on the Open Images V7 dataset\nresults = model.train(data='open-images-v7.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Train a COCO-pretrained YOLOv8n model on the Open Images V7 dataset\nyolo detect train data=open-images-v7.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/open-images-v7/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>Illustrations of the dataset help provide insights into its richness:</p> <p></p> <ul> <li>Open Images V7: This image exemplifies the depth and detail of annotations available, including bounding boxes, relationships, and segmentation masks.</li> </ul> <p>Researchers can gain invaluable insights into the array of computer vision challenges that the dataset addresses, from basic object detection to intricate relationship identification.</p>"},{"location":"datasets/detect/open-images-v7/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>For those employing Open Images V7 in their work, it's prudent to cite the relevant papers and acknowledge the creators:</p> BibTeX <pre><code>@article{OpenImages,\nauthor = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and Tom Duerig and Vittorio Ferrari},\ntitle = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},\nyear = {2020},\njournal = {IJCV}\n}\n</code></pre> <p>A heartfelt acknowledgment goes out to the Google AI team for creating and maintaining the Open Images V7 dataset. For a deep dive into the dataset and its offerings, navigate to the official Open Images V7 website.</p>"},{"location":"datasets/detect/sku-110k/","title":"SKU-110k Dataset","text":"<p>The SKU-110k dataset is a collection of densely packed retail shelf images, designed to support research in object detection tasks. Developed by Eran Goldman et al., the dataset contains over 110,000 unique store keeping unit (SKU) categories with densely packed objects, often looking similar or even identical, positioned in close proximity.</p> <p></p>"},{"location":"datasets/detect/sku-110k/#key-features","title":"Key Features","text":"<ul> <li>SKU-110k contains images of store shelves from around the world, featuring densely packed objects that pose challenges for state-of-the-art object detectors.</li> <li>The dataset includes over 110,000 unique SKU categories, providing a diverse range of object appearances.</li> <li>Annotations include bounding boxes for objects and SKU category labels.</li> </ul>"},{"location":"datasets/detect/sku-110k/#dataset-structure","title":"Dataset Structure","text":"<p>The SKU-110k dataset is organized into three main subsets:</p> <ol> <li>Training set: This subset contains images and annotations used for training object detection models.</li> <li>Validation set: This subset consists of images and annotations used for model validation during training.</li> <li>Test set: This subset is designed for the final evaluation of trained object detection models.</li> </ol>"},{"location":"datasets/detect/sku-110k/#applications","title":"Applications","text":"<p>The SKU-110k dataset is widely used for training and evaluating deep learning models in object detection tasks, especially in densely packed scenes such as retail shelf displays. The dataset's diverse set of SKU categories and densely packed object arrangements make it a valuable resource for researchers and practitioners in the field of computer vision.</p>"},{"location":"datasets/detect/sku-110k/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. For the case of the SKU-110K dataset, the <code>SKU-110K.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/SKU-110K.yaml.</p> <p>ultralytics/cfg/datasets/SKU-110K.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# SKU-110K retail items dataset https://github.com/eg4000/SKU110K_CVPR19 by Trax Retail\n# Example usage: yolo train data=SKU-110K.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 SKU-110K  \u2190 downloads here (13.6 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/SKU-110K  # dataset root dir\ntrain: train.txt  # train images (relative to 'path')  8219 images\nval: val.txt  # val images (relative to 'path')  588 images\ntest: test.txt  # test images (optional)  2936 images\n# Classes\nnames:\n0: object\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nimport shutil\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom ultralytics.utils.downloads import download\nfrom ultralytics.utils.ops import xyxy2xywh\n# Download\ndir = Path(yaml['path'])  # dataset root dir\nparent = Path(dir.parent)  # download dir\nurls = ['http://trax-geometry.s3.amazonaws.com/cvpr_challenge/SKU110K_fixed.tar.gz']\ndownload(urls, dir=parent)\n# Rename directories\nif dir.exists():\nshutil.rmtree(dir)\n(parent / 'SKU110K_fixed').rename(dir)  # rename dir\n(dir / 'labels').mkdir(parents=True, exist_ok=True)  # create labels dir\n# Convert labels\nnames = 'image', 'x1', 'y1', 'x2', 'y2', 'class', 'image_width', 'image_height'  # column names\nfor d in 'annotations_train.csv', 'annotations_val.csv', 'annotations_test.csv':\nx = pd.read_csv(dir / 'annotations' / d, names=names).values  # annotations\nimages, unique_images = x[:, 0], np.unique(x[:, 0])\nwith open((dir / d).with_suffix('.txt').__str__().replace('annotations_', ''), 'w') as f:\nf.writelines(f'./images/{s}\\n' for s in unique_images)\nfor im in tqdm(unique_images, desc=f'Converting {dir / d}'):\ncls = 0  # single-class dataset\nwith open((dir / 'labels' / im).with_suffix('.txt'), 'a') as f:\nfor r in x[images == im]:\nw, h = r[6], r[7]  # image width, height\nxywh = xyxy2xywh(np.array([[r[1] / w, r[2] / h, r[3] / w, r[4] / h]]))[0]  # instance\nf.write(f\"{cls} {xywh[0]:.5f} {xywh[1]:.5f} {xywh[2]:.5f} {xywh[3]:.5f}\\n\")  # write label\n</code></pre>"},{"location":"datasets/detect/sku-110k/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the SKU-110K dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='SKU-110K.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=SKU-110K.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/sku-110k/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The SKU-110k dataset contains a diverse set of retail shelf images with densely packed objects, providing rich context for object detection tasks. Here are some examples of data from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Densely packed retail shelf image: This image demonstrates an example of densely packed objects in a retail shelf setting. Objects are annotated with bounding boxes and SKU category labels.</li> </ul> <p>The example showcases the variety and complexity of the data in the SKU-110k dataset and highlights the importance of high-quality data for object detection tasks.</p>"},{"location":"datasets/detect/sku-110k/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the SKU-110k dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@inproceedings{goldman2019dense,\nauthor    = {Eran Goldman and Roei Herzig and Aviv Eisenschtat and Jacob Goldberger and Tal Hassner},\ntitle     = {Precise Detection in Densely Packed Scenes},\nbooktitle = {Proc. Conf. Comput. Vision Pattern Recognition (CVPR)},\nyear      = {2019}\n}\n</code></pre> <p>We would like to acknowledge Eran Goldman et al. for creating and maintaining the SKU-110k dataset as a valuable resource for the computer vision research community. For more information about the SKU-110k dataset and its creators, visit the SKU-110k dataset GitHub repository.</p>"},{"location":"datasets/detect/visdrone/","title":"VisDrone Dataset","text":"<p>The VisDrone Dataset is a large-scale benchmark created by the AISKYEYE team at the Lab of Machine Learning and Data Mining, Tianjin University, China. It contains carefully annotated ground truth data for various computer vision tasks related to drone-based image and video analysis.</p> <p>VisDrone is composed of 288 video clips with 261,908 frames and 10,209 static images, captured by various drone-mounted cameras. The dataset covers a wide range of aspects, including location (14 different cities across China), environment (urban and rural), objects (pedestrians, vehicles, bicycles, etc.), and density (sparse and crowded scenes). The dataset was collected using various drone platforms under different scenarios and weather and lighting conditions. These frames are manually annotated with over 2.6 million bounding boxes of targets such as pedestrians, cars, bicycles, and tricycles. Attributes like scene visibility, object class, and occlusion are also provided for better data utilization.</p>"},{"location":"datasets/detect/visdrone/#dataset-structure","title":"Dataset Structure","text":"<p>The VisDrone dataset is organized into five main subsets, each focusing on a specific task:</p> <ol> <li>Task 1: Object detection in images</li> <li>Task 2: Object detection in videos</li> <li>Task 3: Single-object tracking</li> <li>Task 4: Multi-object tracking</li> <li>Task 5: Crowd counting</li> </ol>"},{"location":"datasets/detect/visdrone/#applications","title":"Applications","text":"<p>The VisDrone dataset is widely used for training and evaluating deep learning models in drone-based computer vision tasks such as object detection, object tracking, and crowd counting. The dataset's diverse set of sensor data, object annotations, and attributes make it a valuable resource for researchers and practitioners in the field of drone-based computer vision.</p>"},{"location":"datasets/detect/visdrone/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the Visdrone dataset, the <code>VisDrone.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VisDrone.yaml.</p> <p>ultralytics/cfg/datasets/VisDrone.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# VisDrone2019-DET dataset https://github.com/VisDrone/VisDrone-Dataset by Tianjin University\n# Example usage: yolo train data=VisDrone.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 VisDrone  \u2190 downloads here (2.3 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/VisDrone  # dataset root dir\ntrain: VisDrone2019-DET-train/images  # train images (relative to 'path')  6471 images\nval: VisDrone2019-DET-val/images  # val images (relative to 'path')  548 images\ntest: VisDrone2019-DET-test-dev/images  # test images (optional)  1610 images\n# Classes\nnames:\n0: pedestrian\n1: people\n2: bicycle\n3: car\n4: van\n5: truck\n6: tricycle\n7: awning-tricycle\n8: bus\n9: motor\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nimport os\nfrom pathlib import Path\nfrom ultralytics.utils.downloads import download\ndef visdrone2yolo(dir):\nfrom PIL import Image\nfrom tqdm import tqdm\ndef convert_box(size, box):\n# Convert VisDrone box to YOLO xywh box\ndw = 1. / size[0]\ndh = 1. / size[1]\nreturn (box[0] + box[2] / 2) * dw, (box[1] + box[3] / 2) * dh, box[2] * dw, box[3] * dh\n(dir / 'labels').mkdir(parents=True, exist_ok=True)  # make labels directory\npbar = tqdm((dir / 'annotations').glob('*.txt'), desc=f'Converting {dir}')\nfor f in pbar:\nimg_size = Image.open((dir / 'images' / f.name).with_suffix('.jpg')).size\nlines = []\nwith open(f, 'r') as file:  # read annotation.txt\nfor row in [x.split(',') for x in file.read().strip().splitlines()]:\nif row[4] == '0':  # VisDrone 'ignored regions' class 0\ncontinue\ncls = int(row[5]) - 1\nbox = convert_box(img_size, tuple(map(int, row[:4])))\nlines.append(f\"{cls} {' '.join(f'{x:.6f}' for x in box)}\\n\")\nwith open(str(f).replace(f'{os.sep}annotations{os.sep}', f'{os.sep}labels{os.sep}'), 'w') as fl:\nfl.writelines(lines)  # write label.txt\n# Download\ndir = Path(yaml['path'])  # dataset root dir\nurls = ['https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-train.zip',\n'https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-val.zip',\n'https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-test-dev.zip',\n'https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-test-challenge.zip']\ndownload(urls, dir=dir, curl=True, threads=4)\n# Convert\nfor d in 'VisDrone2019-DET-train', 'VisDrone2019-DET-val', 'VisDrone2019-DET-test-dev':\nvisdrone2yolo(dir / d)  # convert VisDrone annotations to YOLO labels\n</code></pre>"},{"location":"datasets/detect/visdrone/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the VisDrone dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='VisDrone.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=VisDrone.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/visdrone/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The VisDrone dataset contains a diverse set of images and videos captured by drone-mounted cameras. Here are some examples of data from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Task 1: Object detection in images - This image demonstrates an example of object detection in images, where objects are annotated with bounding boxes. The dataset provides a wide variety of images taken from different locations, environments, and densities to facilitate the development of models for this task.</li> </ul> <p>The example showcases the variety and complexity of the data in the VisDrone dataset and highlights the importance of high-quality sensor data for drone-based computer vision tasks.</p>"},{"location":"datasets/detect/visdrone/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the VisDrone dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@ARTICLE{9573394,\nauthor={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},\njournal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\ntitle={Detection and Tracking Meet Drones Challenge},\nyear={2021},\nvolume={},\nnumber={},\npages={1-1},\ndoi={10.1109/TPAMI.2021.3119563}}\n</code></pre> <p>We would like to acknowledge the AISKYEYE team at the Lab of Machine Learning and Data Mining, Tianjin University, China, for creating and maintaining the VisDrone dataset as a valuable resource for the drone-based computer vision research community. For more information about the VisDrone dataset and its creators, visit the VisDrone Dataset GitHub repository.</p>"},{"location":"datasets/detect/voc/","title":"VOC Dataset","text":"<p>The PASCAL VOC (Visual Object Classes) dataset is a well-known object detection, segmentation, and classification dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models. It is an essential dataset for researchers and developers working on object detection, segmentation, and classification tasks.</p>"},{"location":"datasets/detect/voc/#key-features","title":"Key Features","text":"<ul> <li>VOC dataset includes two main challenges: VOC2007 and VOC2012.</li> <li>The dataset comprises 20 object categories, including common objects like cars, bicycles, and animals, as well as more specific categories such as boats, sofas, and dining tables.</li> <li>Annotations include object bounding boxes and class labels for object detection and classification tasks, and segmentation masks for the segmentation tasks.</li> <li>VOC provides standardized evaluation metrics like mean Average Precision (mAP) for object detection and classification, making it suitable for comparing model performance.</li> </ul>"},{"location":"datasets/detect/voc/#dataset-structure","title":"Dataset Structure","text":"<p>The VOC dataset is split into three subsets:</p> <ol> <li>Train: This subset contains images for training object detection, segmentation, and classification models.</li> <li>Validation: This subset has images used for validation purposes during model training.</li> <li>Test: This subset consists of images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the PASCAL VOC evaluation server for performance evaluation.</li> </ol>"},{"location":"datasets/detect/voc/#applications","title":"Applications","text":"<p>The VOC dataset is widely used for training and evaluating deep learning models in object detection (such as YOLO, Faster R-CNN, and SSD), instance segmentation (such as Mask R-CNN), and image classification. The dataset's diverse set of object categories, large number of annotated images, and standardized evaluation metrics make it an essential resource for computer vision researchers and practitioners.</p>"},{"location":"datasets/detect/voc/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the VOC dataset, the <code>VOC.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VOC.yaml.</p> <p>ultralytics/cfg/datasets/VOC.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# PASCAL VOC dataset http://host.robots.ox.ac.uk/pascal/VOC by University of Oxford\n# Example usage: yolo train data=VOC.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 VOC  \u2190 downloads here (2.8 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/VOC\ntrain: # train images (relative to 'path')  16551 images\n- images/train2012\n- images/train2007\n- images/val2012\n- images/val2007\nval: # val images (relative to 'path')  4952 images\n- images/test2007\ntest: # test images (optional)\n- images/test2007\n# Classes\nnames:\n0: aeroplane\n1: bicycle\n2: bird\n3: boat\n4: bottle\n5: bus\n6: car\n7: cat\n8: chair\n9: cow\n10: diningtable\n11: dog\n12: horse\n13: motorbike\n14: person\n15: pottedplant\n16: sheep\n17: sofa\n18: train\n19: tvmonitor\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\ndef convert_label(path, lb_path, year, image_id):\ndef convert_box(size, box):\ndw, dh = 1. / size[0], 1. / size[1]\nx, y, w, h = (box[0] + box[1]) / 2.0 - 1, (box[2] + box[3]) / 2.0 - 1, box[1] - box[0], box[3] - box[2]\nreturn x * dw, y * dh, w * dw, h * dh\nin_file = open(path / f'VOC{year}/Annotations/{image_id}.xml')\nout_file = open(lb_path, 'w')\ntree = ET.parse(in_file)\nroot = tree.getroot()\nsize = root.find('size')\nw = int(size.find('width').text)\nh = int(size.find('height').text)\nnames = list(yaml['names'].values())  # names list\nfor obj in root.iter('object'):\ncls = obj.find('name').text\nif cls in names and int(obj.find('difficult').text) != 1:\nxmlbox = obj.find('bndbox')\nbb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\ncls_id = names.index(cls)  # class id\nout_file.write(\" \".join(str(a) for a in (cls_id, *bb)) + '\\n')\n# Download\ndir = Path(yaml['path'])  # dataset root dir\nurl = 'https://github.com/ultralytics/yolov5/releases/download/v1.0/'\nurls = [f'{url}VOCtrainval_06-Nov-2007.zip',  # 446MB, 5012 images\nf'{url}VOCtest_06-Nov-2007.zip',  # 438MB, 4953 images\nf'{url}VOCtrainval_11-May-2012.zip']  # 1.95GB, 17126 images\ndownload(urls, dir=dir / 'images', curl=True, threads=3)\n# Convert\npath = dir / 'images/VOCdevkit'\nfor year, image_set in ('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val'), ('2007', 'test'):\nimgs_path = dir / 'images' / f'{image_set}{year}'\nlbs_path = dir / 'labels' / f'{image_set}{year}'\nimgs_path.mkdir(exist_ok=True, parents=True)\nlbs_path.mkdir(exist_ok=True, parents=True)\nwith open(path / f'VOC{year}/ImageSets/Main/{image_set}.txt') as f:\nimage_ids = f.read().strip().split()\nfor id in tqdm(image_ids, desc=f'{image_set}{year}'):\nf = path / f'VOC{year}/JPEGImages/{id}.jpg'  # old img path\nlb_path = (lbs_path / f.name).with_suffix('.txt')  # new label path\nf.rename(imgs_path / f.name)  # move image\nconvert_label(path, lb_path, year, id)  # convert labels to YOLO format\n</code></pre>"},{"location":"datasets/detect/voc/#usage","title":"Usage","text":"<p>To train a YOLOv8n model on the VOC dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='VOC.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from\na pretrained *.pt model\nyolo detect train data=VOC.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/voc/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The VOC dataset contains a diverse set of images with various object categories and complex scenes. Here are some examples of images from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the VOC dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/detect/voc/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the VOC dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{everingham2010pascal,\ntitle={The PASCAL Visual Object Classes (VOC) Challenge},\nauthor={Mark Everingham and Luc Van Gool and Christopher K. I. Williams and John Winn and Andrew Zisserman},\nyear={2010},\neprint={0909.5206},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the PASCAL VOC Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the VOC dataset and its creators, visit the PASCAL VOC dataset website.</p>"},{"location":"datasets/detect/xview/","title":"xView Dataset","text":"<p>The xView dataset is one of the largest publicly available datasets of overhead imagery, containing images from complex scenes around the world annotated using bounding boxes. The goal of the xView dataset is to accelerate progress in four computer vision frontiers:</p> <ol> <li>Reduce minimum resolution for detection.</li> <li>Improve learning efficiency.</li> <li>Enable discovery of more object classes.</li> <li>Improve detection of fine-grained classes.</li> </ol> <p>xView builds on the success of challenges like Common Objects in Context (COCO) and aims to leverage computer vision to analyze the growing amount of available imagery from space in order to understand the visual world in new ways and address a range of important applications.</p>"},{"location":"datasets/detect/xview/#key-features","title":"Key Features","text":"<ul> <li>xView contains over 1 million object instances across 60 classes.</li> <li>The dataset has a resolution of 0.3 meters, providing higher resolution imagery than most public satellite imagery datasets.</li> <li>xView features a diverse collection of small, rare, fine-grained, and multi-type objects with bounding box annotation.</li> <li>Comes with a pre-trained baseline model using the TensorFlow object detection API and an example for PyTorch.</li> </ul>"},{"location":"datasets/detect/xview/#dataset-structure","title":"Dataset Structure","text":"<p>The xView dataset is composed of satellite images collected from WorldView-3 satellites at a 0.3m ground sample distance. It contains over 1 million objects across 60 classes in over 1,400 km\u00b2 of imagery.</p>"},{"location":"datasets/detect/xview/#applications","title":"Applications","text":"<p>The xView dataset is widely used for training and evaluating deep learning models for object detection in overhead imagery. The dataset's diverse set of object classes and high-resolution imagery make it a valuable resource for researchers and practitioners in the field of computer vision, especially for satellite imagery analysis.</p>"},{"location":"datasets/detect/xview/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the xView dataset, the <code>xView.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/xView.yaml.</p> <p>ultralytics/cfg/datasets/xView.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# DIUx xView 2018 Challenge https://challenge.xviewdataset.org by U.S. National Geospatial-Intelligence Agency (NGA)\n# --------  DOWNLOAD DATA MANUALLY and jar xf val_images.zip to 'datasets/xView' before running train command!  --------\n# Example usage: yolo train data=xView.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 xView  \u2190 downloads here (20.7 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/xView  # dataset root dir\ntrain: images/autosplit_train.txt  # train images (relative to 'path') 90% of 847 train images\nval: images/autosplit_val.txt  # train images (relative to 'path') 10% of 847 train images\n# Classes\nnames:\n0: Fixed-wing Aircraft\n1: Small Aircraft\n2: Cargo Plane\n3: Helicopter\n4: Passenger Vehicle\n5: Small Car\n6: Bus\n7: Pickup Truck\n8: Utility Truck\n9: Truck\n10: Cargo Truck\n11: Truck w/Box\n12: Truck Tractor\n13: Trailer\n14: Truck w/Flatbed\n15: Truck w/Liquid\n16: Crane Truck\n17: Railway Vehicle\n18: Passenger Car\n19: Cargo Car\n20: Flat Car\n21: Tank car\n22: Locomotive\n23: Maritime Vessel\n24: Motorboat\n25: Sailboat\n26: Tugboat\n27: Barge\n28: Fishing Vessel\n29: Ferry\n30: Yacht\n31: Container Ship\n32: Oil Tanker\n33: Engineering Vehicle\n34: Tower crane\n35: Container Crane\n36: Reach Stacker\n37: Straddle Carrier\n38: Mobile Crane\n39: Dump Truck\n40: Haul Truck\n41: Scraper/Tractor\n42: Front loader/Bulldozer\n43: Excavator\n44: Cement Mixer\n45: Ground Grader\n46: Hut/Tent\n47: Shed\n48: Building\n49: Aircraft Hangar\n50: Damaged Building\n51: Facility\n52: Construction Site\n53: Vehicle Lot\n54: Helipad\n55: Storage Tank\n56: Shipping container lot\n57: Shipping Container\n58: Pylon\n59: Tower\n# Download script/URL (optional) ---------------------------------------------------------------------------------------\ndownload: |\nimport json\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom ultralytics.data.utils import autosplit\nfrom ultralytics.utils.ops import xyxy2xywhn\ndef convert_labels(fname=Path('xView/xView_train.geojson')):\n# Convert xView geoJSON labels to YOLO format\npath = fname.parent\nwith open(fname) as f:\nprint(f'Loading {fname}...')\ndata = json.load(f)\n# Make dirs\nlabels = Path(path / 'labels' / 'train')\nos.system(f'rm -rf {labels}')\nlabels.mkdir(parents=True, exist_ok=True)\n# xView classes 11-94 to 0-59\nxview_class2index = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 2, -1, 3, -1, 4, 5, 6, 7, 8, -1, 9, 10, 11,\n12, 13, 14, 15, -1, -1, 16, 17, 18, 19, 20, 21, 22, -1, 23, 24, 25, -1, 26, 27, -1, 28, -1,\n29, 30, 31, 32, 33, 34, 35, 36, 37, -1, 38, 39, 40, 41, 42, 43, 44, 45, -1, -1, -1, -1, 46,\n47, 48, 49, -1, 50, 51, -1, 52, -1, -1, -1, 53, 54, -1, 55, -1, -1, 56, -1, 57, -1, 58, 59]\nshapes = {}\nfor feature in tqdm(data['features'], desc=f'Converting {fname}'):\np = feature['properties']\nif p['bounds_imcoords']:\nid = p['image_id']\nfile = path / 'train_images' / id\nif file.exists():  # 1395.tif missing\ntry:\nbox = np.array([int(num) for num in p['bounds_imcoords'].split(\",\")])\nassert box.shape[0] == 4, f'incorrect box shape {box.shape[0]}'\ncls = p['type_id']\ncls = xview_class2index[int(cls)]  # xView class to 0-60\nassert 59 &gt;= cls &gt;= 0, f'incorrect class index {cls}'\n# Write YOLO label\nif id not in shapes:\nshapes[id] = Image.open(file).size\nbox = xyxy2xywhn(box[None].astype(np.float), w=shapes[id][0], h=shapes[id][1], clip=True)\nwith open((labels / id).with_suffix('.txt'), 'a') as f:\nf.write(f\"{cls} {' '.join(f'{x:.6f}' for x in box[0])}\\n\")  # write label.txt\nexcept Exception as e:\nprint(f'WARNING: skipping one label for {file}: {e}')\n# Download manually from https://challenge.xviewdataset.org\ndir = Path(yaml['path'])  # dataset root dir\n# urls = ['https://d307kc0mrhucc3.cloudfront.net/train_labels.zip',  # train labels\n#         'https://d307kc0mrhucc3.cloudfront.net/train_images.zip',  # 15G, 847 train images\n#         'https://d307kc0mrhucc3.cloudfront.net/val_images.zip']  # 5G, 282 val images (no labels)\n# download(urls, dir=dir)\n# Convert labels\nconvert_labels(dir / 'xView_train.geojson')\n# Move images\nimages = Path(dir / 'images')\nimages.mkdir(parents=True, exist_ok=True)\nPath(dir / 'train_images').rename(dir / 'images' / 'train')\nPath(dir / 'val_images').rename(dir / 'images' / 'val')\n# Split\nautosplit(dir / 'images' / 'train')\n</code></pre>"},{"location":"datasets/detect/xview/#usage","title":"Usage","text":"<p>To train a model on the xView dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='xView.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=xView.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/detect/xview/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>The xView dataset contains high-resolution satellite images with a diverse set of objects annotated using bounding boxes. Here are some examples of data from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Overhead Imagery: This image demonstrates an example of object detection in overhead imagery, where objects are annotated with bounding boxes. The dataset provides high-resolution satellite images to facilitate the development of models for this task.</li> </ul> <p>The example showcases the variety and complexity of the data in the xView dataset and highlights the importance of high-quality satellite imagery for object detection tasks.</p>"},{"location":"datasets/detect/xview/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the xView dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lam2018xview,\ntitle={xView: Objects in Context in Overhead Imagery},\nauthor={Darius Lam and Richard Kuzma and Kevin McGee and Samuel Dooley and Michael Laielli and Matthew Klaric and Yaroslav Bulatov and Brendan McCord},\nyear={2018},\neprint={1802.07856},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the Defense Innovation Unit (DIU) and the creators of the xView dataset for their valuable contribution to the computer vision research community. For more information about the xView dataset and its creators, visit the xView dataset website.</p>"},{"location":"datasets/obb/","title":"Oriented Bounding Box (OBB) Datasets Overview","text":"<p>Training a precise object detection model with oriented bounding boxes (OBB) requires a thorough dataset. This guide explains the various OBB dataset formats compatible with Ultralytics YOLO models, offering insights into their structure, application, and methods for format conversions.</p>"},{"location":"datasets/obb/#supported-obb-dataset-formats","title":"Supported OBB Dataset Formats","text":""},{"location":"datasets/obb/#yolo-obb-format","title":"YOLO OBB Format","text":"<p>The YOLO OBB format designates bounding boxes by their four corner points with coordinates normalized between 0 and 1. It follows this format:</p> <pre><code>class_index, x1, y1, x2, y2, x3, y3, x4, y4\n</code></pre> <p>Internally, YOLO processes losses and outputs in the <code>xywhr</code> format, which represents the bounding box's center point (xy), width, height, and rotation.</p> <p></p> <p>An example of a <code>*.txt</code> label file for the above image, which contains an object of class <code>0</code> in OBB format, could look like:</p> <pre><code>0 0.780811 0.743961 0.782371 0.74686 0.777691 0.752174 0.776131 0.749758\n</code></pre>"},{"location":"datasets/obb/#usage","title":"Usage","text":"<p>To train a model using these OBB formats:</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Create a new YOLOv8n-OBB model from scratch\nmodel = YOLO('yolov8n-obb.yaml')\n# Train the model on the DOTAv2 dataset\nresults = model.train(data='DOTAv2.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Train a new YOLOv8n-OBB model on the DOTAv2 dataset\nyolo detect train data=DOTAv2.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/obb/#supported-datasets","title":"Supported Datasets","text":"<p>Currently, the following datasets with Oriented Bounding Boxes are supported:</p> <ul> <li>DOTA v2: DOTA (A Large-scale Dataset for Object Detection in Aerial Images) version 2, emphasizes detection from aerial perspectives and contains oriented bounding boxes with 1.7 million instances and 11,268 images.</li> </ul>"},{"location":"datasets/obb/#incorporating-your-own-obb-dataset","title":"Incorporating your own OBB dataset","text":"<p>For those looking to introduce their own datasets with oriented bounding boxes, ensure compatibility with the \"YOLO OBB format\" mentioned above. Convert your annotations to this required format and detail the paths, classes, and class names in a corresponding YAML configuration file.</p>"},{"location":"datasets/obb/#convert-label-formats","title":"Convert Label Formats","text":""},{"location":"datasets/obb/#dota-dataset-format-to-yolo-obb-format","title":"DOTA Dataset Format to YOLO OBB Format","text":"<p>Transitioning labels from the DOTA dataset format to the YOLO OBB format can be achieved with this script:</p> <pre><code>from ultralytics.data.converter import convert_dota_to_yolo_obb\nconvert_dota_to_yolo_obb('path/to/DOTA')\n</code></pre> <p>This conversion mechanism is instrumental for datasets in the DOTA format, ensuring alignment with the Ultralytics YOLO OBB format.</p> <p>It's imperative to validate the compatibility of the dataset with your model and adhere to the necessary format conventions. Properly structured datasets are pivotal for training efficient object detection models with oriented bounding boxes.</p>"},{"location":"datasets/obb/dota-v2/","title":"DOTA v2 Dataset with OBB","text":"<p>DOTA v2 stands as a specialized dataset, emphasizing object detection in aerial images. Originating from the DOTA series of datasets, it offers annotated images capturing a diverse array of aerial scenes with Oriented Bounding Boxes (OBB).</p> <p></p>"},{"location":"datasets/obb/dota-v2/#key-features","title":"Key Features","text":"<ul> <li>Collection from various sensors and platforms, with image sizes ranging from 800 \u00d7 800 to 20,000 \u00d7 20,000 pixels.</li> <li>Features more than 1.7M Oriented Bounding Boxes across 18 categories.</li> <li>Encompasses multiscale object detection.</li> <li>Instances are annotated by experts using arbitrary (8 d.o.f.) quadrilateral, capturing objects of different scales, orientations, and shapes.</li> </ul>"},{"location":"datasets/obb/dota-v2/#dataset-versions","title":"Dataset Versions","text":""},{"location":"datasets/obb/dota-v2/#dota-v10","title":"DOTA-v1.0","text":"<ul> <li>Contains 15 common categories.</li> <li>Comprises 2,806 images with 188,282 instances.</li> <li>Split ratios: 1/2 for training, 1/6 for validation, and 1/3 for testing.</li> </ul>"},{"location":"datasets/obb/dota-v2/#dota-v15","title":"DOTA-v1.5","text":"<ul> <li>Incorporates the same images as DOTA-v1.0.</li> <li>Very small instances (less than 10 pixels) are also annotated.</li> <li>Addition of a new category: \"container crane\".</li> <li>A total of 403,318 instances.</li> <li>Released for the DOAI Challenge 2019 on Object Detection in Aerial Images.</li> </ul>"},{"location":"datasets/obb/dota-v2/#dota-v20","title":"DOTA-v2.0","text":"<ul> <li>Collections from Google Earth, GF-2 Satellite, and other aerial images.</li> <li>Contains 18 common categories.</li> <li>Comprises 11,268 images with a whopping 1,793,658 instances.</li> <li>New categories introduced: \"airport\" and \"helipad\".</li> <li>Image splits:<ul> <li>Training: 1,830 images with 268,627 instances.</li> <li>Validation: 593 images with 81,048 instances.</li> <li>Test-dev: 2,792 images with 353,346 instances.</li> <li>Test-challenge: 6,053 images with 1,090,637 instances.</li> </ul> </li> </ul>"},{"location":"datasets/obb/dota-v2/#dataset-structure","title":"Dataset Structure","text":"<p>DOTA v2 exhibits a structured layout tailored for OBB object detection challenges:</p> <ul> <li>Images: A vast collection of high-resolution aerial images capturing diverse terrains and structures.</li> <li>Oriented Bounding Boxes: Annotations in the form of rotated rectangles encapsulating objects irrespective of their orientation, ideal for capturing objects like airplanes, ships, and buildings.</li> </ul>"},{"location":"datasets/obb/dota-v2/#applications","title":"Applications","text":"<p>DOTA v2 serves as a benchmark for training and evaluating models specifically tailored for aerial image analysis. With the inclusion of OBB annotations, it provides a unique challenge, enabling the development of specialized object detection models that cater to aerial imagery's nuances.</p>"},{"location":"datasets/obb/dota-v2/#dataset-yaml","title":"Dataset YAML","text":"<p>Typically, datasets incorporate a YAML (Yet Another Markup Language) file detailing the dataset's configuration. For DOTA v2, a hypothetical <code>DOTAv2.yaml</code> could be used. For accurate paths and configurations, it's vital to consult the dataset's official repository or documentation.</p> <p>DOTAv2.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# DOTA 2.0 dataset https://captain-whu.github.io/DOTA/index.html for object detection in aerial images by Wuhan University\n# Example usage: yolo train model=yolov8n-obb.pt data=DOTAv2.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 dota2  \u2190 downloads here (2GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/DOTAv2  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 1411 images\nval: images/val  # val images (relative to 'path') 458 images\ntest: images/test  # test images (optional) 937 images\n# Classes for DOTA 2.0\nnames:\n0: plane\n1: ship\n2: storage tank\n3: baseball diamond\n4: tennis court\n5: basketball court\n6: ground track field\n7: harbor\n8: bridge\n9: large vehicle\n10: small vehicle\n11: helicopter\n12: roundabout\n13: soccer ball field\n14: swimming pool\n15: container crane\n16: airport\n17: helipad\n# Download script/URL (optional)\ndownload: https://github.com/ultralytics/yolov5/releases/download/v1.0/DOTAv2.zip\n</code></pre>"},{"location":"datasets/obb/dota-v2/#usage","title":"Usage","text":"<p>To train a model on the DOTA v2 dataset, you can utilize the following code snippets. Always refer to your model's documentation for a thorough list of available arguments.</p> <p>Warning</p> <p>Please note that all images and associated annotations in the DOTAv2 dataset can be used for academic purposes, but commercial use is prohibited. Your understanding and respect for the dataset creators' wishes are greatly appreciated!</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Create a new YOLOv8n-OBB model from scratch\nmodel = YOLO('yolov8n-obb.yaml')\n# Train the model on the DOTAv2 dataset\nresults = model.train(data='DOTAv2.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Train a new YOLOv8n-OBB model on the DOTAv2 dataset\nyolo detect train data=DOTAv2.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/obb/dota-v2/#sample-data-and-annotations","title":"Sample Data and Annotations","text":"<p>Having a glance at the dataset illustrates its depth:</p> <p></p> <ul> <li>DOTA v2: This snapshot underlines the complexity of aerial scenes and the significance of Oriented Bounding Box annotations, capturing objects in their natural orientation.</li> </ul> <p>The dataset's richness offers invaluable insights into object detection challenges exclusive to aerial imagery.</p>"},{"location":"datasets/obb/dota-v2/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>For those leveraging DOTA v2 in their endeavors, it's pertinent to cite the relevant research papers:</p> BibTeX <pre><code>@article{9560031,\nauthor={Ding, Jian and Xue, Nan and Xia, Gui-Song and Bai, Xiang and Yang, Wen and Yang, Michael and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei},\njournal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\ntitle={Object Detection in Aerial Images: A Large-Scale Benchmark and Challenges},\nyear={2021},\nvolume={},\nnumber={},\npages={1-1},\ndoi={10.1109/TPAMI.2021.3117983}\n}\n</code></pre> <p>A special note of gratitude to the team behind DOTA v2 for their commendable effort in curating this dataset. For an exhaustive understanding of the dataset and its nuances, please visit the official DOTA v2 website.</p>"},{"location":"datasets/pose/","title":"Pose Estimation Datasets Overview","text":""},{"location":"datasets/pose/#supported-dataset-formats","title":"Supported Dataset Formats","text":""},{"location":"datasets/pose/#ultralytics-yolo-format","title":"Ultralytics YOLO format","text":"<p>** Label Format **</p> <p>The dataset format used for training YOLO pose models is as follows:</p> <ol> <li>One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the \".txt\" extension.</li> <li>One row per object: Each row in the text file corresponds to one object instance in the image.</li> <li>Object information per row: Each row contains the following information about the object instance:<ul> <li>Object class index: An integer representing the class of the object (e.g., 0 for person, 1 for car, etc.).</li> <li>Object center coordinates: The x and y coordinates of the center of the object, normalized to be between 0 and 1.</li> <li>Object width and height: The width and height of the object, normalized to be between 0 and 1.</li> <li>Object keypoint coordinates: The keypoints of the object, normalized to be between 0 and 1.</li> </ul> </li> </ol> <p>Here is an example of the label format for pose estimation task:</p> <p>Format with Dim = 2</p> <pre><code>&lt;class-index&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt; &lt;px1&gt; &lt;py1&gt; &lt;px2&gt; &lt;py2&gt; ... &lt;pxn&gt; &lt;pyn&gt;\n</code></pre> <p>Format with Dim = 3</p> <pre><code>&lt;class-index&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt; &lt;px1&gt; &lt;py1&gt; &lt;p1-visibility&gt; &lt;px2&gt; &lt;py2&gt; &lt;p2-visibility&gt; &lt;pxn&gt; &lt;pyn&gt; &lt;p2-visibility&gt;\n</code></pre> <p>In this format, <code>&lt;class-index&gt;</code> is the index of the class for the object,<code>&lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</code> are coordinates of boudning box, and <code>&lt;px1&gt; &lt;py1&gt; &lt;px2&gt; &lt;py2&gt; ... &lt;pxn&gt; &lt;pyn&gt;</code> are the pixel coordinates of the keypoints. The coordinates are separated by spaces.</p>"},{"location":"datasets/pose/#dataset-yaml-format","title":"Dataset YAML format","text":"<p>The Ultralytics framework uses a YAML file format to define the dataset and model configuration for training Detection Models. Here is an example of the YAML format used for defining a detection dataset:</p> <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8-pose  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Keypoints\nkpt_shape: [17, 3]  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)\nflip_idx: [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n# Classes dictionary\nnames:\n0: person\n</code></pre> <p>The <code>train</code> and <code>val</code> fields specify the paths to the directories containing the training and validation images, respectively.</p> <p><code>names</code> is a dictionary of class names. The order of the names should match the order of the object class indices in the YOLO dataset files.</p> <p>(Optional) if the points are symmetric then need flip_idx, like left-right side of human or face. For example if we assume five keypoints of facial landmark: [left eye, right eye, nose, left mouth, right mouth], and the original index is [0, 1, 2, 3, 4], then flip_idx is [1, 0, 2, 4, 3] (just exchange the left-right index, i.e 0-1 and 3-4, and do not modify others like nose in this example).</p>"},{"location":"datasets/pose/#usage","title":"Usage","text":"PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco128-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco128-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/pose/#supported-datasets","title":"Supported Datasets","text":"<p>This section outlines the datasets that are compatible with Ultralytics YOLO format and can be used for training pose estimation models:</p>"},{"location":"datasets/pose/#coco-pose","title":"COCO-Pose","text":"<ul> <li>Description: COCO-Pose is a large-scale object detection, segmentation, and pose estimation dataset. It is a subset of the popular COCO dataset and focuses on human pose estimation. COCO-Pose includes multiple keypoints for each human instance.</li> <li>Label Format: Same as Ultralytics YOLO format as described above, with keypoints for human poses.</li> <li>Number of Classes: 1 (Human).</li> <li>Keypoints: 17 keypoints including nose, eyes, ears, shoulders, elbows, wrists, hips, knees, and ankles.</li> <li>Usage: Suitable for training human pose estimation models.</li> <li>Additional Notes: The dataset is rich and diverse, containing over 200k labeled images.</li> <li>Read more about COCO-Pose</li> </ul>"},{"location":"datasets/pose/#coco8-pose","title":"COCO8-Pose","text":"<ul> <li>Description: Ultralytics COCO8-Pose is a small, but versatile pose detection dataset composed of the first 8 images of the COCO train 2017 set, 4 for training and 4 for validation.</li> <li>Label Format: Same as Ultralytics YOLO format as described above, with keypoints for human poses.</li> <li>Number of Classes: 1 (Human).</li> <li>Keypoints: 17 keypoints including nose, eyes, ears, shoulders, elbows, wrists, hips, knees, and ankles.</li> <li>Usage: Suitable for testing and debugging object detection models, or for experimenting with new detection approaches.</li> <li>Additional Notes: COCO8-Pose is ideal for sanity checks and CI checks.</li> <li>Read more about COCO8-Pose</li> </ul>"},{"location":"datasets/pose/#adding-your-own-dataset","title":"Adding your own dataset","text":"<p>If you have your own dataset and would like to use it for training pose estimation models with Ultralytics YOLO format, ensure that it follows the format specified above under \"Ultralytics YOLO format\". Convert your annotations to the required format and specify the paths, number of classes, and class names in the YAML configuration file.</p>"},{"location":"datasets/pose/#conversion-tool","title":"Conversion Tool","text":"<p>Ultralytics provides a convenient conversion tool to convert labels from the popular COCO dataset format to YOLO format:</p> <pre><code>from ultralytics.data.converter import convert_coco\nconvert_coco(labels_dir='../coco/annotations/', use_keypoints=True)\n</code></pre> <p>This conversion tool can be used to convert the COCO dataset or any dataset in the COCO format to the Ultralytics YOLO format. The <code>use_keypoints</code> parameter specifies whether to include keypoints (for pose estimation) in the converted labels.</p>"},{"location":"datasets/pose/coco/","title":"COCO-Pose Dataset","text":"<p>The COCO-Pose dataset is a specialized version of the COCO (Common Objects in Context) dataset, designed for pose estimation tasks. It leverages the COCO Keypoints 2017 images and labels to enable the training of models like YOLO for pose estimation tasks.</p> <p></p>"},{"location":"datasets/pose/coco/#key-features","title":"Key Features","text":"<ul> <li>COCO-Pose builds upon the COCO Keypoints 2017 dataset which contains 200K images labeled with keypoints for pose estimation tasks.</li> <li>The dataset supports 17 keypoints for human figures, facilitating detailed pose estimation.</li> <li>Like COCO, it provides standardized evaluation metrics, including Object Keypoint Similarity (OKS) for pose estimation tasks, making it suitable for comparing model performance.</li> </ul>"},{"location":"datasets/pose/coco/#dataset-structure","title":"Dataset Structure","text":"<p>The COCO-Pose dataset is split into three subsets:</p> <ol> <li>Train2017: This subset contains a portion of the 118K images from the COCO dataset, annotated for training pose estimation models.</li> <li>Val2017: This subset has a selection of images used for validation purposes during model training.</li> <li>Test2017: This subset consists of images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the COCO evaluation server for performance evaluation.</li> </ol>"},{"location":"datasets/pose/coco/#applications","title":"Applications","text":"<p>The COCO-Pose dataset is specifically used for training and evaluating deep learning models in keypoint detection and pose estimation tasks, such as OpenPose. The dataset's large number of annotated images and standardized evaluation metrics make it an essential resource for computer vision researchers and practitioners focused on pose estimation.</p>"},{"location":"datasets/pose/coco/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO-Pose dataset, the <code>coco-pose.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco-pose.yaml.</p> <p>ultralytics/cfg/datasets/coco-pose.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO 2017 dataset http://cocodataset.org by Microsoft\n# Example usage: yolo train data=coco-pose.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco-pose  \u2190 downloads here (20.1 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco-pose  # dataset root dir\ntrain: train2017.txt  # train images (relative to 'path') 118287 images\nval: val2017.txt  # val images (relative to 'path') 5000 images\ntest: test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n# Keypoints\nkpt_shape: [17, 3]  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)\nflip_idx: [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n# Classes\nnames:\n0: person\n# Download script/URL (optional)\ndownload: |\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\n# Download labels\ndir = Path(yaml['path'])  # dataset root dir\nurl = 'https://github.com/ultralytics/yolov5/releases/download/v1.0/'\nurls = [url + 'coco2017labels-pose.zip']  # labels\ndownload(urls, dir=dir.parent)\n# Download data\nurls = ['http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n'http://images.cocodataset.org/zips/val2017.zip',  # 1G, 5k images\n'http://images.cocodataset.org/zips/test2017.zip']  # 7G, 41k images (optional)\ndownload(urls, dir=dir / 'images', threads=3)\n</code></pre>"},{"location":"datasets/pose/coco/#usage","title":"Usage","text":"<p>To train a YOLOv8n-pose model on the COCO-Pose dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco-pose.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/pose/coco/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>The COCO-Pose dataset contains a diverse set of images with human figures annotated with keypoints. Here are some examples of images from the dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO-Pose dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/pose/coco/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO-Pose dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO-Pose dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/pose/coco8-pose/","title":"COCO8-Pose Dataset","text":""},{"location":"datasets/pose/coco8-pose/#introduction","title":"Introduction","text":"<p>Ultralytics COCO8-Pose is a small, but versatile pose detection dataset composed of the first 8 images of the COCO train 2017 set, 4 for training and 4 for validation. This dataset is ideal for testing and debugging object detection models, or for experimenting with new detection approaches. With 8 images, it is small enough to be easily manageable, yet diverse enough to test training pipelines for errors and act as a sanity check before training larger datasets.</p> <p>This dataset is intended for use with Ultralytics HUB and YOLOv8.</p>"},{"location":"datasets/pose/coco8-pose/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO8-Pose dataset, the <code>coco8-pose.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco8-pose.yaml.</p> <p>ultralytics/cfg/datasets/coco8-pose.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO8-pose dataset (first 8 images from COCO train2017) by Ultralytics\n# Example usage: yolo train data=coco8-pose.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco8-pose  \u2190 downloads here (1 MB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8-pose  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Keypoints\nkpt_shape: [17, 3]  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)\nflip_idx: [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n# Classes\nnames:\n0: person\n# Download script/URL (optional)\ndownload: https://ultralytics.com/assets/coco8-pose.zip\n</code></pre>"},{"location":"datasets/pose/coco8-pose/#usage","title":"Usage","text":"<p>To train a YOLOv8n-pose model on the COCO8-Pose dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco8-pose.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/pose/coco8-pose/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>Here are some examples of images from the COCO8-Pose dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO8-Pose dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/pose/coco8-pose/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/segment/","title":"Instance Segmentation Datasets Overview","text":""},{"location":"datasets/segment/#supported-dataset-formats","title":"Supported Dataset Formats","text":""},{"location":"datasets/segment/#ultralytics-yolo-format","title":"Ultralytics YOLO format","text":"<p>** Label Format **</p> <p>The dataset format used for training YOLO segmentation models is as follows:</p> <ol> <li>One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the \".txt\" extension.</li> <li>One row per object: Each row in the text file corresponds to one object instance in the image.</li> <li>Object information per row: Each row contains the following information about the object instance:<ul> <li>Object class index: An integer representing the class of the object (e.g., 0 for person, 1 for car, etc.).</li> <li>Object bounding coordinates: The bounding coordinates around the mask area, normalized to be between 0 and 1.</li> </ul> </li> </ol> <p>The format for a single row in the segmentation dataset file is as follows:</p> <pre><code>&lt;class-index&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; ... &lt;xn&gt; &lt;yn&gt;\n</code></pre> <p>In this format, <code>&lt;class-index&gt;</code> is the index of the class for the object, and <code>&lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; ... &lt;xn&gt; &lt;yn&gt;</code> are the bounding coordinates of the object's segmentation mask. The coordinates are separated by spaces.</p> <p>Here is an example of the YOLO dataset format for a single image with two object instances:</p> <pre><code>0 0.6812 0.48541 0.67 0.4875 0.67656 0.487 0.675 0.489 0.66\n1 0.5046 0.0 0.5015 0.004 0.4984 0.00416 0.4937 0.010 0.492 0.0104\n</code></pre> <p>Tip</p> <ul> <li>The length of each row does not have to be equal.</li> <li>Each segmentation label must have a minimum of 3 xy points: <code>&lt;class-index&gt; &lt;x1&gt; &lt;y1&gt; &lt;x2&gt; &lt;y2&gt; &lt;x3&gt; &lt;y3&gt;</code></li> </ul>"},{"location":"datasets/segment/#dataset-yaml-format","title":"Dataset YAML format","text":"<p>The Ultralytics framework uses a YAML file format to define the dataset and model configuration for training Detection Models. Here is an example of the YAML format used for defining a detection dataset:</p> <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8-seg  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Classes (80 COCO classes)\nnames:\n0: person\n1: bicycle\n2: car\n...\n77: teddy bear\n78: hair drier\n79: toothbrush\n</code></pre> <p>The <code>train</code> and <code>val</code> fields specify the paths to the directories containing the training and validation images, respectively.</p> <p><code>names</code> is a dictionary of class names. The order of the names should match the order of the object class indices in the YOLO dataset files.</p>"},{"location":"datasets/segment/#usage","title":"Usage","text":"PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/segment/#supported-datasets","title":"Supported Datasets","text":"<ul> <li>COCO: A large-scale dataset designed for object detection, segmentation, and captioning tasks with over 200K labeled images.</li> <li>COCO8-seg: A smaller dataset for instance segmentation tasks, containing a subset of 8 COCO images with segmentation annotations.</li> </ul>"},{"location":"datasets/segment/#adding-your-own-dataset","title":"Adding your own dataset","text":"<p>If you have your own dataset and would like to use it for training segmentation models with Ultralytics YOLO format, ensure that it follows the format specified above under \"Ultralytics YOLO format\". Convert your annotations to the required format and specify the paths, number of classes, and class names in the YAML configuration file.</p>"},{"location":"datasets/segment/#port-or-convert-label-formats","title":"Port or Convert Label Formats","text":""},{"location":"datasets/segment/#coco-dataset-format-to-yolo-format","title":"COCO Dataset Format to YOLO Format","text":"<p>You can easily convert labels from the popular COCO dataset format to the YOLO format using the following code snippet:</p> <pre><code>from ultralytics.data.converter import convert_coco\nconvert_coco(labels_dir='../coco/annotations/', use_segments=True)\n</code></pre> <p>This conversion tool can be used to convert the COCO dataset or any dataset in the COCO format to the Ultralytics YOLO format.</p> <p>Remember to double-check if the dataset you want to use is compatible with your model and follows the necessary format conventions. Properly formatted datasets are crucial for training successful object detection models.</p>"},{"location":"datasets/segment/#auto-annotation","title":"Auto-Annotation","text":"<p>Auto-annotation is an essential feature that allows you to generate a segmentation dataset using a pre-trained detection model. It enables you to quickly and accurately annotate a large number of images without the need for manual labeling, saving time and effort.</p>"},{"location":"datasets/segment/#generate-segmentation-dataset-using-a-detection-model","title":"Generate Segmentation Dataset Using a Detection Model","text":"<p>To auto-annotate your dataset using the Ultralytics framework, you can use the <code>auto_annotate</code> function as shown below:</p> <pre><code>from ultralytics.data.annotator import auto_annotate\nauto_annotate(data=\"path/to/images\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argument Type Description Default data str Path to a folder containing images to be annotated. det_model str, optional Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'. 'yolov8x.pt' sam_model str, optional Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'. 'sam_b.pt' device str, optional Device to run the models on. Defaults to an empty string (CPU or GPU, if available). output_dir str, None, optional Directory to save the annotated results. Defaults to a 'labels' folder in the same directory as 'data'. None <p>The <code>auto_annotate</code> function takes the path to your images, along with optional arguments for specifying the pre-trained detection and SAM segmentation models, the device to run the models on, and the output directory for saving the annotated results.</p> <p>By leveraging the power of pre-trained models, auto-annotation can significantly reduce the time and effort required for creating high-quality segmentation datasets. This feature is particularly useful for researchers and developers working with large image collections, as it allows them to focus on model development and evaluation rather than manual annotation.</p>"},{"location":"datasets/segment/coco/","title":"COCO-Seg Dataset","text":"<p>The COCO-Seg dataset, an extension of the COCO (Common Objects in Context) dataset, is specially designed to aid research in object instance segmentation. It uses the same images as COCO but introduces more detailed segmentation annotations. This dataset is a crucial resource for researchers and developers working on instance segmentation tasks, especially for training YOLO models.</p>"},{"location":"datasets/segment/coco/#key-features","title":"Key Features","text":"<ul> <li>COCO-Seg retains the original 330K images from COCO.</li> <li>The dataset consists of the same 80 object categories found in the original COCO dataset.</li> <li>Annotations now include more detailed instance segmentation masks for each object in the images.</li> <li>COCO-Seg provides standardized evaluation metrics like mean Average Precision (mAP) for object detection, and mean Average Recall (mAR) for instance segmentation tasks, enabling effective comparison of model performance.</li> </ul>"},{"location":"datasets/segment/coco/#dataset-structure","title":"Dataset Structure","text":"<p>The COCO-Seg dataset is partitioned into three subsets:</p> <ol> <li>Train2017: This subset contains 118K images for training instance segmentation models.</li> <li>Val2017: This subset includes 5K images used for validation purposes during model training.</li> <li>Test2017: This subset encompasses 20K images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the COCO evaluation server for performance evaluation.</li> </ol>"},{"location":"datasets/segment/coco/#applications","title":"Applications","text":"<p>COCO-Seg is widely used for training and evaluating deep learning models in instance segmentation, such as the YOLO models. The large number of annotated images, the diversity of object categories, and the standardized evaluation metrics make it an indispensable resource for computer vision researchers and practitioners.</p>"},{"location":"datasets/segment/coco/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO-Seg dataset, the <code>coco.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml.</p> <p>ultralytics/cfg/datasets/coco.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO 2017 dataset http://cocodataset.org by Microsoft\n# Example usage: yolo train data=coco.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco  \u2190 downloads here (20.1 GB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco  # dataset root dir\ntrain: train2017.txt  # train images (relative to 'path') 118287 images\nval: val2017.txt  # val images (relative to 'path') 5000 images\ntest: test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n# Download script/URL (optional)\ndownload: |\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\n# Download labels\nsegments = True  # segment or box labels\ndir = Path(yaml['path'])  # dataset root dir\nurl = 'https://github.com/ultralytics/yolov5/releases/download/v1.0/'\nurls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\ndownload(urls, dir=dir.parent)\n# Download data\nurls = ['http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n'http://images.cocodataset.org/zips/val2017.zip',  # 1G, 5k images\n'http://images.cocodataset.org/zips/test2017.zip']  # 7G, 41k images (optional)\ndownload(urls, dir=dir / 'images', threads=3)\n</code></pre>"},{"location":"datasets/segment/coco/#usage","title":"Usage","text":"<p>To train a YOLOv8n-seg model on the COCO-Seg dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco-seg.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/segment/coco/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>COCO-Seg, like its predecessor COCO, contains a diverse set of images with various object categories and complex scenes. However, COCO-Seg introduces more detailed instance segmentation masks for each object in the images. Here are some examples of images from the dataset, along with their corresponding instance segmentation masks:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This aids the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO-Seg dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/segment/coco/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO-Seg dataset in your research or development work, please cite the original COCO paper and acknowledge the extension to COCO-Seg:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We extend our thanks to the COCO Consortium for creating and maintaining this invaluable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/segment/coco8-seg/","title":"COCO8-Seg Dataset","text":""},{"location":"datasets/segment/coco8-seg/#introduction","title":"Introduction","text":"<p>Ultralytics COCO8-Seg is a small, but versatile instance segmentation dataset composed of the first 8 images of the COCO train 2017 set, 4 for training and 4 for validation. This dataset is ideal for testing and debugging segmentation models, or for experimenting with new detection approaches. With 8 images, it is small enough to be easily manageable, yet diverse enough to test training pipelines for errors and act as a sanity check before training larger datasets.</p> <p>This dataset is intended for use with Ultralytics HUB and YOLOv8.</p>"},{"location":"datasets/segment/coco8-seg/#dataset-yaml","title":"Dataset YAML","text":"<p>A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO8-Seg dataset, the <code>coco8-seg.yaml</code> file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco8-seg.yaml.</p> <p>ultralytics/cfg/datasets/coco8-seg.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO8-seg dataset (first 8 images from COCO train2017) by Ultralytics\n# Example usage: yolo train data=coco8-seg.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco8-seg  \u2190 downloads here (1 MB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8-seg  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n# Download script/URL (optional)\ndownload: https://ultralytics.com/assets/coco8-seg.zip\n</code></pre>"},{"location":"datasets/segment/coco8-seg/#usage","title":"Usage","text":"<p>To train a YOLOv8n-seg model on the COCO8-Seg dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.</p> <p>Train Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load a pretrained model (recommended for training)\n# Train the model\nresults = model.train(data='coco8-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Start training from a pretrained *.pt model\nyolo detect train data=coco8-seg.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"datasets/segment/coco8-seg/#sample-images-and-annotations","title":"Sample Images and Annotations","text":"<p>Here are some examples of images from the COCO8-Seg dataset, along with their corresponding annotations:</p> <p></p> <ul> <li>Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.</li> </ul> <p>The example showcases the variety and complexity of the images in the COCO8-Seg dataset and the benefits of using mosaicing during the training process.</p>"},{"location":"datasets/segment/coco8-seg/#citations-and-acknowledgments","title":"Citations and Acknowledgments","text":"<p>If you use the COCO dataset in your research or development work, please cite the following paper:</p> BibTeX <pre><code>@misc{lin2015microsoft,\ntitle={Microsoft COCO: Common Objects in Context},\nauthor={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll\u00e1r},\nyear={2015},\neprint={1405.0312},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.</p>"},{"location":"datasets/track/","title":"Multi-object Tracking Datasets Overview","text":""},{"location":"datasets/track/#dataset-format-coming-soon","title":"Dataset Format (Coming Soon)","text":"<p>Multi-Object Detector doesn't need standalone training and directly supports pre-trained detection, segmentation or Pose models. Support for training trackers alone is coming soon</p>"},{"location":"datasets/track/#usage","title":"Usage","text":"PythonCLI <pre><code>from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code>yolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"guides/","title":"Comprehensive Tutorials to Ultralytics YOLO","text":"<p>Welcome to the Ultralytics' YOLO \ud83d\ude80 Guides! Our comprehensive tutorials cover various aspects of the YOLO object detection model, ranging from training and prediction to deployment. Built on PyTorch, YOLO stands out for its exceptional speed and accuracy in real-time object detection tasks.</p> <p>Whether you're a beginner or an expert in deep learning, our tutorials offer valuable insights into the implementation and optimization of YOLO for your computer vision projects. Let's dive in!</p>"},{"location":"guides/#guides","title":"Guides","text":"<p>Here's a compilation of in-depth guides to help you master different aspects of Ultralytics YOLO.</p> <ul> <li>K-Fold Cross Validation \ud83d\ude80 NEW: Learn how to improve model generalization using K-Fold cross-validation technique.</li> </ul> <p>Note: More guides about training, exporting, predicting, and deploying with Ultralytics YOLO are coming soon. Stay tuned!</p>"},{"location":"guides/kfold-cross-validation/","title":"K-Fold Cross Validation with Ultralytics","text":""},{"location":"guides/kfold-cross-validation/#introduction","title":"Introduction","text":"<p>This comprehensive guide illustrates the implementation of K-Fold Cross Validation for object detection datasets within the Ultralytics ecosystem. We'll leverage the YOLO detection format and key Python libraries such as sklearn, pandas, and PyYaml to guide you through the necessary setup, the process of generating feature vectors, and the execution of a K-Fold dataset split.</p> <p> </p> <p>Whether your project involves the Fruit Detection dataset or a custom data source, this tutorial aims to help you comprehend and apply K-Fold Cross Validation to bolster the reliability and robustness of your machine learning models. While we're applying <code>k=5</code> folds for this tutorial, keep in mind that the optimal number of folds can vary depending on your dataset and the specifics of your project.</p> <p>Without further ado, let's dive in!</p>"},{"location":"guides/kfold-cross-validation/#setup","title":"Setup","text":"<ul> <li> <p>Your annotations should be in the YOLO detection format.</p> </li> <li> <p>This guide assumes that annotation files are locally available.</p> <ul> <li> <p>For our demonstration, we use the Fruit Detection dataset.</p> <ul> <li>This dataset contains a total of 8479 images.</li> <li>It includes 6 class labels, each with its total instance counts listed below.</li> </ul> Class Label Instance Count Apple 7049 Grapes 7202 Pineapple 1613 Orange 15549 Banana 3536 Watermelon 1976 </li> </ul> </li> <li> <p>Necessary Python packages include:</p> <ul> <li><code>ultralytics</code></li> <li><code>sklearn</code></li> <li><code>pandas</code></li> <li><code>pyyaml</code></li> </ul> </li> <li> <p>This tutorial operates with <code>k=5</code> folds. However, you should determine the best number of folds for your specific dataset.</p> </li> <li> <p>Initiate a new Python virtual environment (<code>venv</code>) for your project and activate it. Use <code>pip</code> (or your preferred package manager) to install:</p> <ul> <li>The Ultralytics library: <code>pip install -U ultralytics</code>. Alternatively, you can clone the official repo.</li> <li>Scikit-learn, pandas, and PyYAML: <code>pip install -U scikit-learn pandas pyyaml</code>.</li> </ul> </li> <li> <p>Verify that your annotations are in the YOLO detection format.</p> <ul> <li>For this tutorial, all annotation files are found in the <code>Fruit-Detection/labels</code> directory.</li> </ul> </li> </ul>"},{"location":"guides/kfold-cross-validation/#generating-feature-vectors-for-object-detection-dataset","title":"Generating Feature Vectors for Object Detection Dataset","text":"<ol> <li> <p>Start by creating a new Python file and import the required libraries.</p> <pre><code>import datetime\nimport shutil\nfrom pathlib import Path\nfrom collections import Counter\nimport yaml\nimport numpy as np\nimport pandas as pd\nfrom ultralytics import YOLO\nfrom sklearn.model_selection import KFold\n</code></pre> </li> <li> <p>Proceed to retrieve all label files for your dataset.</p> <pre><code>dataset_path = Path('./Fruit-detection') # replace with 'path/to/dataset' for your custom data\nlabels = sorted(dataset_path.rglob(\"*labels/*.txt\")) # all data in 'labels'\n</code></pre> </li> <li> <p>Now, read the contents of the dataset YAML file and extract the indices of the class labels.</p> <pre><code>with open(yaml_file, 'r', encoding=\"utf8\") as y:\nclasses = yaml.safe_load(y)['names']\ncls_idx = sorted(classes.keys())\n</code></pre> </li> <li> <p>Initialize an empty <code>pandas</code> DataFrame.</p> <pre><code>indx = [l.stem for l in labels] # uses base filename as ID (no extension)\nlabels_df = pd.DataFrame([], columns=cls_idx, index=indx)\n</code></pre> </li> <li> <p>Count the instances of each class-label present in the annotation files.</p> <pre><code>for label in labels:\nlbl_counter = Counter()\nwith open(label,'r') as lf:\nlines = lf.readlines()\nfor l in lines:\n# classes for YOLO label uses integer at first position of each line\nlbl_counter[int(l.split(' ')[0])] += 1\nlabels_df.loc[label.stem] = lbl_counter\nlabels_df = labels_df.fillna(0.0) # replace `nan` values with `0.0`\n</code></pre> </li> <li> <p>The following is a sample view of the populated DataFrame:</p> <pre><code>                                                       0    1    2    3    4    5\n'0000a16e4b057580_jpg.rf.00ab48988370f64f5ca8ea4...'  0.0  0.0  0.0  0.0  0.0  7.0\n'0000a16e4b057580_jpg.rf.7e6dce029fb67f01eb19aa7...'  0.0  0.0  0.0  0.0  0.0  7.0\n'0000a16e4b057580_jpg.rf.bc4d31cdcbe229dd022957a...'  0.0  0.0  0.0  0.0  0.0  7.0\n'00020ebf74c4881c_jpg.rf.508192a0a97aa6c4a3b6882...'  0.0  0.0  0.0  1.0  0.0  0.0\n'00020ebf74c4881c_jpg.rf.5af192a2254c8ecc4188a25...'  0.0  0.0  0.0  1.0  0.0  0.0\n ...                                                  ...  ...  ...  ...  ...  ...\n'ff4cd45896de38be_jpg.rf.c4b5e967ca10c7ced3b9e97...'  0.0  0.0  0.0  0.0  0.0  2.0\n'ff4cd45896de38be_jpg.rf.ea4c1d37d2884b3e3cbce08...'  0.0  0.0  0.0  0.0  0.0  2.0\n'ff5fd9c3c624b7dc_jpg.rf.bb519feaa36fc4bf630a033...'  1.0  0.0  0.0  0.0  0.0  0.0\n'ff5fd9c3c624b7dc_jpg.rf.f0751c9c3aa4519ea3c9d6a...'  1.0  0.0  0.0  0.0  0.0  0.0\n'fffe28b31f2a70d4_jpg.rf.7ea16bd637ba0711c53b540...'  0.0  6.0  0.0  0.0  0.0  0.0\n</code></pre> </li> </ol> <p>The rows index the label files, each corresponding to an image in your dataset, and the columns correspond to your class-label indices. Each row represents a pseudo feature-vector, with the count of each class-label present in your dataset. This data structure enables the application of K-Fold Cross Validation to an object detection dataset.</p>"},{"location":"guides/kfold-cross-validation/#k-fold-dataset-split","title":"K-Fold Dataset Split","text":"<ol> <li> <p>Now we will use the <code>KFold</code> class from <code>sklearn.model_selection</code> to generate <code>k</code> splits of the dataset.</p> <ul> <li>Important:<ul> <li>Setting <code>shuffle=True</code> ensures a randomized distribution of classes in your splits.</li> <li>By setting <code>random_state=M</code> where <code>M</code> is a chosen integer, you can obtain repeatable results.</li> </ul> </li> </ul> <pre><code>ksplit = 5\nkf = KFold(n_splits=ksplit, shuffle=True, random_state=20)   # setting random_state for repeatable results\nkfolds = list(kf.split(labels_df))\n</code></pre> </li> <li> <p>The dataset has now been split into <code>k</code> folds, each having a list of <code>train</code> and <code>val</code> indices. We will construct a DataFrame to display these results more clearly.</p> <pre><code>folds = [f'split_{n}' for n in range(1, ksplit + 1)]\nfolds_df = pd.DataFrame(index=indx, columns=folds)\nfor idx, (train, val) in enumerate(kfolds, start=1):\nfolds_df[f'split_{idx}'].loc[labels_df.iloc[train].index] = 'train'\nfolds_df[f'split_{idx}'].loc[labels_df.iloc[val].index] = 'val'\n</code></pre> </li> <li> <p>Now we will calculate the distribution of class labels for each fold as a ratio of the classes present in <code>val</code> to those present in <code>train</code>.</p> <pre><code>fold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)\nfor n, (train_indices, val_indices) in enumerate(kfolds, start=1):\ntrain_totals = labels_df.iloc[train_indices].sum()\nval_totals = labels_df.iloc[val_indices].sum()\n# To avoid division by zero, we add a small value (1E-7) to the denominator\nratio = val_totals / (train_totals + 1E-7)\nfold_lbl_distrb.loc[f'split_{n}'] = ratio\n</code></pre> </li> </ol> <p>The ideal scenario is for all class ratios to be reasonably similar for each split and across classes. This, however, will be subject to the specifics of your dataset.</p> <ol> <li> <p>Next, we create the directories and dataset YAML files for each split.</p> <pre><code>save_path = Path(dataset_path / f'{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-val')\nsave_path.mkdir(parents=True, exist_ok=True)\nimages = sorted((dataset_path / 'images').rglob(\"*.jpg\"))  # change file extension as needed\nds_yamls = []\nfor split in folds_df.columns:\n# Create directories\nsplit_dir = save_path / split\nsplit_dir.mkdir(parents=True, exist_ok=True)\n(split_dir / 'train' / 'images').mkdir(parents=True, exist_ok=True)\n(split_dir / 'train' / 'labels').mkdir(parents=True, exist_ok=True)\n(split_dir / 'val' / 'images').mkdir(parents=True, exist_ok=True)\n(split_dir / 'val' / 'labels').mkdir(parents=True, exist_ok=True)\n# Create dataset YAML files\ndataset_yaml = split_dir / f'{split}_dataset.yaml'\nds_yamls.append(dataset_yaml)\nwith open(dataset_yaml, 'w') as ds_y:\nyaml.safe_dump({\n'path': split_dir.as_posix(),\n'train': 'train',\n'val': 'val',\n'names': classes\n}, ds_y)\n</code></pre> </li> <li> <p>Lastly, copy images and labels into the respective directory ('train' or 'val') for each split.</p> <ul> <li>NOTE: The time required for this portion of the code will vary based on the size of your dataset and your system hardware.</li> </ul> <pre><code>for image, label in zip(images, labels):\nfor split, k_split in folds_df.loc[image.stem].items():\n# Destination directory\nimg_to_path = save_path / split / k_split / 'images'\nlbl_to_path = save_path / split / k_split / 'labels'\n# Copy image and label files to new directory \n# Might throw a SamefileError if file already exists\nshutil.copy(image, img_to_path / image.name)\nshutil.copy(label, lbl_to_path / label.name)\n</code></pre> </li> </ol>"},{"location":"guides/kfold-cross-validation/#save-records-optional","title":"Save Records (Optional)","text":"<p>Optionally, you can save the records of the K-Fold split and label distribution DataFrames as CSV files for future reference.</p> <pre><code>folds_df.to_csv(save_path / \"kfold_datasplit.csv\")\nfold_lbl_distrb.to_csv(save_path / \"kfold_label_distribution.csv\")\n</code></pre>"},{"location":"guides/kfold-cross-validation/#train-yolo-using-k-fold-data-splits","title":"Train YOLO using K-Fold Data Splits","text":"<ol> <li> <p>First, load the YOLO model.</p> <pre><code>weights_path = 'path/to/weights.pt'\nmodel = YOLO(weights_path, task='detect')\n</code></pre> </li> <li> <p>Next, iterate over the dataset YAML files to run training. The results will be saved to a directory specified by the <code>project</code> and <code>name</code> arguments. By default, this directory is 'exp/runs#' where # is an integer index.</p> <pre><code>results = {}\nfor k in range(ksplit):\ndataset_yaml = ds_yamls[k]\nmodel.train(data=dataset_yaml, *args, **kwargs)  # Include any training arguments\nresults[k] = model.metrics  # save output metrics for further analysis\n</code></pre> </li> </ol>"},{"location":"guides/kfold-cross-validation/#conclusion","title":"Conclusion","text":"<p>In this guide, we have explored the process of using K-Fold cross-validation for training the YOLO object detection model. We learned how to split our dataset into K partitions, ensuring a balanced class distribution across the different folds.</p> <p>We also explored the procedure for creating report DataFrames to visualize the data splits and label distributions across these splits, providing us a clear insight into the structure of our training and validation sets.</p> <p>Optionally, we saved our records for future reference, which could be particularly useful in large-scale projects or when troubleshooting model performance.</p> <p>Finally, we implemented the actual model training using each split in a loop, saving our training results for further analysis and comparison.</p> <p>This technique of K-Fold cross-validation is a robust way of making the most out of your available data, and it helps to ensure that your model performance is reliable and consistent across different data subsets. This results in a more generalizable and reliable model that is less likely to overfit to specific data patterns.</p> <p>Remember that although we used YOLO in this guide, these steps are mostly transferable to other machine learning models. Understanding these steps allows you to apply cross-validation effectively in your own machine learning projects. Happy coding!</p>"},{"location":"help/","title":"Help","text":"<p>Welcome to the Ultralytics Help page! We are committed to providing you with comprehensive resources to make your experience with Ultralytics YOLO repositories as smooth and enjoyable as possible. On this page, you'll find essential links to guides and documents that will help you navigate through common tasks and address any questions you might have while using our repositories.</p> <ul> <li>Frequently Asked Questions (FAQ): Find answers to common questions and issues faced by users and contributors of Ultralytics YOLO repositories.</li> <li>Contributing Guide: Learn the best practices for submitting pull requests, reporting bugs, and contributing to the development of our repositories.</li> <li>Continuous Integration (CI) Guide: Understand the CI tests we perform for each Ultralytics repository and see their current statuses.</li> <li>Contributor License Agreement (CLA): Familiarize yourself with our CLA to understand the terms and conditions for contributing to Ultralytics projects.</li> <li>Minimum Reproducible Example (MRE) Guide: Understand how to create an MRE when submitting bug reports to ensure that our team can quickly and efficiently address the issue.</li> <li>Code of Conduct: Learn about our community guidelines and expectations to ensure a welcoming and inclusive environment for all participants.</li> <li>Environmental, Health and Safety (EHS) Policy: Explore Ultralytics' dedicated approach towards maintaining a sustainable, safe, and healthy work environment for all our stakeholders.</li> <li>Security Policy: Understand our security practices and how to report security vulnerabilities responsibly.</li> </ul> <p>We highly recommend going through these guides to make the most of your collaboration with the Ultralytics community. Our goal is to maintain a welcoming and supportive environment for all users and contributors. If you need further assistance, don't hesitate to reach out to us through GitHub Issues or the official discussion forum. Happy coding!</p>"},{"location":"help/CI/","title":"Continuous Integration (CI)","text":"<p>Continuous Integration (CI) is an essential aspect of software development which involves integrating changes and testing them automatically. CI allows us to maintain high-quality code by catching issues early and often in the development process. At Ultralytics, we use various CI tests to ensure the quality and integrity of our codebase.</p>"},{"location":"help/CI/#ci-actions","title":"CI Actions","text":"<p>Here's a brief description of our CI actions:</p> <ul> <li>CI: This is our primary CI test that involves running unit tests, linting checks, and sometimes more comprehensive tests depending on the repository.</li> <li>Docker Deployment: This test checks the deployment of the project using Docker to ensure the Dockerfile and related scripts are working correctly.</li> <li>Broken Links: This test scans the codebase for any broken or dead links in our markdown or HTML files.</li> <li>CodeQL: CodeQL is a tool from GitHub that performs semantic analysis on our code, helping to find potential security vulnerabilities and maintain high-quality code.</li> <li>PyPi Publishing: This test checks if the project can be packaged and published to PyPi without any errors.</li> </ul>"},{"location":"help/CI/#ci-results","title":"CI Results","text":"<p>Below is the table showing the status of these CI tests for our main repositories:</p> Repository CI Docker Deployment Broken Links CodeQL PyPi and Docs Publishing yolov3 yolov5 ultralytics hub docs <p>Each badge shows the status of the last run of the corresponding CI test on the <code>main</code> branch of the respective repository. If a test fails, the badge will display a \"failing\" status, and if it passes, it will display a \"passing\" status.</p> <p>If you notice a test failing, it would be a great help if you could report it through a GitHub issue in the respective repository.</p> <p>Remember, a successful CI test does not mean that everything is perfect. It is always recommended to manually review the code before deployment or merging changes.</p>"},{"location":"help/CI/#code-coverage","title":"Code Coverage","text":"<p>Code coverage is a metric that represents the percentage of your codebase that is executed when your tests run. It provides insight into how well your tests exercise your code and can be crucial in identifying untested parts of your application. A high code coverage percentage is often associated with a lower likelihood of bugs. However, it's essential to understand that code coverage doesn't guarantee the absence of defects. It merely indicates which parts of the code have been executed by the tests.</p>"},{"location":"help/CI/#integration-with-codecovio","title":"Integration with codecov.io","text":"<p>At Ultralytics, we have integrated our repositories with codecov.io, a popular online platform for measuring and visualizing code coverage. Codecov provides detailed insights, coverage comparisons between commits, and visual overlays directly on your code, indicating which lines were covered.</p> <p>By integrating with Codecov, we aim to maintain and improve the quality of our code by focusing on areas that might be prone to errors or need further testing.</p>"},{"location":"help/CI/#coverage-results","title":"Coverage Results","text":"<p>To quickly get a glimpse of the code coverage status of the <code>ultralytics</code> python package, we have included a badge and and sunburst visual of the <code>ultralytics</code> coverage results. These images show the percentage of code covered by our tests, offering an at-a-glance metric of our testing efforts. For full details please see https://codecov.io/github/ultralytics/ultralytics.</p> Repository Code Coverage ultralytics <p>In the sunburst graphic below, the inner-most circle is the entire project, moving away from the center are folders then, finally, a single file. The size and color of each slice is representing the number of statements and the coverage, respectively.</p> <p> </p>"},{"location":"help/CLA/","title":"Ultralytics Individual Contributor License Agreement","text":"<p>Thank you for your interest in contributing to open source software projects (\u201cProjects\u201d) made available by Ultralytics SE or its affiliates (\u201cUltralytics\u201d). This Individual Contributor License Agreement (\u201cAgreement\u201d) sets out the terms governing any source code, object code, bug fixes, configuration changes, tools, specifications, documentation, data, materials, feedback, information or other works of authorship that you submit or have submitted, in any form and in any manner, to Ultralytics in respect of any of the Projects (collectively \u201cContributions\u201d). If you have any questions respecting this Agreement, please contact hello@ultralytics.com.</p> <p>You agree that the following terms apply to all of your past, present and future Contributions. Except for the licenses granted in this Agreement, you retain all of your right, title and interest in and to your Contributions.</p> <p>Copyright License. You hereby grant, and agree to grant, to Ultralytics a non-exclusive, perpetual, irrevocable, worldwide, fully-paid, royalty-free, transferable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, and distribute your Contributions and such derivative works, with the right to sublicense the foregoing rights through multiple tiers of sublicensees.</p> <p>Patent License. You hereby grant, and agree to grant, to Ultralytics a non-exclusive, perpetual, irrevocable, worldwide, fully-paid, royalty-free, transferable patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer your Contributions, where such license applies only to those patent claims licensable by you that are necessarily infringed by your Contributions alone or by combination of your Contributions with the Project to which such Contributions were submitted, with the right to sublicense the foregoing rights through multiple tiers of sublicensees.</p> <p>Moral Rights. To the fullest extent permitted under applicable law, you hereby waive, and agree not to assert, all of your \u201cmoral rights\u201d in or relating to your Contributions for the benefit of Ultralytics, its assigns, and their respective direct and indirect sublicensees.</p> <p>Third Party Content/Rights. If your Contribution includes or is based on any source code, object code, bug fixes, configuration changes, tools, specifications, documentation, data, materials, feedback, information or other works of authorship that were not authored by you (\u201cThird Party Content\u201d) or if you are aware of any third party intellectual property or proprietary rights associated with your Contribution (\u201cThird Party Rights\u201d), then you agree to include with the submission of your Contribution full details respecting such Third Party Content and Third Party Rights, including, without limitation, identification of which aspects of your Contribution contain Third Party Content or are associated with Third Party Rights, the owner/author of the Third Party Content and Third Party Rights, where you obtained the Third Party Content, and any applicable third party license terms or restrictions respecting the Third Party Content and Third Party Rights. For greater certainty, the foregoing obligations respecting the identification of Third Party Content and Third Party Rights do not apply to any portion of a Project that is incorporated into your Contribution to that same Project.</p> <p>Representations. You represent that, other than the Third Party Content and Third Party Rights identified by you in accordance with this Agreement, you are the sole author of your Contributions and are legally entitled to grant the foregoing licenses and waivers in respect of your Contributions. If your Contributions were created in the course of your employment with your past or present employer(s), you represent that such employer(s) has authorized you to make your Contributions on behalf of such employer(s) or such employer (s) has waived all of their right, title or interest in or to your Contributions.</p> <p>Disclaimer. To the fullest extent permitted under applicable law, your Contributions are provided on an \"asis\" basis, without any warranties or conditions, express or implied, including, without limitation, any implied warranties or conditions of non-infringement, merchantability or fitness for a particular purpose. You are not required to provide support for your Contributions, except to the extent you desire to provide support.</p> <p>No Obligation. You acknowledge that Ultralytics is under no obligation to use or incorporate your Contributions into any of the Projects. The decision to use or incorporate your Contributions into any of the Projects will be made at the sole discretion of Ultralytics or its authorized delegates ..</p> <p>Disputes. This Agreement shall be governed by and construed in accordance with the laws of the State of New York, United States of America, without giving effect to its principles or rules regarding conflicts of laws, other than such principles directing application of New York law. The parties hereby submit to venue in, and jurisdiction of the courts located in New York, New York for purposes relating to this Agreement. In the event that any of the provisions of this Agreement shall be held by a court or other tribunal of competent jurisdiction to be unenforceable, the remaining portions hereof shall remain in full force and effect.</p> <p>Assignment. You agree that Ultralytics may assign this Agreement, and all of its rights, obligations and licenses hereunder.</p>"},{"location":"help/FAQ/","title":"Ultralytics YOLO Frequently Asked Questions (FAQ)","text":"<p>This FAQ section addresses some common questions and issues users might encounter while working with Ultralytics YOLO repositories.</p>"},{"location":"help/FAQ/#1-what-are-the-hardware-requirements-for-running-ultralytics-yolo","title":"1. What are the hardware requirements for running Ultralytics YOLO?","text":"<p>Ultralytics YOLO can be run on a variety of hardware configurations, including CPUs, GPUs, and even some edge devices. However, for optimal performance and faster training and inference, we recommend using a GPU with a minimum of 8GB of memory. NVIDIA GPUs with CUDA support are ideal for this purpose.</p>"},{"location":"help/FAQ/#2-how-do-i-fine-tune-a-pre-trained-yolo-model-on-my-custom-dataset","title":"2. How do I fine-tune a pre-trained YOLO model on my custom dataset?","text":"<p>To fine-tune a pre-trained YOLO model on your custom dataset, you'll need to create a dataset configuration file (YAML) that defines the dataset's properties, such as the path to the images, the number of classes, and class names. Next, you'll need to modify the model configuration file to match the number of classes in your dataset. Finally, use the <code>train.py</code> script to start the training process with your custom dataset and the pre-trained model. You can find a detailed guide on fine-tuning YOLO in the Ultralytics documentation.</p>"},{"location":"help/FAQ/#3-how-do-i-convert-a-yolo-model-to-onnx-or-tensorflow-format","title":"3. How do I convert a YOLO model to ONNX or TensorFlow format?","text":"<p>Ultralytics provides built-in support for converting YOLO models to ONNX format. You can use the <code>export.py</code> script to convert a saved model to ONNX format. If you need to convert the model to TensorFlow format, you can use the ONNX model as an intermediary and then use the ONNX-TensorFlow converter to convert the ONNX model to TensorFlow format.</p>"},{"location":"help/FAQ/#4-can-i-use-ultralytics-yolo-for-real-time-object-detection","title":"4. Can I use Ultralytics YOLO for real-time object detection?","text":"<p>Yes, Ultralytics YOLO is designed to be efficient and fast, making it suitable for real-time object detection tasks. The actual performance will depend on your hardware configuration and the complexity of the model. Using a GPU and optimizing the model for your specific use case can help achieve real-time performance.</p>"},{"location":"help/FAQ/#5-how-can-i-improve-the-accuracy-of-my-yolo-model","title":"5. How can I improve the accuracy of my YOLO model?","text":"<p>Improving the accuracy of a YOLO model may involve several strategies, such as:</p> <ul> <li>Fine-tuning the model on more annotated data</li> <li>Data augmentation to increase the variety of training samples</li> <li>Using a larger or more complex model architecture</li> <li>Adjusting the learning rate, batch size, and other hyperparameters</li> <li>Using techniques like transfer learning or knowledge distillation</li> </ul> <p>Remember that there's often a trade-off between accuracy and inference speed, so finding the right balance is crucial for your specific application.</p> <p>If you have any more questions or need assistance, don't hesitate to consult the Ultralytics documentation or reach out to the community through GitHub Issues or the official discussion forum.</p>"},{"location":"help/code_of_conduct/","title":"Ultralytics Contributor Covenant Code of Conduct","text":""},{"location":"help/code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"help/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"help/code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"help/code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"help/code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at hello@ultralytics.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"help/code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"help/code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"help/code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"help/code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"help/code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"help/code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"help/contributing/","title":"Contributing to Ultralytics Open-Source YOLO Repositories","text":"<p>First of all, thank you for your interest in contributing to Ultralytics open-source YOLO repositories! Your contributions will help improve the project and benefit the community. This document provides guidelines and best practices to get you started.</p>"},{"location":"help/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Pull Requests<ul> <li>CLA Signing</li> <li>Google-Style Docstrings</li> <li>GitHub Actions CI Tests</li> </ul> </li> <li>Bug Reports<ul> <li>Minimum Reproducible Example</li> </ul> </li> <li>License and Copyright</li> </ul>"},{"location":"help/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>All contributors are expected to adhere to the Code of Conduct to ensure a welcoming and inclusive environment for everyone.</p>"},{"location":"help/contributing/#pull-requests","title":"Pull Requests","text":"<p>We welcome contributions in the form of pull requests. To make the review process smoother, please follow these guidelines:</p> <ol> <li> <p>Fork the repository: Fork the Ultralytics YOLO repository to your own GitHub account.</p> </li> <li> <p>Create a branch: Create a new branch in your forked repository with a descriptive name for your changes.</p> </li> <li> <p>Make your changes: Make the changes you want to contribute. Ensure that your changes follow the coding style of the project and do not introduce new errors or warnings.</p> </li> <li> <p>Test your changes: Test your changes locally to ensure that they work as expected and do not introduce new issues.</p> </li> <li> <p>Commit your changes: Commit your changes with a descriptive commit message. Make sure to include any relevant issue numbers in your commit message.</p> </li> <li> <p>Create a pull request: Create a pull request from your forked repository to the main Ultralytics YOLO repository. In the pull request description, provide a clear explanation of your changes and how they improve the project.</p> </li> </ol>"},{"location":"help/contributing/#cla-signing","title":"CLA Signing","text":"<p>Before we can accept your pull request, you need to sign a Contributor License Agreement (CLA). This is a legal document stating that you agree to the terms of contributing to the Ultralytics YOLO repositories. The CLA ensures that your contributions are properly licensed and that the project can continue to be distributed under the AGPL-3.0 license.</p> <p>To sign the CLA, follow the instructions provided by the CLA bot after you submit your PR.</p>"},{"location":"help/contributing/#google-style-docstrings","title":"Google-Style Docstrings","text":"<p>When adding new functions or classes, please include a Google-style docstring to provide clear and concise documentation for other developers. This will help ensure that your contributions are easy to understand and maintain.</p> <p>Example Google-style docstring:</p> <pre><code>def example_function(arg1: int, arg2: str) -&gt; bool:\n\"\"\"Example function that demonstrates Google-style docstrings.\n    Args:\n        arg1 (int): The first argument.\n        arg2 (str): The second argument.\n    Returns:\n        bool: True if successful, False otherwise.\n    Raises:\n        ValueError: If `arg1` is negative or `arg2` is empty.\n    \"\"\"\nif arg1 &lt; 0 or not arg2:\nraise ValueError(\"Invalid input values\")\nreturn True\n</code></pre>"},{"location":"help/contributing/#github-actions-ci-tests","title":"GitHub Actions CI Tests","text":"<p>Before your pull request can be merged, all GitHub Actions Continuous Integration (CI) tests must pass. These tests include linting, unit tests, and other checks to ensure that your changes meet the quality standards of the project. Make sure to review the output of the GitHub Actions and fix any issues</p>"},{"location":"help/environmental-health-safety/","title":"Ultralytics Environmental, Health and Safety (EHS) Policy","text":"<p>At Ultralytics, we recognize that the long-term success of our company relies not only on the products and services we offer, but also the manner in which we conduct our business. We are committed to ensuring the safety and well-being of our employees, stakeholders, and the environment, and we will continuously strive to mitigate our impact on the environment while promoting health and safety.</p>"},{"location":"help/environmental-health-safety/#policy-principles","title":"Policy Principles","text":"<ol> <li> <p>Compliance: We will comply with all applicable laws, regulations, and standards related to EHS, and we will strive to exceed these standards where possible.</p> </li> <li> <p>Prevention: We will work to prevent accidents, injuries, and environmental harm by implementing risk management measures and ensuring all our operations and procedures are safe.</p> </li> <li> <p>Continuous Improvement: We will continuously improve our EHS performance by setting measurable objectives, monitoring our performance, auditing our operations, and revising our policies and procedures as needed.</p> </li> <li> <p>Communication: We will communicate openly about our EHS performance and will engage with stakeholders to understand and address their concerns and expectations.</p> </li> <li> <p>Education and Training: We will educate and train our employees and contractors in appropriate EHS procedures and practices.</p> </li> </ol>"},{"location":"help/environmental-health-safety/#implementation-measures","title":"Implementation Measures","text":"<ol> <li> <p>Responsibility and Accountability: Every employee and contractor working at or with Ultralytics is responsible for adhering to this policy. Managers and supervisors are accountable for ensuring this policy is implemented within their areas of control.</p> </li> <li> <p>Risk Management: We will identify, assess, and manage EHS risks associated with our operations and activities to prevent accidents, injuries, and environmental harm.</p> </li> <li> <p>Resource Allocation: We will allocate the necessary resources to ensure the effective implementation of our EHS policy, including the necessary equipment, personnel, and training.</p> </li> <li> <p>Emergency Preparedness and Response: We will develop, maintain, and test emergency preparedness and response plans to ensure we can respond effectively to EHS incidents.</p> </li> <li> <p>Monitoring and Review: We will monitor and review our EHS performance regularly to identify opportunities for improvement and ensure we are meeting our objectives.</p> </li> </ol> <p>This policy reflects our commitment to minimizing our environmental footprint, ensuring the safety and well-being of our employees, and continuously improving our performance.</p> <p>Please remember that the implementation of an effective EHS policy requires the involvement and commitment of everyone working at or with Ultralytics. We encourage you to take personal responsibility for your safety and the safety of others, and to take care of the environment in which we live and work.</p>"},{"location":"help/minimum_reproducible_example/","title":"Creating a Minimum Reproducible Example for Bug Reports in Ultralytics YOLO Repositories","text":"<p>When submitting a bug report for Ultralytics YOLO repositories, it's essential to provide a minimum reproducible example (MRE). An MRE is a small, self-contained piece of code that demonstrates the problem you're experiencing. Providing an MRE helps maintainers and contributors understand the issue and work on a fix more efficiently. This guide explains how to create an MRE when submitting bug reports to Ultralytics YOLO repositories.</p>"},{"location":"help/minimum_reproducible_example/#1-isolate-the-problem","title":"1. Isolate the Problem","text":"<p>The first step in creating an MRE is to isolate the problem. This means removing any unnecessary code or dependencies that are not directly related to the issue. Focus on the specific part of the code that is causing the problem and remove any irrelevant code.</p>"},{"location":"help/minimum_reproducible_example/#2-use-public-models-and-datasets","title":"2. Use Public Models and Datasets","text":"<p>When creating an MRE, use publicly available models and datasets to reproduce the issue. For example, use the 'yolov8n.pt' model and the 'coco8.yaml' dataset. This ensures that the maintainers and contributors can easily run your example and investigate the problem without needing access to proprietary data or custom models.</p>"},{"location":"help/minimum_reproducible_example/#3-include-all-necessary-dependencies","title":"3. Include All Necessary Dependencies","text":"<p>Make sure to include all the necessary dependencies in your MRE. If your code relies on external libraries, specify the required packages and their versions. Ideally, provide a <code>requirements.txt</code> file or list the dependencies in your bug report.</p>"},{"location":"help/minimum_reproducible_example/#4-write-a-clear-description-of-the-issue","title":"4. Write a Clear Description of the Issue","text":"<p>Provide a clear and concise description of the issue you're experiencing. Explain the expected behavior and the actual behavior you're encountering. If applicable, include any relevant error messages or logs.</p>"},{"location":"help/minimum_reproducible_example/#5-format-your-code-properly","title":"5. Format Your Code Properly","text":"<p>When submitting an MRE, format your code properly using code blocks in the issue description. This makes it easier for others to read and understand your code. In GitHub, you can create a code block by wrapping your code with triple backticks (```) and specifying the language:</p> <pre>\n<pre><code># Your Python code goes here\n</code></pre>\n</pre>"},{"location":"help/minimum_reproducible_example/#6-test-your-mre","title":"6. Test Your MRE","text":"<p>Before submitting your MRE, test it to ensure that it accurately reproduces the issue. Make sure that others can run your example without any issues or modifications.</p>"},{"location":"help/minimum_reproducible_example/#example-of-an-mre","title":"Example of an MRE","text":"<p>Here's an example of an MRE for a hypothetical bug report:</p> <p>Bug description:</p> <p>When running the <code>detect.py</code> script on the sample image from the 'coco8.yaml' dataset, I get an error related to the dimensions of the input tensor.</p> <p>MRE:</p> <pre><code>import torch\nfrom ultralytics import YOLO\n# Load the model\nmodel = YOLO(\"yolov8n.pt\")\n# Load a 0-channel image\nimage = torch.rand(1, 0, 640, 640)\n# Run the model\nresults = model(image)\n</code></pre> <p>Error message:</p> <pre><code>RuntimeError: Expected input[1, 0, 640, 640] to have 3 channels, but got 0 channels instead\n</code></pre> <p>Dependencies:</p> <ul> <li>torch==2.0.0</li> <li>ultralytics==8.0.90</li> </ul> <p>In this example, the MRE demonstrates the issue with a minimal amount of code, uses a public model ('yolov8n.pt'), includes all necessary dependencies, and provides a clear description of the problem along with the error message.</p> <p>By following these guidelines, you'll help the maintainers and contributors of Ultralytics YOLO repositories to understand and resolve your issue more efficiently.</p>"},{"location":"hub/","title":"Ultralytics HUB","text":"<p>\ud83d\udc4b Hello from the Ultralytics Team! We've been working hard these last few months to launch Ultralytics HUB, a new web tool for training and deploying all your YOLOv5 and YOLOv8 \ud83d\ude80 models from one spot!</p>"},{"location":"hub/#introduction","title":"Introduction","text":"<p>HUB is designed to be user-friendly and intuitive, with a drag-and-drop interface that allows users to easily upload their data and train new models quickly. It offers a range of pre-trained models and templates to choose from, making it easy for users to get started with training their own models. Once a model is trained, it can be easily deployed and used for real-time object detection, instance segmentation and classification tasks.</p> <p>We hope that the resources here will help you get the most out of HUB. Please browse the HUB Docs for details, raise an issue on GitHub for support, and join our Discord community for questions and discussions!</p> <ul> <li>Quickstart. Start training and deploying YOLO models with HUB in seconds.</li> <li>Datasets: Preparing and Uploading. Learn how to prepare and upload your datasets to HUB in YOLO format.</li> <li>Projects: Creating and Managing. Group your models into projects for improved organization.</li> <li>Models: Training and Exporting. Train YOLOv5 and YOLOv8 models on your custom datasets and export them to various formats for deployment.</li> <li>Integrations: Options. Explore different integration options for your trained models, such as TensorFlow, ONNX, OpenVINO, CoreML, and PaddlePaddle.</li> <li>Ultralytics HUB App. Learn about the Ultralytics App for iOS and Android, which allows you to run models directly on your mobile device.<ul> <li>iOS. Learn about YOLO CoreML models accelerated on Apple's Neural Engine on iPhones and iPads.</li> <li>Android. Explore TFLite acceleration on mobile devices.</li> </ul> </li> <li>Inference API. Understand how to use the Inference API for running your trained models in the cloud to generate predictions.</li> </ul>"},{"location":"hub/datasets/","title":"HUB Datasets","text":"<p>Ultralytics HUB datasets are a practical solution for managing and leveraging your custom datasets.</p> <p>Once uploaded, datasets can be immediately utilized for model training. This integrated approach facilitates a seamless transition from dataset management to model training, significantly simplifying the entire process.</p>"},{"location":"hub/datasets/#upload-dataset","title":"Upload Dataset","text":"<p>Ultralytics HUB datasets are just like YOLOv5 and YOLOv8 \ud83d\ude80 datasets. They use the same structure and the same label formats to keep everything simple.</p> <p>Before you upload a dataset to Ultralytics HUB, make sure to place your dataset YAML file inside the dataset root directory and that your dataset YAML, directory and ZIP have the same name, as shown in the example below, and then zip the dataset directory.</p> <p>For example, if your dataset is called \"coco8\", as our COCO8 example dataset, then you should have a <code>coco8.yaml</code> inside your <code>coco8/</code> directory, which will create a <code>coco8.zip</code> when zipped:</p> <pre><code>zip -r coco8.zip coco8\n</code></pre> <p>You can download our COCO8 example dataset and unzip it to see exactly how to structure your dataset.</p> <p> </p> <p>The dataset YAML is the same standard YOLOv5 and YOLOv8 YAML format.</p> <p>coco8.yaml</p> <pre><code># Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n# COCO8 dataset (first 8 images from COCO train2017) by Ultralytics\n# Example usage: yolo train data=coco8.yaml\n# parent\n# \u251c\u2500\u2500 ultralytics\n# \u2514\u2500\u2500 datasets\n#     \u2514\u2500\u2500 coco8  \u2190 downloads here (1 MB)\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco8  # dataset root dir\ntrain: images/train  # train images (relative to 'path') 4 images\nval: images/val  # val images (relative to 'path') 4 images\ntest:  # test images (optional)\n# Classes\nnames:\n0: person\n1: bicycle\n2: car\n3: motorcycle\n4: airplane\n5: bus\n6: train\n7: truck\n8: boat\n9: traffic light\n10: fire hydrant\n11: stop sign\n12: parking meter\n13: bench\n14: bird\n15: cat\n16: dog\n17: horse\n18: sheep\n19: cow\n20: elephant\n21: bear\n22: zebra\n23: giraffe\n24: backpack\n25: umbrella\n26: handbag\n27: tie\n28: suitcase\n29: frisbee\n30: skis\n31: snowboard\n32: sports ball\n33: kite\n34: baseball bat\n35: baseball glove\n36: skateboard\n37: surfboard\n38: tennis racket\n39: bottle\n40: wine glass\n41: cup\n42: fork\n43: knife\n44: spoon\n45: bowl\n46: banana\n47: apple\n48: sandwich\n49: orange\n50: broccoli\n51: carrot\n52: hot dog\n53: pizza\n54: donut\n55: cake\n56: chair\n57: couch\n58: potted plant\n59: bed\n60: dining table\n61: toilet\n62: tv\n63: laptop\n64: mouse\n65: remote\n66: keyboard\n67: cell phone\n68: microwave\n69: oven\n70: toaster\n71: sink\n72: refrigerator\n73: book\n74: clock\n75: vase\n76: scissors\n77: teddy bear\n78: hair drier\n79: toothbrush\n# Download script/URL (optional)\ndownload: https://ultralytics.com/assets/coco8.zip\n</code></pre> <p>After zipping your dataset, you should validate it before uploading it to Ultralytics HUB. Ultralytics HUB conducts the dataset validation check post-upload, so by ensuring your dataset is correctly formatted and error-free ahead of time, you can forestall any setbacks due to dataset rejection.</p> <pre><code>from ultralytics.hub import check_dataset\ncheck_dataset('path/to/coco8.zip')\n</code></pre> <p>Once your dataset ZIP is ready, navigate to the Datasets page by clicking on the Datasets button in the sidebar.</p> <p></p> Tip <p>You can also upload a dataset directly from the Home page.</p> <p></p> <p>Click on the Upload Dataset button on the top right of the page. This action will trigger the Upload Dataset dialog.</p> <p></p> <p>Upload your dataset in the Dataset .zip file field.</p> <p>You have the additional option to set a custom name and description for your Ultralytics HUB dataset.</p> <p>When you're happy with your dataset configuration, click Upload.</p> <p></p> <p>After your dataset is uploaded and processed, you will be able to access it from the Datasets page.</p> <p></p> <p>You can view the images in your dataset grouped by splits (Train, Validation, Test).</p> <p></p> Tip <p>Each image can be enlarged for better visualization.</p> <p></p> <p></p> <p>Also, you can analyze your dataset by click on the Overview tab.</p> <p></p> <p>Next, train a model on your dataset.</p> <p></p>"},{"location":"hub/datasets/#share-dataset","title":"Share Dataset","text":"<p>Info</p> <p>Ultralytics HUB's sharing functionality provides a convenient way to share datasets with others. This feature is designed to accommodate both existing Ultralytics HUB users and those who have yet to create an account.</p> Note <p>You have control over the general access of your datasets.</p> <p>You can choose to set the general access to \"Private\", in which case, only you will have access to it. Alternatively, you can set the general access to \"Unlisted\" which grants viewing access to anyone who has the direct link to the dataset, regardless of whether they have an Ultralytics HUB account or not.</p> <p>Navigate to the Dataset page of the dataset you want to share, open the dataset actions dropdown and click on the Share option. This action will trigger the Share Dataset dialog.</p> <p></p> Tip <p>You can also share a dataset directly from the Datasets page.</p> <p></p> <p>Set the general access to \"Unlisted\" and click Save.</p> <p></p> <p>Now, anyone who has the direct link to your dataset can view it.</p> Tip <p>You can easily click on the dataset's link shown in the Share Dataset dialog to copy it.</p> <p></p>"},{"location":"hub/datasets/#edit-dataset","title":"Edit Dataset","text":"<p>Navigate to the Dataset page of the dataset you want to edit, open the dataset actions dropdown and click on the Edit option. This action will trigger the Update Dataset dialog.</p> <p></p> Tip <p>You can also edit a dataset directly from the Datasets page.</p> <p></p> <p>Apply the desired modifications to your dataset and then confirm the changes by clicking Save.</p> <p></p>"},{"location":"hub/datasets/#delete-dataset","title":"Delete Dataset","text":"<p>Navigate to the Dataset page of the dataset you want to delete, open the dataset actions dropdown and click on the Delete option. This action will delete the dataset.</p> <p></p> Tip <p>You can also delete a dataset directly from the Datasets page.</p> <p></p> Note <p>If you change your mind, you can restore the dataset from the Trash page.</p> <p></p>"},{"location":"hub/inference_api/","title":"YOLO Inference API","text":"<p>The YOLO Inference API allows you to access the YOLOv8 object detection capabilities via a RESTful API. This enables you to run object detection on images without the need to install and set up the YOLOv8 environment locally.</p> <p> Screenshot of the Inference API section in the trained model Preview tab.</p>"},{"location":"hub/inference_api/#api-url","title":"API URL","text":"<p>The API URL is the address used to access the YOLO Inference API. In this case, the base URL is:</p> <pre><code>https://api.ultralytics.com/v1/predict\n</code></pre>"},{"location":"hub/inference_api/#example-usage-in-python","title":"Example Usage in Python","text":"<p>To access the YOLO Inference API with the specified model and API key using Python, you can use the following code:</p> <pre><code>import requests\n# API URL, use actual MODEL_ID\nurl = f\"https://api.ultralytics.com/v1/predict/MODEL_ID\"\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n# Inference arguments (optional)\ndata = {\"size\": 640, \"confidence\": 0.25, \"iou\": 0.45}\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\nfiles = {\"image\": image_file}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\n</code></pre> <p>In this example, replace <code>API_KEY</code> with your actual API key, <code>MODEL_ID</code> with the desired model ID, and <code>path/to/image.jpg</code> with the path to the image you want to analyze.</p>"},{"location":"hub/inference_api/#example-usage-with-cli","title":"Example Usage with CLI","text":"<p>You can use the YOLO Inference API with the command-line interface (CLI) by utilizing the <code>curl</code> command. Replace <code>API_KEY</code> with your actual API key, <code>MODEL_ID</code> with the desired model ID, and <code>image.jpg</code> with the path to the image you want to analyze:</p> <pre><code>curl -X POST \"https://api.ultralytics.com/v1/predict/MODEL_ID\" \\\n-H \"x-api-key: API_KEY\" \\\n-F \"image=@/path/to/image.jpg\" \\\n-F \"size=640\" \\\n-F \"confidence=0.25\" \\\n-F \"iou=0.45\"\n</code></pre>"},{"location":"hub/inference_api/#passing-arguments","title":"Passing Arguments","text":"<p>This command sends a POST request to the YOLO Inference API with the specified <code>MODEL_ID</code> in the URL and the <code>API_KEY</code> in the request <code>headers</code>, along with the image file specified by <code>@path/to/image.jpg</code>.</p> <p>Here's an example of passing the <code>size</code>, <code>confidence</code>, and <code>iou</code> arguments via the API URL using the <code>requests</code> library in Python:</p> <pre><code>import requests\n# API URL, use actual MODEL_ID\nurl = f\"https://api.ultralytics.com/v1/predict/MODEL_ID\"\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n# Inference arguments (optional)\ndata = {\"size\": 640, \"confidence\": 0.25, \"iou\": 0.45}\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\nfiles = {\"image\": image_file}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\n</code></pre> <p>In this example, the <code>data</code> dictionary contains the query arguments <code>size</code>, <code>confidence</code>, and <code>iou</code>, which tells the API to run inference at image size 640 with confidence and IoU thresholds of 0.25 and 0.45.</p> <p>This will send the query parameters along with the file in the POST request. See the table below for a full list of available inference arguments.</p> Inference Argument Default Type Notes <code>size</code> <code>640</code> <code>int</code> valid range is <code>32</code> - <code>1280</code> pixels <code>confidence</code> <code>0.25</code> <code>float</code> valid range is <code>0.01</code> - <code>1.0</code> <code>iou</code> <code>0.45</code> <code>float</code> valid range is <code>0.0</code> - <code>0.95</code> <code>url</code> <code>''</code> <code>str</code> optional image URL if not image file is passed <code>normalize</code> <code>False</code> <code>bool</code>"},{"location":"hub/inference_api/#return-json-format","title":"Return JSON format","text":"<p>The YOLO Inference API returns a JSON list with the detection results. The format of the JSON list will be the same as the one produced locally by the <code>results[0].tojson()</code> command.</p> <p>The JSON list contains information about the detected objects, their coordinates, classes, and confidence scores.</p>"},{"location":"hub/inference_api/#detect-model-format","title":"Detect Model Format","text":"<p>YOLO detection models, such as <code>yolov8n.pt</code>, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.</p> <p>Detect Model JSON Response</p> LocalCLI APIPython APIJSON Response <pre><code>from ultralytics import YOLO\n# Load model\nmodel = YOLO('yolov8n.pt')\n# Run inference\nresults = model('image.jpg')\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n</code></pre> <pre><code>curl -X POST \"https://api.ultralytics.com/v1/predict/MODEL_ID\" \\\n-H \"x-api-key: API_KEY\" \\\n-F \"image=@/path/to/image.jpg\" \\\n-F \"size=640\" \\\n-F \"confidence=0.25\" \\\n-F \"iou=0.45\"\n</code></pre> <pre><code>import requests\n# API URL, use actual MODEL_ID\nurl = f\"https://api.ultralytics.com/v1/predict/MODEL_ID\"\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n# Inference arguments (optional)\ndata = {\"size\": 640, \"confidence\": 0.25, \"iou\": 0.45}\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\nfiles = {\"image\": image_file}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\n</code></pre> <pre><code>{\n\"success\": True,\n\"message\": \"Inference complete.\",\n\"data\": [\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.8359682559967041,\n\"box\": {\n\"x1\": 0.08974208831787109,\n\"y1\": 0.27418340047200523,\n\"x2\": 0.8706787109375,\n\"y2\": 0.9887352837456598\n}\n},\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.8189555406570435,\n\"box\": {\n\"x1\": 0.5847355842590332,\n\"y1\": 0.05813225640190972,\n\"x2\": 0.8930277824401855,\n\"y2\": 0.9903111775716146\n}\n},\n{\n\"name\": \"tie\",\n\"class\": 27,\n\"confidence\": 0.2909725308418274,\n\"box\": {\n\"x1\": 0.3433395862579346,\n\"y1\": 0.6070465511745877,\n\"x2\": 0.40964522361755373,\n\"y2\": 0.9849439832899306\n}\n}\n]\n}\n</code></pre>"},{"location":"hub/inference_api/#segment-model-format","title":"Segment Model Format","text":"<p>YOLO segmentation models, such as <code>yolov8n-seg.pt</code>, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.</p> <p>Segment Model JSON Response</p> LocalCLI APIPython APIJSON Response <pre><code>from ultralytics import YOLO\n# Load model\nmodel = YOLO('yolov8n-seg.pt')\n# Run inference\nresults = model('image.jpg')\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n</code></pre> <pre><code>curl -X POST \"https://api.ultralytics.com/v1/predict/MODEL_ID\" \\\n-H \"x-api-key: API_KEY\" \\\n-F \"image=@/path/to/image.jpg\" \\\n-F \"size=640\" \\\n-F \"confidence=0.25\" \\\n-F \"iou=0.45\"\n</code></pre> <pre><code>import requests\n# API URL, use actual MODEL_ID\nurl = f\"https://api.ultralytics.com/v1/predict/MODEL_ID\"\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n# Inference arguments (optional)\ndata = {\"size\": 640, \"confidence\": 0.25, \"iou\": 0.45}\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\nfiles = {\"image\": image_file}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\n</code></pre> <p>Note <code>segments</code> <code>x</code> and <code>y</code> lengths may vary from one object to another. Larger or more complex objects may have more segment points. <pre><code>{\n\"success\": True,\n\"message\": \"Inference complete.\",\n\"data\": [\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.856913149356842,\n\"box\": {\n\"x1\": 0.1064866065979004,\n\"y1\": 0.2798851860894097,\n\"x2\": 0.8738358497619629,\n\"y2\": 0.9894873725043403\n},\n\"segments\": {\n\"x\": [\n0.421875,\n0.4203124940395355,\n0.41718751192092896\n...\n],\n\"y\": [\n0.2888889014720917,\n0.2916666567325592,\n0.2916666567325592\n...\n]\n}\n},\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.8512625694274902,\n\"box\": {\n\"x1\": 0.5757311820983887,\n\"y1\": 0.053943040635850696,\n\"x2\": 0.8960096359252929,\n\"y2\": 0.985154045952691\n},\n\"segments\": {\n\"x\": [\n0.7515624761581421,\n0.75,\n0.7437499761581421\n...\n],\n\"y\": [\n0.0555555559694767,\n0.05833333358168602,\n0.05833333358168602\n...\n]\n}\n},\n{\n\"name\": \"tie\",\n\"class\": 27,\n\"confidence\": 0.6485961675643921,\n\"box\": {\n\"x1\": 0.33911995887756347,\n\"y1\": 0.6057066175672743,\n\"x2\": 0.4081430912017822,\n\"y2\": 0.9916408962673611\n},\n\"segments\": {\n\"x\": [\n0.37187498807907104,\n0.37031251192092896,\n0.3687500059604645\n...\n],\n\"y\": [\n0.6111111044883728,\n0.6138888597488403,\n0.6138888597488403\n...\n]\n}\n}\n]\n}\n</code></pre></p>"},{"location":"hub/inference_api/#pose-model-format","title":"Pose Model Format","text":"<p>YOLO pose models, such as <code>yolov8n-pose.pt</code>, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.</p> <p>Pose Model JSON Response</p> LocalCLI APIPython APIJSON Response <pre><code>from ultralytics import YOLO\n# Load model\nmodel = YOLO('yolov8n-seg.pt')\n# Run inference\nresults = model('image.jpg')\n# Print image.jpg results in JSON format\nprint(results[0].tojson())\n</code></pre> <pre><code>curl -X POST \"https://api.ultralytics.com/v1/predict/MODEL_ID\" \\\n-H \"x-api-key: API_KEY\" \\\n-F \"image=@/path/to/image.jpg\" \\\n-F \"size=640\" \\\n-F \"confidence=0.25\" \\\n-F \"iou=0.45\"\n</code></pre> <pre><code>import requests\n# API URL, use actual MODEL_ID\nurl = f\"https://api.ultralytics.com/v1/predict/MODEL_ID\"\n# Headers, use actual API_KEY\nheaders = {\"x-api-key\": \"API_KEY\"}\n# Inference arguments (optional)\ndata = {\"size\": 640, \"confidence\": 0.25, \"iou\": 0.45}\n# Load image and send request\nwith open(\"path/to/image.jpg\", \"rb\") as image_file:\nfiles = {\"image\": image_file}\nresponse = requests.post(url, headers=headers, files=files, data=data)\nprint(response.json())\n</code></pre> <p>Note COCO-keypoints pretrained models will have 17 human keypoints. The <code>visible</code> part of the keypoints indicates whether a keypoint is visible or obscured. Obscured keypoints may be outside the image or may not be visible, i.e. a person's eyes facing away from the camera. <pre><code>{\n\"success\": True,\n\"message\": \"Inference complete.\",\n\"data\": [\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.8439509868621826,\n\"box\": {\n\"x1\": 0.1125,\n\"y1\": 0.28194444444444444,\n\"x2\": 0.7953125,\n\"y2\": 0.9902777777777778\n},\n\"keypoints\": {\n\"x\": [\n0.5058594942092896,\n0.5103894472122192,\n0.4920862317085266\n...\n],\n\"y\": [\n0.48964157700538635,\n0.4643048942089081,\n0.4465252459049225\n...\n],\n\"visible\": [\n0.8726999163627625,\n0.653947651386261,\n0.9130823612213135\n...\n]\n}\n},\n{\n\"name\": \"person\",\n\"class\": 0,\n\"confidence\": 0.7474289536476135,\n\"box\": {\n\"x1\": 0.58125,\n\"y1\": 0.0625,\n\"x2\": 0.8859375,\n\"y2\": 0.9888888888888889\n},\n\"keypoints\": {\n\"x\": [\n0.778544008731842,\n0.7976160049438477,\n0.7530890107154846\n...\n],\n\"y\": [\n0.27595141530036926,\n0.2378823608160019,\n0.23644638061523438\n...\n],\n\"visible\": [\n0.8900790810585022,\n0.789978563785553,\n0.8974530100822449\n...\n]\n}\n}\n]\n}\n</code></pre></p>"},{"location":"hub/integrations/","title":"\ud83d\udea7 Page Under Construction \u2692","text":"<p>This page is currently under construction!\ufe0f \ud83d\udc77Please check back later for updates. \ud83d\ude03\ud83d\udd1c</p>"},{"location":"hub/models/","title":"Ultralytics HUB Models","text":"<p>Ultralytics HUB models provide a streamlined solution for training vision AI models on your custom datasets.</p> <p>The process is user-friendly and efficient, involving a simple three-step creation and accelerated training powered by Utralytics YOLOv8. During training, real-time updates on model metrics are available so that you can monitor each step of the progress. Once training is completed, you can preview your model and easily deploy it to real-world applications. Therefore, Ultralytics HUB offers a comprehensive yet straightforward system for model creation, training, evaluation, and deployment.</p>"},{"location":"hub/models/#train-model","title":"Train Model","text":"<p>Navigate to the Models page by clicking on the Models button in the sidebar.</p> <p></p> Tip <p>You can also train a model directly from the Home page.</p> <p></p> <p>Click on the Train Model button on the top right of the page. This action will trigger the Train Model dialog.</p> <p></p> <p>The Train Model dialog has three simple steps, explained below.</p>"},{"location":"hub/models/#1-dataset","title":"1. Dataset","text":"<p>In this step, you have to select the dataset you want to train your model on. After you selected a dataset, click Continue.</p> <p></p> Tip <p>You can skip this step if you train a model directly from the Dataset page.</p> <p></p>"},{"location":"hub/models/#2-model","title":"2. Model","text":"<p>In this step, you have to choose the project in which you want to create your model, the name of your model and your model's architecture.</p> Note <p>Ultralytics HUB will try to pre-select the project.</p> <p>If you opened the Train Model dialog as described above, Ultralytics HUB will pre-select the last project you used.</p> <p>If you opened the Train Model dialog from the Project page, Ultralytics HUB will pre-select the project you were inside of.</p> <p></p> <p>In case you don't have a project created yet, you can set the name of your project in this step and it will be created together with your model.</p> <p></p> <p>Info</p> <p>You can read more about the available YOLOv8 (and YOLOv5) architectures in our documentation.</p> <p>When you're happy with your model configuration, click Continue.</p> <p></p> Note <p>By default, your model will use a pre-trained model (trained on the COCO dataset) to reduce training time.</p> <p>You can change this behaviour by opening the Advanced Options accordion.</p>"},{"location":"hub/models/#3-train","title":"3. Train","text":"<p>In this step, you will start training you model.</p> <p>Ultralytics HUB offers three training options:</p> <ul> <li>Ultralytics Cloud (COMING SOON)</li> <li>Google Colab</li> <li>Bring your own agent</li> </ul> <p>In order to start training your model, follow the instructions presented in this step.</p> <p></p> Note <p>When you are on this step, before the training starts, you can change the default training configuration by opening the Advanced Options accordion.</p> <p></p> Note <p>When you are on this step, you have the option to close the Train Model dialog and start training your model from the Model page later.</p> <p></p> <p>To start training your model using Google Colab, simply follow the instructions shown above or on the Google Colab notebook.</p> <p> </p> <p>When the training starts, you can click Done and monitor the training progress on the Model page.</p> <p></p> <p></p> Note <p>In case the training stops and a checkpoint was saved, you can resume training your model from the Model page.</p> <p></p>"},{"location":"hub/models/#preview-model","title":"Preview Model","text":"<p>Ultralytics HUB offers a variety of ways to preview your trained model.</p> <p>You can preview your model if you click on the Preview tab and upload an image in the Test card.</p> <p></p> <p>You can also use our Ultralytics Cloud API to effortlessly run inference with your custom model.</p> <p></p> <p>Furthermore, you can preview your model in real-time directly on your iOS or Android mobile device by downloading our Ultralytics HUB Mobile Application.</p> <p></p>"},{"location":"hub/models/#deploy-model","title":"Deploy Model","text":"<p>You can export your model to 13 different formats, including ONNX, OpenVINO, CoreML, TensorFlow, Paddle and many others.</p> <p></p> Tip <p>You can customize the export options of each format if you open the export actions dropdown and click on the Advanced option.</p> <p></p>"},{"location":"hub/models/#share-model","title":"Share Model","text":"<p>Info</p> <p>Ultralytics HUB's sharing functionality provides a convenient way to share models with others. This feature is designed to accommodate both existing Ultralytics HUB users and those who have yet to create an account.</p> Note <p>You have control over the general access of your models.</p> <p>You can choose to set the general access to \"Private\", in which case, only you will have access to it. Alternatively, you can set the general access to \"Unlisted\" which grants viewing access to anyone who has the direct link to the model, regardless of whether they have an Ultralytics HUB account or not.</p> <p>Navigate to the Model page of the model you want to share, open the model actions dropdown and click on the Share option. This action will trigger the Share Model dialog.</p> <p></p> Tip <p>You can also share a model directly from the Models page or from the Project page of the project where your model is located.</p> <p></p> <p>Set the general access to \"Unlisted\" and click Save.</p> <p></p> <p>Now, anyone who has the direct link to your model can view it.</p> Tip <p>You can easily click on the models's link shown in the Share Model dialog to copy it.</p> <p></p>"},{"location":"hub/models/#edit-model","title":"Edit Model","text":"<p>Navigate to the Model page of the model you want to edit, open the model actions dropdown and click on the Edit option. This action will trigger the Update Model dialog.</p> <p></p> Tip <p>You can also edit a model directly from the Models page or from the Project page of the project where your model is located.</p> <p></p> <p>Apply the desired modifications to your model and then confirm the changes by clicking Save.</p> <p></p>"},{"location":"hub/models/#delete-model","title":"Delete Model","text":"<p>Navigate to the Model page of the model you want to delete, open the model actions dropdown and click on the Delete option. This action will delete the model.</p> <p></p> Tip <p>You can also delete a model directly from the Models page or from the Project page of the project where your model is located.</p> <p></p> Note <p>If you change your mind, you can restore the model from the Trash page.</p> <p></p>"},{"location":"hub/projects/","title":"Ultralytics HUB Projects","text":"<p>Ultralytics HUB projects provide an effective solution for consolidating and managing your models. If you are working with several models that perform similar tasks or have related purposes, Ultralytics HUB projects allow you to group these models together.</p> <p>This creates a unified and organized workspace that facilitates easier model management, comparison and development. Having similar models or various iterations together can facilitate rapid benchmarking, as you can compare their effectiveness. This can lead to faster, more insightful iterative development and refinement of your models.</p>"},{"location":"hub/projects/#create-project","title":"Create Project","text":"<p>Navigate to the Projects page by clicking on the Projects button in the sidebar.</p> <p></p> Tip <p>You can also create a project directly from the Home page.</p> <p></p> <p>Click on the Create Project button on the top right of the page. This action will trigger the Create Project dialog, opening up a suite of options for tailoring your project to your needs.</p> <p></p> <p>Type the name of your project in the Project name field or keep the default name and finalize the project creation with a single click.</p> <p>You have the additional option to enrich your project with a description and a unique image, enhancing its recognizability on the Projects page.</p> <p>When you're happy with your project configuration, click Create.</p> <p></p> <p>After your project is created, you will be able to access it from the Projects page.</p> <p></p> <p>Next, train a model inside your project.</p> <p></p>"},{"location":"hub/projects/#share-project","title":"Share Project","text":"<p>Info</p> <p>Ultralytics HUB's sharing functionality provides a convenient way to share projects with others. This feature is designed to accommodate both existing Ultralytics HUB users and those who have yet to create an account.</p> Note <p>You have control over the general access of your projects.</p> <p>You can choose to set the general access to \"Private\", in which case, only you will have access to it. Alternatively, you can set the general access to \"Unlisted\" which grants viewing access to anyone who has the direct link to the project, regardless of whether they have an Ultralytics HUB account or not.</p> <p>Navigate to the Project page of the project you want to share, open the project actions dropdown and click on the Share option. This action will trigger the Share Project dialog.</p> <p></p> Tip <p>You can also share a project directly from the Projects page.</p> <p></p> <p>Set the general access to \"Unlisted\" and click Save.</p> <p></p> <p>Warning</p> <p>When changing the general access of a project, the general access of the models inside the project will be changed as well.</p> <p>Now, anyone who has the direct link to your project can view it.</p> Tip <p>You can easily click on the project's link shown in the Share Project dialog to copy it.</p> <p></p>"},{"location":"hub/projects/#edit-project","title":"Edit Project","text":"<p>Navigate to the Project page of the project you want to edit, open the project actions dropdown and click on the Edit option. This action will trigger the Update Project dialog.</p> <p></p> Tip <p>You can also edit a project directly from the Projects page.</p> <p></p> <p>Apply the desired modifications to your project and then confirm the changes by clicking Save.</p> <p></p>"},{"location":"hub/projects/#delete-project","title":"Delete Project","text":"<p>Navigate to the Project page of the project you want to delete, open the project actions dropdown and click on the Delete option. This action will delete the project.</p> <p></p> Tip <p>You can also delete a project directly from the Projects page.</p> <p></p> <p>Warning</p> <p>When deleting a project, the the models inside the project will be deleted as well.</p> Note <p>If you change your mind, you can restore the project from the Trash page.</p> <p></p>"},{"location":"hub/projects/#compare-models","title":"Compare Models","text":"<p>Navigate to the Project page of the project where the models you want to compare are located. To use the model comparison feature, click on the Charts tab.</p> <p></p> <p>This will display all the relevant charts. Each chart corresponds to a different metric and contains the performance of each model for that metric. The models are represented by different colors and you can hover over each data point to get more information.</p> <p></p> Tip <p>Each chart can be enlarged for better visualization.</p> <p></p> <p></p> Tip <p>You have the flexibility to customize your view by selectively hiding certain models. This feature allows you to concentrate on the models of interest.</p> <p></p>"},{"location":"hub/projects/#reorder-models","title":"Reorder Models","text":"Note <p>Ultralytics HUB's reordering functionality works only inside projects you own.</p> <p>Navigate to the Project page of the project where the models you want to reorder are located. Click on the designated reorder icon of the model you want to move and drag it to the desired location.</p> <p></p>"},{"location":"hub/projects/#transfer-models","title":"Transfer Models","text":"<p>Navigate to the Project page of the project where the model you want to mode is located, open the project actions dropdown and click on the Transfer option. This action will trigger the Transfer Model dialog.</p> <p></p> Tip <p>You can also transfer a model directly from the Models page.</p> <p></p> <p>Select the project you want to transfer the model to and click Save.</p> <p></p>"},{"location":"hub/quickstart/","title":"\ud83d\udea7 Page Under Construction \u2692","text":"<p>This page is currently under construction!\ufe0f \ud83d\udc77Please check back later for updates. \ud83d\ude03\ud83d\udd1c</p>"},{"location":"hub/app/","title":"Ultralytics HUB App","text":"<p>Welcome to the Ultralytics HUB App! We are excited to introduce this powerful mobile app that allows you to run YOLOv5 and YOLOv8 models directly on your iOS and Android devices. With the HUB App, you can utilize hardware acceleration features like Apple's Neural Engine (ANE) or Android GPU and Neural Network API (NNAPI) delegates to achieve impressive performance on your mobile device.</p>"},{"location":"hub/app/#features","title":"Features","text":"<ul> <li>Run YOLOv5 and YOLOv8 models: Experience the power of YOLO models on your mobile device for real-time object detection and image recognition tasks.</li> <li>Hardware Acceleration: Benefit from Apple ANE on iOS devices or Android GPU and NNAPI delegates for optimized performance.</li> <li>Custom Model Training: Train custom models with the Ultralytics HUB platform and preview them live using the HUB App.</li> <li>Mobile Compatibility: The HUB App supports both iOS and Android devices, bringing the power of YOLO models to a wide range of users.</li> </ul>"},{"location":"hub/app/#app-documentation","title":"App Documentation","text":"<ul> <li>iOS: Learn about YOLO CoreML models accelerated on Apple's Neural Engine for iPhones and iPads.</li> <li>Android: Explore TFLite acceleration on Android mobile devices.</li> </ul> <p>Get started today by downloading the Ultralytics HUB App on your mobile device and unlock the potential of YOLOv5 and YOLOv8 models on-the-go. Don't forget to check out our comprehensive HUB Docs for more information on training, deploying, and using your custom models with the Ultralytics HUB platform.</p>"},{"location":"hub/app/android/","title":"Ultralytics Android App: Real-time Object Detection with YOLO Models","text":"<p>The Ultralytics Android App is a powerful tool that allows you to run YOLO models directly on your Android device for real-time object detection. This app utilizes TensorFlow Lite for model optimization and various hardware delegates for acceleration, enabling fast and efficient object detection.</p>"},{"location":"hub/app/android/#quantization-and-acceleration","title":"Quantization and Acceleration","text":"<p>To achieve real-time performance on your Android device, YOLO models are quantized to either FP16 or INT8 precision. Quantization is a process that reduces the numerical precision of the model's weights and biases, thus reducing the model's size and the amount of computation required. This results in faster inference times without significantly affecting the model's accuracy.</p>"},{"location":"hub/app/android/#fp16-quantization","title":"FP16 Quantization","text":"<p>FP16 (or half-precision) quantization converts the model's 32-bit floating-point numbers to 16-bit floating-point numbers. This reduces the model's size by half and speeds up the inference process, while maintaining a good balance between accuracy and performance.</p>"},{"location":"hub/app/android/#int8-quantization","title":"INT8 Quantization","text":"<p>INT8 (or 8-bit integer) quantization further reduces the model's size and computation requirements by converting its 32-bit floating-point numbers to 8-bit integers. This quantization method can result in a significant speedup, but it may lead to a slight reduction in mean average precision (mAP) due to the lower numerical precision.</p> <p>mAP Reduction in INT8 Models</p> <p>The reduced numerical precision in INT8 models can lead to some loss of information during the quantization process, which may result in a slight decrease in mAP. However, this trade-off is often acceptable considering the substantial performance gains offered by INT8 quantization.</p>"},{"location":"hub/app/android/#delegates-and-performance-variability","title":"Delegates and Performance Variability","text":"<p>Different delegates are available on Android devices to accelerate model inference. These delegates include CPU, GPU, Hexagon and NNAPI. The performance of these delegates varies depending on the device's hardware vendor, product line, and specific chipsets used in the device.</p> <ol> <li>CPU: The default option, with reasonable performance on most devices.</li> <li>GPU: Utilizes the device's GPU for faster inference. It can provide a significant performance boost on devices with powerful GPUs.</li> <li>Hexagon: Leverages Qualcomm's Hexagon DSP for faster and more efficient processing. This option is available on devices with Qualcomm Snapdragon processors.</li> <li>NNAPI: The Android Neural Networks API (NNAPI) serves as an abstraction layer for running ML models on Android devices. NNAPI can utilize various hardware accelerators, such as CPU, GPU, and dedicated AI chips (e.g., Google's Edge TPU, or the Pixel Neural Core).</li> </ol> <p>Here's a table showing the primary vendors, their product lines, popular devices, and supported delegates:</p> Vendor Product Lines Popular Devices Delegates Supported Qualcomm Snapdragon (e.g., 800 series) Samsung Galaxy S21, OnePlus 9, Google Pixel 6 CPU, GPU, Hexagon, NNAPI Samsung Exynos (e.g., Exynos 2100) Samsung Galaxy S21 (Global version) CPU, GPU, NNAPI MediaTek Dimensity (e.g., Dimensity 1200) Realme GT, Xiaomi Redmi Note CPU, GPU, NNAPI HiSilicon Kirin (e.g., Kirin 990) Huawei P40 Pro, Huawei Mate 30 Pro CPU, GPU, NNAPI NVIDIA Tegra (e.g., Tegra X1) NVIDIA Shield TV, Nintendo Switch CPU, GPU, NNAPI <p>Please note that the list of devices mentioned is not exhaustive and may vary depending on the specific chipsets and device models. Always test your models on your target devices to ensure compatibility and optimal performance.</p> <p>Keep in mind that the choice of delegate can affect performance and model compatibility. For example, some models may not work with certain delegates, or a delegate may not be available on a specific device. As such, it's essential to test your model and the chosen delegate on your target devices for the best results.</p>"},{"location":"hub/app/android/#getting-started-with-the-ultralytics-android-app","title":"Getting Started with the Ultralytics Android App","text":"<p>To get started with the Ultralytics Android App, follow these steps:</p> <ol> <li> <p>Download the Ultralytics App from the Google Play Store.</p> </li> <li> <p>Launch the app on your Android device and sign in with your Ultralytics account. If you don't have an account yet, create one here.</p> </li> <li> <p>Once signed in, you will see a list of your trained YOLO models. Select a model to use for object detection.</p> </li> <li> <p>Grant the app permission to access your device's camera.</p> </li> <li> <p>Point your device's camera at objects you want to detect. The app will display bounding boxes and class labels in real-time as it detects objects.</p> </li> <li> <p>Explore the app's settings to adjust the detection threshold, enable or disable specific object classes, and more.</p> </li> </ol> <p>With the Ultralytics Android App, you now have the power of real-time object detection using YOLO models right at your fingertips. Enjoy exploring the app's features and optimizing its settings to suit your specific use cases.</p>"},{"location":"hub/app/ios/","title":"Ultralytics iOS App: Real-time Object Detection with YOLO Models","text":"<p>The Ultralytics iOS App is a powerful tool that allows you to run YOLO models directly on your iPhone or iPad for real-time object detection. This app utilizes the Apple Neural Engine and Core ML for model optimization and acceleration, enabling fast and efficient object detection.</p>"},{"location":"hub/app/ios/#quantization-and-acceleration","title":"Quantization and Acceleration","text":"<p>To achieve real-time performance on your iOS device, YOLO models are quantized to either FP16 or INT8 precision. Quantization is a process that reduces the numerical precision of the model's weights and biases, thus reducing the model's size and the amount of computation required. This results in faster inference times without significantly affecting the model's accuracy.</p>"},{"location":"hub/app/ios/#fp16-quantization","title":"FP16 Quantization","text":"<p>FP16 (or half-precision) quantization converts the model's 32-bit floating-point numbers to 16-bit floating-point numbers. This reduces the model's size by half and speeds up the inference process, while maintaining a good balance between accuracy and performance.</p>"},{"location":"hub/app/ios/#int8-quantization","title":"INT8 Quantization","text":"<p>INT8 (or 8-bit integer) quantization further reduces the model's size and computation requirements by converting its 32-bit floating-point numbers to 8-bit integers. This quantization method can result in a significant speedup, but it may lead to a slight reduction in accuracy.</p>"},{"location":"hub/app/ios/#apple-neural-engine","title":"Apple Neural Engine","text":"<p>The Apple Neural Engine (ANE) is a dedicated hardware component integrated into Apple's A-series and M-series chips. It's designed to accelerate machine learning tasks, particularly for neural networks, allowing for faster and more efficient execution of your YOLO models.</p> <p>By combining quantized YOLO models with the Apple Neural Engine, the Ultralytics iOS App achieves real-time object detection on your iOS device without compromising on accuracy or performance.</p> Release Year iPhone Name Chipset Name Node Size ANE TOPs 2017 iPhone X A11 Bionic 10 nm 0.6 2018 iPhone XS A12 Bionic 7 nm 5 2019 iPhone 11 A13 Bionic 7 nm 6 2020 iPhone 12 A14 Bionic 5 nm 11 2021 iPhone 13 A15 Bionic 5 nm 15.8 2022 iPhone 14 A16 Bionic 4 nm 17.0 <p>Please note that this list only includes iPhone models from 2017 onwards, and the ANE TOPs values are approximate.</p>"},{"location":"hub/app/ios/#getting-started-with-the-ultralytics-ios-app","title":"Getting Started with the Ultralytics iOS App","text":"<p>To get started with the Ultralytics iOS App, follow these steps:</p> <ol> <li> <p>Download the Ultralytics App from the App Store.</p> </li> <li> <p>Launch the app on your iOS device and sign in with your Ultralytics account. If you don't have an account yet, create one here.</p> </li> <li> <p>Once signed in, you will see a list of your trained YOLO models. Select a model to use for object detection.</p> </li> <li> <p>Grant the app permission to access your device's camera.</p> </li> <li> <p>Point your device's camera at objects you want to detect. The app will display bounding boxes and class labels in real-time as it detects objects.</p> </li> <li> <p>Explore the app's settings to adjust the detection threshold, enable or disable specific object classes, and more.</p> </li> </ol> <p>With the Ultralytics iOS App, you can now leverage the power of YOLO models for real-time object detection on your iPhone or iPad, powered by the Apple Neural Engine and optimized with FP16 or INT8 quantization.</p>"},{"location":"integrations/","title":"Ultralytics Integrations","text":"<p>Welcome to the Ultralytics Integrations page! This page provides an overview of our partnerships with various tools and platforms, designed to streamline your machine learning workflows, enhance dataset management, simplify model training, and facilitate efficient deployment.</p> <p></p>"},{"location":"integrations/#datasets-integrations","title":"Datasets Integrations","text":"<ul> <li>Roboflow: Facilitate seamless dataset management for Ultralytics models, offering robust annotation, preprocessing, and augmentation capabilities.</li> </ul>"},{"location":"integrations/#training-integrations","title":"Training Integrations","text":"<ul> <li> <p>Comet ML: Enhance your model development with Ultralytics by tracking, comparing, and optimizing your machine learning experiments.</p> </li> <li> <p>ClearML: Automate your Ultralytics ML workflows, monitor experiments, and foster team collaboration.</p> </li> <li> <p>DVC: Implement version control for your Ultralytics machine learning projects, synchronizing data, code, and models effectively.</p> </li> <li> <p>Ultralytics HUB: Access and contribute to a community of pre-trained Ultralytics models.</p> </li> <li> <p>MLFlow: Streamline the entire ML lifecycle of Ultralytics models, from experimentation and reproducibility to deployment.</p> </li> <li> <p>Neptune: Maintain a comprehensive log of your ML experiments with Ultralytics in this metadata store designed for MLOps.</p> </li> <li> <p>Ray Tune: Optimize the hyperparameters of your Ultralytics models at any scale.</p> </li> <li> <p>TensorBoard: Visualize your Ultralytics ML workflows, monitor model metrics, and foster team collaboration.</p> </li> <li> <p>Weights &amp; Biases (W&amp;B): Monitor experiments, visualize metrics, and foster reproducibility and collaboration on Ultralytics projects.</p> </li> </ul>"},{"location":"integrations/#deployment-integrations","title":"Deployment Integrations","text":"<ul> <li>Neural Magic: Leverage Quantization Aware Training (QAT) and pruning techniques to optimize Ultralytics models for superior performance and leaner size.</li> </ul>"},{"location":"integrations/#export-formats","title":"Export Formats","text":"<p>We also support a variety of model export formats for deployment in different environments. Here are the available formats:</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> NCNN <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Explore the links to learn more about each integration and how to get the most out of them with Ultralytics.</p>"},{"location":"integrations/openvino/","title":"OpenVINO","text":"<p>Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this guide, we specifically cover exporting to OpenVINO, which can provide up to 3x CPU speedup as well as accelerating on other Intel hardware (iGPU, dGPU, VPU, etc.).</p> <p>OpenVINO, short for Open Visual Inference &amp; Neural Network Optimization toolkit, is a comprehensive toolkit for optimizing and deploying AI inference models. Even though the name contains Visual, OpenVINO also supports various additional tasks including language, audio, time series, etc.</p>"},{"location":"integrations/openvino/#usage-examples","title":"Usage Examples","text":"<p>Export a YOLOv8n model to OpenVINO format and run inference with the exported model.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a YOLOv8n PyTorch model\nmodel = YOLO('yolov8n.pt')\n# Export the model\nmodel.export(format='openvino')  # creates 'yolov8n_openvino_model/'\n# Load the exported OpenVINO model\nov_model = YOLO('yolov8n_openvino_model/')\n# Run inference\nresults = ov_model('https://ultralytics.com/images/bus.jpg')\n</code></pre> <pre><code># Export a YOLOv8n PyTorch model to OpenVINO format\nyolo export model=yolov8n.pt format=openvino  # creates 'yolov8n_openvino_model/'\n# Run inference with the exported model\nyolo predict model=yolov8n_openvino_model source='https://ultralytics.com/images/bus.jpg'\n</code></pre>"},{"location":"integrations/openvino/#arguments","title":"Arguments","text":"Key Value Description <code>format</code> <code>'openvino'</code> format to export to <code>imgsz</code> <code>640</code> image size as scalar or (h, w) list, i.e. (640, 480) <code>half</code> <code>False</code> FP16 quantization"},{"location":"integrations/openvino/#benefits-of-openvino","title":"Benefits of OpenVINO","text":"<ol> <li>Performance: OpenVINO delivers high-performance inference by utilizing the power of Intel CPUs, integrated and discrete GPUs, and FPGAs.</li> <li>Support for Heterogeneous Execution: OpenVINO provides an API to write once and deploy on any supported Intel hardware (CPU, GPU, FPGA, VPU, etc.).</li> <li>Model Optimizer: OpenVINO provides a Model Optimizer that imports, converts, and optimizes models from popular deep learning frameworks such as PyTorch, TensorFlow, TensorFlow Lite, Keras, ONNX, PaddlePaddle, and Caffe.</li> <li>Ease of Use: The toolkit comes with more than 80 tutorial notebooks (including YOLOv8 optimization) teaching different aspects of the toolkit.</li> </ol>"},{"location":"integrations/openvino/#openvino-export-structure","title":"OpenVINO Export Structure","text":"<p>When you export a model to OpenVINO format, it results in a directory containing the following:</p> <ol> <li>XML file: Describes the network topology.</li> <li>BIN file: Contains the weights and biases binary data.</li> <li>Mapping file: Holds mapping of original model output tensors to OpenVINO tensor names.</li> </ol> <p>You can use these files to run inference with the OpenVINO Inference Engine.</p>"},{"location":"integrations/openvino/#using-openvino-export-in-deployment","title":"Using OpenVINO Export in Deployment","text":"<p>Once you have the OpenVINO files, you can use the OpenVINO Runtime to run the model. The Runtime provides a unified API to inference across all supported Intel hardware. It also provides advanced capabilities like load balancing across Intel hardware and asynchronous execution. For more information on running the inference, refer to the Inference with OpenVINO Runtime Guide.</p> <p>Remember, you'll need the XML and BIN files as well as any application-specific settings like input size, scale factor for normalization, etc., to correctly set up and use the model with the Runtime.</p> <p>In your deployment application, you would typically do the following steps:</p> <ol> <li>Initialize OpenVINO by creating <code>core = Core()</code>.</li> <li>Load the model using the <code>core.read_model()</code> method.</li> <li>Compile the model using the <code>core.compile_model()</code> function.</li> <li>Prepare the input (image, text, audio, etc.).</li> <li>Run inference using <code>compiled_model(input_data)</code>.</li> </ol> <p>For more detailed steps and code snippets, refer to the OpenVINO documentation or API tutorial.</p>"},{"location":"integrations/openvino/#openvino-yolov8-benchmarks","title":"OpenVINO YOLOv8 Benchmarks","text":"<p>YOLOv8 benchmarks below were run by the Ultralytics team on 4 different model formats measuring speed and accuracy: PyTorch, TorchScript, ONNX and OpenVINO. Benchmarks were run on Intel Flex and Arc GPUs, and on Intel Xeon CPUs at FP32 precision (with the <code>half=False</code> argument).</p> <p>Note</p> <p>The benchmarking results below are for reference and might vary based on the exact hardware and software configuration of a system, as well as the current workload of the system at the time the benchmarks are run.</p> <p>All benchmarks run with <code>openvino</code> python package version 2023.0.1.</p>"},{"location":"integrations/openvino/#intel-flex-gpu","title":"Intel Flex GPU","text":"<p>The Intel\u00ae Data Center GPU Flex Series is a versatile and robust solution designed for the intelligent visual cloud. This GPU supports a wide array of workloads including media streaming, cloud gaming, AI visual inference, and virtual desktop Infrastructure workloads. It stands out for its open architecture and built-in support for the AV1 encode, providing a standards-based software stack for high-performance, cross-architecture applications. The Flex Series GPU is optimized for density and quality, offering high reliability, availability, and scalability.</p> <p>Benchmarks below run on Intel\u00ae Data Center GPU Flex 170 at FP32 precision.</p> Model Format Status Size (MB) mAP50-95(B) Inference time (ms/im) YOLOv8n PyTorch \u2705 6.2 0.3709 21.79 YOLOv8n TorchScript \u2705 12.4 0.3704 23.24 YOLOv8n ONNX \u2705 12.2 0.3704 37.22 YOLOv8n OpenVINO \u2705 12.3 0.3703 3.29 YOLOv8s PyTorch \u2705 21.5 0.4471 31.89 YOLOv8s TorchScript \u2705 42.9 0.4472 32.71 YOLOv8s ONNX \u2705 42.8 0.4472 43.42 YOLOv8s OpenVINO \u2705 42.9 0.4470 3.92 YOLOv8m PyTorch \u2705 49.7 0.5013 50.75 YOLOv8m TorchScript \u2705 99.2 0.4999 47.90 YOLOv8m ONNX \u2705 99.0 0.4999 63.16 YOLOv8m OpenVINO \u2705 49.8 0.4997 7.11 YOLOv8l PyTorch \u2705 83.7 0.5293 77.45 YOLOv8l TorchScript \u2705 167.2 0.5268 85.71 YOLOv8l ONNX \u2705 166.8 0.5268 88.94 YOLOv8l OpenVINO \u2705 167.0 0.5264 9.37 YOLOv8x PyTorch \u2705 130.5 0.5404 100.09 YOLOv8x TorchScript \u2705 260.7 0.5371 114.64 YOLOv8x ONNX \u2705 260.4 0.5371 110.32 YOLOv8x OpenVINO \u2705 260.6 0.5367 15.02 <p>This table represents the benchmark results for five different models (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l, YOLOv8x) across four different formats (PyTorch, TorchScript, ONNX, OpenVINO), giving us the status, size, mAP50-95(B) metric, and inference time for each combination.</p>"},{"location":"integrations/openvino/#intel-arc-gpu","title":"Intel Arc GPU","text":"<p>Intel\u00ae Arc\u2122 represents Intel's foray into the dedicated GPU market. The Arc\u2122 series, designed to compete with leading GPU manufacturers like AMD and Nvidia, caters to both the laptop and desktop markets. The series includes mobile versions for compact devices like laptops, and larger, more powerful versions for desktop computers.</p> <p>The Arc\u2122 series is divided into three categories: Arc\u2122 3, Arc\u2122 5, and Arc\u2122 7, with each number indicating the performance level. Each category includes several models, and the 'M' in the GPU model name signifies a mobile, integrated variant.</p> <p>Early reviews have praised the Arc\u2122 series, particularly the integrated A770M GPU, for its impressive graphics performance. The availability of the Arc\u2122 series varies by region, and additional models are expected to be released soon. Intel\u00ae Arc\u2122 GPUs offer high-performance solutions for a range of computing needs, from gaming to content creation.</p> <p>Benchmarks below run on Intel\u00ae Arc 770 GPU at FP32 precision.</p> Model Format Status Size (MB) metrics/mAP50-95(B) Inference time (ms/im) YOLOv8n PyTorch \u2705 6.2 0.3709 88.79 YOLOv8n TorchScript \u2705 12.4 0.3704 102.66 YOLOv8n ONNX \u2705 12.2 0.3704 57.98 YOLOv8n OpenVINO \u2705 12.3 0.3703 8.52 YOLOv8s PyTorch \u2705 21.5 0.4471 189.83 YOLOv8s TorchScript \u2705 42.9 0.4472 227.58 YOLOv8s ONNX \u2705 42.7 0.4472 142.03 YOLOv8s OpenVINO \u2705 42.9 0.4469 9.19 YOLOv8m PyTorch \u2705 49.7 0.5013 411.64 YOLOv8m TorchScript \u2705 99.2 0.4999 517.12 YOLOv8m ONNX \u2705 98.9 0.4999 298.68 YOLOv8m OpenVINO \u2705 99.1 0.4996 12.55 YOLOv8l PyTorch \u2705 83.7 0.5293 725.73 YOLOv8l TorchScript \u2705 167.1 0.5268 892.83 YOLOv8l ONNX \u2705 166.8 0.5268 576.11 YOLOv8l OpenVINO \u2705 167.0 0.5262 17.62 YOLOv8x PyTorch \u2705 130.5 0.5404 988.92 YOLOv8x TorchScript \u2705 260.7 0.5371 1186.42 YOLOv8x ONNX \u2705 260.4 0.5371 768.90 YOLOv8x OpenVINO \u2705 260.6 0.5367 19"},{"location":"integrations/openvino/#intel-xeon-cpu","title":"Intel Xeon CPU","text":"<p>The Intel\u00ae Xeon\u00ae CPU is a high-performance, server-grade processor designed for complex and demanding workloads. From high-end cloud computing and virtualization to artificial intelligence and machine learning applications, Xeon\u00ae CPUs provide the power, reliability, and flexibility required for today's data centers.</p> <p>Notably, Xeon\u00ae CPUs deliver high compute density and scalability, making them ideal for both small businesses and large enterprises. By choosing Intel\u00ae Xeon\u00ae CPUs, organizations can confidently handle their most demanding computing tasks and foster innovation while maintaining cost-effectiveness and operational efficiency.</p> <p>Benchmarks below run on 4th Gen Intel\u00ae Xeon\u00ae Scalable CPU at FP32 precision.</p> Model Format Status Size (MB) metrics/mAP50-95(B) Inference time (ms/im) YOLOv8n PyTorch \u2705 6.2 0.3709 24.36 YOLOv8n TorchScript \u2705 12.4 0.3704 23.93 YOLOv8n ONNX \u2705 12.2 0.3704 39.86 YOLOv8n OpenVINO \u2705 12.3 0.3704 11.34 YOLOv8s PyTorch \u2705 21.5 0.4471 33.77 YOLOv8s TorchScript \u2705 42.9 0.4472 34.84 YOLOv8s ONNX \u2705 42.8 0.4472 43.23 YOLOv8s OpenVINO \u2705 42.9 0.4471 13.86 YOLOv8m PyTorch \u2705 49.7 0.5013 53.91 YOLOv8m TorchScript \u2705 99.2 0.4999 53.51 YOLOv8m ONNX \u2705 99.0 0.4999 64.16 YOLOv8m OpenVINO \u2705 99.1 0.4996 28.79 YOLOv8l PyTorch \u2705 83.7 0.5293 75.78 YOLOv8l TorchScript \u2705 167.2 0.5268 79.13 YOLOv8l ONNX \u2705 166.8 0.5268 88.45 YOLOv8l OpenVINO \u2705 167.0 0.5263 56.23 YOLOv8x PyTorch \u2705 130.5 0.5404 96.60 YOLOv8x TorchScript \u2705 260.7 0.5371 114.28 YOLOv8x ONNX \u2705 260.4 0.5371 111.02 YOLOv8x OpenVINO \u2705 260.6 0.5371 83.28"},{"location":"integrations/openvino/#intel-core-cpu","title":"Intel Core CPU","text":"<p>The Intel\u00ae Core\u00ae series is a range of high-performance processors by Intel. The lineup includes Core i3 (entry-level), Core i5 (mid-range), Core i7 (high-end), and Core i9 (extreme performance). Each series caters to different computing needs and budgets, from everyday tasks to demanding professional workloads. With each new generation, improvements are made to performance, energy efficiency, and features.</p> <p>Benchmarks below run on 13th Gen Intel\u00ae Core\u00ae i7-13700H CPU at FP32 precision.</p> Model Format Status Size (MB) metrics/mAP50-95(B) Inference time (ms/im) YOLOv8n PyTorch \u2705 6.2 0.4478 104.61 YOLOv8n TorchScript \u2705 12.4 0.4525 112.39 YOLOv8n ONNX \u2705 12.2 0.4525 28.02 YOLOv8n OpenVINO \u2705 12.3 0.4504 23.53 YOLOv8s PyTorch \u2705 21.5 0.5885 194.83 YOLOv8s TorchScript \u2705 43.0 0.5962 202.01 YOLOv8s ONNX \u2705 42.8 0.5962 65.74 YOLOv8s OpenVINO \u2705 42.9 0.5966 38.66 YOLOv8m PyTorch \u2705 49.7 0.6101 355.23 YOLOv8m TorchScript \u2705 99.2 0.6120 424.78 YOLOv8m ONNX \u2705 99.0 0.6120 173.39 YOLOv8m OpenVINO \u2705 99.1 0.6091 69.80 YOLOv8l PyTorch \u2705 83.7 0.6591 593.00 YOLOv8l TorchScript \u2705 167.2 0.6580 697.54 YOLOv8l ONNX \u2705 166.8 0.6580 342.15 YOLOv8l OpenVINO \u2705 167.0 0.0708 117.69 YOLOv8x PyTorch \u2705 130.5 0.6651 804.65 YOLOv8x TorchScript \u2705 260.8 0.6650 921.46 YOLOv8x ONNX \u2705 260.4 0.6650 526.66 YOLOv8x OpenVINO \u2705 260.6 0.6619 158.73"},{"location":"integrations/openvino/#reproduce-our-results","title":"Reproduce Our Results","text":"<p>To reproduce the Ultralytics benchmarks above on all export formats run this code:</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a YOLOv8n PyTorch model\nmodel = YOLO('yolov8n.pt')\n# Benchmark YOLOv8n speed and accuracy on the COCO128 dataset for all all export formats\nresults= model.benchmarks(data='coco128.yaml')\n</code></pre> <pre><code># Benchmark YOLOv8n speed and accuracy on the COCO128 dataset for all all export formats\nyolo benchmark model=yolov8n.pt data=coco128.yaml\n</code></pre> <p>Note that benchmarking results might vary based on the exact hardware and software configuration of a system, as well as the current workload of the system at the time the benchmarks are run. For the most reliable results use a dataset with a large number of images, i.e. <code>data='coco128.yaml' (128 val images), or</code>data='coco.yaml'` (5000 val images).</p>"},{"location":"integrations/openvino/#conclusion","title":"Conclusion","text":"<p>The benchmarking results clearly demonstrate the benefits of exporting the YOLOv8 model to the OpenVINO format. Across different models and hardware platforms, the OpenVINO format consistently outperforms other formats in terms of inference speed while maintaining comparable accuracy.</p> <p>For the Intel\u00ae Data Center GPU Flex Series, the OpenVINO format was able to deliver inference speeds almost 10 times faster than the original PyTorch format. On the Xeon CPU, the OpenVINO format was twice as fast as the PyTorch format. The accuracy of the models remained nearly identical across the different formats.</p> <p>The benchmarks underline the effectiveness of OpenVINO as a tool for deploying deep learning models. By converting models to the OpenVINO format, developers can achieve significant performance improvements, making it easier to deploy these models in real-world applications.</p> <p>For more detailed information and instructions on using OpenVINO, refer to the official OpenVINO documentation.</p>"},{"location":"integrations/ray-tune/","title":"Efficient Hyperparameter Tuning with Ray Tune and YOLOv8","text":"<p>Hyperparameter tuning is vital in achieving peak model performance by discovering the optimal set of hyperparameters. This involves running trials with different hyperparameters and evaluating each trial\u2019s performance.</p>"},{"location":"integrations/ray-tune/#accelerate-tuning-with-ultralytics-yolov8-and-ray-tune","title":"Accelerate Tuning with Ultralytics YOLOv8 and Ray Tune","text":"<p>Ultralytics YOLOv8 incorporates Ray Tune for hyperparameter tuning, streamlining the optimization of YOLOv8 model hyperparameters. With Ray Tune, you can utilize advanced search strategies, parallelism, and early stopping to expedite the tuning process.</p>"},{"location":"integrations/ray-tune/#ray-tune","title":"Ray Tune","text":"<p>Ray Tune is a hyperparameter tuning library designed for efficiency and flexibility. It supports various search strategies, parallelism, and early stopping strategies, and seamlessly integrates with popular machine learning frameworks, including Ultralytics YOLOv8.</p>"},{"location":"integrations/ray-tune/#integration-with-weights-biases","title":"Integration with Weights &amp; Biases","text":"<p>YOLOv8 also allows optional integration with Weights &amp; Biases for monitoring the tuning process.</p>"},{"location":"integrations/ray-tune/#installation","title":"Installation","text":"<p>To install the required packages, run:</p> <p>Installation</p> <pre><code># Install and update Ultralytics and Ray Tune packages\npip install -U ultralytics \"ray[tune]\"\n# Optionally install W&amp;B for logging\npip install wandb\n</code></pre>"},{"location":"integrations/ray-tune/#usage","title":"Usage","text":"<p>Usage</p> <pre><code>from ultralytics import YOLO\n# Load a YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n# Start tuning hyperparameters for YOLOv8n training on the COCO128 dataset\nresult_grid = model.tune(data=\"coco128.yaml\")\n</code></pre>"},{"location":"integrations/ray-tune/#tune-method-parameters","title":"<code>tune()</code> Method Parameters","text":"<p>The <code>tune()</code> method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:</p> Parameter Type Description Default Value <code>data</code> <code>str</code> The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings. <code>space</code> <code>dict, optional</code> A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters. <code>grace_period</code> <code>int, optional</code> The grace period in epochs for the ASHA scheduler in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. 10 <code>gpu_per_trial</code> <code>int, optional</code> The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs. None <code>max_samples</code> <code>int, optional</code> The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely. 10 <code>**train_args</code> <code>dict, optional</code> Additional arguments to pass to the <code>train()</code> method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations. {} <p>By customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.</p>"},{"location":"integrations/ray-tune/#default-search-space-description","title":"Default Search Space Description","text":"<p>The following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by <code>tune.uniform()</code>.</p> Parameter Value Range Description <code>lr0</code> <code>tune.uniform(1e-5, 1e-1)</code> Initial learning rate <code>lrf</code> <code>tune.uniform(0.01, 1.0)</code> Final learning rate factor <code>momentum</code> <code>tune.uniform(0.6, 0.98)</code> Momentum <code>weight_decay</code> <code>tune.uniform(0.0, 0.001)</code> Weight decay <code>warmup_epochs</code> <code>tune.uniform(0.0, 5.0)</code> Warmup epochs <code>warmup_momentum</code> <code>tune.uniform(0.0, 0.95)</code> Warmup momentum <code>box</code> <code>tune.uniform(0.02, 0.2)</code> Box loss weight <code>cls</code> <code>tune.uniform(0.2, 4.0)</code> Class loss weight <code>hsv_h</code> <code>tune.uniform(0.0, 0.1)</code> Hue augmentation range <code>hsv_s</code> <code>tune.uniform(0.0, 0.9)</code> Saturation augmentation range <code>hsv_v</code> <code>tune.uniform(0.0, 0.9)</code> Value (brightness) augmentation range <code>degrees</code> <code>tune.uniform(0.0, 45.0)</code> Rotation augmentation range (degrees) <code>translate</code> <code>tune.uniform(0.0, 0.9)</code> Translation augmentation range <code>scale</code> <code>tune.uniform(0.0, 0.9)</code> Scaling augmentation range <code>shear</code> <code>tune.uniform(0.0, 10.0)</code> Shear augmentation range (degrees) <code>perspective</code> <code>tune.uniform(0.0, 0.001)</code> Perspective augmentation range <code>flipud</code> <code>tune.uniform(0.0, 1.0)</code> Vertical flip augmentation probability <code>fliplr</code> <code>tune.uniform(0.0, 1.0)</code> Horizontal flip augmentation probability <code>mosaic</code> <code>tune.uniform(0.0, 1.0)</code> Mosaic augmentation probability <code>mixup</code> <code>tune.uniform(0.0, 1.0)</code> Mixup augmentation probability <code>copy_paste</code> <code>tune.uniform(0.0, 1.0)</code> Copy-paste augmentation probability"},{"location":"integrations/ray-tune/#custom-search-space-example","title":"Custom Search Space Example","text":"<p>In this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.</p> <p>Usage</p> <pre><code>from ultralytics import YOLO\n# Define a YOLO model\nmodel = YOLO(\"yolov8n.pt\")\n# Run Ray Tune on the model\nresult_grid = model.tune(data=\"coco128.yaml\",\nspace={\"lr0\": tune.uniform(1e-5, 1e-1)},\nepochs=50)\n</code></pre> <p>In the code snippet above, we create a YOLO model with the \"yolov8n.pt\" pretrained weights. Then, we call the <code>tune()</code> method, specifying the dataset configuration with \"coco128.yaml\". We provide a custom search space for the initial learning rate <code>lr0</code> using a dictionary with the key \"lr0\" and the value <code>tune.uniform(1e-5, 1e-1)</code>. Finally, we pass additional training arguments, such as the number of epochs directly to the tune method as <code>epochs=50</code>.</p>"},{"location":"integrations/ray-tune/#processing-ray-tune-results","title":"Processing Ray Tune Results","text":"<p>After running a hyperparameter tuning experiment with Ray Tune, you might want to perform various analyses on the obtained results. This guide will take you through common workflows for processing and analyzing these results.</p>"},{"location":"integrations/ray-tune/#loading-tune-experiment-results-from-a-directory","title":"Loading Tune Experiment Results from a Directory","text":"<p>After running the tuning experiment with <code>tuner.fit()</code>, you can load the results from a directory. This is useful, especially if you're performing the analysis after the initial training script has exited.</p> <pre><code>experiment_path = f\"{storage_path}/{exp_name}\"\nprint(f\"Loading results from {experiment_path}...\")\nrestored_tuner = tune.Tuner.restore(experiment_path, trainable=train_mnist)\nresult_grid = restored_tuner.get_results()\n</code></pre>"},{"location":"integrations/ray-tune/#basic-experiment-level-analysis","title":"Basic Experiment-Level Analysis","text":"<p>Get an overview of how trials performed. You can quickly check if there were any errors during the trials.</p> <pre><code>if result_grid.errors:\nprint(\"One or more trials failed!\")\nelse:\nprint(\"No errors!\")\n</code></pre>"},{"location":"integrations/ray-tune/#basic-trial-level-analysis","title":"Basic Trial-Level Analysis","text":"<p>Access individual trial hyperparameter configurations and the last reported metrics.</p> <pre><code>for i, result in enumerate(result_grid):\nprint(f\"Trial #{i}: Configuration: {result.config}, Last Reported Metrics: {result.metrics}\")\n</code></pre>"},{"location":"integrations/ray-tune/#plotting-the-entire-history-of-reported-metrics-for-a-trial","title":"Plotting the Entire History of Reported Metrics for a Trial","text":"<p>You can plot the history of reported metrics for each trial to see how the metrics evolved over time.</p> <pre><code>import matplotlib.pyplot as plt\nfor result in result_grid:\nplt.plot(result.metrics_dataframe[\"training_iteration\"], result.metrics_dataframe[\"mean_accuracy\"], label=f\"Trial {i}\")\nplt.xlabel('Training Iterations')\nplt.ylabel('Mean Accuracy')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"integrations/ray-tune/#summary","title":"Summary","text":"<p>In this documentation, we covered common workflows to analyze the results of experiments run with Ray Tune using Ultralytics. The key steps include loading the experiment results from a directory, performing basic experiment-level and trial-level analysis and plotting metrics.</p> <p>Explore further by looking into Ray Tune\u2019s Analyze Results docs page to get the most out of your hyperparameter tuning experiments.</p>"},{"location":"models/","title":"Models","text":"<p>Ultralytics supports many models and architectures with more to come in the future. Want to add your model architecture? Here's how you can contribute.</p> <p>In this documentation, we provide information on four major models:</p> <ol> <li>YOLOv3: The third iteration of the YOLO model family originally by Joseph Redmon, known for its efficient real-time object detection capabilities.</li> <li>YOLOv4: A darknet-native update to YOLOv3 released by Alexey Bochkovskiy in 2020.</li> <li>YOLOv5: An improved version of the YOLO architecture by Ultralytics, offering better performance and speed tradeoffs compared to previous versions.</li> <li>YOLOv6: Released by Meituan in 2022 and is in use in many of the company's autonomous delivery robots.</li> <li>YOLOv7: Updated YOLO models released in 2022 by the authors of YOLOv4.</li> <li>YOLOv8: The latest version of the YOLO family, featuring enhanced capabilities such as instance segmentation, pose/keypoints estimation, and classification.</li> <li>Segment Anything Model (SAM): Meta's Segment Anything Model (SAM).</li> <li>Mobile Segment Anything Model (MobileSAM): MobileSAM for mobile applications by Kyung Hee University.</li> <li>Fast Segment Anything Model (FastSAM): FastSAM by Image &amp; Video Analysis Group, Institute of Automation, Chinese Academy of Sciences.</li> <li>YOLO-NAS: YOLO Neural Architecture Search (NAS) Models.</li> <li>Realtime Detection Transformers (RT-DETR): Baidu's PaddlePaddle Realtime Detection Transformer (RT-DETR) models.</li> </ol> <p>You can use many of these models directly in the Command Line Interface (CLI) or in a Python environment. Below are examples of how to use the models with CLI and Python:</p>"},{"location":"models/#usage","title":"Usage","text":"<p>This example provides simple inference code for YOLO, SAM and RTDETR models. For more options including handling inference results see Predict mode. For using models with additional modes see Train, Val and Export.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> and <code>RTDETR()</code> classes to create a model instance in python:</p> <pre><code>from ultralytics import YOLO\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the YOLOv8n model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Load a COCO-pretrained YOLOv8n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n# Load a COCO-pretrained YOLOv8n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre> <p>For more details on each model, their supported tasks, modes, and performance, please visit their respective documentation pages linked above.</p>"},{"location":"models/fast-sam/","title":"Fast Segment Anything Model (FastSAM)","text":"<p>The Fast Segment Anything Model (FastSAM) is a novel, real-time CNN-based solution for the Segment Anything task. This task is designed to segment any object within an image based on various possible user interaction prompts. FastSAM significantly reduces computational demands while maintaining competitive performance, making it a practical choice for a variety of vision tasks.</p> <p></p>"},{"location":"models/fast-sam/#overview","title":"Overview","text":"<p>FastSAM is designed to address the limitations of the Segment Anything Model (SAM), a heavy Transformer model with substantial computational resource requirements. The FastSAM decouples the segment anything task into two sequential stages: all-instance segmentation and prompt-guided selection. The first stage uses YOLOv8-seg to produce the segmentation masks of all instances in the image. In the second stage, it outputs the region-of-interest corresponding to the prompt.</p>"},{"location":"models/fast-sam/#key-features","title":"Key Features","text":"<ol> <li> <p>Real-time Solution: By leveraging the computational efficiency of CNNs, FastSAM provides a real-time solution for the segment anything task, making it valuable for industrial applications that require quick results.</p> </li> <li> <p>Efficiency and Performance: FastSAM offers a significant reduction in computational and resource demands without compromising on performance quality. It achieves comparable performance to SAM but with drastically reduced computational resources, enabling real-time application.</p> </li> <li> <p>Prompt-guided Segmentation: FastSAM can segment any object within an image guided by various possible user interaction prompts, providing flexibility and adaptability in different scenarios.</p> </li> <li> <p>Based on YOLOv8-seg: FastSAM is based on YOLOv8-seg, an object detector equipped with an instance segmentation branch. This allows it to effectively produce the segmentation masks of all instances in an image.</p> </li> <li> <p>Competitive Results on Benchmarks: On the object proposal task on MS COCO, FastSAM achieves high scores at a significantly faster speed than SAM on a single NVIDIA RTX 3090, demonstrating its efficiency and capability.</p> </li> <li> <p>Practical Applications: The proposed approach provides a new, practical solution for a large number of vision tasks at a really high speed, tens or hundreds of times faster than current methods.</p> </li> <li> <p>Model Compression Feasibility: FastSAM demonstrates the feasibility of a path that can significantly reduce the computational effort by introducing an artificial prior to the structure, thus opening new possibilities for large model architecture for general vision tasks.</p> </li> </ol>"},{"location":"models/fast-sam/#usage","title":"Usage","text":""},{"location":"models/fast-sam/#python-api","title":"Python API","text":"<p>The FastSAM models are easy to integrate into your Python applications. Ultralytics provides a user-friendly Python API to streamline the process.</p>"},{"location":"models/fast-sam/#predict-usage","title":"Predict Usage","text":"<p>To perform object detection on an image, use the <code>predict</code> method as shown below:</p> PythonCLI <pre><code>from ultralytics import FastSAM\nfrom ultralytics.models.fastsam import FastSAMPrompt\n# Define an inference source\nsource = 'path/to/bus.jpg'\n# Create a FastSAM model\nmodel = FastSAM('FastSAM-s.pt')  # or FastSAM-x.pt\n# Run inference on an image\neverything_results = model(source, device='cpu', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n# Prepare a Prompt Process object\nprompt_process = FastSAMPrompt(source, everything_results, device='cpu')\n# Everything prompt\nann = prompt_process.everything_prompt()\n# Bbox default shape [0,0,0,0] -&gt; [x1,y1,x2,y2]\nann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n# Text prompt\nann = prompt_process.text_prompt(text='a photo of a dog')\n# Point prompt\n# points default [[0,0]] [[x1,y1],[x2,y2]]\n# point_label default [0] [1,0] 0:background, 1:foreground\nann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\nprompt_process.plot(annotations=ann, output='./')\n</code></pre> <pre><code># Load a FastSAM model and segment everything with it\nyolo segment predict model=FastSAM-s.pt source=path/to/bus.jpg imgsz=640\n</code></pre> <p>This snippet demonstrates the simplicity of loading a pre-trained model and running a prediction on an image.</p>"},{"location":"models/fast-sam/#val-usage","title":"Val Usage","text":"<p>Validation of the model on a dataset can be done as follows:</p> PythonCLI <pre><code>from ultralytics import FastSAM\n# Create a FastSAM model\nmodel = FastSAM('FastSAM-s.pt')  # or FastSAM-x.pt\n# Validate the model\nresults = model.val(data='coco8-seg.yaml')\n</code></pre> <pre><code># Load a FastSAM model and validate it on the COCO8 example dataset at image size 640\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n</code></pre> <p>Please note that FastSAM only supports detection and segmentation of a single class of object. This means it will recognize and segment all objects as the same class. Therefore, when preparing the dataset, you need to convert all object category IDs to 0.</p>"},{"location":"models/fast-sam/#fastsam-official-usage","title":"FastSAM official Usage","text":"<p>FastSAM is also available directly from the https://github.com/CASIA-IVA-Lab/FastSAM repository. Here is a brief overview of the typical steps you might take to use FastSAM:</p>"},{"location":"models/fast-sam/#installation","title":"Installation","text":"<ol> <li> <p>Clone the FastSAM repository:    <pre><code>git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n</code></pre></p> </li> <li> <p>Create and activate a Conda environment with Python 3.9:    <pre><code>conda create -n FastSAM python=3.9\nconda activate FastSAM\n</code></pre></p> </li> <li> <p>Navigate to the cloned repository and install the required packages:    <pre><code>cd FastSAM\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install the CLIP model:    <pre><code>pip install git+https://github.com/openai/CLIP.git\n</code></pre></p> </li> </ol>"},{"location":"models/fast-sam/#example-usage","title":"Example Usage","text":"<ol> <li> <p>Download a model checkpoint.</p> </li> <li> <p>Use FastSAM for inference. Example commands:</p> <ul> <li> <p>Segment everything in an image:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n</code></pre></p> </li> <li> <p>Segment specific objects using text prompt:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"the yellow dog\"\n</code></pre></p> </li> <li> <p>Segment objects within a bounding box (provide box coordinates in xywh format):   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n</code></pre></p> </li> <li> <p>Segment objects near specific points:   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n</code></pre></p> </li> </ul> </li> </ol> <p>Additionally, you can try FastSAM through a Colab demo or on the HuggingFace web demo for a visual experience.</p>"},{"location":"models/fast-sam/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>We would like to acknowledge the FastSAM authors for their significant contributions in the field of real-time instance segmentation:</p> BibTeX <pre><code>@misc{zhao2023fast,\ntitle={Fast Segment Anything},\nauthor={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\nyear={2023},\neprint={2306.12156},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>The original FastSAM paper can be found on arXiv. The authors have made their work publicly available, and the codebase can be accessed on GitHub. We appreciate their efforts in advancing the field and making their work accessible to the broader community.</p>"},{"location":"models/mobile-sam/","title":"MobileSAM (Mobile Segment Anything Model)","text":""},{"location":"models/mobile-sam/#mobile-segment-anything-mobilesam","title":"Mobile Segment Anything (MobileSAM)","text":"<p>The MobileSAM paper is now available on arXiv.</p> <p>A demonstration of MobileSAM running on a CPU can be accessed at this demo link. The performance on a Mac i5 CPU takes approximately 3 seconds. On the Hugging Face demo, the interface and lower-performance CPUs contribute to a slower response, but it continues to function effectively.</p> <p>MobileSAM is implemented in various projects including Grounding-SAM, AnyLabeling, and Segment Anything in 3D.</p> <p>MobileSAM is trained on a single GPU with a 100k dataset (1% of the original images) in less than a day. The code for this training will be made available in the future.</p>"},{"location":"models/mobile-sam/#adapting-from-sam-to-mobilesam","title":"Adapting from SAM to MobileSAM","text":"<p>Since MobileSAM retains the same pipeline as the original SAM, we have incorporated the original's pre-processing, post-processing, and all other interfaces. Consequently, those currently using the original SAM can transition to MobileSAM with minimal effort.</p> <p>MobileSAM performs comparably to the original SAM and retains the same pipeline except for a change in the image encoder. Specifically, we replace the original heavyweight ViT-H encoder (632M) with a smaller Tiny-ViT (5M). On a single GPU, MobileSAM operates at about 12ms per image: 8ms on the image encoder and 4ms on the mask decoder.</p> <p>The following table provides a comparison of ViT-based image encoders:</p> Image Encoder Original SAM MobileSAM Parameters 611M 5M Speed 452ms 8ms <p>Both the original SAM and MobileSAM utilize the same prompt-guided mask decoder:</p> Mask Decoder Original SAM MobileSAM Parameters 3.876M 3.876M Speed 4ms 4ms <p>Here is the comparison of the whole pipeline:</p> Whole Pipeline (Enc+Dec) Original SAM MobileSAM Parameters 615M 9.66M Speed 456ms 12ms <p>The performance of MobileSAM and the original SAM are demonstrated using both a point and a box as prompts.</p> <p></p> <p></p> <p>With its superior performance, MobileSAM is approximately 5 times smaller and 7 times faster than the current FastSAM. More details are available at the MobileSAM project page.</p>"},{"location":"models/mobile-sam/#testing-mobilesam-in-ultralytics","title":"Testing MobileSAM in Ultralytics","text":"<p>Just like the original SAM, we offer a straightforward testing method in Ultralytics, including modes for both Point and Box prompts.</p>"},{"location":"models/mobile-sam/#model-download","title":"Model Download","text":"<p>You can download the model here.</p>"},{"location":"models/mobile-sam/#point-prompt","title":"Point Prompt","text":"Python <pre><code>from ultralytics import SAM\n# Load the model\nmodel = SAM('mobile_sam.pt')\n# Predict a segment based on a point prompt\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre>"},{"location":"models/mobile-sam/#box-prompt","title":"Box Prompt","text":"Python <pre><code>from ultralytics import SAM\n# Load the model\nmodel = SAM('mobile_sam.pt')\n# Predict a segment based on a box prompt\nmodel.predict('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n</code></pre> <p>We have implemented <code>MobileSAM</code> and <code>SAM</code> using the same API. For more usage information, please see the SAM page.</p>"},{"location":"models/mobile-sam/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you find MobileSAM useful in your research or development work, please consider citing our paper:</p> BibTeX <pre><code>@article{mobile_sam,\ntitle={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},\nauthor={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},\njournal={arXiv preprint arXiv:2306.14289},\nyear={2023}\n}\n</code></pre>"},{"location":"models/rtdetr/","title":"Baidu's RT-DETR: A Vision Transformer-Based Real-Time Object Detector","text":""},{"location":"models/rtdetr/#overview","title":"Overview","text":"<p>Real-Time Detection Transformer (RT-DETR), developed by Baidu, is a cutting-edge end-to-end object detector that provides real-time performance while maintaining high accuracy. It leverages the power of Vision Transformers (ViT) to efficiently process multiscale features by decoupling intra-scale interaction and cross-scale fusion. RT-DETR is highly adaptable, supporting flexible adjustment of inference speed using different decoder layers without retraining. The model excels on accelerated backends like CUDA with TensorRT, outperforming many other real-time object detectors.</p> <p> Overview of Baidu's RT-DETR. The RT-DETR model architecture diagram shows the last three stages of the backbone {S3, S4, S5} as the input to the encoder. The efficient hybrid encoder transforms multiscale features into a sequence of image features through intrascale feature interaction (AIFI) and cross-scale feature-fusion module (CCFM). The IoU-aware query selection is employed to select a fixed number of image features to serve as initial object queries for the decoder. Finally, the decoder with auxiliary prediction heads iteratively optimizes object queries to generate boxes and confidence scores (source).</p>"},{"location":"models/rtdetr/#key-features","title":"Key Features","text":"<ul> <li>Efficient Hybrid Encoder: Baidu's RT-DETR uses an efficient hybrid encoder that processes multiscale features by decoupling intra-scale interaction and cross-scale fusion. This unique Vision Transformers-based design reduces computational costs and allows for real-time object detection.</li> <li>IoU-aware Query Selection: Baidu's RT-DETR improves object query initialization by utilizing IoU-aware query selection. This allows the model to focus on the most relevant objects in the scene, enhancing the detection accuracy.</li> <li>Adaptable Inference Speed: Baidu's RT-DETR supports flexible adjustments of inference speed by using different decoder layers without the need for retraining. This adaptability facilitates practical application in various real-time object detection scenarios.</li> </ul>"},{"location":"models/rtdetr/#pre-trained-models","title":"Pre-trained Models","text":"<p>The Ultralytics Python API provides pre-trained PaddlePaddle RT-DETR models with different scales:</p> <ul> <li>RT-DETR-L: 53.0% AP on COCO val2017, 114 FPS on T4 GPU</li> <li>RT-DETR-X: 54.8% AP on COCO val2017, 74 FPS on T4 GPU</li> </ul>"},{"location":"models/rtdetr/#usage","title":"Usage","text":"<p>You can use RT-DETR for object detection tasks using the <code>ultralytics</code> pip package. The following is a sample code snippet showing how to use RT-DETR models for training and inference:</p> <p>This example provides simple inference code for RT-DETR. For more options including handling inference results see Predict mode. For using RT-DETR with additional modes see Train, Val and Export.</p> PythonCLI <pre><code>from ultralytics import RTDETR\n# Load a COCO-pretrained RT-DETR-l model\nmodel = RTDETR('rtdetr-l.pt')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the RT-DETR-l model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <pre><code># Load a COCO-pretrained RT-DETR-l model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n# Load a COCO-pretrained RT-DETR-l model and run inference on the 'bus.jpg' image\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/rtdetr/#supported-tasks","title":"Supported Tasks","text":"Model Type Pre-trained Weights Tasks Supported RT-DETR Large <code>rtdetr-l.pt</code> Object Detection RT-DETR Extra-Large <code>rtdetr-x.pt</code> Object Detection"},{"location":"models/rtdetr/#supported-modes","title":"Supported Modes","text":"Mode Supported Inference Validation Training"},{"location":"models/rtdetr/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you use Baidu's RT-DETR in your research or development work, please cite the original paper:</p> BibTeX <pre><code>@misc{lv2023detrs,\ntitle={DETRs Beat YOLOs on Real-time Object Detection},\nauthor={Wenyu Lv and Shangliang Xu and Yian Zhao and Guanzhong Wang and Jinman Wei and Cheng Cui and Yuning Du and Qingqing Dang and Yi Liu},\nyear={2023},\neprint={2304.08069},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to acknowledge Baidu and the PaddlePaddle team for creating and maintaining this valuable resource for the computer vision community. Their contribution to the field with the development of the Vision Transformers-based real-time object detector, RT-DETR, is greatly appreciated.</p> <p>Keywords: RT-DETR, Transformer, ViT, Vision Transformers, Baidu RT-DETR, PaddlePaddle, Paddle Paddle RT-DETR, real-time object detection, Vision Transformers-based object detection, pre-trained PaddlePaddle RT-DETR models, Baidu's RT-DETR usage, Ultralytics Python API</p>"},{"location":"models/sam/","title":"Segment Anything Model (SAM)","text":"<p>Welcome to the frontier of image segmentation with the Segment Anything Model, or SAM. This revolutionary model has changed the game by introducing promptable image segmentation with real-time performance, setting new standards in the field.</p>"},{"location":"models/sam/#introduction-to-sam-the-segment-anything-model","title":"Introduction to SAM: The Segment Anything Model","text":"<p>The Segment Anything Model, or SAM, is a cutting-edge image segmentation model that allows for promptable segmentation, providing unparalleled versatility in image analysis tasks. SAM forms the heart of the Segment Anything initiative, a groundbreaking project that introduces a novel model, task, and dataset for image segmentation.</p> <p>SAM's advanced design allows it to adapt to new image distributions and tasks without prior knowledge, a feature known as zero-shot transfer. Trained on the expansive SA-1B dataset, which contains more than 1 billion masks spread over 11 million carefully curated images, SAM has displayed impressive zero-shot performance, surpassing previous fully supervised results in many cases.</p> <p> Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated fully automatically by SAM, and as verified by human ratings and numerous experiments, are of high quality and diversity. Images are grouped by number of masks per image for visualization (there are \u223c100 masks per image on average).</p>"},{"location":"models/sam/#key-features-of-the-segment-anything-model-sam","title":"Key Features of the Segment Anything Model (SAM)","text":"<ul> <li>Promptable Segmentation Task: SAM was designed with a promptable segmentation task in mind, allowing it to generate valid segmentation masks from any given prompt, such as spatial or text clues identifying an object.</li> <li>Advanced Architecture: The Segment Anything Model employs a powerful image encoder, a prompt encoder, and a lightweight mask decoder. This unique architecture enables flexible prompting, real-time mask computation, and ambiguity awareness in segmentation tasks.</li> <li>The SA-1B Dataset: Introduced by the Segment Anything project, the SA-1B dataset features over 1 billion masks on 11 million images. As the largest segmentation dataset to date, it provides SAM with a diverse and large-scale training data source.</li> <li>Zero-Shot Performance: SAM displays outstanding zero-shot performance across various segmentation tasks, making it a ready-to-use tool for diverse applications with minimal need for prompt engineering.</li> </ul> <p>For an in-depth look at the Segment Anything Model and the SA-1B dataset, please visit the Segment Anything website and check out the research paper Segment Anything.</p>"},{"location":"models/sam/#how-to-use-sam-versatility-and-power-in-image-segmentation","title":"How to Use SAM: Versatility and Power in Image Segmentation","text":"<p>The Segment Anything Model can be employed for a multitude of downstream tasks that go beyond its training data. This includes edge detection, object proposal generation, instance segmentation, and preliminary text-to-mask prediction. With prompt engineering, SAM can swiftly adapt to new tasks and data distributions in a zero-shot manner, establishing it as a versatile and potent tool for all your image segmentation needs.</p>"},{"location":"models/sam/#sam-prediction-example","title":"SAM prediction example","text":"<p>Segment with prompts</p> <p>Segment image with given prompts.</p> Python <pre><code>from ultralytics import SAM\n# Load a model\nmodel = SAM('sam_b.pt')\n# Display model information (optional)\nmodel.info()\n# Run inference with bboxes prompt\nmodel('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n# Run inference with points prompt\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre> <p>Segment everything</p> <p>Segment the whole image.</p> PythonCLI <pre><code>from ultralytics import SAM\n# Load a model\nmodel = SAM('sam_b.pt')\n# Display model information (optional)\nmodel.info()\n# Run inference\nmodel('path/to/image.jpg')\n</code></pre> <pre><code># Run inference with a SAM model\nyolo predict model=sam_b.pt source=path/to/image.jpg\n</code></pre> <ul> <li>The logic here is to segment the whole image if you don't pass any prompts(bboxes/points/masks).</li> </ul> <p>SAMPredictor example</p> <p>This way you can set image once and run prompts inference multiple times without running image encoder multiple times.</p> Prompt inference <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n# Create SAMPredictor\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n# Set image\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # set with image file\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # set with np.ndarray\nresults = predictor(bboxes=[439, 437, 524, 709])\nresults = predictor(points=[900, 370], labels=[1])\n# Reset image\npredictor.reset_image()\n</code></pre> <p>Segment everything with additional args.</p> Segment everything <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n# Create SAMPredictor\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n# Segment with additional args\nresults = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n</code></pre> <ul> <li>More additional args for <code>Segment everything</code> see <code>Predictor/generate</code> Reference.</li> </ul>"},{"location":"models/sam/#available-models-and-supported-tasks","title":"Available Models and Supported Tasks","text":"Model Type Pre-trained Weights Tasks Supported SAM base <code>sam_b.pt</code> Instance Segmentation SAM large <code>sam_l.pt</code> Instance Segmentation"},{"location":"models/sam/#operating-modes","title":"Operating Modes","text":"Mode Supported Inference Validation Training"},{"location":"models/sam/#sam-comparison-vs-yolov8","title":"SAM comparison vs YOLOv8","text":"<p>Here we compare Meta's smallest SAM model, SAM-b, with Ultralytics smallest segmentation model, YOLOv8n-seg:</p> Model Size Parameters Speed (CPU) Meta's SAM-b 358 MB 94.7 M 51096 ms/im MobileSAM 40.7 MB 10.1 M 46122 ms/im FastSAM-s with YOLOv8 backbone 23.7 MB 11.8 M 115 ms/im Ultralytics YOLOv8n-seg 6.7 MB (53.4x smaller) 3.4 M (27.9x less) 59 ms/im (866x faster) <p>This comparison shows the order-of-magnitude differences in the model sizes and speeds between models. Whereas SAM presents unique capabilities for automatic segmenting, it is not a direct competitor to YOLOv8 segment models, which are smaller, faster and more efficient.</p> <p>Tests run on a 2023 Apple M2 Macbook with 16GB of RAM. To reproduce this test:</p> Python <pre><code>from ultralytics import FastSAM, SAM, YOLO\n# Profile SAM-b\nmodel = SAM('sam_b.pt')\nmodel.info()\nmodel('ultralytics/assets')\n# Profile MobileSAM\nmodel = SAM('mobile_sam.pt')\nmodel.info()\nmodel('ultralytics/assets')\n# Profile FastSAM-s\nmodel = FastSAM('FastSAM-s.pt')\nmodel.info()\nmodel('ultralytics/assets')\n# Profile YOLOv8n-seg\nmodel = YOLO('yolov8n-seg.pt')\nmodel.info()\nmodel('ultralytics/assets')\n</code></pre>"},{"location":"models/sam/#auto-annotation-a-quick-path-to-segmentation-datasets","title":"Auto-Annotation: A Quick Path to Segmentation Datasets","text":"<p>Auto-annotation is a key feature of SAM, allowing users to generate a segmentation dataset using a pre-trained detection model. This feature enables rapid and accurate annotation of a large number of images, bypassing the need for time-consuming manual labeling.</p>"},{"location":"models/sam/#generate-your-segmentation-dataset-using-a-detection-model","title":"Generate Your Segmentation Dataset Using a Detection Model","text":"<p>To auto-annotate your dataset with the Ultralytics framework, use the <code>auto_annotate</code> function as shown below:</p> Python <pre><code>from ultralytics.data.annotator import auto_annotate\nauto_annotate(data=\"path/to/images\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argument Type Description Default data str Path to a folder containing images to be annotated. det_model str, optional Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'. 'yolov8x.pt' sam_model str, optional Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'. 'sam_b.pt' device str, optional Device to run the models on. Defaults to an empty string (CPU or GPU, if available). output_dir str, None, optional Directory to save the annotated results. Defaults to a 'labels' folder in the same directory as 'data'. None <p>The <code>auto_annotate</code> function takes the path to your images, with optional arguments for specifying the pre-trained detection and SAM segmentation models, the device to run the models on, and the output directory for saving the annotated results.</p> <p>Auto-annotation with pre-trained models can dramatically cut down the time and effort required for creating high-quality segmentation datasets. This feature is especially beneficial for researchers and developers dealing with large image collections, as it allows them to focus on model development and evaluation rather than manual annotation.</p>"},{"location":"models/sam/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you find SAM useful in your research or development work, please consider citing our paper:</p> BibTeX <pre><code>@misc{kirillov2023segment,\ntitle={Segment Anything},\nauthor={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross Girshick},\nyear={2023},\neprint={2304.02643},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>We would like to express our gratitude to Meta AI for creating and maintaining this valuable resource for the computer vision community.</p> <p>keywords: Segment Anything, Segment Anything Model, SAM, Meta SAM, image segmentation, promptable segmentation, zero-shot performance, SA-1B dataset, advanced architecture, auto-annotation, Ultralytics, pre-trained models, SAM base, SAM large, instance segmentation, computer vision, AI, artificial intelligence, machine learning, data annotation, segmentation masks, detection model, YOLO detection model, bibtex, Meta AI.</p>"},{"location":"models/yolo-nas/","title":"YOLO-NAS","text":""},{"location":"models/yolo-nas/#overview","title":"Overview","text":"<p>Developed by Deci AI, YOLO-NAS is a groundbreaking object detection foundational model. It is the product of advanced Neural Architecture Search technology, meticulously designed to address the limitations of previous YOLO models. With significant improvements in quantization support and accuracy-latency trade-offs, YOLO-NAS represents a major leap in object detection.</p> <p> Overview of YOLO-NAS. YOLO-NAS employs quantization-aware blocks and selective quantization for optimal performance. The model, when converted to its INT8 quantized version, experiences a minimal precision drop, a significant improvement over other models. These advancements culminate in a superior architecture with unprecedented object detection capabilities and outstanding performance.</p>"},{"location":"models/yolo-nas/#key-features","title":"Key Features","text":"<ul> <li>Quantization-Friendly Basic Block: YOLO-NAS introduces a new basic block that is friendly to quantization, addressing one of the significant limitations of previous YOLO models.</li> <li>Sophisticated Training and Quantization: YOLO-NAS leverages advanced training schemes and post-training quantization to enhance performance.</li> <li>AutoNAC Optimization and Pre-training: YOLO-NAS utilizes AutoNAC optimization and is pre-trained on prominent datasets such as COCO, Objects365, and Roboflow 100. This pre-training makes it extremely suitable for downstream object detection tasks in production environments.</li> </ul>"},{"location":"models/yolo-nas/#pre-trained-models","title":"Pre-trained Models","text":"<p>Experience the power of next-generation object detection with the pre-trained YOLO-NAS models provided by Ultralytics. These models are designed to deliver top-notch performance in terms of both speed and accuracy. Choose from a variety of options tailored to your specific needs:</p> Model mAP Latency (ms) YOLO-NAS S 47.5 3.21 YOLO-NAS M 51.55 5.85 YOLO-NAS L 52.22 7.87 YOLO-NAS S INT-8 47.03 2.36 YOLO-NAS M INT-8 51.0 3.78 YOLO-NAS L INT-8 52.1 4.78 <p>Each model variant is designed to offer a balance between Mean Average Precision (mAP) and latency, helping you optimize your object detection tasks for both performance and speed.</p>"},{"location":"models/yolo-nas/#usage","title":"Usage","text":"<p>Ultralytics has made YOLO-NAS models easy to integrate into your Python applications via our <code>ultralytics</code> python package. The package provides a user-friendly Python API to streamline the process.</p> <p>The following examples show how to use YOLO-NAS models with the <code>ultralytics</code> package for inference and validation:</p>"},{"location":"models/yolo-nas/#inference-and-validation-examples","title":"Inference and Validation Examples","text":"<p>In this example we validate YOLO-NAS-s on the COCO8 dataset.</p> <p>This example provides simple inference and validation code for YOLO-NAS. For handling inference results see Predict mode. For using YOLO-NAS with additional modes see Val and Export. YOLO-NAS on the <code>ultralytics</code> package does not support training.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models files can be passed to the <code>NAS()</code> class to create a model instance in python:</p> <pre><code>from ultralytics import NAS\n# Load a COCO-pretrained YOLO-NAS-s model\nmodel = NAS('yolo_nas_s.pt')\n# Display model information (optional)\nmodel.info()\n# Validate the model on the COCO8 example dataset\nresults = model.val(data='coco8.yaml')\n# Run inference with the YOLO-NAS-s model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Load a COCO-pretrained YOLO-NAS-s model and validate it's performance on the COCO8 example dataset\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Load a COCO-pretrained YOLO-NAS-s model and run inference on the 'bus.jpg' image\nyolo predict model=yolo_nas_s.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolo-nas/#supported-tasks","title":"Supported Tasks","text":"<p>The YOLO-NAS models are primarily designed for object detection tasks. You can download the pre-trained weights for each variant of the model as follows:</p> Model Type Pre-trained Weights Tasks Supported YOLO-NAS-s yolo_nas_s.pt Object Detection YOLO-NAS-m yolo_nas_m.pt Object Detection YOLO-NAS-l yolo_nas_l.pt Object Detection"},{"location":"models/yolo-nas/#supported-modes","title":"Supported Modes","text":"<p>The YOLO-NAS models support both inference and validation modes, allowing you to predict and validate results with ease. Training mode, however, is currently not supported.</p> Mode Supported Inference Validation Training <p>Harness the power of the YOLO-NAS models to drive your object detection tasks to new heights of performance and speed.</p>"},{"location":"models/yolo-nas/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you employ YOLO-NAS in your research or development work, please cite SuperGradients:</p> BibTeX <pre><code>@misc{supergradients,\ndoi = {10.5281/ZENODO.7789328},\nurl = {https://zenodo.org/record/7789328},\nauthor = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\ntitle = {Super-Gradients},\npublisher = {GitHub},\njournal = {GitHub repository},\nyear = {2021},\n}\n</code></pre> <p>We express our gratitude to Deci AI's SuperGradients team for their efforts in creating and maintaining this valuable resource for the computer vision community. We believe YOLO-NAS, with its innovative architecture and superior object detection capabilities, will become a critical tool for developers and researchers alike.</p> <p>Keywords: YOLO-NAS, Deci AI, object detection, deep learning, neural architecture search, Ultralytics Python API, YOLO model, SuperGradients, pre-trained models, quantization-friendly basic block, advanced training schemes, post-training quantization, AutoNAC optimization, COCO, Objects365, Roboflow 100</p>"},{"location":"models/yolov3/","title":"YOLOv3, YOLOv3-Ultralytics, and YOLOv3u","text":""},{"location":"models/yolov3/#overview","title":"Overview","text":"<p>This document presents an overview of three closely related object detection models, namely YOLOv3, YOLOv3-Ultralytics, and YOLOv3u.</p> <ol> <li> <p>YOLOv3: This is the third version of the You Only Look Once (YOLO) object detection algorithm. Originally developed by Joseph Redmon, YOLOv3 improved on its predecessors by introducing features such as multiscale predictions and three different sizes of detection kernels.</p> </li> <li> <p>YOLOv3-Ultralytics: This is Ultralytics' implementation of the YOLOv3 model. It reproduces the original YOLOv3 architecture and offers additional functionalities, such as support for more pre-trained models and easier customization options.</p> </li> <li> <p>YOLOv3u: This is an updated version of YOLOv3-Ultralytics that incorporates the anchor-free, objectness-free split head used in YOLOv8 models. YOLOv3u maintains the same backbone and neck architecture as YOLOv3 but with the updated detection head from YOLOv8.</p> </li> </ol> <p></p>"},{"location":"models/yolov3/#key-features","title":"Key Features","text":"<ul> <li> <p>YOLOv3: Introduced the use of three different scales for detection, leveraging three different sizes of detection kernels: 13x13, 26x26, and 52x52. This significantly improved detection accuracy for objects of different sizes. Additionally, YOLOv3 added features such as multi-label predictions for each bounding box and a better feature extractor network.</p> </li> <li> <p>YOLOv3-Ultralytics: Ultralytics' implementation of YOLOv3 provides the same performance as the original model but comes with added support for more pre-trained models, additional training methods, and easier customization options. This makes it more versatile and user-friendly for practical applications.</p> </li> <li> <p>YOLOv3u: This updated model incorporates the anchor-free, objectness-free split head from YOLOv8. By eliminating the need for pre-defined anchor boxes and objectness scores, this detection head design can improve the model's ability to detect objects of varying sizes and shapes. This makes YOLOv3u more robust and accurate for object detection tasks.</p> </li> </ul>"},{"location":"models/yolov3/#supported-tasks","title":"Supported Tasks","text":"<p>YOLOv3, YOLOv3-Ultralytics, and YOLOv3u all support the following tasks:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"models/yolov3/#supported-modes","title":"Supported Modes","text":"<p>All three models support the following modes:</p> <ul> <li>Inference</li> <li>Validation</li> <li>Training</li> <li>Export</li> </ul>"},{"location":"models/yolov3/#performance","title":"Performance","text":"<p>Below is a comparison of the performance of the three models. The performance is measured in terms of the Mean Average Precision (mAP) on the COCO dataset:</p> <p>TODO</p>"},{"location":"models/yolov3/#usage","title":"Usage","text":"<p>You can use YOLOv3 for object detection tasks using the Ultralytics repository. The following is a sample code snippet showing how to use YOLOv3 model for inference:</p> <p>This example provides simple inference code for YOLOv3. For more options including handling inference results see Predict mode. For using YOLOv3 with additional modes see Train, Val and Export.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLO()</code> class to create a model instance in python:</p> <pre><code>from ultralytics import YOLO\n# Load a COCO-pretrained YOLOv3n model\nmodel = YOLO('yolov3n.pt')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the YOLOv3n model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Load a COCO-pretrained YOLOv3n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov3n.pt data=coco8.yaml epochs=100 imgsz=640\n# Load a COCO-pretrained YOLOv3n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov3n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov3/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you use YOLOv3 in your research, please cite the original YOLO papers and the Ultralytics YOLOv3 repository:</p> BibTeX <pre><code>@article{redmon2018yolov3,\ntitle={YOLOv3: An Incremental Improvement},\nauthor={Redmon, Joseph and Farhadi, Ali},\njournal={arXiv preprint arXiv:1804.02767},\nyear={2018}\n}\n</code></pre> <p>Thank you to Joseph Redmon and Ali Farhadi for developing the original YOLOv3.</p>"},{"location":"models/yolov4/","title":"YOLOv4: High-Speed and Precise Object Detection","text":"<p>Welcome to the Ultralytics documentation page for YOLOv4, a state-of-the-art, real-time object detector launched in 2020 by Alexey Bochkovskiy at https://github.com/AlexeyAB/darknet. YOLOv4 is designed to provide the optimal balance between speed and accuracy, making it an excellent choice for many applications.</p> <p> YOLOv4 architecture diagram. Showcasing the intricate network design of YOLOv4, including the backbone, neck, and head components, and their interconnected layers for optimal real-time object detection.</p>"},{"location":"models/yolov4/#introduction","title":"Introduction","text":"<p>YOLOv4 stands for You Only Look Once version 4. It is a real-time object detection model developed to address the limitations of previous YOLO versions like YOLOv3 and other object detection models. Unlike other convolutional neural network (CNN) based object detectors, YOLOv4 is not only applicable for recommendation systems but also for standalone process management and human input reduction. Its operation on conventional graphics processing units (GPUs) allows for mass usage at an affordable price, and it is designed to work in real-time on a conventional GPU while requiring only one such GPU for training.</p>"},{"location":"models/yolov4/#architecture","title":"Architecture","text":"<p>YOLOv4 makes use of several innovative features that work together to optimize its performance. These include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT), Mish-activation, Mosaic data augmentation, DropBlock regularization, and CIoU loss. These features are combined to achieve state-of-the-art results.</p> <p>A typical object detector is composed of several parts including the input, the backbone, the neck, and the head. The backbone of YOLOv4 is pre-trained on ImageNet and is used to predict classes and bounding boxes of objects. The backbone could be from several models including VGG, ResNet, ResNeXt, or DenseNet. The neck part of the detector is used to collect feature maps from different stages and usually includes several bottom-up paths and several top-down paths. The head part is what is used to make the final object detections and classifications.</p>"},{"location":"models/yolov4/#bag-of-freebies","title":"Bag of Freebies","text":"<p>YOLOv4 also makes use of methods known as \"bag of freebies,\" which are techniques that improve the accuracy of the model during training without increasing the cost of inference. Data augmentation is a common bag of freebies technique used in object detection, which increases the variability of the input images to improve the robustness of the model. Some examples of data augmentation include photometric distortions (adjusting the brightness, contrast, hue, saturation, and noise of an image) and geometric distortions (adding random scaling, cropping, flipping, and rotating). These techniques help the model to generalize better to different types of images.</p>"},{"location":"models/yolov4/#features-and-performance","title":"Features and Performance","text":"<p>YOLOv4 is designed for optimal speed and accuracy in object detection. The architecture of YOLOv4 includes CSPDarknet53 as the backbone, PANet as the neck, and YOLOv3 as the detection head. This design allows YOLOv4 to perform object detection at an impressive speed, making it suitable for real-time applications. YOLOv4 also excels in accuracy, achieving state-of-the-art results in object detection benchmarks.</p>"},{"location":"models/yolov4/#usage-examples","title":"Usage Examples","text":"<p>As of the time of writing, Ultralytics does not currently support YOLOv4 models. Therefore, any users interested in using YOLOv4 will need to refer directly to the YOLOv4 GitHub repository for installation and usage instructions.</p> <p>Here is a brief overview of the typical steps you might take to use YOLOv4:</p> <ol> <li> <p>Visit the YOLOv4 GitHub repository: https://github.com/AlexeyAB/darknet.</p> </li> <li> <p>Follow the instructions provided in the README file for installation. This typically involves cloning the repository, installing necessary dependencies, and setting up any necessary environment variables.</p> </li> <li> <p>Once installation is complete, you can train and use the model as per the usage instructions provided in the repository. This usually involves preparing your dataset, configuring the model parameters, training the model, and then using the trained model to perform object detection.</p> </li> </ol> <p>Please note that the specific steps may vary depending on your specific use case and the current state of the YOLOv4 repository. Therefore, it is strongly recommended to refer directly to the instructions provided in the YOLOv4 GitHub repository.</p> <p>We regret any inconvenience this may cause and will strive to update this document with usage examples for Ultralytics once support for YOLOv4 is implemented.</p>"},{"location":"models/yolov4/#conclusion","title":"Conclusion","text":"<p>YOLOv4 is a powerful and efficient object detection model that strikes a balance between speed and accuracy. Its use of unique features and bag of freebies techniques during training allows it to perform excellently in real-time object detection tasks. YOLOv4 can be trained and used by anyone with a conventional GPU, making it accessible and practical for a wide range of applications.</p>"},{"location":"models/yolov4/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>We would like to acknowledge the YOLOv4 authors for their significant contributions in the field of real-time object detection:</p> BibTeX <pre><code>@misc{bochkovskiy2020yolov4,\ntitle={YOLOv4: Optimal Speed and Accuracy of Object Detection},\nauthor={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\nyear={2020},\neprint={2004.10934},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>The original YOLOv4 paper can be found on arXiv. The authors have made their work publicly available, and the codebase can be accessed on GitHub. We appreciate their efforts in advancing the field and making their work accessible to the broader community.</p>"},{"location":"models/yolov5/","title":"YOLOv5","text":""},{"location":"models/yolov5/#overview","title":"Overview","text":"<p>YOLOv5u represents an advancement in object detection methodologies. Originating from the foundational architecture of the YOLOv5 model developed by Ultralytics, YOLOv5u integrates the anchor-free, objectness-free split head, a feature previously introduced in the YOLOv8 models. This adaptation refines the model's architecture, leading to an improved accuracy-speed tradeoff in object detection tasks. Given the empirical results and its derived features, YOLOv5u provides an efficient alternative for those seeking robust solutions in both research and practical applications.</p> <p></p>"},{"location":"models/yolov5/#key-features","title":"Key Features","text":"<ul> <li> <p>Anchor-free Split Ultralytics Head: Traditional object detection models rely on predefined anchor boxes to predict object locations. However, YOLOv5u modernizes this approach. By adopting an anchor-free split Ultralytics head, it ensures a more flexible and adaptive detection mechanism, consequently enhancing the performance in diverse scenarios.</p> </li> <li> <p>Optimized Accuracy-Speed Tradeoff: Speed and accuracy often pull in opposite directions. But YOLOv5u challenges this tradeoff. It offers a calibrated balance, ensuring real-time detections without compromising on accuracy. This feature is particularly invaluable for applications that demand swift responses, such as autonomous vehicles, robotics, and real-time video analytics.</p> </li> <li> <p>Variety of Pre-trained Models: Understanding that different tasks require different toolsets, YOLOv5u provides a plethora of pre-trained models. Whether you're focusing on Inference, Validation, or Training, there's a tailor-made model awaiting you. This variety ensures you're not just using a one-size-fits-all solution, but a model specifically fine-tuned for your unique challenge.</p> </li> </ul>"},{"location":"models/yolov5/#supported-tasks","title":"Supported Tasks","text":"Model Type Pre-trained Weights Task YOLOv5u <code>yolov5nu</code>, <code>yolov5su</code>, <code>yolov5mu</code>, <code>yolov5lu</code>, <code>yolov5xu</code>, <code>yolov5n6u</code>, <code>yolov5s6u</code>, <code>yolov5m6u</code>, <code>yolov5l6u</code>, <code>yolov5x6u</code> Detection"},{"location":"models/yolov5/#supported-modes","title":"Supported Modes","text":"Mode Supported Inference Validation Training <p>Performance</p> Detection Model YAML size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) yolov5nu.pt yolov5n.yaml 640 34.3 73.6 1.06 2.6 7.7 yolov5su.pt yolov5s.yaml 640 43.0 120.7 1.27 9.1 24.0 yolov5mu.pt yolov5m.yaml 640 49.0 233.9 1.86 25.1 64.2 yolov5lu.pt yolov5l.yaml 640 52.2 408.4 2.50 53.2 135.0 yolov5xu.pt yolov5x.yaml 640 53.2 763.2 3.81 97.2 246.4 yolov5n6u.pt yolov5n6.yaml 1280 42.1 211.0 1.83 4.3 7.8 yolov5s6u.pt yolov5s6.yaml 1280 48.6 422.6 2.34 15.3 24.6 yolov5m6u.pt yolov5m6.yaml 1280 53.6 810.9 4.36 41.2 65.7 yolov5l6u.pt yolov5l6.yaml 1280 55.7 1470.9 5.47 86.1 137.4 yolov5x6u.pt yolov5x6.yaml 1280 56.8 2436.5 8.98 155.4 250.7"},{"location":"models/yolov5/#usage","title":"Usage","text":"<p>You can use YOLOv5u for object detection tasks using the Ultralytics repository. The following is a sample code snippet showing how to use YOLOv5u model for inference:</p> <p>This example provides simple inference code for YOLOv5. For more options including handling inference results see Predict mode. For using YOLOv5 with additional modes see Train, Val and Export.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLO()</code> class to create a model instance in python:</p> <pre><code>from ultralytics import YOLO\n# Load a COCO-pretrained YOLOv5n model\nmodel = YOLO('yolov5n.pt')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the YOLOv5n model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Load a COCO-pretrained YOLOv5n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n# Load a COCO-pretrained YOLOv5n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov5/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you use YOLOv5 or YOLOv5u in your research, please cite the Ultralytics YOLOv5 repository as follows:</p> BibTeX <pre><code>@software{yolov5,\ntitle = {Ultralytics YOLOv5},\nauthor = {Glenn Jocher},\nyear = {2020},\nversion = {7.0},\nlicense = {AGPL-3.0},\nurl = {https://github.com/ultralytics/yolov5},\ndoi = {10.5281/zenodo.3908559},\norcid = {0000-0001-5950-6979}\n}\n</code></pre> <p>Special thanks to Glenn Jocher and the Ultralytics team for their work on developing and maintaining the YOLOv5 and YOLOv5u models.</p>"},{"location":"models/yolov6/","title":"Meituan YOLOv6","text":""},{"location":"models/yolov6/#overview","title":"Overview","text":"<p>Meituan YOLOv6 is a cutting-edge object detector that offers remarkable balance between speed and accuracy, making it a popular choice for real-time applications. This model introduces several notable enhancements on its architecture and training scheme, including the implementation of a Bi-directional Concatenation (BiC) module, an anchor-aided training (AAT) strategy, and an improved backbone and neck design for state-of-the-art accuracy on the COCO dataset.</p> <p> Overview of YOLOv6. Model architecture diagram showing the redesigned network components and training strategies that have led to significant performance improvements. (a) The neck of YOLOv6 (N and S are shown). Note for M/L, RepBlocks is replaced with CSPStackRep. (b) The structure of a BiC module. (c) A SimCSPSPPF block. (source).</p>"},{"location":"models/yolov6/#key-features","title":"Key Features","text":"<ul> <li>Bidirectional Concatenation (BiC) Module: YOLOv6 introduces a BiC module in the neck of the detector, enhancing localization signals and delivering performance gains with negligible speed degradation.</li> <li>Anchor-Aided Training (AAT) Strategy: This model proposes AAT to enjoy the benefits of both anchor-based and anchor-free paradigms without compromising inference efficiency.</li> <li>Enhanced Backbone and Neck Design: By deepening YOLOv6 to include another stage in the backbone and neck, this model achieves state-of-the-art performance on the COCO dataset at high-resolution input.</li> <li>Self-Distillation Strategy: A new self-distillation strategy is implemented to boost the performance of smaller models of YOLOv6, enhancing the auxiliary regression branch during training and removing it at inference to avoid a marked speed decline.</li> </ul>"},{"location":"models/yolov6/#pre-trained-models","title":"Pre-trained Models","text":"<p>YOLOv6 provides various pre-trained models with different scales:</p> <ul> <li>YOLOv6-N: 37.5% AP on COCO val2017 at 1187 FPS with NVIDIA Tesla T4 GPU.</li> <li>YOLOv6-S: 45.0% AP at 484 FPS.</li> <li>YOLOv6-M: 50.0% AP at 226 FPS.</li> <li>YOLOv6-L: 52.8% AP at 116 FPS.</li> <li>YOLOv6-L6: State-of-the-art accuracy in real-time.</li> </ul> <p>YOLOv6 also provides quantized models for different precisions and models optimized for mobile platforms.</p>"},{"location":"models/yolov6/#usage","title":"Usage","text":"<p>You can use YOLOv6 for object detection tasks using the Ultralytics pip package. The following is a sample code snippet showing how to use YOLOv6 models for training:</p> <p>This example provides simple training code for YOLOv6. For more options including training settings see Train mode. For using YOLOv6 with additional modes see Predict, Val and Export.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLO()</code> class to create a model instance in python:</p> <pre><code>from ultralytics import YOLO\n# Build a YOLOv6n model from scratch\nmodel = YOLO('yolov6n.yaml')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the YOLOv6n model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Build a YOLOv6n model from scratch and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n# Build a YOLOv6n model from scratch and run inference on the 'bus.jpg' image\nyolo predict model=yolov6n.yaml source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov6/#supported-tasks","title":"Supported Tasks","text":"Model Type Pre-trained Weights Tasks Supported YOLOv6-N <code>yolov6-n.pt</code> Object Detection YOLOv6-S <code>yolov6-s.pt</code> Object Detection YOLOv6-M <code>yolov6-m.pt</code> Object Detection YOLOv6-L <code>yolov6-l.pt</code> Object Detection YOLOv6-L6 <code>yolov6-l6.pt</code> Object Detection"},{"location":"models/yolov6/#supported-modes","title":"Supported Modes","text":"Mode Supported Inference Validation Training"},{"location":"models/yolov6/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>We would like to acknowledge the authors for their significant contributions in the field of real-time object detection:</p> BibTeX <pre><code>@misc{li2023yolov6,\ntitle={YOLOv6 v3.0: A Full-Scale Reloading},\nauthor={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\nyear={2023},\neprint={2301.05586},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n</code></pre> <p>The original YOLOv6 paper can be found on arXiv. The authors have made their work publicly available, and the codebase can be accessed on GitHub. We appreciate their efforts in advancing the field and making their work accessible to the broader community.</p>"},{"location":"models/yolov7/","title":"YOLOv7: Trainable Bag-of-Freebies","text":"<p>YOLOv7 is a state-of-the-art real-time object detector that surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS. It has the highest accuracy (56.8% AP) among all known real-time object detectors with 30 FPS or higher on GPU V100. Moreover, YOLOv7 outperforms other object detectors such as YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, and many others in speed and accuracy. The model is trained on the MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code for YOLOv7 is available on GitHub.</p> <p> Comparison of state-of-the-art object detectors. From the results in Table 2 we know that the proposed method has the best speed-accuracy trade-off comprehensively. If we compare YOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method is 127 fps faster and 10.7% more accurate on AP. In addition, YOLOv7 has 51.4% AP at frame rate of 161 fps, while PPYOLOE-L with the same AP has only 78 fps frame rate. In terms of parameter usage, YOLOv7 is 41% less than PPYOLOE-L. If we compare YOLOv7-X with 114 fps inference speed to YOLOv5-L (r6.1) with 99 fps inference speed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-X is compared with YOLOv5-X (r6.1) of similar scale, the inference speed of YOLOv7-X is 31 fps faster. In addition, in terms the amount of parameters and computation, YOLOv7-X reduces 22% of parameters and 8% of computation compared to YOLOv5-X (r6.1), but improves AP by 2.2% (Source).</p>"},{"location":"models/yolov7/#overview","title":"Overview","text":"<p>Real-time object detection is an important component in many computer vision systems, including multi-object tracking, autonomous driving, robotics, and medical image analysis. In recent years, real-time object detection development has focused on designing efficient architectures and improving the inference speed of various CPUs, GPUs, and neural processing units (NPUs). YOLOv7 supports both mobile GPU and GPU devices, from the edge to the cloud.</p> <p>Unlike traditional real-time object detectors that focus on architecture optimization, YOLOv7 introduces a focus on the optimization of the training process. This includes modules and optimization methods designed to improve the accuracy of object detection without increasing the inference cost, a concept known as the \"trainable bag-of-freebies\".</p>"},{"location":"models/yolov7/#key-features","title":"Key Features","text":"<p>YOLOv7 introduces several key features:</p> <ol> <li> <p>Model Re-parameterization: YOLOv7 proposes a planned re-parameterized model, which is a strategy applicable to layers in different networks with the concept of gradient propagation path.</p> </li> <li> <p>Dynamic Label Assignment: The training of the model with multiple output layers presents a new issue: \"How to assign dynamic targets for the outputs of different branches?\" To solve this problem, YOLOv7 introduces a new label assignment method called coarse-to-fine lead guided label assignment.</p> </li> <li> <p>Extended and Compound Scaling: YOLOv7 proposes \"extend\" and \"compound scaling\" methods for the real-time object detector that can effectively utilize parameters and computation.</p> </li> <li> <p>Efficiency: The method proposed by YOLOv7 can effectively reduce about 40% parameters and 50% computation of state-of-the-art real-time object detector, and has faster inference speed and higher detection accuracy.</p> </li> </ol>"},{"location":"models/yolov7/#usage-examples","title":"Usage Examples","text":"<p>As of the time of writing, Ultralytics does not currently support YOLOv7 models. Therefore, any users interested in using YOLOv7 will need to refer directly to the YOLOv7 GitHub repository for installation and usage instructions.</p> <p>Here is a brief overview of the typical steps you might take to use YOLOv7:</p> <ol> <li> <p>Visit the YOLOv7 GitHub repository: https://github.com/WongKinYiu/yolov7.</p> </li> <li> <p>Follow the instructions provided in the README file for installation. This typically involves cloning the repository, installing necessary dependencies, and setting up any necessary environment variables.</p> </li> <li> <p>Once installation is complete, you can train and use the model as per the usage instructions provided in the repository. This usually involves preparing your dataset, configuring the model parameters, training the model, and then using the trained model to perform object detection.</p> </li> </ol> <p>Please note that the specific steps may vary depending on your specific use case and the current state of the YOLOv7 repository. Therefore, it is strongly recommended to refer directly to the instructions provided in the YOLOv7 GitHub repository.</p> <p>We regret any inconvenience this may cause and will strive to update this document with usage examples for Ultralytics once support for YOLOv7 is implemented.</p>"},{"location":"models/yolov7/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>We would like to acknowledge the YOLOv7 authors for their significant contributions in the field of real-time object detection:</p> BibTeX <pre><code>@article{wang2022yolov7,\ntitle={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\nauthor={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\njournal={arXiv preprint arXiv:2207.02696},\nyear={2022}\n}\n</code></pre> <p>The original YOLOv7 paper can be found on arXiv. The authors have made their work publicly available, and the codebase can be accessed on GitHub. We appreciate their efforts in advancing the field and making their work accessible to the broader community.</p>"},{"location":"models/yolov8/","title":"YOLOv8","text":""},{"location":"models/yolov8/#overview","title":"Overview","text":"<p>YOLOv8 is the latest iteration in the YOLO series of real-time object detectors, offering cutting-edge performance in terms of accuracy and speed. Building upon the advancements of previous YOLO versions, YOLOv8 introduces new features and optimizations that make it an ideal choice for various object detection tasks in a wide range of applications.</p> <p></p>"},{"location":"models/yolov8/#key-features","title":"Key Features","text":"<ul> <li>Advanced Backbone and Neck Architectures: YOLOv8 employs state-of-the-art backbone and neck architectures, resulting in improved feature extraction and object detection performance.</li> <li>Anchor-free Split Ultralytics Head: YOLOv8 adopts an anchor-free split Ultralytics head, which contributes to better accuracy and a more efficient detection process compared to anchor-based approaches.</li> <li>Optimized Accuracy-Speed Tradeoff: With a focus on maintaining an optimal balance between accuracy and speed, YOLOv8 is suitable for real-time object detection tasks in diverse application areas.</li> <li>Variety of Pre-trained Models: YOLOv8 offers a range of pre-trained models to cater to various tasks and performance requirements, making it easier to find the right model for your specific use case.</li> </ul>"},{"location":"models/yolov8/#supported-tasks","title":"Supported Tasks","text":"Model Type Pre-trained Weights Task YOLOv8 <code>yolov8n.pt</code>, <code>yolov8s.pt</code>, <code>yolov8m.pt</code>, <code>yolov8l.pt</code>, <code>yolov8x.pt</code> Detection YOLOv8-seg <code>yolov8n-seg.pt</code>, <code>yolov8s-seg.pt</code>, <code>yolov8m-seg.pt</code>, <code>yolov8l-seg.pt</code>, <code>yolov8x-seg.pt</code> Instance Segmentation YOLOv8-pose <code>yolov8n-pose.pt</code>, <code>yolov8s-pose.pt</code>, <code>yolov8m-pose.pt</code>, <code>yolov8l-pose.pt</code>, <code>yolov8x-pose.pt</code>, <code>yolov8x-pose-p6.pt</code> Pose/Keypoints YOLOv8-cls <code>yolov8n-cls.pt</code>, <code>yolov8s-cls.pt</code>, <code>yolov8m-cls.pt</code>, <code>yolov8l-cls.pt</code>, <code>yolov8x-cls.pt</code> Classification"},{"location":"models/yolov8/#supported-modes","title":"Supported Modes","text":"Mode Supported Inference Validation Training <p>Performance</p> DetectionSegmentationClassificationPose Model size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 Model size<sup>(pixels) mAP<sup>box50-95 mAP<sup>mask50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 Model size<sup>(pixels) acc<sup>top1 acc<sup>top5 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) at 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 Model size<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4"},{"location":"models/yolov8/#usage","title":"Usage","text":"<p>You can use YOLOv8 for object detection tasks using the Ultralytics pip package. The following is a sample code snippet showing how to use YOLOv8 models for inference:</p> <p>This example provides simple inference code for YOLOv8. For more options including handling inference results see Predict mode. For using YOLOv8 with additional modes see Train, Val and Export.</p> PythonCLI <p>PyTorch pretrained <code>*.pt</code> models as well as configuration <code>*.yaml</code> files can be passed to the <code>YOLO()</code> class to create a model instance in python:</p> <pre><code>from ultralytics import YOLO\n# Load a COCO-pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Display model information (optional)\nmodel.info()\n# Train the model on the COCO8 example dataset for 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n# Run inference with the YOLOv8n model on the 'bus.jpg' image\nresults = model('path/to/bus.jpg')\n</code></pre> <p>CLI commands are available to directly run the models:</p> <pre><code># Load a COCO-pretrained YOLOv8n model and train it on the COCO8 example dataset for 100 epochs\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n# Load a COCO-pretrained YOLOv8n model and run inference on the 'bus.jpg' image\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov8/#citations-and-acknowledgements","title":"Citations and Acknowledgements","text":"<p>If you use the YOLOv8 model or any other software from this repository in your work, please cite it using the following format:</p> BibTeX <pre><code>@software{yolov8_ultralytics,\nauthor = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\ntitle = {Ultralytics YOLOv8},\nversion = {8.0.0},\nyear = {2023},\nurl = {https://github.com/ultralytics/ultralytics},\norcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\nlicense = {AGPL-3.0}\n}\n</code></pre> <p>Please note that the DOI is pending and will be added to the citation once it is available. The usage of the software is in accordance with the AGPL-3.0 license.</p>"},{"location":"modes/","title":"Ultralytics YOLOv8 Modes","text":"<p>Ultralytics YOLOv8 supports several modes that can be used to perform different tasks. These modes are:</p> <ul> <li>Train: For training a YOLOv8 model on a custom dataset.</li> <li>Val: For validating a YOLOv8 model after it has been trained.</li> <li>Predict: For making predictions using a trained YOLOv8 model on new images or videos.</li> <li>Export: For exporting a YOLOv8 model to a format that can be used for deployment.</li> <li>Track: For tracking objects in real-time using a YOLOv8 model.</li> <li>Benchmark: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.</li> </ul>"},{"location":"modes/#train","title":"Train","text":"<p>Train mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can accurately predict the classes and locations of objects in an image.</p> <p>Train Examples</p>"},{"location":"modes/#val","title":"Val","text":"<p>Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters of the model to improve its performance.</p> <p>Val Examples</p>"},{"location":"modes/#predict","title":"Predict","text":"<p>Predict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the model is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model predicts the classes and locations of objects in the input images or videos.</p> <p>Predict Examples</p>"},{"location":"modes/#export","title":"Export","text":"<p>Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is converted to a format that can be used by other software applications or hardware devices. This mode is useful when deploying the model to production environments.</p> <p>Export Examples</p>"},{"location":"modes/#track","title":"Track","text":"<p>Track mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a checkpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful for applications such as surveillance systems or self-driving cars.</p> <p>Track Examples</p>"},{"location":"modes/#benchmark","title":"Benchmark","text":"<p>Benchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide information on the size of the exported format, its <code>mAP50-95</code> metrics (for object detection, segmentation and pose) or <code>accuracy_top5</code> metrics (for classification), and the inference time in milliseconds per image across various export formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for their specific use case based on their requirements for speed and accuracy.</p> <p>Benchmark Examples</p>"},{"location":"modes/benchmark/","title":"Benchmark","text":"<p>Benchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide information on the size of the exported format, its <code>mAP50-95</code> metrics (for object detection, segmentation and pose) or <code>accuracy_top5</code> metrics (for classification), and the inference time in milliseconds per image across various export formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for their specific use case based on their requirements for speed and accuracy.</p> <p>Tip</p> <ul> <li>Export to ONNX or OpenVINO for up to 3x CPU speedup.</li> <li>Export to TensorRT for up to 5x GPU speedup.</li> </ul>"},{"location":"modes/benchmark/#usage-examples","title":"Usage Examples","text":"<p>Run YOLOv8n benchmarks on all supported export formats including ONNX, TensorRT etc. See Arguments section below for a full list of export arguments.</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n# Benchmark on GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#arguments","title":"Arguments","text":"<p>Arguments such as <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code>, and <code>verbose</code> provide users with the flexibility to fine-tune the benchmarks to their specific needs and compare the performance of different export formats with ease.</p> Key Value Description <code>model</code> <code>None</code> path to model file, i.e. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> path to YAML referencing the benchmarking dataset (under <code>val</code> label) <code>imgsz</code> <code>640</code> image size as scalar or (h, w) list, i.e. (640, 480) <code>half</code> <code>False</code> FP16 quantization <code>int8</code> <code>False</code> INT8 quantization <code>device</code> <code>None</code> device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu <code>verbose</code> <code>False</code> do not continue on error (bool), or val floor threshold (float)"},{"location":"modes/benchmark/#export-formats","title":"Export Formats","text":"<p>Benchmarks will attempt to run automatically on all possible export formats below.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>See full <code>export</code> details in the Export page.</p>"},{"location":"modes/export/","title":"Export","text":"<p>Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is converted to a format that can be used by other software applications or hardware devices. This mode is useful when deploying the model to production environments.</p> <p>Tip</p> <ul> <li>Export to ONNX or OpenVINO for up to 3x CPU speedup.</li> <li>Export to TensorRT for up to 5x GPU speedup.</li> </ul>"},{"location":"modes/export/#usage-examples","title":"Usage Examples","text":"<p>Export a YOLOv8n model to a different format like ONNX or TensorRT. See Arguments section below for a full list of export arguments.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom trained\n# Export the model\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre>"},{"location":"modes/export/#arguments","title":"Arguments","text":"<p>Export settings for YOLO models refer to the various configurations and options used to save or export the model for use in other environments or platforms. These settings can affect the model's performance, size, and compatibility with different systems. Some common YOLO export settings include the format of the exported model file (e.g. ONNX, TensorFlow SavedModel), the device on which the model will be run (e.g. CPU, GPU), and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the export process include the specific task the model is being used for and the requirements or constraints of the target environment or platform. It is important to carefully consider and configure these settings to ensure that the exported model is optimized for the intended use case and can be used effectively in the target environment.</p> Key Value Description <code>format</code> <code>'torchscript'</code> format to export to <code>imgsz</code> <code>640</code> image size as scalar or (h, w) list, i.e. (640, 480) <code>keras</code> <code>False</code> use Keras for TF SavedModel export <code>optimize</code> <code>False</code> TorchScript: optimize for mobile <code>half</code> <code>False</code> FP16 quantization <code>int8</code> <code>False</code> INT8 quantization <code>dynamic</code> <code>False</code> ONNX/TensorRT: dynamic axes <code>simplify</code> <code>False</code> ONNX/TensorRT: simplify model <code>opset</code> <code>None</code> ONNX: opset version (optional, defaults to latest) <code>workspace</code> <code>4</code> TensorRT: workspace size (GB) <code>nms</code> <code>False</code> CoreML: add NMS"},{"location":"modes/export/#export-formats","title":"Export Formats","text":"<p>Available YOLOv8 export formats are in the table below. You can export to any format using the <code>format</code> argument, i.e. <code>format='onnx'</code> or <code>format='engine'</code>.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Predict","text":"<p>YOLOv8 predict mode can generate predictions for various tasks, returning either a list of <code>Results</code> objects or a memory-efficient generator of <code>Results</code> objects when using the streaming mode. Enable streaming mode by passing <code>stream=True</code> in the predictor's call method.</p> <p>Predict</p> Return a list with <code>stream=False</code>Return a generator with <code>stream=True</code> <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n# Run batched inference on a list of images\nresults = model(['im1.jpg', 'im2.jpg'])  # return a list of Results objects\n# Process results list\nfor result in results:\nboxes = result.boxes  # Boxes object for bbox outputs\nmasks = result.masks  # Masks object for segmentation masks outputs\nkeypoints = result.keypoints  # Keypoints object for pose outputs\nprobs = result.probs  # Probs object for classification outputs\n</code></pre> <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n# Run batched inference on a list of images\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # return a generator of Results objects\n# Process results generator\nfor result in results:\nboxes = result.boxes  # Boxes object for bbox outputs\nmasks = result.masks  # Masks object for segmentation masks outputs\nkeypoints = result.keypoints  # Keypoints object for pose outputs\nprobs = result.probs  # Probs object for classification outputs\n</code></pre>"},{"location":"modes/predict/#inference-sources","title":"Inference Sources","text":"<p>YOLOv8 can process different types of input sources for inference, as shown in the table below. The sources include static images, video streams, and various data formats. The table also indicates whether each source can be used in streaming mode with the argument <code>stream=True</code> \u2705. Streaming mode is beneficial for processing videos or live streams as it creates a generator of results instead of loading all frames into memory.</p> <p>Tip</p> <p>Use <code>stream=True</code> for processing long videos or large datasets to efficiently manage memory. When <code>stream=False</code>, the results for all frames or data points are stored in memory, which can quickly add up and cause out-of-memory errors for large inputs. In contrast, <code>stream=True</code> utilizes a generator, which only keeps the results of the current frame or data point in memory, significantly reducing memory consumption and preventing out-of-memory issues.</p> Source Argument Type Notes image <code>'image.jpg'</code> <code>str</code> or <code>Path</code> Single image file. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL to an image. screenshot <code>'screen'</code> <code>str</code> Capture a screenshot. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> HWC format with RGB channels. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> HWC format with BGR channels <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> HWC format with BGR channels <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> BCHW format with RGB channels <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> or <code>Path</code> CSV file containing paths to images, videos, or directories. video \u2705 <code>'video.mp4'</code> <code>str</code> or <code>Path</code> Video file in formats like MP4, AVI, etc. directory \u2705 <code>'path/'</code> <code>str</code> or <code>Path</code> Path to a directory containing images or videos. glob \u2705 <code>'path/*.jpg'</code> <code>str</code> Glob pattern to match multiple files. Use the <code>*</code> character as a wildcard. YouTube \u2705 <code>'https://youtu.be/Zgi9g1ksQHc'</code> <code>str</code> URL to a YouTube video. stream \u2705 <code>'rtsp://example.com/media.mp4'</code> <code>str</code> URL for streaming protocols such as RTSP, RTMP, or an IP address. multi-stream \u2705 <code>'list.streams'</code> <code>str</code> or <code>Path</code> <code>*.streams</code> text file with one stream URL per row, i.e. 8 streams will run at batch-size 8. <p>Below are code examples for using each source type:</p> <p>Prediction sources</p> imagescreenshotURLPILOpenCVnumpytorchCSVvideodirectoryglobYouTubeStreams <p>Run inference on an image file. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define path to the image file\nsource = 'path/to/image.jpg'\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on the current screen content as a screenshot. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define current screenshot as source\nsource = 'screen'\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on an image or video hosted remotely via URL. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define remote image or video URL\nsource = 'https://ultralytics.com/images/bus.jpg'\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on an image opened with Python Imaging Library (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Open an image using PIL\nsource = Image.open('path/to/image.jpg')\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on an image read with OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Read an image using OpenCV\nsource = cv2.imread('path/to/image.jpg')\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on an image represented as a numpy array. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Create a random numpy array of HWC shape (640, 640, 3) with values in range [0, 255] and type uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on an image represented as a PyTorch tensor. <pre><code>import torch\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Create a random torch tensor of BCHW shape (1, 3, 640, 640) with values in range [0, 1] and type float32\nsource = torch.rand(1, 3, 640, 640, dtype=torch.float32)\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on a collection of images, URLs, videos and directories listed in a CSV file. <pre><code>import torch\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define a path to a CSV file with images, URLs, videos and directories\nsource = 'path/to/file.csv'\n# Run inference on the source\nresults = model(source)  # list of Results objects\n</code></pre></p> <p>Run inference on a video file. By using <code>stream=True</code>, you can create a generator of Results objects to reduce memory usage. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define path to video file\nsource = 'path/to/video.mp4'\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n</code></pre></p> <p>Run inference on all images and videos in a directory. To also capture images and videos in subdirectories use a glob pattern, i.e. <code>path/to/dir/**/*</code>. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define path to directory containing images and videos for inference\nsource = 'path/to/dir'\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n</code></pre></p> <p>Run inference on all images and videos that match a glob expression with <code>*</code> characters. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define a glob search for all JPG files in a directory\nsource = 'path/to/dir/*.jpg'\n# OR define a recursive glob search for all JPG files including subdirectories\nsource = 'path/to/dir/**/*.jpg'\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n</code></pre></p> <p>Run inference on a YouTube video. By using <code>stream=True</code>, you can create a generator of Results objects to reduce memory usage for long videos. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Define source as YouTube video URL\nsource = 'https://youtu.be/Zgi9g1ksQHc'\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n</code></pre></p> <p>Run inference on remote streaming sources using RTSP, RTMP, and IP address protocols. If mutliple streams are provided in a <code>*.streams</code> text file then batched inference will run, i.e. 8 streams will run at batch-size 8, otherwise single streams will run at batch-size 1. <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Single stream with batch-size 1 inference\nsource = 'rtsp://example.com/media.mp4'  # RTSP, RTMP or IP streaming address\n# Multiple streams with batched inference (i.e. batch-size 8 for 8 streams)\nsource = 'path/to/list.streams'  # *.streams text file with one streaming address per row\n# Run inference on the source\nresults = model(source, stream=True)  # generator of Results objects\n</code></pre></p>"},{"location":"modes/predict/#inference-arguments","title":"Inference Arguments","text":"<p><code>model.predict()</code> accepts multiple arguments that can be passed at inference time to override defaults:</p> <p>Example</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Run inference on 'bus.jpg' with arguments\nmodel.predict('bus.jpg', save=True, imgsz=320, conf=0.5)\n</code></pre> <p>All supported arguments:</p> Name Type Default Description <code>source</code> <code>str</code> <code>'ultralytics/assets'</code> source directory for images or videos <code>conf</code> <code>float</code> <code>0.25</code> object confidence threshold for detection <code>iou</code> <code>float</code> <code>0.7</code> intersection over union (IoU) threshold for NMS <code>imgsz</code> <code>int or tuple</code> <code>640</code> image size as scalar or (h, w) list, i.e. (640, 480) <code>half</code> <code>bool</code> <code>False</code> use half precision (FP16) <code>device</code> <code>None or str</code> <code>None</code> device to run on, i.e. cuda device=0/1/2/3 or device=cpu <code>show</code> <code>bool</code> <code>False</code> show results if possible <code>save</code> <code>bool</code> <code>False</code> save images with results <code>save_txt</code> <code>bool</code> <code>False</code> save results as .txt file <code>save_conf</code> <code>bool</code> <code>False</code> save results with confidence scores <code>save_crop</code> <code>bool</code> <code>False</code> save cropped images with results <code>hide_labels</code> <code>bool</code> <code>False</code> hide labels <code>hide_conf</code> <code>bool</code> <code>False</code> hide confidence scores <code>max_det</code> <code>int</code> <code>300</code> maximum number of detections per image <code>vid_stride</code> <code>bool</code> <code>False</code> video frame-rate stride <code>line_width</code> <code>None or int</code> <code>None</code> The line width of the bounding boxes. If None, it is scaled to the image size. <code>visualize</code> <code>bool</code> <code>False</code> visualize model features <code>augment</code> <code>bool</code> <code>False</code> apply image augmentation to prediction sources <code>agnostic_nms</code> <code>bool</code> <code>False</code> class-agnostic NMS <code>retina_masks</code> <code>bool</code> <code>False</code> use high-resolution segmentation masks <code>classes</code> <code>None or list</code> <code>None</code> filter results by class, i.e. classes=0, or classes=[0,2,3] <code>boxes</code> <code>bool</code> <code>True</code> Show boxes in segmentation predictions"},{"location":"modes/predict/#image-and-video-formats","title":"Image and Video Formats","text":"<p>YOLOv8 supports various image and video formats, as specified in data/utils.py. See the tables below for the valid suffixes and example predict commands.</p>"},{"location":"modes/predict/#images","title":"Images","text":"<p>The below table contains valid Ultralytics image formats.</p> Image Suffixes Example Predict Command Reference .bmp <code>yolo predict source=image.bmp</code> Microsoft BMP File Format .dng <code>yolo predict source=image.dng</code> Adobe DNG .jpeg <code>yolo predict source=image.jpeg</code> JPEG .jpg <code>yolo predict source=image.jpg</code> JPEG .mpo <code>yolo predict source=image.mpo</code> Multi Picture Object .png <code>yolo predict source=image.png</code> Portable Network Graphics .tif <code>yolo predict source=image.tif</code> Tag Image File Format .tiff <code>yolo predict source=image.tiff</code> Tag Image File Format .webp <code>yolo predict source=image.webp</code> WebP .pfm <code>yolo predict source=image.pfm</code> Portable FloatMap"},{"location":"modes/predict/#videos","title":"Videos","text":"<p>The below table contains valid Ultralytics video formats.</p> Video Suffixes Example Predict Command Reference .asf <code>yolo predict source=video.asf</code> Advanced Systems Format .avi <code>yolo predict source=video.avi</code> Audio Video Interleave .gif <code>yolo predict source=video.gif</code> Graphics Interchange Format .m4v <code>yolo predict source=video.m4v</code> MPEG-4 Part 14 .mkv <code>yolo predict source=video.mkv</code> Matroska .mov <code>yolo predict source=video.mov</code> QuickTime File Format .mp4 <code>yolo predict source=video.mp4</code> MPEG-4 Part 14 - Wikipedia .mpeg <code>yolo predict source=video.mpeg</code> MPEG-1 Part 2 .mpg <code>yolo predict source=video.mpg</code> MPEG-1 Part 2 .ts <code>yolo predict source=video.ts</code> MPEG Transport Stream .wmv <code>yolo predict source=video.wmv</code> Windows Media Video .webm <code>yolo predict source=video.webm</code> WebM Project"},{"location":"modes/predict/#working-with-results","title":"Working with Results","text":"<p>All Ultralytics <code>predict()</code> calls will return a list of <code>Results</code> objects:</p> <p>Results</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Run inference on an image\nresults = model('bus.jpg')  # list of 1 Results object\nresults = model(['bus.jpg', 'zidane.jpg'])  # list of 2 Results objects\n</code></pre> <p><code>Results</code> objects have the following attributes:</p> Attribute Type Description <code>orig_img</code> <code>numpy.ndarray</code> The original image as a numpy array. <code>orig_shape</code> <code>tuple</code> The original image shape in (height, width) format. <code>boxes</code> <code>Boxes, optional</code> A Boxes object containing the detection bounding boxes. <code>masks</code> <code>Masks, optional</code> A Masks object containing the detection masks. <code>probs</code> <code>Probs, optional</code> A Probs object containing probabilities of each class for classification task. <code>keypoints</code> <code>Keypoints, optional</code> A Keypoints object containing detected keypoints for each object. <code>speed</code> <code>dict</code> A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image. <code>names</code> <code>dict</code> A dictionary of class names. <code>path</code> <code>str</code> The path to the image file. <p><code>Results</code> objects have the following methods:</p> Method Return Type Description <code>__getitem__()</code> <code>Results</code> Return a Results object for the specified index. <code>__len__()</code> <code>int</code> Return the number of detections in the Results object. <code>update()</code> <code>None</code> Update the boxes, masks, and probs attributes of the Results object. <code>cpu()</code> <code>Results</code> Return a copy of the Results object with all tensors on CPU memory. <code>numpy()</code> <code>Results</code> Return a copy of the Results object with all tensors as numpy arrays. <code>cuda()</code> <code>Results</code> Return a copy of the Results object with all tensors on GPU memory. <code>to()</code> <code>Results</code> Return a copy of the Results object with tensors on the specified device and dtype. <code>new()</code> <code>Results</code> Return a new Results object with the same image, path, and names. <code>keys()</code> <code>List[str]</code> Return a list of non-empty attribute names. <code>plot()</code> <code>numpy.ndarray</code> Plots the detection results. Returns a numpy array of the annotated image. <code>verbose()</code> <code>str</code> Return log string for each task. <code>save_txt()</code> <code>None</code> Save predictions into a txt file. <code>save_crop()</code> <code>None</code> Save cropped predictions to <code>save_dir/cls/file_name.jpg</code>. <code>tojson()</code> <code>None</code> Convert the object to JSON format. <p>For more details see the <code>Results</code> class documentation.</p>"},{"location":"modes/predict/#boxes","title":"Boxes","text":"<p><code>Boxes</code> object can be used to index, manipulate, and convert bounding boxes to different formats.</p> <p>Boxes</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Run inference on an image\nresults = model('bus.jpg')  # results list\n# View results\nfor r in results:\nprint(r.boxes)  # print the Boxes object containing the detection bounding boxes\n</code></pre> <p>Here is a table for the <code>Boxes</code> class methods and properties, including their name, type, and description:</p> Name Type Description <code>cpu()</code> Method Move the object to CPU memory. <code>numpy()</code> Method Convert the object to a numpy array. <code>cuda()</code> Method Move the object to CUDA memory. <code>to()</code> Method Move the object to the specified device. <code>xyxy</code> Property (<code>torch.Tensor</code>) Return the boxes in xyxy format. <code>conf</code> Property (<code>torch.Tensor</code>) Return the confidence values of the boxes. <code>cls</code> Property (<code>torch.Tensor</code>) Return the class values of the boxes. <code>id</code> Property (<code>torch.Tensor</code>) Return the track IDs of the boxes (if available). <code>xywh</code> Property (<code>torch.Tensor</code>) Return the boxes in xywh format. <code>xyxyn</code> Property (<code>torch.Tensor</code>) Return the boxes in xyxy format normalized by original image size. <code>xywhn</code> Property (<code>torch.Tensor</code>) Return the boxes in xywh format normalized by original image size. <p>For more details see the <code>Boxes</code> class documentation.</p>"},{"location":"modes/predict/#masks","title":"Masks","text":"<p><code>Masks</code> object can be used index, manipulate and convert masks to segments.</p> <p>Masks</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n-seg Segment model\nmodel = YOLO('yolov8n-seg.pt')\n# Run inference on an image\nresults = model('bus.jpg')  # results list\n# View results\nfor r in results:\nprint(r.masks)  # print the Masks object containing the detected instance masks\n</code></pre> <p>Here is a table for the <code>Masks</code> class methods and properties, including their name, type, and description:</p> Name Type Description <code>cpu()</code> Method Returns the masks tensor on CPU memory. <code>numpy()</code> Method Returns the masks tensor as a numpy array. <code>cuda()</code> Method Returns the masks tensor on GPU memory. <code>to()</code> Method Returns the masks tensor with the specified device and dtype. <code>xyn</code> Property (<code>torch.Tensor</code>) A list of normalized segments represented as tensors. <code>xy</code> Property (<code>torch.Tensor</code>) A list of segments in pixel coordinates represented as tensors. <p>For more details see the <code>Masks</code> class documentation.</p>"},{"location":"modes/predict/#keypoints","title":"Keypoints","text":"<p><code>Keypoints</code> object can be used index, manipulate and normalize coordinates.</p> <p>Keypoints</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n-pose Pose model\nmodel = YOLO('yolov8n-pose.pt')\n# Run inference on an image\nresults = model('bus.jpg')  # results list\n# View results\nfor r in results:\nprint(r.keypoints)  # print the Keypoints object containing the detected keypoints\n</code></pre> <p>Here is a table for the <code>Keypoints</code> class methods and properties, including their name, type, and description:</p> Name Type Description <code>cpu()</code> Method Returns the keypoints tensor on CPU memory. <code>numpy()</code> Method Returns the keypoints tensor as a numpy array. <code>cuda()</code> Method Returns the keypoints tensor on GPU memory. <code>to()</code> Method Returns the keypoints tensor with the specified device and dtype. <code>xyn</code> Property (<code>torch.Tensor</code>) A list of normalized keypoints represented as tensors. <code>xy</code> Property (<code>torch.Tensor</code>) A list of keypoints in pixel coordinates represented as tensors. <code>conf</code> Property (<code>torch.Tensor</code>) Returns confidence values of keypoints if available, else None. <p>For more details see the <code>Keypoints</code> class documentation.</p>"},{"location":"modes/predict/#probs","title":"Probs","text":"<p><code>Probs</code> object can be used index, get <code>top1</code> and <code>top5</code> indices and scores of classification.</p> <p>Probs</p> <pre><code>from ultralytics import YOLO\n# Load a pretrained YOLOv8n-cls Classify model\nmodel = YOLO('yolov8n-cls.pt')\n# Run inference on an image\nresults = model('bus.jpg')  # results list\n# View results\nfor r in results:\nprint(r.probs)  # print the Probs object containing the detected class probabilities\n</code></pre> <p>Here's a table summarizing the methods and properties for the <code>Probs</code> class:</p> Name Type Description <code>cpu()</code> Method Returns a copy of the probs tensor on CPU memory. <code>numpy()</code> Method Returns a copy of the probs tensor as a numpy array. <code>cuda()</code> Method Returns a copy of the probs tensor on GPU memory. <code>to()</code> Method Returns a copy of the probs tensor with the specified device and dtype. <code>top1</code> Property (<code>int</code>) Index of the top 1 class. <code>top5</code> Property (<code>list[int]</code>) Indices of the top 5 classes. <code>top1conf</code> Property (<code>torch.Tensor</code>) Confidence of the top 1 class. <code>top5conf</code> Property (<code>torch.Tensor</code>) Confidences of the top 5 classes. <p>For more details see the <code>Probs</code> class documentation.</p>"},{"location":"modes/predict/#plotting-results","title":"Plotting Results","text":"<p>You can use the <code>plot()</code> method of a <code>Result</code> objects to visualize predictions. It plots all prediction types (boxes, masks, keypoints, probabilities, etc.) contained in the <code>Results</code> object onto a numpy array that can then be shown or saved.</p> <p>Plotting</p> <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n# Run inference on 'bus.jpg'\nresults = model('bus.jpg')  # results list\n# Show the results\nfor r in results:\nim_array = r.plot()  # plot a BGR numpy array of predictions\nim = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\nim.show()  # show image\nim.save('results.jpg')  # save image\n</code></pre> <p>The <code>plot()</code> method supports the following arguments:</p> Argument Type Description Default <code>conf</code> <code>bool</code> Whether to plot the detection confidence score. <code>True</code> <code>line_width</code> <code>float</code> The line width of the bounding boxes. If None, it is scaled to the image size. <code>None</code> <code>font_size</code> <code>float</code> The font size of the text. If None, it is scaled to the image size. <code>None</code> <code>font</code> <code>str</code> The font to use for the text. <code>'Arial.ttf'</code> <code>pil</code> <code>bool</code> Whether to return the image as a PIL Image. <code>False</code> <code>img</code> <code>numpy.ndarray</code> Plot to another image. if not, plot to original image. <code>None</code> <code>im_gpu</code> <code>torch.Tensor</code> Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting. <code>None</code> <code>kpt_radius</code> <code>int</code> Radius of the drawn keypoints. Default is 5. <code>5</code> <code>kpt_line</code> <code>bool</code> Whether to draw lines connecting keypoints. <code>True</code> <code>labels</code> <code>bool</code> Whether to plot the label of bounding boxes. <code>True</code> <code>boxes</code> <code>bool</code> Whether to plot the bounding boxes. <code>True</code> <code>masks</code> <code>bool</code> Whether to plot the masks. <code>True</code> <code>probs</code> <code>bool</code> Whether to plot classification probability <code>True</code>"},{"location":"modes/predict/#streaming-source-for-loop","title":"Streaming Source <code>for</code>-loop","text":"<p>Here's a Python script using OpenCV (<code>cv2</code>) and YOLOv8 to run inference on video frames. This script assumes you have already installed the necessary packages (<code>opencv-python</code> and <code>ultralytics</code>).</p> <p>Streaming for-loop</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n# Open the video file\nvideo_path = \"path/to/your/video/file.mp4\"\ncap = cv2.VideoCapture(video_path)\n# Loop through the video frames\nwhile cap.isOpened():\n# Read a frame from the video\nsuccess, frame = cap.read()\nif success:\n# Run YOLOv8 inference on the frame\nresults = model(frame)\n# Visualize the results on the frame\nannotated_frame = results[0].plot()\n# Display the annotated frame\ncv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n# Break the loop if 'q' is pressed\nif cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\nbreak\nelse:\n# Break the loop if the end of the video is reached\nbreak\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>This script will run predictions on each frame of the video, visualize the results, and display them in a window. The loop can be exited by pressing 'q'.</p>"},{"location":"modes/track/","title":"Track","text":"<p>Object tracking is a task that involves identifying the location and class of objects, then assigning a unique ID to that detection in video streams.</p> <p>The output of tracker is the same as detection with an added object ID.</p>"},{"location":"modes/track/#available-trackers","title":"Available Trackers","text":"<p>Ultralytics YOLO supports the following tracking algorithms. They can be enabled by passing the relevant YAML configuration file such as <code>tracker=tracker_type.yaml</code>:</p> <ul> <li>BoT-SORT - Use <code>botsort.yaml</code> to enable this tracker.</li> <li>ByteTrack - Use <code>bytetrack.yaml</code> to enable this tracker.</li> </ul> <p>The default tracker is BoT-SORT.</p>"},{"location":"modes/track/#tracking","title":"Tracking","text":"<p>To run the tracker on video streams, use a trained Detect, Segment or Pose model such as YOLOv8n, YOLOv8n-seg and YOLOv8n-pose.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load an official or custom model\nmodel = YOLO('yolov8n.pt')  # Load an official Detect model\nmodel = YOLO('yolov8n-seg.pt')  # Load an official Segment model\nmodel = YOLO('yolov8n-pose.pt')  # Load an official Pose model\nmodel = YOLO('path/to/best.pt')  # Load a custom trained model\n# Perform tracking with the model\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True)  # Tracking with default tracker\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\")  # Tracking with ByteTrack tracker\n</code></pre> <pre><code># Perform tracking with various models using the command line interface\nyolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\"  # Official Detect model\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/Zgi9g1ksQHc\"  # Official Segment model\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/Zgi9g1ksQHc\"  # Official Pose model\nyolo track model=path/to/best.pt source=\"https://youtu.be/Zgi9g1ksQHc\"  # Custom trained model\n# Track using ByteTrack tracker\nyolo track model=path/to/best.pt tracker=\"bytetrack.yaml\" </code></pre> <p>As can be seen in the above usage, tracking is available for all Detect, Segment and Pose models run on videos or streaming sources.</p>"},{"location":"modes/track/#configuration","title":"Configuration","text":""},{"location":"modes/track/#tracking-arguments","title":"Tracking Arguments","text":"<p>Tracking configuration shares properties with Predict mode, such as <code>conf</code>, <code>iou</code>, and <code>show</code>. For further configurations, refer to the Predict model page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Configure the tracking parameters and run the tracker\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configure tracking parameters and run the tracker using the command line interface\nyolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#tracker-selection","title":"Tracker Selection","text":"<p>Ultralytics also allows you to use a modified tracker configuration file. To do this, simply make a copy of a tracker config file (for example, <code>custom_tracker.yaml</code>) from ultralytics/cfg/trackers and modify any configurations (except the <code>tracker_type</code>) as per your needs.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load the model and run the tracker with a custom configuration file\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Load the model and run the tracker with a custom configuration file using the command line interface\nyolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\" tracker='custom_tracker.yaml'\n</code></pre> <p>For a comprehensive list of tracking arguments, refer to the ultralytics/cfg/trackers page.</p>"},{"location":"modes/track/#python-examples","title":"Python Examples","text":""},{"location":"modes/track/#persisting-tracks-loop","title":"Persisting Tracks Loop","text":"<p>Here is a Python script using OpenCV (<code>cv2</code>) and YOLOv8 to run object tracking on video frames. This script still assumes you have already installed the necessary packages (<code>opencv-python</code> and <code>ultralytics</code>).</p> <p>Streaming for-loop with tracking</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n# Loop through the video frames\nwhile cap.isOpened():\n# Read a frame from the video\nsuccess, frame = cap.read()\nif success:\n# Run YOLOv8 tracking on the frame, persisting tracks between frames\nresults = model.track(frame, persist=True)\n# Visualize the results on the frame\nannotated_frame = results[0].plot()\n# Display the annotated frame\ncv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n# Break the loop if 'q' is pressed\nif cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\nbreak\nelse:\n# Break the loop if the end of the video is reached\nbreak\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Please note the change from <code>model(frame)</code> to <code>model.track(frame)</code>, which enables object tracking instead of simple detection. This modified script will run the tracker on each frame of the video, visualize the results, and display them in a window. The loop can be exited by pressing 'q'.</p>"},{"location":"modes/track/#plotting-tracks-over-time","title":"Plotting Tracks Over Time","text":"<p>Visualizing object tracks over consecutive frames can provide valuable insights into the movement patterns and behavior of detected objects within a video. With Ultralytics YOLOv8, plotting these tracks is a seamless and efficient process.</p> <p>In the following example, we demonstrate how to utilize YOLOv8's tracking capabilities to plot the movement of detected objects across multiple video frames. This script involves opening a video file, reading it frame by frame, and utilizing the YOLO model to identify and track various objects. By retaining the center points of the detected bounding boxes and connecting them, we can draw lines that represent the paths followed by the tracked objects.</p> <p>Plotting tracks over multiple video frames</p> <pre><code>from collections import defaultdict\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n# Load the YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n# Open the video file\nvideo_path = \"path/to/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n# Loop through the video frames\nwhile cap.isOpened():\n# Read a frame from the video\nsuccess, frame = cap.read()\nif success:\n# Run YOLOv8 tracking on the frame, persisting tracks between frames\nresults = model.track(frame, persist=True)\n# Get the boxes and track IDs\nboxes = results[0].boxes.xywh.cpu()\ntrack_ids = results[0].boxes.id.int().cpu().tolist()\n# Visualize the results on the frame\nannotated_frame = results[0].plot()\n# Plot the tracks\nfor box, track_id in zip(boxes, track_ids):\nx, y, w, h = box\ntrack = track_history[track_id]\ntrack.append((float(x), float(y)))  # x, y center point\nif len(track) &gt; 30:  # retain 90 tracks for 90 frames\ntrack.pop(0)\n# Draw the tracking lines\npoints = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\ncv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n# Display the annotated frame\ncv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n# Break the loop if 'q' is pressed\nif cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\nbreak\nelse:\n# Break the loop if the end of the video is reached\nbreak\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modes/track/#multithreaded-tracking","title":"Multithreaded Tracking","text":"<p>Multithreaded tracking provides the capability to run object tracking on multiple video streams simultaneously. This is particularly useful when handling multiple video inputs, such as from multiple surveillance cameras, where concurrent processing can greatly enhance efficiency and performance.</p> <p>In the provided Python script, we make use of Python's <code>threading</code> module to run multiple instances of the tracker concurrently. Each thread is responsible for running the tracker on one video file, and all the threads run simultaneously in the background.</p> <p>To ensure that each thread receives the correct parameters (the video file and the model to use), we define a function <code>run_tracker_in_thread</code> that accepts these parameters and contains the main tracking loop. This function reads the video frame by frame, runs the tracker, and displays the results.</p> <p>Two different models are used in this example: <code>yolov8n.pt</code> and <code>yolov8n-seg.pt</code>, each tracking objects in a different video file. The video files are specified in <code>video_file1</code> and <code>video_file2</code>.</p> <p>The <code>daemon=True</code> parameter in <code>threading.Thread</code> means that these threads will be closed as soon as the main program finishes. We then start the threads with <code>start()</code> and use <code>join()</code> to make the main thread wait until both tracker threads have finished.</p> <p>Finally, after all threads have completed their task, the windows displaying the results are closed using <code>cv2.destroyAllWindows()</code>.</p> <p>Streaming for-loop with tracking</p> <pre><code>import threading\nimport cv2\nfrom ultralytics import YOLO\ndef run_tracker_in_thread(filename, model):\nvideo = cv2.VideoCapture(filename)\nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\nfor _ in range(frames):\nret, frame = video.read()\nif ret:\nresults = model.track(source=frame, persist=True)\nres_plotted = results[0].plot()\ncv2.imshow('p', res_plotted)\nif cv2.waitKey(1) == ord('q'):\nbreak\n# Load the models\nmodel1 = YOLO('yolov8n.pt')\nmodel2 = YOLO('yolov8n-seg.pt')\n# Define the video files for the trackers\nvideo_file1 = 'path/to/video1.mp4'\nvideo_file2 = 'path/to/video2.mp4'\n# Create the tracker threads\ntracker_thread1 = threading.Thread(target=run_tracker_in_thread, args=(video_file1, model1), daemon=True)\ntracker_thread2 = threading.Thread(target=run_tracker_in_thread, args=(video_file2, model2), daemon=True)\n# Start the tracker threads\ntracker_thread1.start()\ntracker_thread2.start()\n# Wait for the tracker threads to finish\ntracker_thread1.join()\ntracker_thread2.join()\n# Clean up and close windows\ncv2.destroyAllWindows()\n</code></pre> <p>This example can easily be extended to handle more video files and models by creating more threads and applying the same methodology.</p>"},{"location":"modes/train/","title":"Train","text":"<p>Train mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can accurately predict the classes and locations of objects in an image.</p> <p>Tip</p> <ul> <li>YOLOv8 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#usage-examples","title":"Usage Examples","text":"<p>Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. See Arguments section below for a full list of training arguments.</p> <p>Single-GPU and CPU Training Example</p> <p>Device is determined automatically. If a GPU is available then it will be used, otherwise training will start on CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n# Train the model\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Build a new model from YAML and start training from scratch\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n# Start training from a pretrained *.pt model\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>The training device can be specified using the <code>device</code> argument. If no argument is passed GPU <code>device=0</code> will be used if available, otherwise <code>device=cpu</code> will be used.</p> <p>Multi-GPU Training Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model with 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Start training from a pretrained *.pt model using GPUs 0 and 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#apple-m1-and-m2-mps-training","title":"Apple M1 and M2 MPS Training","text":"<p>With the support for Apple M1 and M2 chips integrated in the Ultralytics YOLO models, it's now possible to train your models on devices utilizing the powerful Metal Performance Shaders (MPS) framework. The MPS offers a high-performance way of executing computation and image processing tasks on Apple's custom silicon.</p> <p>To enable training on Apple M1 and M2 chips, you should specify 'mps' as your device when initiating the training process. Below is an example of how you could do this in Python and via the command line:</p> <p>MPS Training Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n# Train the model with 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Start training from a pretrained *.pt model using GPUs 0 and 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>While leveraging the computational power of the M1/M2 chips, this enables more efficient processing of the training tasks. For more detailed guidance and advanced configuration options, please refer to the PyTorch MPS documentation.</p>"},{"location":"modes/train/#resuming-interrupted-trainings","title":"Resuming Interrupted Trainings","text":"<p>Resuming training from a previously saved state is a crucial feature when working with deep learning models. This can come in handy in various scenarios, like when the training process has been unexpectedly interrupted, or when you wish to continue training a model with new data or for more epochs.</p> <p>When training is resumed, Ultralytics YOLO loads the weights from the last saved model and also restores the optimizer state, learning rate scheduler, and the epoch number. This allows you to continue the training process seamlessly from where it was left off.</p> <p>You can easily resume training in Ultralytics YOLO by setting the <code>resume</code> argument to <code>True</code> when calling the <code>train</code> method, and specifying the path to the <code>.pt</code> file containing the partially trained model weights.</p> <p>Below is an example of how to resume an interrupted training using Python and via the command line:</p> <p>Resume Training Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('path/to/last.pt')  # load a partially trained model\n# Resume training\nresults = model.train(resume=True)\n</code></pre> <pre><code># Resume an interrupted training\nyolo train resume model=path/to/last.pt\n</code></pre> <p>By setting <code>resume=True</code>, the <code>train</code> function will continue training from where it left off, using the state stored in the 'path/to/last.pt' file. If the <code>resume</code> argument is omitted or set to <code>False</code>, the <code>train</code> function will start a new training session.</p> <p>Remember that checkpoints are saved at the end of every epoch by default, or at fixed interval using the <code>save_period</code> argument, so you must complete at least 1 epoch to resume a training run.</p>"},{"location":"modes/train/#arguments","title":"Arguments","text":"<p>Training settings for YOLO models refer to the various hyperparameters and configurations used to train the model on a dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO training settings include the batch size, learning rate, momentum, and weight decay. Other factors that may affect the training process include the choice of optimizer, the choice of loss function, and the size and composition of the training dataset. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.</p> Key Value Description <code>model</code> <code>None</code> path to model file, i.e. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> path to data file, i.e. coco128.yaml <code>epochs</code> <code>100</code> number of epochs to train for <code>patience</code> <code>50</code> epochs to wait for no observable improvement for early stopping of training <code>batch</code> <code>16</code> number of images per batch (-1 for AutoBatch) <code>imgsz</code> <code>640</code> size of input images as integer <code>save</code> <code>True</code> save train checkpoints and predict results <code>save_period</code> <code>-1</code> Save checkpoint every x epochs (disabled if &lt; 1) <code>cache</code> <code>False</code> True/ram, disk or False. Use cache for data loading <code>device</code> <code>None</code> device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu <code>workers</code> <code>8</code> number of worker threads for data loading (per RANK if DDP) <code>project</code> <code>None</code> project name <code>name</code> <code>None</code> experiment name <code>exist_ok</code> <code>False</code> whether to overwrite existing experiment <code>pretrained</code> <code>False</code> whether to use a pretrained model <code>optimizer</code> <code>'auto'</code> optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto] <code>verbose</code> <code>False</code> whether to print verbose output <code>seed</code> <code>0</code> random seed for reproducibility <code>deterministic</code> <code>True</code> whether to enable deterministic mode <code>single_cls</code> <code>False</code> train multi-class data as single-class <code>rect</code> <code>False</code> rectangular training with each batch collated for minimum padding <code>cos_lr</code> <code>False</code> use cosine learning rate scheduler <code>close_mosaic</code> <code>10</code> (int) disable mosaic augmentation for final epochs (0 to disable) <code>resume</code> <code>False</code> resume training from last checkpoint <code>amp</code> <code>True</code> Automatic Mixed Precision (AMP) training, choices=[True, False] <code>fraction</code> <code>1.0</code> dataset fraction to train on (default is 1.0, all images in train set) <code>profile</code> <code>False</code> profile ONNX and TensorRT speeds during training for loggers <code>freeze</code> <code>None</code> (int or list, optional) freeze first n layers, or freeze list of layer indices during training <code>lr0</code> <code>0.01</code> initial learning rate (i.e. SGD=1E-2, Adam=1E-3) <code>lrf</code> <code>0.01</code> final learning rate (lr0 * lrf) <code>momentum</code> <code>0.937</code> SGD momentum/Adam beta1 <code>weight_decay</code> <code>0.0005</code> optimizer weight decay 5e-4 <code>warmup_epochs</code> <code>3.0</code> warmup epochs (fractions ok) <code>warmup_momentum</code> <code>0.8</code> warmup initial momentum <code>warmup_bias_lr</code> <code>0.1</code> warmup initial bias lr <code>box</code> <code>7.5</code> box loss gain <code>cls</code> <code>0.5</code> cls loss gain (scale with pixels) <code>dfl</code> <code>1.5</code> dfl loss gain <code>pose</code> <code>12.0</code> pose loss gain (pose-only) <code>kobj</code> <code>2.0</code> keypoint obj loss gain (pose-only) <code>label_smoothing</code> <code>0.0</code> label smoothing (fraction) <code>nbs</code> <code>64</code> nominal batch size <code>overlap_mask</code> <code>True</code> masks should overlap during training (segment train only) <code>mask_ratio</code> <code>4</code> mask downsample ratio (segment train only) <code>dropout</code> <code>0.0</code> use dropout regularization (classify train only) <code>val</code> <code>True</code> validate/test during training"},{"location":"modes/train/#logging","title":"Logging","text":"<p>In training a YOLOv8 model, you might find it valuable to keep track of the model's performance over time. This is where logging comes into play. Ultralytics' YOLO provides support for three types of loggers - Comet, ClearML, and TensorBoard.</p> <p>To use a logger, select it from the dropdown menu in the code snippet above and run it. The chosen logger will be installed and initialized.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet is a platform that allows data scientists and developers to track, compare, explain and optimize experiments and models. It provides functionalities such as real-time metrics, code diffs, and hyperparameters tracking.</p> <p>To use Comet:</p> Python <pre><code># pip install comet_ml\nimport comet_ml\ncomet_ml.init()\n</code></pre> <p>Remember to sign in to your Comet account on their website and get your API key. You will need to add this to your environment variables or your script to log your experiments.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML is an open-source platform that automates tracking of experiments and helps with efficient sharing of resources. It is designed to help teams manage, execute, and reproduce their ML work more efficiently.</p> <p>To use ClearML:</p> Python <pre><code># pip install clearml\nimport clearml\nclearml.browser_login()\n</code></pre> <p>After running this script, you will need to sign in to your ClearML account on the browser and authenticate your session.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard is a visualization toolkit for TensorFlow. It allows you to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.</p> <p>To use TensorBoard in Google Colab:</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # replace with 'runs' directory\n</code></pre> <p>To use TensorBoard locally run the below command and view results at http://localhost:6006/.</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # replace with 'runs' directory\n</code></pre> <p>This will load TensorBoard and direct it to the directory where your training logs are saved.</p> <p>After setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model's performance over time, compare different models, and identify areas for improvement.</p>"},{"location":"modes/val/","title":"Val","text":"<p>Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters of the model to improve its performance.</p> <p>Tip</p> <ul> <li>YOLOv8 models automatically remember their training settings, so you can validate a model at the same image size and on the original dataset easily with just <code>yolo val model=yolov8n.pt</code> or <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#usage-examples","title":"Usage Examples","text":"<p>Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes. See Arguments section below for a full list of export arguments.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # a list contains map50-95 of each category\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val official model\nyolo detect val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"modes/val/#arguments","title":"Arguments","text":"<p>Validation settings for YOLO models refer to the various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO validation settings include the batch size, the frequency with which validation is performed during training, and the metrics used to evaluate the model's performance. Other factors that may affect the validation process include the size and composition of the validation dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the model is performing well on the validation dataset and to detect and prevent overfitting.</p> Key Value Description <code>data</code> <code>None</code> path to data file, i.e. coco128.yaml <code>imgsz</code> <code>640</code> size of input images as integer <code>batch</code> <code>16</code> number of images per batch (-1 for AutoBatch) <code>save_json</code> <code>False</code> save results to JSON file <code>save_hybrid</code> <code>False</code> save hybrid version of labels (labels + additional predictions) <code>conf</code> <code>0.001</code> object confidence threshold for detection <code>iou</code> <code>0.6</code> intersection over union (IoU) threshold for NMS <code>max_det</code> <code>300</code> maximum number of detections per image <code>half</code> <code>True</code> use half precision (FP16) <code>device</code> <code>None</code> device to run on, i.e. cuda device=0/1/2/3 or device=cpu <code>dnn</code> <code>False</code> use OpenCV DNN for ONNX inference <code>plots</code> <code>False</code> show plots during training <code>rect</code> <code>False</code> rectangular val with each batch collated for minimum padding <code>split</code> <code>val</code> dataset split to use for validation, i.e. 'val', 'test' or 'train'"},{"location":"reference/cfg/__init__/","title":"Reference for <code>ultralytics/cfg/__init__.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/init.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.cfg2dict","title":"<code>ultralytics.cfg.cfg2dict(cfg)</code>","text":"<p>Convert a configuration object to a dictionary, whether it is a file path, a string, or a SimpleNamespace object.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str | Path | dict | SimpleNamespace</code> <p>Configuration object to be converted to a dictionary.</p> required <p>Returns:</p> Name Type Description <code>cfg</code> <code>dict</code> <p>Configuration object in dictionary format.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def cfg2dict(cfg):\n\"\"\"\n    Convert a configuration object to a dictionary, whether it is a file path, a string, or a SimpleNamespace object.\n    Args:\n        cfg (str | Path | dict | SimpleNamespace): Configuration object to be converted to a dictionary.\n    Returns:\n        cfg (dict): Configuration object in dictionary format.\n    \"\"\"\nif isinstance(cfg, (str, Path)):\ncfg = yaml_load(cfg)  # load dict\nelif isinstance(cfg, SimpleNamespace):\ncfg = vars(cfg)  # convert to dict\nreturn cfg\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.get_cfg","title":"<code>ultralytics.cfg.get_cfg(cfg=DEFAULT_CFG_DICT, overrides=None)</code>","text":"<p>Load and merge configuration data from a file or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str | Path | Dict | SimpleNamespace</code> <p>Configuration data.</p> <code>DEFAULT_CFG_DICT</code> <code>overrides</code> <code>str | Dict | optional</code> <p>Overrides in the form of a file name or a dictionary. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleNamespace</code> <p>Training arguments namespace.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def get_cfg(cfg: Union[str, Path, Dict, SimpleNamespace] = DEFAULT_CFG_DICT, overrides: Dict = None):\n\"\"\"\n    Load and merge configuration data from a file or dictionary.\n    Args:\n        cfg (str | Path | Dict | SimpleNamespace): Configuration data.\n        overrides (str | Dict | optional): Overrides in the form of a file name or a dictionary. Default is None.\n    Returns:\n        (SimpleNamespace): Training arguments namespace.\n    \"\"\"\ncfg = cfg2dict(cfg)\n# Merge overrides\nif overrides:\noverrides = cfg2dict(overrides)\noverrides.pop('save_dir', None)  # special override keys to ignore\ncheck_dict_alignment(cfg, overrides)\ncfg = {**cfg, **overrides}  # merge cfg and overrides dicts (prefer overrides)\n# Special handling for numeric project/name\nfor k in 'project', 'name':\nif k in cfg and isinstance(cfg[k], (int, float)):\ncfg[k] = str(cfg[k])\nif cfg.get('name') == 'model':  # assign model to 'name' arg\ncfg['name'] = cfg.get('model', '').split('.')[0]\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'name=model' automatically updated to 'name={cfg['name']}'.\")\n# Type and Value checks\nfor k, v in cfg.items():\nif v is not None:  # None values may be from optional args\nif k in CFG_FLOAT_KEYS and not isinstance(v, (int, float)):\nraise TypeError(f\"'{k}={v}' is of invalid type {type(v).__name__}. \"\nf\"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')\")\nelif k in CFG_FRACTION_KEYS:\nif not isinstance(v, (int, float)):\nraise TypeError(f\"'{k}={v}' is of invalid type {type(v).__name__}. \"\nf\"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')\")\nif not (0.0 &lt;= v &lt;= 1.0):\nraise ValueError(f\"'{k}={v}' is an invalid value. \"\nf\"Valid '{k}' values are between 0.0 and 1.0.\")\nelif k in CFG_INT_KEYS and not isinstance(v, int):\nraise TypeError(f\"'{k}={v}' is of invalid type {type(v).__name__}. \"\nf\"'{k}' must be an int (i.e. '{k}=8')\")\nelif k in CFG_BOOL_KEYS and not isinstance(v, bool):\nraise TypeError(f\"'{k}={v}' is of invalid type {type(v).__name__}. \"\nf\"'{k}' must be a bool (i.e. '{k}=True' or '{k}=False')\")\n# Return instance\nreturn IterableSimpleNamespace(**cfg)\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg._handle_deprecation","title":"<code>ultralytics.cfg._handle_deprecation(custom)</code>","text":"<p>Hardcoded function to handle deprecated config keys</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def _handle_deprecation(custom):\n\"\"\"Hardcoded function to handle deprecated config keys\"\"\"\nfor key in custom.copy().keys():\nif key == 'hide_labels':\ndeprecation_warn(key, 'show_labels')\ncustom['show_labels'] = custom.pop('hide_labels') == 'False'\nif key == 'hide_conf':\ndeprecation_warn(key, 'show_conf')\ncustom['show_conf'] = custom.pop('hide_conf') == 'False'\nif key == 'line_thickness':\ndeprecation_warn(key, 'line_width')\ncustom['line_width'] = custom.pop('line_thickness')\nreturn custom\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.check_dict_alignment","title":"<code>ultralytics.cfg.check_dict_alignment(base, custom, e=None)</code>","text":"<p>This function checks for any mismatched keys between a custom configuration list and a base configuration list. If any mismatched keys are found, the function prints out similar keys from the base list and exits the program.</p> <p>Parameters:</p> Name Type Description Default <code>custom</code> <code>dict</code> <p>a dictionary of custom configuration options</p> required <code>base</code> <code>dict</code> <p>a dictionary of base configuration options</p> required Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def check_dict_alignment(base: Dict, custom: Dict, e=None):\n\"\"\"\n    This function checks for any mismatched keys between a custom configuration list and a base configuration list.\n    If any mismatched keys are found, the function prints out similar keys from the base list and exits the program.\n    Args:\n        custom (dict): a dictionary of custom configuration options\n        base (dict): a dictionary of base configuration options\n    \"\"\"\ncustom = _handle_deprecation(custom)\nbase_keys, custom_keys = (set(x.keys()) for x in (base, custom))\nmismatched = [k for k in custom_keys if k not in base_keys]\nif mismatched:\nstring = ''\nfor x in mismatched:\nmatches = get_close_matches(x, base_keys)  # key list\nmatches = [f'{k}={base[k]}' if base.get(k) is not None else k for k in matches]\nmatch_str = f'Similar arguments are i.e. {matches}.' if matches else ''\nstring += f\"'{colorstr('red', 'bold', x)}' is not a valid YOLO argument. {match_str}\\n\"\nraise SyntaxError(string + CLI_HELP_MSG) from e\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.merge_equals_args","title":"<code>ultralytics.cfg.merge_equals_args(args)</code>","text":"<p>Merges arguments around isolated '=' args in a list of strings. The function considers cases where the first argument ends with '=' or the second starts with '=', as well as when the middle one is an equals sign.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>A list of strings where each element is an argument.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings where the arguments around isolated '=' are merged.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def merge_equals_args(args: List[str]) -&gt; List[str]:\n\"\"\"\n    Merges arguments around isolated '=' args in a list of strings.\n    The function considers cases where the first argument ends with '=' or the second starts with '=',\n    as well as when the middle one is an equals sign.\n    Args:\n        args (List[str]): A list of strings where each element is an argument.\n    Returns:\n        List[str]: A list of strings where the arguments around isolated '=' are merged.\n    \"\"\"\nnew_args = []\nfor i, arg in enumerate(args):\nif arg == '=' and 0 &lt; i &lt; len(args) - 1:  # merge ['arg', '=', 'val']\nnew_args[-1] += f'={args[i + 1]}'\ndel args[i + 1]\nelif arg.endswith('=') and i &lt; len(args) - 1 and '=' not in args[i + 1]:  # merge ['arg=', 'val']\nnew_args.append(f'{arg}{args[i + 1]}')\ndel args[i + 1]\nelif arg.startswith('=') and i &gt; 0:  # merge ['arg', '=val']\nnew_args[-1] += arg\nelse:\nnew_args.append(arg)\nreturn new_args\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.handle_yolo_hub","title":"<code>ultralytics.cfg.handle_yolo_hub(args)</code>","text":"<p>Handle Ultralytics HUB command-line interface (CLI) commands.</p> <p>This function processes Ultralytics HUB CLI commands such as login and logout. It should be called when executing a script with arguments related to HUB authentication.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>A list of command line arguments</p> required Example <pre><code>python my_script.py hub login your_api_key\n</code></pre> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def handle_yolo_hub(args: List[str]) -&gt; None:\n\"\"\"\n    Handle Ultralytics HUB command-line interface (CLI) commands.\n    This function processes Ultralytics HUB CLI commands such as login and logout.\n    It should be called when executing a script with arguments related to HUB authentication.\n    Args:\n        args (List[str]): A list of command line arguments\n    Example:\n        ```bash\n        python my_script.py hub login your_api_key\n        ```\n    \"\"\"\nfrom ultralytics import hub\nif args[0] == 'login':\nkey = args[1] if len(args) &gt; 1 else ''\n# Log in to Ultralytics HUB using the provided API key\nhub.login(key)\nelif args[0] == 'logout':\n# Log out from Ultralytics HUB\nhub.logout()\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.handle_yolo_settings","title":"<code>ultralytics.cfg.handle_yolo_settings(args)</code>","text":"<p>Handle YOLO settings command-line interface (CLI) commands.</p> <p>This function processes YOLO settings CLI commands such as reset. It should be called when executing a script with arguments related to YOLO settings management.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>A list of command line arguments for YOLO settings management.</p> required Example <pre><code>python my_script.py yolo settings reset\n</code></pre> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def handle_yolo_settings(args: List[str]) -&gt; None:\n\"\"\"\n    Handle YOLO settings command-line interface (CLI) commands.\n    This function processes YOLO settings CLI commands such as reset.\n    It should be called when executing a script with arguments related to YOLO settings management.\n    Args:\n        args (List[str]): A list of command line arguments for YOLO settings management.\n    Example:\n        ```bash\n        python my_script.py yolo settings reset\n        ```\n    \"\"\"\nurl = 'https://docs.ultralytics.com/quickstart/#ultralytics-settings'  # help URL\ntry:\nif any(args):\nif args[0] == 'reset':\nSETTINGS_YAML.unlink()  # delete the settings file\nSETTINGS.reset()  # create new settings\nLOGGER.info('Settings reset successfully')  # inform the user that settings have been reset\nelse:  # save a new setting\nnew = dict(parse_key_value_pair(a) for a in args)\ncheck_dict_alignment(SETTINGS, new)\nSETTINGS.update(new)\nLOGGER.info(f'\ud83d\udca1 Learn about settings at {url}')\nyaml_print(SETTINGS_YAML)  # print the current settings\nexcept Exception as e:\nLOGGER.warning(f\"WARNING \u26a0\ufe0f settings error: '{e}'. Please see {url} for help.\")\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.parse_key_value_pair","title":"<code>ultralytics.cfg.parse_key_value_pair(pair)</code>","text":"<p>Parse one 'key=value' pair and return key and value.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def parse_key_value_pair(pair):\n\"\"\"Parse one 'key=value' pair and return key and value.\"\"\"\nre.sub(r' *= *', '=', pair)  # remove spaces around equals sign\nk, v = pair.split('=', 1)  # split on first '=' sign\nassert v, f\"missing '{k}' value\"\nreturn k, smart_value(v)\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.smart_value","title":"<code>ultralytics.cfg.smart_value(v)</code>","text":"<p>Convert a string to an underlying type such as int, float, bool, etc.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def smart_value(v):\n\"\"\"Convert a string to an underlying type such as int, float, bool, etc.\"\"\"\nif v.lower() == 'none':\nreturn None\nelif v.lower() == 'true':\nreturn True\nelif v.lower() == 'false':\nreturn False\nelse:\nwith contextlib.suppress(Exception):\nreturn eval(v)\nreturn v\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.entrypoint","title":"<code>ultralytics.cfg.entrypoint(debug='')</code>","text":"<p>This function is the ultralytics package entrypoint, it's responsible for parsing the command line arguments passed to the package.</p> <p>This function allows for: - passing mandatory YOLO args as a list of strings - specifying the task to be performed, either 'detect', 'segment' or 'classify' - specifying the mode, either 'train', 'val', 'test', or 'predict' - running special modes like 'checks' - passing overrides to the package's configuration</p> <p>It uses the package's default cfg and initializes it using the passed overrides. Then it calls the CLI function with the composed cfg</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def entrypoint(debug=''):\n\"\"\"\n    This function is the ultralytics package entrypoint, it's responsible for parsing the command line arguments passed\n    to the package.\n    This function allows for:\n    - passing mandatory YOLO args as a list of strings\n    - specifying the task to be performed, either 'detect', 'segment' or 'classify'\n    - specifying the mode, either 'train', 'val', 'test', or 'predict'\n    - running special modes like 'checks'\n    - passing overrides to the package's configuration\n    It uses the package's default cfg and initializes it using the passed overrides.\n    Then it calls the CLI function with the composed cfg\n    \"\"\"\nargs = (debug.split(' ') if debug else sys.argv)[1:]\nif not args:  # no arguments passed\nLOGGER.info(CLI_HELP_MSG)\nreturn\nspecial = {\n'help': lambda: LOGGER.info(CLI_HELP_MSG),\n'checks': checks.check_yolo,\n'version': lambda: LOGGER.info(__version__),\n'settings': lambda: handle_yolo_settings(args[1:]),\n'cfg': lambda: yaml_print(DEFAULT_CFG_PATH),\n'hub': lambda: handle_yolo_hub(args[1:]),\n'login': lambda: handle_yolo_hub(args),\n'copy-cfg': copy_default_cfg}\nfull_args_dict = {**DEFAULT_CFG_DICT, **{k: None for k in TASKS}, **{k: None for k in MODES}, **special}\n# Define common mis-uses of special commands, i.e. -h, -help, --help\nspecial.update({k[0]: v for k, v in special.items()})  # singular\nspecial.update({k[:-1]: v for k, v in special.items() if len(k) &gt; 1 and k.endswith('s')})  # singular\nspecial = {**special, **{f'-{k}': v for k, v in special.items()}, **{f'--{k}': v for k, v in special.items()}}\noverrides = {}  # basic overrides, i.e. imgsz=320\nfor a in merge_equals_args(args):  # merge spaces around '=' sign\nif a.startswith('--'):\nLOGGER.warning(f\"WARNING \u26a0\ufe0f '{a}' does not require leading dashes '--', updating to '{a[2:]}'.\")\na = a[2:]\nif a.endswith(','):\nLOGGER.warning(f\"WARNING \u26a0\ufe0f '{a}' does not require trailing comma ',', updating to '{a[:-1]}'.\")\na = a[:-1]\nif '=' in a:\ntry:\nk, v = parse_key_value_pair(a)\nif k == 'cfg':  # custom.yaml passed\nLOGGER.info(f'Overriding {DEFAULT_CFG_PATH} with {v}')\noverrides = {k: val for k, val in yaml_load(checks.check_yaml(v)).items() if k != 'cfg'}\nelse:\noverrides[k] = v\nexcept (NameError, SyntaxError, ValueError, AssertionError) as e:\ncheck_dict_alignment(full_args_dict, {a: ''}, e)\nelif a in TASKS:\noverrides['task'] = a\nelif a in MODES:\noverrides['mode'] = a\nelif a.lower() in special:\nspecial[a.lower()]()\nreturn\nelif a in DEFAULT_CFG_DICT and isinstance(DEFAULT_CFG_DICT[a], bool):\noverrides[a] = True  # auto-True for default bool args, i.e. 'yolo show' sets show=True\nelif a in DEFAULT_CFG_DICT:\nraise SyntaxError(f\"'{colorstr('red', 'bold', a)}' is a valid YOLO argument but is missing an '=' sign \"\nf\"to set its value, i.e. try '{a}={DEFAULT_CFG_DICT[a]}'\\n{CLI_HELP_MSG}\")\nelse:\ncheck_dict_alignment(full_args_dict, {a: ''})\n# Check keys\ncheck_dict_alignment(full_args_dict, overrides)\n# Mode\nmode = overrides.get('mode')\nif mode is None:\nmode = DEFAULT_CFG.mode or 'predict'\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'mode' is missing. Valid modes are {MODES}. Using default 'mode={mode}'.\")\nelif mode not in MODES:\nif mode not in ('checks', checks):\nraise ValueError(f\"Invalid 'mode={mode}'. Valid modes are {MODES}.\\n{CLI_HELP_MSG}\")\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'yolo mode=checks' is deprecated. Use 'yolo checks' instead.\")\nchecks.check_yolo()\nreturn\n# Task\ntask = overrides.pop('task', None)\nif task:\nif task not in TASKS:\nraise ValueError(f\"Invalid 'task={task}'. Valid tasks are {TASKS}.\\n{CLI_HELP_MSG}\")\nif 'model' not in overrides:\noverrides['model'] = TASK2MODEL[task]\n# Model\nmodel = overrides.pop('model', DEFAULT_CFG.model)\nif model is None:\nmodel = 'yolov8n.pt'\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'model' is missing. Using default 'model={model}'.\")\noverrides['model'] = model\nif 'rtdetr' in model.lower():  # guess architecture\nfrom ultralytics import RTDETR\nmodel = RTDETR(model)  # no task argument\nelif 'fastsam' in model.lower():\nfrom ultralytics import FastSAM\nmodel = FastSAM(model)\nelif 'sam' in model.lower():\nfrom ultralytics import SAM\nmodel = SAM(model)\nelse:\nfrom ultralytics import YOLO\nmodel = YOLO(model, task=task)\nif isinstance(overrides.get('pretrained'), str):\nmodel.load(overrides['pretrained'])\n# Task Update\nif task != model.task:\nif task:\nLOGGER.warning(f\"WARNING \u26a0\ufe0f conflicting 'task={task}' passed with 'task={model.task}' model. \"\nf\"Ignoring 'task={task}' and updating to 'task={model.task}' to match model.\")\ntask = model.task\n# Mode\nif mode in ('predict', 'track') and 'source' not in overrides:\noverrides['source'] = DEFAULT_CFG.source or ASSETS\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'source' is missing. Using default 'source={overrides['source']}'.\")\nelif mode in ('train', 'val'):\nif 'data' not in overrides and 'resume' not in overrides:\noverrides['data'] = TASK2DATA.get(task or DEFAULT_CFG.task, DEFAULT_CFG.data)\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'data' is missing. Using default 'data={overrides['data']}'.\")\nelif mode == 'export':\nif 'format' not in overrides:\noverrides['format'] = DEFAULT_CFG.format or 'torchscript'\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'format' is missing. Using default 'format={overrides['format']}'.\")\n# Run command in python\n# getattr(model, mode)(**vars(get_cfg(overrides=overrides)))  # default args using default.yaml\ngetattr(model, mode)(**overrides)  # default args from model\n</code></pre>"},{"location":"reference/cfg/__init__/#ultralytics.cfg.copy_default_cfg","title":"<code>ultralytics.cfg.copy_default_cfg()</code>","text":"<p>Copy and create a new default configuration file with '_copy' appended to its name.</p> Source code in <code>ultralytics/cfg/__init__.py</code> <pre><code>def copy_default_cfg():\n\"\"\"Copy and create a new default configuration file with '_copy' appended to its name.\"\"\"\nnew_file = Path.cwd() / DEFAULT_CFG_PATH.name.replace('.yaml', '_copy.yaml')\nshutil.copy2(DEFAULT_CFG_PATH, new_file)\nLOGGER.info(f'{DEFAULT_CFG_PATH} copied to {new_file}\\n'\nf\"Example YOLO command with this new custom cfg:\\n    yolo cfg='{new_file}' imgsz=320 batch=8\")\n</code></pre>"},{"location":"reference/data/annotator/","title":"Reference for <code>ultralytics/data/annotator.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/annotator.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/data/annotator/#ultralytics.data.annotator.auto_annotate","title":"<code>ultralytics.data.annotator.auto_annotate(data, det_model='yolov8x.pt', sam_model='sam_b.pt', device='', output_dir=None)</code>","text":"<p>Automatically annotates images using a YOLO object detection model and a SAM segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Path to a folder containing images to be annotated.</p> required <code>det_model</code> <code>str</code> <p>Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'.</p> <code>'yolov8x.pt'</code> <code>sam_model</code> <code>str</code> <p>Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'.</p> <code>'sam_b.pt'</code> <code>device</code> <code>str</code> <p>Device to run the models on. Defaults to an empty string (CPU or GPU, if available).</p> <code>''</code> <code>output_dir</code> <code>str | None | optional</code> <p>Directory to save the annotated results. Defaults to a 'labels' folder in the same directory as 'data'.</p> <code>None</code> Example <pre><code>from ultralytics.data.annotator import auto_annotate\nauto_annotate(data='ultralytics/assets', det_model='yolov8n.pt', sam_model='mobile_sam.pt')\n</code></pre> Source code in <code>ultralytics/data/annotator.py</code> <pre><code>def auto_annotate(data, det_model='yolov8x.pt', sam_model='sam_b.pt', device='', output_dir=None):\n\"\"\"\n    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.\n    Args:\n        data (str): Path to a folder containing images to be annotated.\n        det_model (str, optional): Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'.\n        sam_model (str, optional): Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'.\n        device (str, optional): Device to run the models on. Defaults to an empty string (CPU or GPU, if available).\n        output_dir (str | None | optional): Directory to save the annotated results.\n            Defaults to a 'labels' folder in the same directory as 'data'.\n    Example:\n        ```python\n        from ultralytics.data.annotator import auto_annotate\n        auto_annotate(data='ultralytics/assets', det_model='yolov8n.pt', sam_model='mobile_sam.pt')\n        ```\n    \"\"\"\ndet_model = YOLO(det_model)\nsam_model = SAM(sam_model)\ndata = Path(data)\nif not output_dir:\noutput_dir = data.parent / f'{data.stem}_auto_annotate_labels'\nPath(output_dir).mkdir(exist_ok=True, parents=True)\ndet_results = det_model(data, stream=True, device=device)\nfor result in det_results:\nclass_ids = result.boxes.cls.int().tolist()  # noqa\nif len(class_ids):\nboxes = result.boxes.xyxy  # Boxes object for bbox outputs\nsam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=False, device=device)\nsegments = sam_results[0].masks.xyn  # noqa\nwith open(f'{str(Path(output_dir) / Path(result.path).stem)}.txt', 'w') as f:\nfor i in range(len(segments)):\ns = segments[i]\nif len(s) == 0:\ncontinue\nsegment = map(str, segments[i].reshape(-1).tolist())\nf.write(f'{class_ids[i]} ' + ' '.join(segment) + '\\n')\n</code></pre>"},{"location":"reference/data/augment/","title":"Reference for <code>ultralytics/data/augment.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/augment.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseTransform","title":"<code>ultralytics.data.augment.BaseTransform</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class BaseTransform:\ndef __init__(self) -&gt; None:\npass\ndef apply_image(self, labels):\n\"\"\"Applies image transformation to labels.\"\"\"\npass\ndef apply_instances(self, labels):\n\"\"\"Applies transformations to input 'labels' and returns object instances.\"\"\"\npass\ndef apply_semantic(self, labels):\n\"\"\"Applies semantic segmentation to an image.\"\"\"\npass\ndef __call__(self, labels):\n\"\"\"Applies label transformations to an image, instances and semantic masks.\"\"\"\nself.apply_image(labels)\nself.apply_instances(labels)\nself.apply_semantic(labels)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseTransform.__call__","title":"<code>__call__(labels)</code>","text":"<p>Applies label transformations to an image, instances and semantic masks.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Applies label transformations to an image, instances and semantic masks.\"\"\"\nself.apply_image(labels)\nself.apply_instances(labels)\nself.apply_semantic(labels)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseTransform.apply_image","title":"<code>apply_image(labels)</code>","text":"<p>Applies image transformation to labels.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_image(self, labels):\n\"\"\"Applies image transformation to labels.\"\"\"\npass\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseTransform.apply_instances","title":"<code>apply_instances(labels)</code>","text":"<p>Applies transformations to input 'labels' and returns object instances.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_instances(self, labels):\n\"\"\"Applies transformations to input 'labels' and returns object instances.\"\"\"\npass\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseTransform.apply_semantic","title":"<code>apply_semantic(labels)</code>","text":"<p>Applies semantic segmentation to an image.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_semantic(self, labels):\n\"\"\"Applies semantic segmentation to an image.\"\"\"\npass\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose","title":"<code>ultralytics.data.augment.Compose</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class Compose:\ndef __init__(self, transforms):\n\"\"\"Initializes the Compose object with a list of transforms.\"\"\"\nself.transforms = transforms\ndef __call__(self, data):\n\"\"\"Applies a series of transformations to input data.\"\"\"\nfor t in self.transforms:\ndata = t(data)\nreturn data\ndef append(self, transform):\n\"\"\"Appends a new transform to the existing list of transforms.\"\"\"\nself.transforms.append(transform)\ndef tolist(self):\n\"\"\"Converts list of transforms to a standard Python list.\"\"\"\nreturn self.transforms\ndef __repr__(self):\n\"\"\"Return string representation of object.\"\"\"\nformat_string = f'{self.__class__.__name__}('\nfor t in self.transforms:\nformat_string += '\\n'\nformat_string += f'    {t}'\nformat_string += '\\n)'\nreturn format_string\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose.__call__","title":"<code>__call__(data)</code>","text":"<p>Applies a series of transformations to input data.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, data):\n\"\"\"Applies a series of transformations to input data.\"\"\"\nfor t in self.transforms:\ndata = t(data)\nreturn data\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose.__init__","title":"<code>__init__(transforms)</code>","text":"<p>Initializes the Compose object with a list of transforms.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, transforms):\n\"\"\"Initializes the Compose object with a list of transforms.\"\"\"\nself.transforms = transforms\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of object.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __repr__(self):\n\"\"\"Return string representation of object.\"\"\"\nformat_string = f'{self.__class__.__name__}('\nfor t in self.transforms:\nformat_string += '\\n'\nformat_string += f'    {t}'\nformat_string += '\\n)'\nreturn format_string\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose.append","title":"<code>append(transform)</code>","text":"<p>Appends a new transform to the existing list of transforms.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def append(self, transform):\n\"\"\"Appends a new transform to the existing list of transforms.\"\"\"\nself.transforms.append(transform)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Compose.tolist","title":"<code>tolist()</code>","text":"<p>Converts list of transforms to a standard Python list.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def tolist(self):\n\"\"\"Converts list of transforms to a standard Python list.\"\"\"\nreturn self.transforms\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseMixTransform","title":"<code>ultralytics.data.augment.BaseMixTransform</code>","text":"<p>This implementation is from mmyolo.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class BaseMixTransform:\n\"\"\"This implementation is from mmyolo.\"\"\"\ndef __init__(self, dataset, pre_transform=None, p=0.0) -&gt; None:\nself.dataset = dataset\nself.pre_transform = pre_transform\nself.p = p\ndef __call__(self, labels):\n\"\"\"Applies pre-processing transforms and mixup/mosaic transforms to labels data.\"\"\"\nif random.uniform(0, 1) &gt; self.p:\nreturn labels\n# Get index of one or three other images\nindexes = self.get_indexes()\nif isinstance(indexes, int):\nindexes = [indexes]\n# Get images information will be used for Mosaic or MixUp\nmix_labels = [self.dataset.get_image_and_label(i) for i in indexes]\nif self.pre_transform is not None:\nfor i, data in enumerate(mix_labels):\nmix_labels[i] = self.pre_transform(data)\nlabels['mix_labels'] = mix_labels\n# Mosaic or MixUp\nlabels = self._mix_transform(labels)\nlabels.pop('mix_labels', None)\nreturn labels\ndef _mix_transform(self, labels):\n\"\"\"Applies MixUp or Mosaic augmentation to the label dictionary.\"\"\"\nraise NotImplementedError\ndef get_indexes(self):\n\"\"\"Gets a list of shuffled indexes for mosaic augmentation.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseMixTransform.__call__","title":"<code>__call__(labels)</code>","text":"<p>Applies pre-processing transforms and mixup/mosaic transforms to labels data.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Applies pre-processing transforms and mixup/mosaic transforms to labels data.\"\"\"\nif random.uniform(0, 1) &gt; self.p:\nreturn labels\n# Get index of one or three other images\nindexes = self.get_indexes()\nif isinstance(indexes, int):\nindexes = [indexes]\n# Get images information will be used for Mosaic or MixUp\nmix_labels = [self.dataset.get_image_and_label(i) for i in indexes]\nif self.pre_transform is not None:\nfor i, data in enumerate(mix_labels):\nmix_labels[i] = self.pre_transform(data)\nlabels['mix_labels'] = mix_labels\n# Mosaic or MixUp\nlabels = self._mix_transform(labels)\nlabels.pop('mix_labels', None)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.BaseMixTransform.get_indexes","title":"<code>get_indexes()</code>","text":"<p>Gets a list of shuffled indexes for mosaic augmentation.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def get_indexes(self):\n\"\"\"Gets a list of shuffled indexes for mosaic augmentation.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Mosaic","title":"<code>ultralytics.data.augment.Mosaic</code>","text":"<p>             Bases: <code>BaseMixTransform</code></p> <p>Mosaic augmentation.</p> <p>This class performs mosaic augmentation by combining multiple (4 or 9) images into a single mosaic image. The augmentation is applied to a dataset with a given probability.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <p>The dataset on which the mosaic augmentation is applied.</p> <code>imgsz</code> <code>int</code> <p>Image size (height and width) after mosaic pipeline of a single image. Default to 640.</p> <code>p</code> <code>float</code> <p>Probability of applying the mosaic augmentation. Must be in the range 0-1. Default to 1.0.</p> <code>n</code> <code>int</code> <p>The grid size, either 4 (for 2x2) or 9 (for 3x3).</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class Mosaic(BaseMixTransform):\n\"\"\"\n    Mosaic augmentation.\n    This class performs mosaic augmentation by combining multiple (4 or 9) images into a single mosaic image.\n    The augmentation is applied to a dataset with a given probability.\n    Attributes:\n        dataset: The dataset on which the mosaic augmentation is applied.\n        imgsz (int, optional): Image size (height and width) after mosaic pipeline of a single image. Default to 640.\n        p (float, optional): Probability of applying the mosaic augmentation. Must be in the range 0-1. Default to 1.0.\n        n (int, optional): The grid size, either 4 (for 2x2) or 9 (for 3x3).\n    \"\"\"\ndef __init__(self, dataset, imgsz=640, p=1.0, n=4):\n\"\"\"Initializes the object with a dataset, image size, probability, and border.\"\"\"\nassert 0 &lt;= p &lt;= 1.0, f'The probability should be in range [0, 1], but got {p}.'\nassert n in (4, 9), 'grid must be equal to 4 or 9.'\nsuper().__init__(dataset=dataset, p=p)\nself.dataset = dataset\nself.imgsz = imgsz\nself.border = (-imgsz // 2, -imgsz // 2)  # width, height\nself.n = n\ndef get_indexes(self, buffer=True):\n\"\"\"Return a list of random indexes from the dataset.\"\"\"\nif buffer:  # select images from buffer\nreturn random.choices(list(self.dataset.buffer), k=self.n - 1)\nelse:  # select any images\nreturn [random.randint(0, len(self.dataset) - 1) for _ in range(self.n - 1)]\ndef _mix_transform(self, labels):\n\"\"\"Apply mixup transformation to the input image and labels.\"\"\"\nassert labels.get('rect_shape', None) is None, 'rect and mosaic are mutually exclusive.'\nassert len(labels.get('mix_labels', [])), 'There are no other images for mosaic augment.'\nreturn self._mosaic4(labels) if self.n == 4 else self._mosaic9(labels)\ndef _mosaic4(self, labels):\n\"\"\"Create a 2x2 image mosaic.\"\"\"\nmosaic_labels = []\ns = self.imgsz\nyc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.border)  # mosaic center x, y\nfor i in range(4):\nlabels_patch = labels if i == 0 else labels['mix_labels'][i - 1]\n# Load image\nimg = labels_patch['img']\nh, w = labels_patch.pop('resized_shape')\n# Place img in img4\nif i == 0:  # top left\nimg4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\nx1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\nx1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\nelif i == 1:  # top right\nx1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\nx1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\nelif i == 2:  # bottom left\nx1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\nx1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\nelif i == 3:  # bottom right\nx1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\nx1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\nimg4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\npadw = x1a - x1b\npadh = y1a - y1b\nlabels_patch = self._update_labels(labels_patch, padw, padh)\nmosaic_labels.append(labels_patch)\nfinal_labels = self._cat_labels(mosaic_labels)\nfinal_labels['img'] = img4\nreturn final_labels\ndef _mosaic9(self, labels):\n\"\"\"Create a 3x3 image mosaic.\"\"\"\nmosaic_labels = []\ns = self.imgsz\nhp, wp = -1, -1  # height, width previous\nfor i in range(9):\nlabels_patch = labels if i == 0 else labels['mix_labels'][i - 1]\n# Load image\nimg = labels_patch['img']\nh, w = labels_patch.pop('resized_shape')\n# Place img in img9\nif i == 0:  # center\nimg9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\nh0, w0 = h, w\nc = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\nelif i == 1:  # top\nc = s, s - h, s + w, s\nelif i == 2:  # top right\nc = s + wp, s - h, s + wp + w, s\nelif i == 3:  # right\nc = s + w0, s, s + w0 + w, s + h\nelif i == 4:  # bottom right\nc = s + w0, s + hp, s + w0 + w, s + hp + h\nelif i == 5:  # bottom\nc = s + w0 - w, s + h0, s + w0, s + h0 + h\nelif i == 6:  # bottom left\nc = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\nelif i == 7:  # left\nc = s - w, s + h0 - h, s, s + h0\nelif i == 8:  # top left\nc = s - w, s + h0 - hp - h, s, s + h0 - hp\npadw, padh = c[:2]\nx1, y1, x2, y2 = (max(x, 0) for x in c)  # allocate coords\n# Image\nimg9[y1:y2, x1:x2] = img[y1 - padh:, x1 - padw:]  # img9[ymin:ymax, xmin:xmax]\nhp, wp = h, w  # height, width previous for next iteration\n# Labels assuming imgsz*2 mosaic size\nlabels_patch = self._update_labels(labels_patch, padw + self.border[0], padh + self.border[1])\nmosaic_labels.append(labels_patch)\nfinal_labels = self._cat_labels(mosaic_labels)\nfinal_labels['img'] = img9[-self.border[0]:self.border[0], -self.border[1]:self.border[1]]\nreturn final_labels\n@staticmethod\ndef _update_labels(labels, padw, padh):\n\"\"\"Update labels.\"\"\"\nnh, nw = labels['img'].shape[:2]\nlabels['instances'].convert_bbox(format='xyxy')\nlabels['instances'].denormalize(nw, nh)\nlabels['instances'].add_padding(padw, padh)\nreturn labels\ndef _cat_labels(self, mosaic_labels):\n\"\"\"Return labels with mosaic border instances clipped.\"\"\"\nif len(mosaic_labels) == 0:\nreturn {}\ncls = []\ninstances = []\nimgsz = self.imgsz * 2  # mosaic imgsz\nfor labels in mosaic_labels:\ncls.append(labels['cls'])\ninstances.append(labels['instances'])\nfinal_labels = {\n'im_file': mosaic_labels[0]['im_file'],\n'ori_shape': mosaic_labels[0]['ori_shape'],\n'resized_shape': (imgsz, imgsz),\n'cls': np.concatenate(cls, 0),\n'instances': Instances.concatenate(instances, axis=0),\n'mosaic_border': self.border}  # final_labels\nfinal_labels['instances'].clip(imgsz, imgsz)\ngood = final_labels['instances'].remove_zero_area_boxes()\nfinal_labels['cls'] = final_labels['cls'][good]\nreturn final_labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Mosaic.__init__","title":"<code>__init__(dataset, imgsz=640, p=1.0, n=4)</code>","text":"<p>Initializes the object with a dataset, image size, probability, and border.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, dataset, imgsz=640, p=1.0, n=4):\n\"\"\"Initializes the object with a dataset, image size, probability, and border.\"\"\"\nassert 0 &lt;= p &lt;= 1.0, f'The probability should be in range [0, 1], but got {p}.'\nassert n in (4, 9), 'grid must be equal to 4 or 9.'\nsuper().__init__(dataset=dataset, p=p)\nself.dataset = dataset\nself.imgsz = imgsz\nself.border = (-imgsz // 2, -imgsz // 2)  # width, height\nself.n = n\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Mosaic.get_indexes","title":"<code>get_indexes(buffer=True)</code>","text":"<p>Return a list of random indexes from the dataset.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def get_indexes(self, buffer=True):\n\"\"\"Return a list of random indexes from the dataset.\"\"\"\nif buffer:  # select images from buffer\nreturn random.choices(list(self.dataset.buffer), k=self.n - 1)\nelse:  # select any images\nreturn [random.randint(0, len(self.dataset) - 1) for _ in range(self.n - 1)]\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.MixUp","title":"<code>ultralytics.data.augment.MixUp</code>","text":"<p>             Bases: <code>BaseMixTransform</code></p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class MixUp(BaseMixTransform):\ndef __init__(self, dataset, pre_transform=None, p=0.0) -&gt; None:\nsuper().__init__(dataset=dataset, pre_transform=pre_transform, p=p)\ndef get_indexes(self):\n\"\"\"Get a random index from the dataset.\"\"\"\nreturn random.randint(0, len(self.dataset) - 1)\ndef _mix_transform(self, labels):\n\"\"\"Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf.\"\"\"\nr = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\nlabels2 = labels['mix_labels'][0]\nlabels['img'] = (labels['img'] * r + labels2['img'] * (1 - r)).astype(np.uint8)\nlabels['instances'] = Instances.concatenate([labels['instances'], labels2['instances']], axis=0)\nlabels['cls'] = np.concatenate([labels['cls'], labels2['cls']], 0)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.MixUp.get_indexes","title":"<code>get_indexes()</code>","text":"<p>Get a random index from the dataset.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def get_indexes(self):\n\"\"\"Get a random index from the dataset.\"\"\"\nreturn random.randint(0, len(self.dataset) - 1)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective","title":"<code>ultralytics.data.augment.RandomPerspective</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class RandomPerspective:\ndef __init__(self,\ndegrees=0.0,\ntranslate=0.1,\nscale=0.5,\nshear=0.0,\nperspective=0.0,\nborder=(0, 0),\npre_transform=None):\nself.degrees = degrees\nself.translate = translate\nself.scale = scale\nself.shear = shear\nself.perspective = perspective\n# Mosaic border\nself.border = border\nself.pre_transform = pre_transform\ndef affine_transform(self, img, border):\n\"\"\"Center.\"\"\"\nC = np.eye(3, dtype=np.float32)\nC[0, 2] = -img.shape[1] / 2  # x translation (pixels)\nC[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n# Perspective\nP = np.eye(3, dtype=np.float32)\nP[2, 0] = random.uniform(-self.perspective, self.perspective)  # x perspective (about y)\nP[2, 1] = random.uniform(-self.perspective, self.perspective)  # y perspective (about x)\n# Rotation and Scale\nR = np.eye(3, dtype=np.float32)\na = random.uniform(-self.degrees, self.degrees)\n# a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\ns = random.uniform(1 - self.scale, 1 + self.scale)\n# s = 2 ** random.uniform(-scale, scale)\nR[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n# Shear\nS = np.eye(3, dtype=np.float32)\nS[0, 1] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # x shear (deg)\nS[1, 0] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # y shear (deg)\n# Translation\nT = np.eye(3, dtype=np.float32)\nT[0, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[0]  # x translation (pixels)\nT[1, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[1]  # y translation (pixels)\n# Combined rotation matrix\nM = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n# Affine image\nif (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\nif self.perspective:\nimg = cv2.warpPerspective(img, M, dsize=self.size, borderValue=(114, 114, 114))\nelse:  # affine\nimg = cv2.warpAffine(img, M[:2], dsize=self.size, borderValue=(114, 114, 114))\nreturn img, M, s\ndef apply_bboxes(self, bboxes, M):\n\"\"\"\n        Apply affine to bboxes only.\n        Args:\n            bboxes (ndarray): list of bboxes, xyxy format, with shape (num_bboxes, 4).\n            M (ndarray): affine matrix.\n        Returns:\n            new_bboxes (ndarray): bboxes after affine, [num_bboxes, 4].\n        \"\"\"\nn = len(bboxes)\nif n == 0:\nreturn bboxes\nxy = np.ones((n * 4, 3), dtype=bboxes.dtype)\nxy[:, :2] = bboxes[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\nxy = xy @ M.T  # transform\nxy = (xy[:, :2] / xy[:, 2:3] if self.perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n# Create new boxes\nx = xy[:, [0, 2, 4, 6]]\ny = xy[:, [1, 3, 5, 7]]\nreturn np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1)), dtype=bboxes.dtype).reshape(4, n).T\ndef apply_segments(self, segments, M):\n\"\"\"\n        Apply affine to segments and generate new bboxes from segments.\n        Args:\n            segments (ndarray): list of segments, [num_samples, 500, 2].\n            M (ndarray): affine matrix.\n        Returns:\n            new_segments (ndarray): list of segments after affine, [num_samples, 500, 2].\n            new_bboxes (ndarray): bboxes after affine, [N, 4].\n        \"\"\"\nn, num = segments.shape[:2]\nif n == 0:\nreturn [], segments\nxy = np.ones((n * num, 3), dtype=segments.dtype)\nsegments = segments.reshape(-1, 2)\nxy[:, :2] = segments\nxy = xy @ M.T  # transform\nxy = xy[:, :2] / xy[:, 2:3]\nsegments = xy.reshape(n, -1, 2)\nbboxes = np.stack([segment2box(xy, self.size[0], self.size[1]) for xy in segments], 0)\nreturn bboxes, segments\ndef apply_keypoints(self, keypoints, M):\n\"\"\"\n        Apply affine to keypoints.\n        Args:\n            keypoints (ndarray): keypoints, [N, 17, 3].\n            M (ndarray): affine matrix.\n        Returns:\n            new_keypoints (ndarray): keypoints after affine, [N, 17, 3].\n        \"\"\"\nn, nkpt = keypoints.shape[:2]\nif n == 0:\nreturn keypoints\nxy = np.ones((n * nkpt, 3), dtype=keypoints.dtype)\nvisible = keypoints[..., 2].reshape(n * nkpt, 1)\nxy[:, :2] = keypoints[..., :2].reshape(n * nkpt, 2)\nxy = xy @ M.T  # transform\nxy = xy[:, :2] / xy[:, 2:3]  # perspective rescale or affine\nout_mask = (xy[:, 0] &lt; 0) | (xy[:, 1] &lt; 0) | (xy[:, 0] &gt; self.size[0]) | (xy[:, 1] &gt; self.size[1])\nvisible[out_mask] = 0\nreturn np.concatenate([xy, visible], axis=-1).reshape(n, nkpt, 3)\ndef __call__(self, labels):\n\"\"\"\n        Affine images and targets.\n        Args:\n            labels (dict): a dict of `bboxes`, `segments`, `keypoints`.\n        \"\"\"\nif self.pre_transform and 'mosaic_border' not in labels:\nlabels = self.pre_transform(labels)\nlabels.pop('ratio_pad', None)  # do not need ratio pad\nimg = labels['img']\ncls = labels['cls']\ninstances = labels.pop('instances')\n# Make sure the coord formats are right\ninstances.convert_bbox(format='xyxy')\ninstances.denormalize(*img.shape[:2][::-1])\nborder = labels.pop('mosaic_border', self.border)\nself.size = img.shape[1] + border[1] * 2, img.shape[0] + border[0] * 2  # w, h\n# M is affine matrix\n# scale for func:`box_candidates`\nimg, M, scale = self.affine_transform(img, border)\nbboxes = self.apply_bboxes(instances.bboxes, M)\nsegments = instances.segments\nkeypoints = instances.keypoints\n# Update bboxes if there are segments.\nif len(segments):\nbboxes, segments = self.apply_segments(segments, M)\nif keypoints is not None:\nkeypoints = self.apply_keypoints(keypoints, M)\nnew_instances = Instances(bboxes, segments, keypoints, bbox_format='xyxy', normalized=False)\n# Clip\nnew_instances.clip(*self.size)\n# Filter instances\ninstances.scale(scale_w=scale, scale_h=scale, bbox_only=True)\n# Make the bboxes have the same scale with new_bboxes\ni = self.box_candidates(box1=instances.bboxes.T,\nbox2=new_instances.bboxes.T,\narea_thr=0.01 if len(segments) else 0.10)\nlabels['instances'] = new_instances[i]\nlabels['cls'] = cls[i]\nlabels['img'] = img\nlabels['resized_shape'] = img.shape[:2]\nreturn labels\ndef box_candidates(self, box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n# Compute box candidates: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\nw1, h1 = box1[2] - box1[0], box1[3] - box1[1]\nw2, h2 = box2[2] - box2[0], box2[3] - box2[1]\nar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\nreturn (w2 &gt; wh_thr) &amp; (h2 &gt; wh_thr) &amp; (w2 * h2 / (w1 * h1 + eps) &gt; area_thr) &amp; (ar &lt; ar_thr)  # candidates\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective.__call__","title":"<code>__call__(labels)</code>","text":"<p>Affine images and targets.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>dict</code> <p>a dict of <code>bboxes</code>, <code>segments</code>, <code>keypoints</code>.</p> required Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"\n    Affine images and targets.\n    Args:\n        labels (dict): a dict of `bboxes`, `segments`, `keypoints`.\n    \"\"\"\nif self.pre_transform and 'mosaic_border' not in labels:\nlabels = self.pre_transform(labels)\nlabels.pop('ratio_pad', None)  # do not need ratio pad\nimg = labels['img']\ncls = labels['cls']\ninstances = labels.pop('instances')\n# Make sure the coord formats are right\ninstances.convert_bbox(format='xyxy')\ninstances.denormalize(*img.shape[:2][::-1])\nborder = labels.pop('mosaic_border', self.border)\nself.size = img.shape[1] + border[1] * 2, img.shape[0] + border[0] * 2  # w, h\n# M is affine matrix\n# scale for func:`box_candidates`\nimg, M, scale = self.affine_transform(img, border)\nbboxes = self.apply_bboxes(instances.bboxes, M)\nsegments = instances.segments\nkeypoints = instances.keypoints\n# Update bboxes if there are segments.\nif len(segments):\nbboxes, segments = self.apply_segments(segments, M)\nif keypoints is not None:\nkeypoints = self.apply_keypoints(keypoints, M)\nnew_instances = Instances(bboxes, segments, keypoints, bbox_format='xyxy', normalized=False)\n# Clip\nnew_instances.clip(*self.size)\n# Filter instances\ninstances.scale(scale_w=scale, scale_h=scale, bbox_only=True)\n# Make the bboxes have the same scale with new_bboxes\ni = self.box_candidates(box1=instances.bboxes.T,\nbox2=new_instances.bboxes.T,\narea_thr=0.01 if len(segments) else 0.10)\nlabels['instances'] = new_instances[i]\nlabels['cls'] = cls[i]\nlabels['img'] = img\nlabels['resized_shape'] = img.shape[:2]\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective.affine_transform","title":"<code>affine_transform(img, border)</code>","text":"<p>Center.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def affine_transform(self, img, border):\n\"\"\"Center.\"\"\"\nC = np.eye(3, dtype=np.float32)\nC[0, 2] = -img.shape[1] / 2  # x translation (pixels)\nC[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n# Perspective\nP = np.eye(3, dtype=np.float32)\nP[2, 0] = random.uniform(-self.perspective, self.perspective)  # x perspective (about y)\nP[2, 1] = random.uniform(-self.perspective, self.perspective)  # y perspective (about x)\n# Rotation and Scale\nR = np.eye(3, dtype=np.float32)\na = random.uniform(-self.degrees, self.degrees)\n# a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\ns = random.uniform(1 - self.scale, 1 + self.scale)\n# s = 2 ** random.uniform(-scale, scale)\nR[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n# Shear\nS = np.eye(3, dtype=np.float32)\nS[0, 1] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # x shear (deg)\nS[1, 0] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # y shear (deg)\n# Translation\nT = np.eye(3, dtype=np.float32)\nT[0, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[0]  # x translation (pixels)\nT[1, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[1]  # y translation (pixels)\n# Combined rotation matrix\nM = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n# Affine image\nif (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\nif self.perspective:\nimg = cv2.warpPerspective(img, M, dsize=self.size, borderValue=(114, 114, 114))\nelse:  # affine\nimg = cv2.warpAffine(img, M[:2], dsize=self.size, borderValue=(114, 114, 114))\nreturn img, M, s\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective.apply_bboxes","title":"<code>apply_bboxes(bboxes, M)</code>","text":"<p>Apply affine to bboxes only.</p> <p>Parameters:</p> Name Type Description Default <code>bboxes</code> <code>ndarray</code> <p>list of bboxes, xyxy format, with shape (num_bboxes, 4).</p> required <code>M</code> <code>ndarray</code> <p>affine matrix.</p> required <p>Returns:</p> Name Type Description <code>new_bboxes</code> <code>ndarray</code> <p>bboxes after affine, [num_bboxes, 4].</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_bboxes(self, bboxes, M):\n\"\"\"\n    Apply affine to bboxes only.\n    Args:\n        bboxes (ndarray): list of bboxes, xyxy format, with shape (num_bboxes, 4).\n        M (ndarray): affine matrix.\n    Returns:\n        new_bboxes (ndarray): bboxes after affine, [num_bboxes, 4].\n    \"\"\"\nn = len(bboxes)\nif n == 0:\nreturn bboxes\nxy = np.ones((n * 4, 3), dtype=bboxes.dtype)\nxy[:, :2] = bboxes[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\nxy = xy @ M.T  # transform\nxy = (xy[:, :2] / xy[:, 2:3] if self.perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n# Create new boxes\nx = xy[:, [0, 2, 4, 6]]\ny = xy[:, [1, 3, 5, 7]]\nreturn np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1)), dtype=bboxes.dtype).reshape(4, n).T\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective.apply_keypoints","title":"<code>apply_keypoints(keypoints, M)</code>","text":"<p>Apply affine to keypoints.</p> <p>Parameters:</p> Name Type Description Default <code>keypoints</code> <code>ndarray</code> <p>keypoints, [N, 17, 3].</p> required <code>M</code> <code>ndarray</code> <p>affine matrix.</p> required <p>Returns:</p> Name Type Description <code>new_keypoints</code> <code>ndarray</code> <p>keypoints after affine, [N, 17, 3].</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_keypoints(self, keypoints, M):\n\"\"\"\n    Apply affine to keypoints.\n    Args:\n        keypoints (ndarray): keypoints, [N, 17, 3].\n        M (ndarray): affine matrix.\n    Returns:\n        new_keypoints (ndarray): keypoints after affine, [N, 17, 3].\n    \"\"\"\nn, nkpt = keypoints.shape[:2]\nif n == 0:\nreturn keypoints\nxy = np.ones((n * nkpt, 3), dtype=keypoints.dtype)\nvisible = keypoints[..., 2].reshape(n * nkpt, 1)\nxy[:, :2] = keypoints[..., :2].reshape(n * nkpt, 2)\nxy = xy @ M.T  # transform\nxy = xy[:, :2] / xy[:, 2:3]  # perspective rescale or affine\nout_mask = (xy[:, 0] &lt; 0) | (xy[:, 1] &lt; 0) | (xy[:, 0] &gt; self.size[0]) | (xy[:, 1] &gt; self.size[1])\nvisible[out_mask] = 0\nreturn np.concatenate([xy, visible], axis=-1).reshape(n, nkpt, 3)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomPerspective.apply_segments","title":"<code>apply_segments(segments, M)</code>","text":"<p>Apply affine to segments and generate new bboxes from segments.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>ndarray</code> <p>list of segments, [num_samples, 500, 2].</p> required <code>M</code> <code>ndarray</code> <p>affine matrix.</p> required <p>Returns:</p> Name Type Description <code>new_segments</code> <code>ndarray</code> <p>list of segments after affine, [num_samples, 500, 2].</p> <code>new_bboxes</code> <code>ndarray</code> <p>bboxes after affine, [N, 4].</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def apply_segments(self, segments, M):\n\"\"\"\n    Apply affine to segments and generate new bboxes from segments.\n    Args:\n        segments (ndarray): list of segments, [num_samples, 500, 2].\n        M (ndarray): affine matrix.\n    Returns:\n        new_segments (ndarray): list of segments after affine, [num_samples, 500, 2].\n        new_bboxes (ndarray): bboxes after affine, [N, 4].\n    \"\"\"\nn, num = segments.shape[:2]\nif n == 0:\nreturn [], segments\nxy = np.ones((n * num, 3), dtype=segments.dtype)\nsegments = segments.reshape(-1, 2)\nxy[:, :2] = segments\nxy = xy @ M.T  # transform\nxy = xy[:, :2] / xy[:, 2:3]\nsegments = xy.reshape(n, -1, 2)\nbboxes = np.stack([segment2box(xy, self.size[0], self.size[1]) for xy in segments], 0)\nreturn bboxes, segments\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomHSV","title":"<code>ultralytics.data.augment.RandomHSV</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class RandomHSV:\ndef __init__(self, hgain=0.5, sgain=0.5, vgain=0.5) -&gt; None:\nself.hgain = hgain\nself.sgain = sgain\nself.vgain = vgain\ndef __call__(self, labels):\n\"\"\"Applies random horizontal or vertical flip to an image with a given probability.\"\"\"\nimg = labels['img']\nif self.hgain or self.sgain or self.vgain:\nr = np.random.uniform(-1, 1, 3) * [self.hgain, self.sgain, self.vgain] + 1  # random gains\nhue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\ndtype = img.dtype  # uint8\nx = np.arange(0, 256, dtype=r.dtype)\nlut_hue = ((x * r[0]) % 180).astype(dtype)\nlut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\nlut_val = np.clip(x * r[2], 0, 255).astype(dtype)\nim_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\ncv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomHSV.__call__","title":"<code>__call__(labels)</code>","text":"<p>Applies random horizontal or vertical flip to an image with a given probability.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Applies random horizontal or vertical flip to an image with a given probability.\"\"\"\nimg = labels['img']\nif self.hgain or self.sgain or self.vgain:\nr = np.random.uniform(-1, 1, 3) * [self.hgain, self.sgain, self.vgain] + 1  # random gains\nhue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\ndtype = img.dtype  # uint8\nx = np.arange(0, 256, dtype=r.dtype)\nlut_hue = ((x * r[0]) % 180).astype(dtype)\nlut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\nlut_val = np.clip(x * r[2], 0, 255).astype(dtype)\nim_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\ncv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomFlip","title":"<code>ultralytics.data.augment.RandomFlip</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class RandomFlip:\ndef __init__(self, p=0.5, direction='horizontal', flip_idx=None) -&gt; None:\nassert direction in ['horizontal', 'vertical'], f'Support direction `horizontal` or `vertical`, got {direction}'\nassert 0 &lt;= p &lt;= 1.0\nself.p = p\nself.direction = direction\nself.flip_idx = flip_idx\ndef __call__(self, labels):\n\"\"\"Resize image and padding for detection, instance segmentation, pose.\"\"\"\nimg = labels['img']\ninstances = labels.pop('instances')\ninstances.convert_bbox(format='xywh')\nh, w = img.shape[:2]\nh = 1 if instances.normalized else h\nw = 1 if instances.normalized else w\n# Flip up-down\nif self.direction == 'vertical' and random.random() &lt; self.p:\nimg = np.flipud(img)\ninstances.flipud(h)\nif self.direction == 'horizontal' and random.random() &lt; self.p:\nimg = np.fliplr(img)\ninstances.fliplr(w)\n# For keypoints\nif self.flip_idx is not None and instances.keypoints is not None:\ninstances.keypoints = np.ascontiguousarray(instances.keypoints[:, self.flip_idx, :])\nlabels['img'] = np.ascontiguousarray(img)\nlabels['instances'] = instances\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.RandomFlip.__call__","title":"<code>__call__(labels)</code>","text":"<p>Resize image and padding for detection, instance segmentation, pose.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Resize image and padding for detection, instance segmentation, pose.\"\"\"\nimg = labels['img']\ninstances = labels.pop('instances')\ninstances.convert_bbox(format='xywh')\nh, w = img.shape[:2]\nh = 1 if instances.normalized else h\nw = 1 if instances.normalized else w\n# Flip up-down\nif self.direction == 'vertical' and random.random() &lt; self.p:\nimg = np.flipud(img)\ninstances.flipud(h)\nif self.direction == 'horizontal' and random.random() &lt; self.p:\nimg = np.fliplr(img)\ninstances.fliplr(w)\n# For keypoints\nif self.flip_idx is not None and instances.keypoints is not None:\ninstances.keypoints = np.ascontiguousarray(instances.keypoints[:, self.flip_idx, :])\nlabels['img'] = np.ascontiguousarray(img)\nlabels['instances'] = instances\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.LetterBox","title":"<code>ultralytics.data.augment.LetterBox</code>","text":"<p>Resize image and padding for detection, instance segmentation, pose.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class LetterBox:\n\"\"\"Resize image and padding for detection, instance segmentation, pose.\"\"\"\ndef __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n\"\"\"Initialize LetterBox object with specific parameters.\"\"\"\nself.new_shape = new_shape\nself.auto = auto\nself.scaleFill = scaleFill\nself.scaleup = scaleup\nself.stride = stride\nself.center = center  # Put the image in the middle or top-left\ndef __call__(self, labels=None, image=None):\n\"\"\"Return updated labels and image with added border.\"\"\"\nif labels is None:\nlabels = {}\nimg = labels.get('img') if image is None else image\nshape = img.shape[:2]  # current shape [height, width]\nnew_shape = labels.pop('rect_shape', self.new_shape)\nif isinstance(new_shape, int):\nnew_shape = (new_shape, new_shape)\n# Scale ratio (new / old)\nr = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\nif not self.scaleup:  # only scale down, do not scale up (for better val mAP)\nr = min(r, 1.0)\n# Compute padding\nratio = r, r  # width, height ratios\nnew_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\ndw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\nif self.auto:  # minimum rectangle\ndw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\nelif self.scaleFill:  # stretch\ndw, dh = 0.0, 0.0\nnew_unpad = (new_shape[1], new_shape[0])\nratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\nif self.center:\ndw /= 2  # divide padding into 2 sides\ndh /= 2\nif labels.get('ratio_pad'):\nlabels['ratio_pad'] = (labels['ratio_pad'], (dw, dh))  # for evaluation\nif shape[::-1] != new_unpad:  # resize\nimg = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\ntop, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\nleft, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\nimg = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\nvalue=(114, 114, 114))  # add border\nif len(labels):\nlabels = self._update_labels(labels, ratio, dw, dh)\nlabels['img'] = img\nlabels['resized_shape'] = new_shape\nreturn labels\nelse:\nreturn img\ndef _update_labels(self, labels, ratio, padw, padh):\n\"\"\"Update labels.\"\"\"\nlabels['instances'].convert_bbox(format='xyxy')\nlabels['instances'].denormalize(*labels['img'].shape[:2][::-1])\nlabels['instances'].scale(*ratio)\nlabels['instances'].add_padding(padw, padh)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.LetterBox.__call__","title":"<code>__call__(labels=None, image=None)</code>","text":"<p>Return updated labels and image with added border.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels=None, image=None):\n\"\"\"Return updated labels and image with added border.\"\"\"\nif labels is None:\nlabels = {}\nimg = labels.get('img') if image is None else image\nshape = img.shape[:2]  # current shape [height, width]\nnew_shape = labels.pop('rect_shape', self.new_shape)\nif isinstance(new_shape, int):\nnew_shape = (new_shape, new_shape)\n# Scale ratio (new / old)\nr = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\nif not self.scaleup:  # only scale down, do not scale up (for better val mAP)\nr = min(r, 1.0)\n# Compute padding\nratio = r, r  # width, height ratios\nnew_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\ndw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\nif self.auto:  # minimum rectangle\ndw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\nelif self.scaleFill:  # stretch\ndw, dh = 0.0, 0.0\nnew_unpad = (new_shape[1], new_shape[0])\nratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\nif self.center:\ndw /= 2  # divide padding into 2 sides\ndh /= 2\nif labels.get('ratio_pad'):\nlabels['ratio_pad'] = (labels['ratio_pad'], (dw, dh))  # for evaluation\nif shape[::-1] != new_unpad:  # resize\nimg = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\ntop, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\nleft, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\nimg = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\nvalue=(114, 114, 114))  # add border\nif len(labels):\nlabels = self._update_labels(labels, ratio, dw, dh)\nlabels['img'] = img\nlabels['resized_shape'] = new_shape\nreturn labels\nelse:\nreturn img\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.LetterBox.__init__","title":"<code>__init__(new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32)</code>","text":"<p>Initialize LetterBox object with specific parameters.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n\"\"\"Initialize LetterBox object with specific parameters.\"\"\"\nself.new_shape = new_shape\nself.auto = auto\nself.scaleFill = scaleFill\nself.scaleup = scaleup\nself.stride = stride\nself.center = center  # Put the image in the middle or top-left\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.CopyPaste","title":"<code>ultralytics.data.augment.CopyPaste</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class CopyPaste:\ndef __init__(self, p=0.5) -&gt; None:\nself.p = p\ndef __call__(self, labels):\n\"\"\"Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy).\"\"\"\nim = labels['img']\ncls = labels['cls']\nh, w = im.shape[:2]\ninstances = labels.pop('instances')\ninstances.convert_bbox(format='xyxy')\ninstances.denormalize(w, h)\nif self.p and len(instances.segments):\nn = len(instances)\n_, w, _ = im.shape  # height, width, channels\nim_new = np.zeros(im.shape, np.uint8)\n# Calculate ioa first then select indexes randomly\nins_flip = deepcopy(instances)\nins_flip.fliplr(w)\nioa = bbox_ioa(ins_flip.bboxes, instances.bboxes)  # intersection over area, (N, M)\nindexes = np.nonzero((ioa &lt; 0.30).all(1))[0]  # (N, )\nn = len(indexes)\nfor j in random.sample(list(indexes), k=round(self.p * n)):\ncls = np.concatenate((cls, cls[[j]]), axis=0)\ninstances = Instances.concatenate((instances, ins_flip[[j]]), axis=0)\ncv2.drawContours(im_new, instances.segments[[j]].astype(np.int32), -1, (1, 1, 1), cv2.FILLED)\nresult = cv2.flip(im, 1)  # augment segments (flip left-right)\ni = cv2.flip(im_new, 1).astype(bool)\nim[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\nlabels['img'] = im\nlabels['cls'] = cls\nlabels['instances'] = instances\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.CopyPaste.__call__","title":"<code>__call__(labels)</code>","text":"<p>Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy).</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy).\"\"\"\nim = labels['img']\ncls = labels['cls']\nh, w = im.shape[:2]\ninstances = labels.pop('instances')\ninstances.convert_bbox(format='xyxy')\ninstances.denormalize(w, h)\nif self.p and len(instances.segments):\nn = len(instances)\n_, w, _ = im.shape  # height, width, channels\nim_new = np.zeros(im.shape, np.uint8)\n# Calculate ioa first then select indexes randomly\nins_flip = deepcopy(instances)\nins_flip.fliplr(w)\nioa = bbox_ioa(ins_flip.bboxes, instances.bboxes)  # intersection over area, (N, M)\nindexes = np.nonzero((ioa &lt; 0.30).all(1))[0]  # (N, )\nn = len(indexes)\nfor j in random.sample(list(indexes), k=round(self.p * n)):\ncls = np.concatenate((cls, cls[[j]]), axis=0)\ninstances = Instances.concatenate((instances, ins_flip[[j]]), axis=0)\ncv2.drawContours(im_new, instances.segments[[j]].astype(np.int32), -1, (1, 1, 1), cv2.FILLED)\nresult = cv2.flip(im, 1)  # augment segments (flip left-right)\ni = cv2.flip(im_new, 1).astype(bool)\nim[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\nlabels['img'] = im\nlabels['cls'] = cls\nlabels['instances'] = instances\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Albumentations","title":"<code>ultralytics.data.augment.Albumentations</code>","text":"<p>YOLOv8 Albumentations class (optional, only used if package is installed)</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class Albumentations:\n\"\"\"YOLOv8 Albumentations class (optional, only used if package is installed)\"\"\"\ndef __init__(self, p=1.0):\n\"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\nself.p = p\nself.transform = None\nprefix = colorstr('albumentations: ')\ntry:\nimport albumentations as A\ncheck_version(A.__version__, '1.0.3', hard=True)  # version requirement\nT = [\nA.Blur(p=0.01),\nA.MedianBlur(p=0.01),\nA.ToGray(p=0.01),\nA.CLAHE(p=0.01),\nA.RandomBrightnessContrast(p=0.0),\nA.RandomGamma(p=0.0),\nA.ImageCompression(quality_lower=75, p=0.0)]  # transforms\nself.transform = A.Compose(T, bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\nLOGGER.info(prefix + ', '.join(f'{x}'.replace('always_apply=False, ', '') for x in T if x.p))\nexcept ImportError:  # package not installed, skip\npass\nexcept Exception as e:\nLOGGER.info(f'{prefix}{e}')\ndef __call__(self, labels):\n\"\"\"Generates object detections and returns a dictionary with detection results.\"\"\"\nim = labels['img']\ncls = labels['cls']\nif len(cls):\nlabels['instances'].convert_bbox('xywh')\nlabels['instances'].normalize(*im.shape[:2][::-1])\nbboxes = labels['instances'].bboxes\n# TODO: add supports of segments and keypoints\nif self.transform and random.random() &lt; self.p:\nnew = self.transform(image=im, bboxes=bboxes, class_labels=cls)  # transformed\nif len(new['class_labels']) &gt; 0:  # skip update if no bbox in new im\nlabels['img'] = new['image']\nlabels['cls'] = np.array(new['class_labels'])\nbboxes = np.array(new['bboxes'], dtype=np.float32)\nlabels['instances'].update(bboxes=bboxes)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Albumentations.__call__","title":"<code>__call__(labels)</code>","text":"<p>Generates object detections and returns a dictionary with detection results.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Generates object detections and returns a dictionary with detection results.\"\"\"\nim = labels['img']\ncls = labels['cls']\nif len(cls):\nlabels['instances'].convert_bbox('xywh')\nlabels['instances'].normalize(*im.shape[:2][::-1])\nbboxes = labels['instances'].bboxes\n# TODO: add supports of segments and keypoints\nif self.transform and random.random() &lt; self.p:\nnew = self.transform(image=im, bboxes=bboxes, class_labels=cls)  # transformed\nif len(new['class_labels']) &gt; 0:  # skip update if no bbox in new im\nlabels['img'] = new['image']\nlabels['cls'] = np.array(new['class_labels'])\nbboxes = np.array(new['bboxes'], dtype=np.float32)\nlabels['instances'].update(bboxes=bboxes)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Albumentations.__init__","title":"<code>__init__(p=1.0)</code>","text":"<p>Initialize the transform object for YOLO bbox formatted params.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, p=1.0):\n\"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\nself.p = p\nself.transform = None\nprefix = colorstr('albumentations: ')\ntry:\nimport albumentations as A\ncheck_version(A.__version__, '1.0.3', hard=True)  # version requirement\nT = [\nA.Blur(p=0.01),\nA.MedianBlur(p=0.01),\nA.ToGray(p=0.01),\nA.CLAHE(p=0.01),\nA.RandomBrightnessContrast(p=0.0),\nA.RandomGamma(p=0.0),\nA.ImageCompression(quality_lower=75, p=0.0)]  # transforms\nself.transform = A.Compose(T, bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\nLOGGER.info(prefix + ', '.join(f'{x}'.replace('always_apply=False, ', '') for x in T if x.p))\nexcept ImportError:  # package not installed, skip\npass\nexcept Exception as e:\nLOGGER.info(f'{prefix}{e}')\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Format","title":"<code>ultralytics.data.augment.Format</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>class Format:\ndef __init__(self,\nbbox_format='xywh',\nnormalize=True,\nreturn_mask=False,\nreturn_keypoint=False,\nmask_ratio=4,\nmask_overlap=True,\nbatch_idx=True):\nself.bbox_format = bbox_format\nself.normalize = normalize\nself.return_mask = return_mask  # set False when training detection only\nself.return_keypoint = return_keypoint\nself.mask_ratio = mask_ratio\nself.mask_overlap = mask_overlap\nself.batch_idx = batch_idx  # keep the batch indexes\ndef __call__(self, labels):\n\"\"\"Return formatted image, classes, bounding boxes &amp; keypoints to be used by 'collate_fn'.\"\"\"\nimg = labels.pop('img')\nh, w = img.shape[:2]\ncls = labels.pop('cls')\ninstances = labels.pop('instances')\ninstances.convert_bbox(format=self.bbox_format)\ninstances.denormalize(w, h)\nnl = len(instances)\nif self.return_mask:\nif nl:\nmasks, instances, cls = self._format_segments(instances, cls, w, h)\nmasks = torch.from_numpy(masks)\nelse:\nmasks = torch.zeros(1 if self.mask_overlap else nl, img.shape[0] // self.mask_ratio,\nimg.shape[1] // self.mask_ratio)\nlabels['masks'] = masks\nif self.normalize:\ninstances.normalize(w, h)\nlabels['img'] = self._format_img(img)\nlabels['cls'] = torch.from_numpy(cls) if nl else torch.zeros(nl)\nlabels['bboxes'] = torch.from_numpy(instances.bboxes) if nl else torch.zeros((nl, 4))\nif self.return_keypoint:\nlabels['keypoints'] = torch.from_numpy(instances.keypoints)\n# Then we can use collate_fn\nif self.batch_idx:\nlabels['batch_idx'] = torch.zeros(nl)\nreturn labels\ndef _format_img(self, img):\n\"\"\"Format the image for YOLOv5 from Numpy array to PyTorch tensor.\"\"\"\nif len(img.shape) &lt; 3:\nimg = np.expand_dims(img, -1)\nimg = np.ascontiguousarray(img.transpose(2, 0, 1)[::-1])\nimg = torch.from_numpy(img)\nreturn img\ndef _format_segments(self, instances, cls, w, h):\n\"\"\"convert polygon points to bitmap.\"\"\"\nsegments = instances.segments\nif self.mask_overlap:\nmasks, sorted_idx = polygons2masks_overlap((h, w), segments, downsample_ratio=self.mask_ratio)\nmasks = masks[None]  # (640, 640) -&gt; (1, 640, 640)\ninstances = instances[sorted_idx]\ncls = cls[sorted_idx]\nelse:\nmasks = polygons2masks((h, w), segments, color=1, downsample_ratio=self.mask_ratio)\nreturn masks, instances, cls\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.Format.__call__","title":"<code>__call__(labels)</code>","text":"<p>Return formatted image, classes, bounding boxes &amp; keypoints to be used by 'collate_fn'.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __call__(self, labels):\n\"\"\"Return formatted image, classes, bounding boxes &amp; keypoints to be used by 'collate_fn'.\"\"\"\nimg = labels.pop('img')\nh, w = img.shape[:2]\ncls = labels.pop('cls')\ninstances = labels.pop('instances')\ninstances.convert_bbox(format=self.bbox_format)\ninstances.denormalize(w, h)\nnl = len(instances)\nif self.return_mask:\nif nl:\nmasks, instances, cls = self._format_segments(instances, cls, w, h)\nmasks = torch.from_numpy(masks)\nelse:\nmasks = torch.zeros(1 if self.mask_overlap else nl, img.shape[0] // self.mask_ratio,\nimg.shape[1] // self.mask_ratio)\nlabels['masks'] = masks\nif self.normalize:\ninstances.normalize(w, h)\nlabels['img'] = self._format_img(img)\nlabels['cls'] = torch.from_numpy(cls) if nl else torch.zeros(nl)\nlabels['bboxes'] = torch.from_numpy(instances.bboxes) if nl else torch.zeros((nl, 4))\nif self.return_keypoint:\nlabels['keypoints'] = torch.from_numpy(instances.keypoints)\n# Then we can use collate_fn\nif self.batch_idx:\nlabels['batch_idx'] = torch.zeros(nl)\nreturn labels\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.ClassifyLetterBox","title":"<code>ultralytics.data.augment.ClassifyLetterBox</code>","text":"<p>YOLOv8 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class ClassifyLetterBox:\n\"\"\"YOLOv8 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\"\"\"\ndef __init__(self, size=(640, 640), auto=False, stride=32):\n\"\"\"Resizes image and crops it to center with max dimensions 'h' and 'w'.\"\"\"\nsuper().__init__()\nself.h, self.w = (size, size) if isinstance(size, int) else size\nself.auto = auto  # pass max size integer, automatically solve for short side using stride\nself.stride = stride  # used with auto\ndef __call__(self, im):  # im = np.array HWC\nimh, imw = im.shape[:2]\nr = min(self.h / imh, self.w / imw)  # ratio of new/old\nh, w = round(imh * r), round(imw * r)  # resized image\nhs, ws = (math.ceil(x / self.stride) * self.stride for x in (h, w)) if self.auto else self.h, self.w\ntop, left = round((hs - h) / 2 - 0.1), round((ws - w) / 2 - 0.1)\nim_out = np.full((self.h, self.w, 3), 114, dtype=im.dtype)\nim_out[top:top + h, left:left + w] = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)\nreturn im_out\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.ClassifyLetterBox.__init__","title":"<code>__init__(size=(640, 640), auto=False, stride=32)</code>","text":"<p>Resizes image and crops it to center with max dimensions 'h' and 'w'.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, size=(640, 640), auto=False, stride=32):\n\"\"\"Resizes image and crops it to center with max dimensions 'h' and 'w'.\"\"\"\nsuper().__init__()\nself.h, self.w = (size, size) if isinstance(size, int) else size\nself.auto = auto  # pass max size integer, automatically solve for short side using stride\nself.stride = stride  # used with auto\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.CenterCrop","title":"<code>ultralytics.data.augment.CenterCrop</code>","text":"<p>YOLOv8 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class CenterCrop:\n\"\"\"YOLOv8 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])\"\"\"\ndef __init__(self, size=640):\n\"\"\"Converts an image from numpy array to PyTorch tensor.\"\"\"\nsuper().__init__()\nself.h, self.w = (size, size) if isinstance(size, int) else size\ndef __call__(self, im):  # im = np.array HWC\nimh, imw = im.shape[:2]\nm = min(imh, imw)  # min dimension\ntop, left = (imh - m) // 2, (imw - m) // 2\nreturn cv2.resize(im[top:top + m, left:left + m], (self.w, self.h), interpolation=cv2.INTER_LINEAR)\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.CenterCrop.__init__","title":"<code>__init__(size=640)</code>","text":"<p>Converts an image from numpy array to PyTorch tensor.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, size=640):\n\"\"\"Converts an image from numpy array to PyTorch tensor.\"\"\"\nsuper().__init__()\nself.h, self.w = (size, size) if isinstance(size, int) else size\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.ToTensor","title":"<code>ultralytics.data.augment.ToTensor</code>","text":"<p>YOLOv8 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()]).</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>class ToTensor:\n\"\"\"YOLOv8 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()]).\"\"\"\ndef __init__(self, half=False):\n\"\"\"Initialize YOLOv8 ToTensor object with optional half-precision support.\"\"\"\nsuper().__init__()\nself.half = half\ndef __call__(self, im):  # im = np.array HWC in BGR order\nim = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])  # HWC to CHW -&gt; BGR to RGB -&gt; contiguous\nim = torch.from_numpy(im)  # to torch\nim = im.half() if self.half else im.float()  # uint8 to fp16/32\nim /= 255.0  # 0-255 to 0.0-1.0\nreturn im\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.ToTensor.__init__","title":"<code>__init__(half=False)</code>","text":"<p>Initialize YOLOv8 ToTensor object with optional half-precision support.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def __init__(self, half=False):\n\"\"\"Initialize YOLOv8 ToTensor object with optional half-precision support.\"\"\"\nsuper().__init__()\nself.half = half\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.v8_transforms","title":"<code>ultralytics.data.augment.v8_transforms(dataset, imgsz, hyp, stretch=False)</code>","text":"<p>Convert images to a size suitable for YOLOv8 training.</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def v8_transforms(dataset, imgsz, hyp, stretch=False):\n\"\"\"Convert images to a size suitable for YOLOv8 training.\"\"\"\npre_transform = Compose([\nMosaic(dataset, imgsz=imgsz, p=hyp.mosaic),\nCopyPaste(p=hyp.copy_paste),\nRandomPerspective(\ndegrees=hyp.degrees,\ntranslate=hyp.translate,\nscale=hyp.scale,\nshear=hyp.shear,\nperspective=hyp.perspective,\npre_transform=None if stretch else LetterBox(new_shape=(imgsz, imgsz)),\n)])\nflip_idx = dataset.data.get('flip_idx', [])  # for keypoints augmentation\nif dataset.use_keypoints:\nkpt_shape = dataset.data.get('kpt_shape', None)\nif len(flip_idx) == 0 and hyp.fliplr &gt; 0.0:\nhyp.fliplr = 0.0\nLOGGER.warning(\"WARNING \u26a0\ufe0f No 'flip_idx' array defined in data.yaml, setting augmentation 'fliplr=0.0'\")\nelif flip_idx and (len(flip_idx) != kpt_shape[0]):\nraise ValueError(f'data.yaml flip_idx={flip_idx} length must be equal to kpt_shape[0]={kpt_shape[0]}')\nreturn Compose([\npre_transform,\nMixUp(dataset, pre_transform=pre_transform, p=hyp.mixup),\nAlbumentations(p=1.0),\nRandomHSV(hgain=hyp.hsv_h, sgain=hyp.hsv_s, vgain=hyp.hsv_v),\nRandomFlip(direction='vertical', p=hyp.flipud),\nRandomFlip(direction='horizontal', p=hyp.fliplr, flip_idx=flip_idx)])  # transforms\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.classify_transforms","title":"<code>ultralytics.data.augment.classify_transforms(size=224, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0))</code>","text":"Source code in <code>ultralytics/data/augment.py</code> <pre><code>def classify_transforms(size=224, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)):  # IMAGENET_MEAN, IMAGENET_STD\n# Transforms to apply if albumentations not installed\nif not isinstance(size, int):\nraise TypeError(f'classify_transforms() size {size} must be integer, not (list, tuple)')\nif any(mean) or any(std):\nreturn T.Compose([CenterCrop(size), ToTensor(), T.Normalize(mean, std, inplace=True)])\nelse:\nreturn T.Compose([CenterCrop(size), ToTensor()])\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.hsv2colorjitter","title":"<code>ultralytics.data.augment.hsv2colorjitter(h, s, v)</code>","text":"<p>Map HSV (hue, saturation, value) jitter into ColorJitter values (brightness, contrast, saturation, hue)</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def hsv2colorjitter(h, s, v):\n\"\"\"Map HSV (hue, saturation, value) jitter into ColorJitter values (brightness, contrast, saturation, hue)\"\"\"\nreturn v, v, s, h\n</code></pre>"},{"location":"reference/data/augment/#ultralytics.data.augment.classify_albumentations","title":"<code>ultralytics.data.augment.classify_albumentations(augment=True, size=224, scale=(0.08, 1.0), hflip=0.5, vflip=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), auto_aug=False)</code>","text":"<p>YOLOv8 classification Albumentations (optional, only used if package is installed).</p> Source code in <code>ultralytics/data/augment.py</code> <pre><code>def classify_albumentations(\naugment=True,\nsize=224,\nscale=(0.08, 1.0),\nhflip=0.5,\nvflip=0.0,\nhsv_h=0.015,  # image HSV-Hue augmentation (fraction)\nhsv_s=0.7,  # image HSV-Saturation augmentation (fraction)\nhsv_v=0.4,  # image HSV-Value augmentation (fraction)\nmean=(0.0, 0.0, 0.0),  # IMAGENET_MEAN\nstd=(1.0, 1.0, 1.0),  # IMAGENET_STD\nauto_aug=False,\n):\n\"\"\"YOLOv8 classification Albumentations (optional, only used if package is installed).\"\"\"\nprefix = colorstr('albumentations: ')\ntry:\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\ncheck_version(A.__version__, '1.0.3', hard=True)  # version requirement\nif augment:  # Resize and crop\nT = [A.RandomResizedCrop(height=size, width=size, scale=scale)]\nif auto_aug:\n# TODO: implement AugMix, AutoAug &amp; RandAug in albumentations\nLOGGER.info(f'{prefix}auto augmentations are currently not supported')\nelse:\nif hflip &gt; 0:\nT += [A.HorizontalFlip(p=hflip)]\nif vflip &gt; 0:\nT += [A.VerticalFlip(p=vflip)]\nif any((hsv_h, hsv_s, hsv_v)):\nT += [A.ColorJitter(*hsv2colorjitter(hsv_h, hsv_s, hsv_v))]  # brightness, contrast, saturation, hue\nelse:  # Use fixed crop for eval set (reproducibility)\nT = [A.SmallestMaxSize(max_size=size), A.CenterCrop(height=size, width=size)]\nT += [A.Normalize(mean=mean, std=std), ToTensorV2()]  # Normalize and convert to Tensor\nLOGGER.info(prefix + ', '.join(f'{x}'.replace('always_apply=False, ', '') for x in T if x.p))\nreturn A.Compose(T)\nexcept ImportError:  # package not installed, skip\npass\nexcept Exception as e:\nLOGGER.info(f'{prefix}{e}')\n</code></pre>"},{"location":"reference/data/base/","title":"Reference for <code>ultralytics/data/base.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/base.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset","title":"<code>ultralytics.data.base.BaseDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Base dataset class for loading and processing image data.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the folder containing images.</p> required <code>imgsz</code> <code>int</code> <p>Image size. Defaults to 640.</p> <code>640</code> <code>cache</code> <code>bool</code> <p>Cache images to RAM or disk during training. Defaults to False.</p> <code>False</code> <code>augment</code> <code>bool</code> <p>If True, data augmentation is applied. Defaults to True.</p> <code>True</code> <code>hyp</code> <code>dict</code> <p>Hyperparameters to apply data augmentation. Defaults to None.</p> <code>DEFAULT_CFG</code> <code>prefix</code> <code>str</code> <p>Prefix to print in log messages. Defaults to ''.</p> <code>''</code> <code>rect</code> <code>bool</code> <p>If True, rectangular training is used. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Size of batches. Defaults to None.</p> <code>16</code> <code>stride</code> <code>int</code> <p>Stride. Defaults to 32.</p> <code>32</code> <code>pad</code> <code>float</code> <p>Padding. Defaults to 0.0.</p> <code>0.5</code> <code>single_cls</code> <code>bool</code> <p>If True, single class training is used. Defaults to False.</p> <code>False</code> <code>classes</code> <code>list</code> <p>List of included classes. Default is None.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of dataset to utilize. Default is 1.0 (use all data).</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>im_files</code> <code>list</code> <p>List of image file paths.</p> <code>labels</code> <code>list</code> <p>List of label data dictionaries.</p> <code>ni</code> <code>int</code> <p>Number of images in the dataset.</p> <code>ims</code> <code>list</code> <p>List of loaded images.</p> <code>npy_files</code> <code>list</code> <p>List of numpy file paths.</p> <code>transforms</code> <code>callable</code> <p>Image transformation function.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>class BaseDataset(Dataset):\n\"\"\"\n    Base dataset class for loading and processing image data.\n    Args:\n        img_path (str): Path to the folder containing images.\n        imgsz (int, optional): Image size. Defaults to 640.\n        cache (bool, optional): Cache images to RAM or disk during training. Defaults to False.\n        augment (bool, optional): If True, data augmentation is applied. Defaults to True.\n        hyp (dict, optional): Hyperparameters to apply data augmentation. Defaults to None.\n        prefix (str, optional): Prefix to print in log messages. Defaults to ''.\n        rect (bool, optional): If True, rectangular training is used. Defaults to False.\n        batch_size (int, optional): Size of batches. Defaults to None.\n        stride (int, optional): Stride. Defaults to 32.\n        pad (float, optional): Padding. Defaults to 0.0.\n        single_cls (bool, optional): If True, single class training is used. Defaults to False.\n        classes (list): List of included classes. Default is None.\n        fraction (float): Fraction of dataset to utilize. Default is 1.0 (use all data).\n    Attributes:\n        im_files (list): List of image file paths.\n        labels (list): List of label data dictionaries.\n        ni (int): Number of images in the dataset.\n        ims (list): List of loaded images.\n        npy_files (list): List of numpy file paths.\n        transforms (callable): Image transformation function.\n    \"\"\"\ndef __init__(self,\nimg_path,\nimgsz=640,\ncache=False,\naugment=True,\nhyp=DEFAULT_CFG,\nprefix='',\nrect=False,\nbatch_size=16,\nstride=32,\npad=0.5,\nsingle_cls=False,\nclasses=None,\nfraction=1.0):\nsuper().__init__()\nself.img_path = img_path\nself.imgsz = imgsz\nself.augment = augment\nself.single_cls = single_cls\nself.prefix = prefix\nself.fraction = fraction\nself.im_files = self.get_img_files(self.img_path)\nself.labels = self.get_labels()\nself.update_labels(include_class=classes)  # single_cls and include_class\nself.ni = len(self.labels)  # number of images\nself.rect = rect\nself.batch_size = batch_size\nself.stride = stride\nself.pad = pad\nif self.rect:\nassert self.batch_size is not None\nself.set_rectangle()\n# Buffer thread for mosaic images\nself.buffer = []  # buffer size = batch size\nself.max_buffer_length = min((self.ni, self.batch_size * 8, 1000)) if self.augment else 0\n# Cache stuff\nif cache == 'ram' and not self.check_cache_ram():\ncache = False\nself.ims, self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni, [None] * self.ni\nself.npy_files = [Path(f).with_suffix('.npy') for f in self.im_files]\nif cache:\nself.cache_images(cache)\n# Transforms\nself.transforms = self.build_transforms(hyp=hyp)\ndef get_img_files(self, img_path):\n\"\"\"Read image files.\"\"\"\ntry:\nf = []  # image files\nfor p in img_path if isinstance(img_path, list) else [img_path]:\np = Path(p)  # os-agnostic\nif p.is_dir():  # dir\nf += glob.glob(str(p / '**' / '*.*'), recursive=True)\n# F = list(p.rglob('*.*'))  # pathlib\nelif p.is_file():  # file\nwith open(p) as t:\nt = t.read().strip().splitlines()\nparent = str(p.parent) + os.sep\nf += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n# F += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\nelse:\nraise FileNotFoundError(f'{self.prefix}{p} does not exist')\nim_files = sorted(x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS)\n# self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\nassert im_files, f'{self.prefix}No images found'\nexcept Exception as e:\nraise FileNotFoundError(f'{self.prefix}Error loading data from {img_path}\\n{HELP_URL}') from e\nif self.fraction &lt; 1:\nim_files = im_files[:round(len(im_files) * self.fraction)]\nreturn im_files\ndef update_labels(self, include_class: Optional[list]):\n\"\"\"include_class, filter labels to include only these classes (optional).\"\"\"\ninclude_class_array = np.array(include_class).reshape(1, -1)\nfor i in range(len(self.labels)):\nif include_class is not None:\ncls = self.labels[i]['cls']\nbboxes = self.labels[i]['bboxes']\nsegments = self.labels[i]['segments']\nkeypoints = self.labels[i]['keypoints']\nj = (cls == include_class_array).any(1)\nself.labels[i]['cls'] = cls[j]\nself.labels[i]['bboxes'] = bboxes[j]\nif segments:\nself.labels[i]['segments'] = [segments[si] for si, idx in enumerate(j) if idx]\nif keypoints is not None:\nself.labels[i]['keypoints'] = keypoints[j]\nif self.single_cls:\nself.labels[i]['cls'][:, 0] = 0\ndef load_image(self, i):\n\"\"\"Loads 1 image from dataset index 'i', returns (im, resized hw).\"\"\"\nim, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\nif im is None:  # not cached in RAM\nif fn.exists():  # load npy\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif im is None:\nraise FileNotFoundError(f'Image Not Found {f}')\nh0, w0 = im.shape[:2]  # orig hw\nr = self.imgsz / max(h0, w0)  # ratio\nif r != 1:  # if sizes are not equal\ninterp = cv2.INTER_LINEAR if (self.augment or r &gt; 1) else cv2.INTER_AREA\nim = cv2.resize(im, (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz)),\ninterpolation=interp)\n# Add to buffer if training with augmentations\nif self.augment:\nself.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\nself.buffer.append(i)\nif len(self.buffer) &gt;= self.max_buffer_length:\nj = self.buffer.pop(0)\nself.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\nreturn im, (h0, w0), im.shape[:2]\nreturn self.ims[i], self.im_hw0[i], self.im_hw[i]\ndef cache_images(self, cache):\n\"\"\"Cache images to memory or disk.\"\"\"\nb, gb = 0, 1 &lt;&lt; 30  # bytes of cached images, bytes per gigabytes\nfcn = self.cache_images_to_disk if cache == 'disk' else self.load_image\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(fcn, range(self.ni))\npbar = tqdm(enumerate(results), total=self.ni, bar_format=TQDM_BAR_FORMAT, disable=LOCAL_RANK &gt; 0)\nfor i, x in pbar:\nif cache == 'disk':\nb += self.npy_files[i].stat().st_size\nelse:  # 'ram'\nself.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\nb += self.ims[i].nbytes\npbar.desc = f'{self.prefix}Caching images ({b / gb:.1f}GB {cache})'\npbar.close()\ndef cache_images_to_disk(self, i):\n\"\"\"Saves an image as an *.npy file for faster loading.\"\"\"\nf = self.npy_files[i]\nif not f.exists():\nnp.save(f.as_posix(), cv2.imread(self.im_files[i]))\ndef check_cache_ram(self, safety_margin=0.5):\n\"\"\"Check image caching requirements vs available memory.\"\"\"\nb, gb = 0, 1 &lt;&lt; 30  # bytes of cached images, bytes per gigabytes\nn = min(self.ni, 30)  # extrapolate from 30 random images\nfor _ in range(n):\nim = cv2.imread(random.choice(self.im_files))  # sample image\nratio = self.imgsz / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio\nb += im.nbytes * ratio ** 2\nmem_required = b * self.ni / n * (1 + safety_margin)  # GB required to cache dataset into RAM\nmem = psutil.virtual_memory()\ncache = mem_required &lt; mem.available  # to cache or not to cache, that is the question\nif not cache:\nLOGGER.info(f'{self.prefix}{mem_required / gb:.1f}GB RAM required to cache images '\nf'with {int(safety_margin * 100)}% safety margin but only '\nf'{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, '\nf\"{'caching images \u2705' if cache else 'not caching images \u26a0\ufe0f'}\")\nreturn cache\ndef set_rectangle(self):\n\"\"\"Sets the shape of bounding boxes for YOLO detections as rectangles.\"\"\"\nbi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index\nnb = bi[-1] + 1  # number of batches\ns = np.array([x.pop('shape') for x in self.labels])  # hw\nar = s[:, 0] / s[:, 1]  # aspect ratio\nirect = ar.argsort()\nself.im_files = [self.im_files[i] for i in irect]\nself.labels = [self.labels[i] for i in irect]\nar = ar[irect]\n# Set training image shapes\nshapes = [[1, 1]] * nb\nfor i in range(nb):\nari = ar[bi == i]\nmini, maxi = ari.min(), ari.max()\nif maxi &lt; 1:\nshapes[i] = [maxi, 1]\nelif mini &gt; 1:\nshapes[i] = [1, 1 / mini]\nself.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride\nself.batch = bi  # batch index of image\ndef __getitem__(self, index):\n\"\"\"Returns transformed label information for given index.\"\"\"\nreturn self.transforms(self.get_image_and_label(index))\ndef get_image_and_label(self, index):\n\"\"\"Get and return label information from the dataset.\"\"\"\nlabel = deepcopy(self.labels[index])  # requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\nlabel.pop('shape', None)  # shape is for rect, remove it\nlabel['img'], label['ori_shape'], label['resized_shape'] = self.load_image(index)\nlabel['ratio_pad'] = (label['resized_shape'][0] / label['ori_shape'][0],\nlabel['resized_shape'][1] / label['ori_shape'][1])  # for evaluation\nif self.rect:\nlabel['rect_shape'] = self.batch_shapes[self.batch[index]]\nreturn self.update_labels_info(label)\ndef __len__(self):\n\"\"\"Returns the length of the labels list for the dataset.\"\"\"\nreturn len(self.labels)\ndef update_labels_info(self, label):\n\"\"\"custom your label format here.\"\"\"\nreturn label\ndef build_transforms(self, hyp=None):\n\"\"\"Users can custom augmentations here\n        like:\n            if self.augment:\n                # Training transforms\n                return Compose([])\n            else:\n                # Val transforms\n                return Compose([])\n        \"\"\"\nraise NotImplementedError\ndef get_labels(self):\n\"\"\"Users can custom their own format here.\n        Make sure your output is a list with each element like below:\n            dict(\n                im_file=im_file,\n                shape=shape,  # format: (height, width)\n                cls=cls,\n                bboxes=bboxes, # xywh\n                segments=segments,  # xy\n                keypoints=keypoints, # xy\n                normalized=True, # or False\n                bbox_format=\"xyxy\",  # or xywh, ltwh\n            )\n        \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns transformed label information for given index.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def __getitem__(self, index):\n\"\"\"Returns transformed label information for given index.\"\"\"\nreturn self.transforms(self.get_image_and_label(index))\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the labels list for the dataset.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the length of the labels list for the dataset.\"\"\"\nreturn len(self.labels)\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.build_transforms","title":"<code>build_transforms(hyp=None)</code>","text":"<p>Users can custom augmentations here</p> like <p>if self.augment:     # Training transforms     return Compose([]) else:     # Val transforms     return Compose([])</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def build_transforms(self, hyp=None):\n\"\"\"Users can custom augmentations here\n    like:\n        if self.augment:\n            # Training transforms\n            return Compose([])\n        else:\n            # Val transforms\n            return Compose([])\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.cache_images","title":"<code>cache_images(cache)</code>","text":"<p>Cache images to memory or disk.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def cache_images(self, cache):\n\"\"\"Cache images to memory or disk.\"\"\"\nb, gb = 0, 1 &lt;&lt; 30  # bytes of cached images, bytes per gigabytes\nfcn = self.cache_images_to_disk if cache == 'disk' else self.load_image\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(fcn, range(self.ni))\npbar = tqdm(enumerate(results), total=self.ni, bar_format=TQDM_BAR_FORMAT, disable=LOCAL_RANK &gt; 0)\nfor i, x in pbar:\nif cache == 'disk':\nb += self.npy_files[i].stat().st_size\nelse:  # 'ram'\nself.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\nb += self.ims[i].nbytes\npbar.desc = f'{self.prefix}Caching images ({b / gb:.1f}GB {cache})'\npbar.close()\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.cache_images_to_disk","title":"<code>cache_images_to_disk(i)</code>","text":"<p>Saves an image as an *.npy file for faster loading.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def cache_images_to_disk(self, i):\n\"\"\"Saves an image as an *.npy file for faster loading.\"\"\"\nf = self.npy_files[i]\nif not f.exists():\nnp.save(f.as_posix(), cv2.imread(self.im_files[i]))\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.check_cache_ram","title":"<code>check_cache_ram(safety_margin=0.5)</code>","text":"<p>Check image caching requirements vs available memory.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def check_cache_ram(self, safety_margin=0.5):\n\"\"\"Check image caching requirements vs available memory.\"\"\"\nb, gb = 0, 1 &lt;&lt; 30  # bytes of cached images, bytes per gigabytes\nn = min(self.ni, 30)  # extrapolate from 30 random images\nfor _ in range(n):\nim = cv2.imread(random.choice(self.im_files))  # sample image\nratio = self.imgsz / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio\nb += im.nbytes * ratio ** 2\nmem_required = b * self.ni / n * (1 + safety_margin)  # GB required to cache dataset into RAM\nmem = psutil.virtual_memory()\ncache = mem_required &lt; mem.available  # to cache or not to cache, that is the question\nif not cache:\nLOGGER.info(f'{self.prefix}{mem_required / gb:.1f}GB RAM required to cache images '\nf'with {int(safety_margin * 100)}% safety margin but only '\nf'{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, '\nf\"{'caching images \u2705' if cache else 'not caching images \u26a0\ufe0f'}\")\nreturn cache\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.get_image_and_label","title":"<code>get_image_and_label(index)</code>","text":"<p>Get and return label information from the dataset.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def get_image_and_label(self, index):\n\"\"\"Get and return label information from the dataset.\"\"\"\nlabel = deepcopy(self.labels[index])  # requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\nlabel.pop('shape', None)  # shape is for rect, remove it\nlabel['img'], label['ori_shape'], label['resized_shape'] = self.load_image(index)\nlabel['ratio_pad'] = (label['resized_shape'][0] / label['ori_shape'][0],\nlabel['resized_shape'][1] / label['ori_shape'][1])  # for evaluation\nif self.rect:\nlabel['rect_shape'] = self.batch_shapes[self.batch[index]]\nreturn self.update_labels_info(label)\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.get_img_files","title":"<code>get_img_files(img_path)</code>","text":"<p>Read image files.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def get_img_files(self, img_path):\n\"\"\"Read image files.\"\"\"\ntry:\nf = []  # image files\nfor p in img_path if isinstance(img_path, list) else [img_path]:\np = Path(p)  # os-agnostic\nif p.is_dir():  # dir\nf += glob.glob(str(p / '**' / '*.*'), recursive=True)\n# F = list(p.rglob('*.*'))  # pathlib\nelif p.is_file():  # file\nwith open(p) as t:\nt = t.read().strip().splitlines()\nparent = str(p.parent) + os.sep\nf += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n# F += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\nelse:\nraise FileNotFoundError(f'{self.prefix}{p} does not exist')\nim_files = sorted(x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS)\n# self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\nassert im_files, f'{self.prefix}No images found'\nexcept Exception as e:\nraise FileNotFoundError(f'{self.prefix}Error loading data from {img_path}\\n{HELP_URL}') from e\nif self.fraction &lt; 1:\nim_files = im_files[:round(len(im_files) * self.fraction)]\nreturn im_files\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.get_labels","title":"<code>get_labels()</code>","text":"<p>Users can custom their own format here.</p> Make sure your output is a list with each element like below <p>dict(     im_file=im_file,     shape=shape,  # format: (height, width)     cls=cls,     bboxes=bboxes, # xywh     segments=segments,  # xy     keypoints=keypoints, # xy     normalized=True, # or False     bbox_format=\"xyxy\",  # or xywh, ltwh )</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def get_labels(self):\n\"\"\"Users can custom their own format here.\n    Make sure your output is a list with each element like below:\n        dict(\n            im_file=im_file,\n            shape=shape,  # format: (height, width)\n            cls=cls,\n            bboxes=bboxes, # xywh\n            segments=segments,  # xy\n            keypoints=keypoints, # xy\n            normalized=True, # or False\n            bbox_format=\"xyxy\",  # or xywh, ltwh\n        )\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.load_image","title":"<code>load_image(i)</code>","text":"<p>Loads 1 image from dataset index 'i', returns (im, resized hw).</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def load_image(self, i):\n\"\"\"Loads 1 image from dataset index 'i', returns (im, resized hw).\"\"\"\nim, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\nif im is None:  # not cached in RAM\nif fn.exists():  # load npy\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif im is None:\nraise FileNotFoundError(f'Image Not Found {f}')\nh0, w0 = im.shape[:2]  # orig hw\nr = self.imgsz / max(h0, w0)  # ratio\nif r != 1:  # if sizes are not equal\ninterp = cv2.INTER_LINEAR if (self.augment or r &gt; 1) else cv2.INTER_AREA\nim = cv2.resize(im, (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz)),\ninterpolation=interp)\n# Add to buffer if training with augmentations\nif self.augment:\nself.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\nself.buffer.append(i)\nif len(self.buffer) &gt;= self.max_buffer_length:\nj = self.buffer.pop(0)\nself.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\nreturn im, (h0, w0), im.shape[:2]\nreturn self.ims[i], self.im_hw0[i], self.im_hw[i]\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.set_rectangle","title":"<code>set_rectangle()</code>","text":"<p>Sets the shape of bounding boxes for YOLO detections as rectangles.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def set_rectangle(self):\n\"\"\"Sets the shape of bounding boxes for YOLO detections as rectangles.\"\"\"\nbi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index\nnb = bi[-1] + 1  # number of batches\ns = np.array([x.pop('shape') for x in self.labels])  # hw\nar = s[:, 0] / s[:, 1]  # aspect ratio\nirect = ar.argsort()\nself.im_files = [self.im_files[i] for i in irect]\nself.labels = [self.labels[i] for i in irect]\nar = ar[irect]\n# Set training image shapes\nshapes = [[1, 1]] * nb\nfor i in range(nb):\nari = ar[bi == i]\nmini, maxi = ari.min(), ari.max()\nif maxi &lt; 1:\nshapes[i] = [maxi, 1]\nelif mini &gt; 1:\nshapes[i] = [1, 1 / mini]\nself.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride\nself.batch = bi  # batch index of image\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.update_labels","title":"<code>update_labels(include_class)</code>","text":"<p>include_class, filter labels to include only these classes (optional).</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def update_labels(self, include_class: Optional[list]):\n\"\"\"include_class, filter labels to include only these classes (optional).\"\"\"\ninclude_class_array = np.array(include_class).reshape(1, -1)\nfor i in range(len(self.labels)):\nif include_class is not None:\ncls = self.labels[i]['cls']\nbboxes = self.labels[i]['bboxes']\nsegments = self.labels[i]['segments']\nkeypoints = self.labels[i]['keypoints']\nj = (cls == include_class_array).any(1)\nself.labels[i]['cls'] = cls[j]\nself.labels[i]['bboxes'] = bboxes[j]\nif segments:\nself.labels[i]['segments'] = [segments[si] for si, idx in enumerate(j) if idx]\nif keypoints is not None:\nself.labels[i]['keypoints'] = keypoints[j]\nif self.single_cls:\nself.labels[i]['cls'][:, 0] = 0\n</code></pre>"},{"location":"reference/data/base/#ultralytics.data.base.BaseDataset.update_labels_info","title":"<code>update_labels_info(label)</code>","text":"<p>custom your label format here.</p> Source code in <code>ultralytics/data/base.py</code> <pre><code>def update_labels_info(self, label):\n\"\"\"custom your label format here.\"\"\"\nreturn label\n</code></pre>"},{"location":"reference/data/build/","title":"Reference for <code>ultralytics/data/build.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/build.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/data/build/#ultralytics.data.build.InfiniteDataLoader","title":"<code>ultralytics.data.build.InfiniteDataLoader</code>","text":"<p>             Bases: <code>DataLoader</code></p> <p>Dataloader that reuses workers. Uses same syntax as vanilla DataLoader.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>class InfiniteDataLoader(dataloader.DataLoader):\n\"\"\"Dataloader that reuses workers. Uses same syntax as vanilla DataLoader.\"\"\"\ndef __init__(self, *args, **kwargs):\n\"\"\"Dataloader that infinitely recycles workers, inherits from DataLoader.\"\"\"\nsuper().__init__(*args, **kwargs)\nobject.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\nself.iterator = super().__iter__()\ndef __len__(self):\n\"\"\"Returns the length of the batch sampler's sampler.\"\"\"\nreturn len(self.batch_sampler.sampler)\ndef __iter__(self):\n\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\nfor _ in range(len(self)):\nyield next(self.iterator)\ndef reset(self):\n\"\"\"Reset iterator.\n        This is useful when we want to modify settings of dataset while training.\n        \"\"\"\nself.iterator = self._get_iterator()\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.InfiniteDataLoader.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Dataloader that infinitely recycles workers, inherits from DataLoader.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Dataloader that infinitely recycles workers, inherits from DataLoader.\"\"\"\nsuper().__init__(*args, **kwargs)\nobject.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\nself.iterator = super().__iter__()\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.InfiniteDataLoader.__iter__","title":"<code>__iter__()</code>","text":"<p>Creates a sampler that repeats indefinitely.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def __iter__(self):\n\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\nfor _ in range(len(self)):\nyield next(self.iterator)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.InfiniteDataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the batch sampler's sampler.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the length of the batch sampler's sampler.\"\"\"\nreturn len(self.batch_sampler.sampler)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.InfiniteDataLoader.reset","title":"<code>reset()</code>","text":"<p>Reset iterator. This is useful when we want to modify settings of dataset while training.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def reset(self):\n\"\"\"Reset iterator.\n    This is useful when we want to modify settings of dataset while training.\n    \"\"\"\nself.iterator = self._get_iterator()\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build._RepeatSampler","title":"<code>ultralytics.data.build._RepeatSampler</code>","text":"<p>Sampler that repeats forever.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>sampler</code> <p>The sampler to repeat.</p> required Source code in <code>ultralytics/data/build.py</code> <pre><code>class _RepeatSampler:\n\"\"\"\n    Sampler that repeats forever.\n    Args:\n        sampler (Dataset.sampler): The sampler to repeat.\n    \"\"\"\ndef __init__(self, sampler):\n\"\"\"Initializes an object that repeats a given sampler indefinitely.\"\"\"\nself.sampler = sampler\ndef __iter__(self):\n\"\"\"Iterates over the 'sampler' and yields its contents.\"\"\"\nwhile True:\nyield from iter(self.sampler)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build._RepeatSampler.__init__","title":"<code>__init__(sampler)</code>","text":"<p>Initializes an object that repeats a given sampler indefinitely.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def __init__(self, sampler):\n\"\"\"Initializes an object that repeats a given sampler indefinitely.\"\"\"\nself.sampler = sampler\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build._RepeatSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the 'sampler' and yields its contents.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterates over the 'sampler' and yields its contents.\"\"\"\nwhile True:\nyield from iter(self.sampler)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.seed_worker","title":"<code>ultralytics.data.build.seed_worker(worker_id)</code>","text":"<p>Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def seed_worker(worker_id):  # noqa\n\"\"\"Set dataloader worker seed https://pytorch.org/docs/stable/notes/randomness.html#dataloader.\"\"\"\nworker_seed = torch.initial_seed() % 2 ** 32\nnp.random.seed(worker_seed)\nrandom.seed(worker_seed)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.build_yolo_dataset","title":"<code>ultralytics.data.build.build_yolo_dataset(cfg, img_path, batch, data, mode='train', rect=False, stride=32)</code>","text":"<p>Build YOLO Dataset</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def build_yolo_dataset(cfg, img_path, batch, data, mode='train', rect=False, stride=32):\n\"\"\"Build YOLO Dataset\"\"\"\nreturn YOLODataset(\nimg_path=img_path,\nimgsz=cfg.imgsz,\nbatch_size=batch,\naugment=mode == 'train',  # augmentation\nhyp=cfg,  # TODO: probably add a get_hyps_from_cfg function\nrect=cfg.rect or rect,  # rectangular batches\ncache=cfg.cache or None,\nsingle_cls=cfg.single_cls or False,\nstride=int(stride),\npad=0.0 if mode == 'train' else 0.5,\nprefix=colorstr(f'{mode}: '),\nuse_segments=cfg.task == 'segment',\nuse_keypoints=cfg.task == 'pose',\nclasses=cfg.classes,\ndata=data,\nfraction=cfg.fraction if mode == 'train' else 1.0)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.build_dataloader","title":"<code>ultralytics.data.build.build_dataloader(dataset, batch, workers, shuffle=True, rank=-1)</code>","text":"<p>Return an InfiniteDataLoader or DataLoader for training or validation set.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def build_dataloader(dataset, batch, workers, shuffle=True, rank=-1):\n\"\"\"Return an InfiniteDataLoader or DataLoader for training or validation set.\"\"\"\nbatch = min(batch, len(dataset))\nnd = torch.cuda.device_count()  # number of CUDA devices\nnw = min([os.cpu_count() // max(nd, 1), batch if batch &gt; 1 else 0, workers])  # number of workers\nsampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)\ngenerator = torch.Generator()\ngenerator.manual_seed(6148914691236517205 + RANK)\nreturn InfiniteDataLoader(dataset=dataset,\nbatch_size=batch,\nshuffle=shuffle and sampler is None,\nnum_workers=nw,\nsampler=sampler,\npin_memory=PIN_MEMORY,\ncollate_fn=getattr(dataset, 'collate_fn', None),\nworker_init_fn=seed_worker,\ngenerator=generator)\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.check_source","title":"<code>ultralytics.data.build.check_source(source)</code>","text":"<p>Check source type and return corresponding flag values.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def check_source(source):\n\"\"\"Check source type and return corresponding flag values.\"\"\"\nwebcam, screenshot, from_img, in_memory, tensor = False, False, False, False, False\nif isinstance(source, (str, int, Path)):  # int for local usb camera\nsource = str(source)\nis_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\nis_url = source.lower().startswith(('https://', 'http://', 'rtsp://', 'rtmp://'))\nwebcam = source.isnumeric() or source.endswith('.streams') or (is_url and not is_file)\nscreenshot = source.lower() == 'screen'\nif is_url and is_file:\nsource = check_file(source)  # download\nelif isinstance(source, LOADERS):\nin_memory = True\nelif isinstance(source, (list, tuple)):\nsource = autocast_list(source)  # convert all list elements to PIL or np arrays\nfrom_img = True\nelif isinstance(source, (Image.Image, np.ndarray)):\nfrom_img = True\nelif isinstance(source, torch.Tensor):\ntensor = True\nelse:\nraise TypeError('Unsupported image type. For supported types see https://docs.ultralytics.com/modes/predict')\nreturn source, webcam, screenshot, from_img, in_memory, tensor\n</code></pre>"},{"location":"reference/data/build/#ultralytics.data.build.load_inference_source","title":"<code>ultralytics.data.build.load_inference_source(source=None, imgsz=640, vid_stride=1)</code>","text":"<p>Loads an inference source for object detection and applies necessary transformations.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>(str, Path, Tensor, Image, ndarray)</code> <p>The input source for inference.</p> <code>None</code> <code>imgsz</code> <code>int</code> <p>The size of the image for inference. Default is 640.</p> <code>640</code> <code>vid_stride</code> <code>int</code> <p>The frame interval for video sources. Default is 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dataset</code> <code>Dataset</code> <p>A dataset object for the specified input source.</p> Source code in <code>ultralytics/data/build.py</code> <pre><code>def load_inference_source(source=None, imgsz=640, vid_stride=1):\n\"\"\"\n    Loads an inference source for object detection and applies necessary transformations.\n    Args:\n        source (str, Path, Tensor, PIL.Image, np.ndarray): The input source for inference.\n        imgsz (int, optional): The size of the image for inference. Default is 640.\n        vid_stride (int, optional): The frame interval for video sources. Default is 1.\n    Returns:\n        dataset (Dataset): A dataset object for the specified input source.\n    \"\"\"\nsource, webcam, screenshot, from_img, in_memory, tensor = check_source(source)\nsource_type = source.source_type if in_memory else SourceTypes(webcam, screenshot, from_img, tensor)\n# Dataloader\nif tensor:\ndataset = LoadTensor(source)\nelif in_memory:\ndataset = source\nelif webcam:\ndataset = LoadStreams(source, imgsz=imgsz, vid_stride=vid_stride)\nelif screenshot:\ndataset = LoadScreenshots(source, imgsz=imgsz)\nelif from_img:\ndataset = LoadPilAndNumpy(source, imgsz=imgsz)\nelse:\ndataset = LoadImages(source, imgsz=imgsz, vid_stride=vid_stride)\n# Attach source types to the dataset\nsetattr(dataset, 'source_type', source_type)\nreturn dataset\n</code></pre>"},{"location":"reference/data/converter/","title":"Reference for <code>ultralytics/data/converter.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/converter.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/data/converter/#ultralytics.data.converter.coco91_to_coco80_class","title":"<code>ultralytics.data.converter.coco91_to_coco80_class()</code>","text":"<p>Converts 91-index COCO class IDs to 80-index COCO class IDs.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of 91 class IDs where the index represents the 80-index class ID and the value is the corresponding 91-index class ID.</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def coco91_to_coco80_class():\n\"\"\"Converts 91-index COCO class IDs to 80-index COCO class IDs.\n    Returns:\n        (list): A list of 91 class IDs where the index represents the 80-index class ID and the value is the\n            corresponding 91-index class ID.\n    \"\"\"\nreturn [\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None, 24, 25, None,\nNone, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, None, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n51, 52, 53, 54, 55, 56, 57, 58, 59, None, 60, None, None, 61, None, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\nNone, 73, 74, 75, 76, 77, 78, 79, None]\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.coco80_to_coco91_class","title":"<code>ultralytics.data.converter.coco80_to_coco91_class()</code>","text":"<pre><code>Converts 80-index (val2014) to 91-index (paper).\nFor details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.\n\nExample:\n    ```python\n    import numpy as np\n\n    a = np.loadtxt('data/coco.names', dtype='str', delimiter='\n</code></pre> <p>')         b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter=' ')         x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco         x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet         ```</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def coco80_to_coco91_class():  #\n\"\"\"\n    Converts 80-index (val2014) to 91-index (paper).\n    For details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.\n    Example:\n        ```python\n        import numpy as np\n        a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n        b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n        x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n        x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n        ```\n    \"\"\"\nreturn [\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.convert_coco","title":"<code>ultralytics.data.converter.convert_coco(labels_dir='../coco/annotations/', use_segments=False, use_keypoints=False, cls91to80=True)</code>","text":"<p>Converts COCO dataset annotations to a format suitable for training YOLOv5 models.</p> <p>Parameters:</p> Name Type Description Default <code>labels_dir</code> <code>str</code> <p>Path to directory containing COCO dataset annotation files.</p> <code>'../coco/annotations/'</code> <code>use_segments</code> <code>bool</code> <p>Whether to include segmentation masks in the output.</p> <code>False</code> <code>use_keypoints</code> <code>bool</code> <p>Whether to include keypoint annotations in the output.</p> <code>False</code> <code>cls91to80</code> <code>bool</code> <p>Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs.</p> <code>True</code> Example <pre><code>from ultralytics.data.converter import convert_coco\nconvert_coco('../datasets/coco/annotations/', use_segments=True, use_keypoints=False, cls91to80=True)\n</code></pre> Output <p>Generates output files in the specified output directory.</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def convert_coco(labels_dir='../coco/annotations/', use_segments=False, use_keypoints=False, cls91to80=True):\n\"\"\"Converts COCO dataset annotations to a format suitable for training YOLOv5 models.\n    Args:\n        labels_dir (str, optional): Path to directory containing COCO dataset annotation files.\n        use_segments (bool, optional): Whether to include segmentation masks in the output.\n        use_keypoints (bool, optional): Whether to include keypoint annotations in the output.\n        cls91to80 (bool, optional): Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs.\n    Example:\n        ```python\n        from ultralytics.data.converter import convert_coco\n        convert_coco('../datasets/coco/annotations/', use_segments=True, use_keypoints=False, cls91to80=True)\n        ```\n    Output:\n        Generates output files in the specified output directory.\n    \"\"\"\n# Create dataset directory\nsave_dir = Path('yolo_labels')\nif save_dir.exists():\nshutil.rmtree(save_dir)  # delete dir\nfor p in save_dir / 'labels', save_dir / 'images':\np.mkdir(parents=True, exist_ok=True)  # make dir\n# Convert classes\ncoco80 = coco91_to_coco80_class()\n# Import json\nfor json_file in sorted(Path(labels_dir).resolve().glob('*.json')):\nfn = Path(save_dir) / 'labels' / json_file.stem.replace('instances_', '')  # folder name\nfn.mkdir(parents=True, exist_ok=True)\nwith open(json_file) as f:\ndata = json.load(f)\n# Create image dict\nimages = {f'{x[\"id\"]:d}': x for x in data['images']}\n# Create image-annotations dict\nimgToAnns = defaultdict(list)\nfor ann in data['annotations']:\nimgToAnns[ann['image_id']].append(ann)\n# Write labels file\nfor img_id, anns in tqdm(imgToAnns.items(), desc=f'Annotations {json_file}'):\nimg = images[f'{img_id:d}']\nh, w, f = img['height'], img['width'], img['file_name']\nbboxes = []\nsegments = []\nkeypoints = []\nfor ann in anns:\nif ann['iscrowd']:\ncontinue\n# The COCO box format is [top left x, top left y, width, height]\nbox = np.array(ann['bbox'], dtype=np.float64)\nbox[:2] += box[2:] / 2  # xy top-left corner to center\nbox[[0, 2]] /= w  # normalize x\nbox[[1, 3]] /= h  # normalize y\nif box[2] &lt;= 0 or box[3] &lt;= 0:  # if w &lt;= 0 and h &lt;= 0\ncontinue\ncls = coco80[ann['category_id'] - 1] if cls91to80 else ann['category_id'] - 1  # class\nbox = [cls] + box.tolist()\nif box not in bboxes:\nbboxes.append(box)\nif use_segments and ann.get('segmentation') is not None:\nif len(ann['segmentation']) == 0:\nsegments.append([])\ncontinue\nif isinstance(ann['segmentation'], dict):\nann['segmentation'] = rle2polygon(ann['segmentation'])\nif len(ann['segmentation']) &gt; 1:\ns = merge_multi_segment(ann['segmentation'])\ns = (np.concatenate(s, axis=0) / np.array([w, h])).reshape(-1).tolist()\nelse:\ns = [j for i in ann['segmentation'] for j in i]  # all segments concatenated\ns = (np.array(s).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()\ns = [cls] + s\nif s not in segments:\nsegments.append(s)\nif use_keypoints and ann.get('keypoints') is not None:\nk = (np.array(ann['keypoints']).reshape(-1, 3) / np.array([w, h, 1])).reshape(-1).tolist()\nk = box + k\nkeypoints.append(k)\n# Write\nwith open((fn / f).with_suffix('.txt'), 'a') as file:\nfor i in range(len(bboxes)):\nif use_keypoints:\nline = *(keypoints[i]),  # cls, box, keypoints\nelse:\nline = *(segments[i]\nif use_segments and len(segments[i]) &gt; 0 else bboxes[i]),  # cls, box or segments\nfile.write(('%g ' * len(line)).rstrip() % line + '\\n')\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.convert_dota_to_yolo_obb","title":"<code>ultralytics.data.converter.convert_dota_to_yolo_obb(dota_root_path)</code>","text":"<p>Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format.</p> <p>The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.</p> <p>Parameters:</p> Name Type Description Default <code>dota_root_path</code> <code>str</code> <p>The root directory path of the DOTA dataset.</p> required Example <pre><code>from ultralytics.data.converter import convert_dota_to_yolo_obb\nconvert_dota_to_yolo_obb('path/to/DOTA')\n</code></pre> Notes <p>The directory structure assumed for the DOTA dataset:     - DOTA         - images             - train             - val         - labels             - train_original             - val_original</p> <p>After the function execution, the new labels will be saved in:     - DOTA         - labels             - train             - val</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def convert_dota_to_yolo_obb(dota_root_path: str):\n\"\"\"\n    Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format.\n    The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the\n    associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.\n    Args:\n        dota_root_path (str): The root directory path of the DOTA dataset.\n    Example:\n        ```python\n        from ultralytics.data.converter import convert_dota_to_yolo_obb\n        convert_dota_to_yolo_obb('path/to/DOTA')\n        ```\n    Notes:\n        The directory structure assumed for the DOTA dataset:\n            - DOTA\n                - images\n                    - train\n                    - val\n                - labels\n                    - train_original\n                    - val_original\n        After the function execution, the new labels will be saved in:\n            - DOTA\n                - labels\n                    - train\n                    - val\n    \"\"\"\ndota_root_path = Path(dota_root_path)\n# Class names to indices mapping\nclass_mapping = {\n'plane': 0,\n'ship': 1,\n'storage-tank': 2,\n'baseball-diamond': 3,\n'tennis-court': 4,\n'basketball-court': 5,\n'ground-track-field': 6,\n'harbor': 7,\n'bridge': 8,\n'large-vehicle': 9,\n'small-vehicle': 10,\n'helicopter': 11,\n'roundabout': 12,\n'soccer ball-field': 13,\n'swimming-pool': 14,\n'container-crane': 15,\n'airport': 16,\n'helipad': 17}\ndef convert_label(image_name, image_width, image_height, orig_label_dir, save_dir):\norig_label_path = orig_label_dir / f'{image_name}.txt'\nsave_path = save_dir / f'{image_name}.txt'\nwith orig_label_path.open('r') as f, save_path.open('w') as g:\nlines = f.readlines()\nfor line in lines:\nparts = line.strip().split()\nif len(parts) &lt; 9:\ncontinue\nclass_name = parts[8]\nclass_idx = class_mapping[class_name]\ncoords = [float(p) for p in parts[:8]]\nnormalized_coords = [\ncoords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)]\nformatted_coords = ['{:.6g}'.format(coord) for coord in normalized_coords]\ng.write(f\"{class_idx} {' '.join(formatted_coords)}\\n\")\nfor phase in ['train', 'val']:\nimage_dir = dota_root_path / 'images' / phase\norig_label_dir = dota_root_path / 'labels' / f'{phase}_original'\nsave_dir = dota_root_path / 'labels' / phase\nsave_dir.mkdir(parents=True, exist_ok=True)\nimage_paths = list(image_dir.iterdir())\nfor image_path in tqdm(image_paths, desc=f'Processing {phase} images'):\nif image_path.suffix != '.png':\ncontinue\nimage_name_without_ext = image_path.stem\nimg = cv2.imread(str(image_path))\nh, w = img.shape[:2]\nconvert_label(image_name_without_ext, w, h, orig_label_dir, save_dir)\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.rle2polygon","title":"<code>ultralytics.data.converter.rle2polygon(segmentation)</code>","text":"<p>Convert Run-Length Encoding (RLE) mask to polygon coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation</code> <code>(dict, list)</code> <p>RLE mask representation of the object segmentation.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists representing the polygon coordinates for each contour.</p> Note <p>Requires the 'pycocotools' package to be installed.</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def rle2polygon(segmentation):\n\"\"\"\n    Convert Run-Length Encoding (RLE) mask to polygon coordinates.\n    Args:\n        segmentation (dict, list): RLE mask representation of the object segmentation.\n    Returns:\n        (list): A list of lists representing the polygon coordinates for each contour.\n    Note:\n        Requires the 'pycocotools' package to be installed.\n    \"\"\"\ncheck_requirements('pycocotools')\nfrom pycocotools import mask\nm = mask.decode(segmentation)\nm[m &gt; 0] = 255\ncontours, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\npolygons = []\nfor contour in contours:\nepsilon = 0.001 * cv2.arcLength(contour, True)\ncontour_approx = cv2.approxPolyDP(contour, epsilon, True)\npolygon = contour_approx.flatten().tolist()\npolygons.append(polygon)\nreturn polygons\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.min_index","title":"<code>ultralytics.data.converter.min_index(arr1, arr2)</code>","text":"<p>Find a pair of indexes with the shortest distance between two arrays of 2D points.</p> <p>Parameters:</p> Name Type Description Default <code>arr1</code> <code>array</code> <p>A NumPy array of shape (N, 2) representing N 2D points.</p> required <code>arr2</code> <code>array</code> <p>A NumPy array of shape (M, 2) representing M 2D points.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the indexes of the points with the shortest distance in arr1 and arr2 respectively.</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def min_index(arr1, arr2):\n\"\"\"\n    Find a pair of indexes with the shortest distance between two arrays of 2D points.\n    Args:\n        arr1 (np.array): A NumPy array of shape (N, 2) representing N 2D points.\n        arr2 (np.array): A NumPy array of shape (M, 2) representing M 2D points.\n    Returns:\n        (tuple): A tuple containing the indexes of the points with the shortest distance in arr1 and arr2 respectively.\n    \"\"\"\ndis = ((arr1[:, None, :] - arr2[None, :, :]) ** 2).sum(-1)\nreturn np.unravel_index(np.argmin(dis, axis=None), dis.shape)\n</code></pre>"},{"location":"reference/data/converter/#ultralytics.data.converter.merge_multi_segment","title":"<code>ultralytics.data.converter.merge_multi_segment(segments)</code>","text":"<p>Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment. This function connects these coordinates with a thin line to merge all segments into one.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[List]</code> <p>Original segmentations in COCO's JSON file.                    Each element is a list of coordinates, like [segmentation1, segmentation2,...].</p> required <p>Returns:</p> Name Type Description <code>s</code> <code>List[numpy.ndarray]</code> <p>A list of connected segments represented as NumPy arrays.</p> Source code in <code>ultralytics/data/converter.py</code> <pre><code>def merge_multi_segment(segments):\n\"\"\"\n    Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment.\n    This function connects these coordinates with a thin line to merge all segments into one.\n    Args:\n        segments (List[List]): Original segmentations in COCO's JSON file.\n                               Each element is a list of coordinates, like [segmentation1, segmentation2,...].\n    Returns:\n        s (List[np.ndarray]): A list of connected segments represented as NumPy arrays.\n    \"\"\"\ns = []\nsegments = [np.array(i).reshape(-1, 2) for i in segments]\nidx_list = [[] for _ in range(len(segments))]\n# record the indexes with min distance between each segment\nfor i in range(1, len(segments)):\nidx1, idx2 = min_index(segments[i - 1], segments[i])\nidx_list[i - 1].append(idx1)\nidx_list[i].append(idx2)\n# use two round to connect all the segments\nfor k in range(2):\n# forward connection\nif k == 0:\nfor i, idx in enumerate(idx_list):\n# middle segments have two indexes\n# reverse the index of middle segments\nif len(idx) == 2 and idx[0] &gt; idx[1]:\nidx = idx[::-1]\nsegments[i] = segments[i][::-1, :]\nsegments[i] = np.roll(segments[i], -idx[0], axis=0)\nsegments[i] = np.concatenate([segments[i], segments[i][:1]])\n# deal with the first segment and the last one\nif i in [0, len(idx_list) - 1]:\ns.append(segments[i])\nelse:\nidx = [0, idx[1] - idx[0]]\ns.append(segments[i][idx[0]:idx[1] + 1])\nelse:\nfor i in range(len(idx_list) - 1, -1, -1):\nif i not in [0, len(idx_list) - 1]:\nidx = idx_list[i]\nnidx = abs(idx[1] - idx[0])\ns.append(segments[i][nidx:])\nreturn s\n</code></pre>"},{"location":"reference/data/dataset/","title":"Reference for <code>ultralytics/data/dataset.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/dataset.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset","title":"<code>ultralytics.data.dataset.YOLODataset</code>","text":"<p>             Bases: <code>BaseDataset</code></p> <p>Dataset class for loading object detection and/or segmentation labels in YOLO format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dataset YAML dictionary. Defaults to None.</p> <code>None</code> <code>use_segments</code> <code>bool</code> <p>If True, segmentation masks are used as labels. Defaults to False.</p> <code>False</code> <code>use_keypoints</code> <code>bool</code> <p>If True, keypoints are used as labels. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A PyTorch dataset object that can be used for training an object detection model.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>class YOLODataset(BaseDataset):\n\"\"\"\n    Dataset class for loading object detection and/or segmentation labels in YOLO format.\n    Args:\n        data (dict, optional): A dataset YAML dictionary. Defaults to None.\n        use_segments (bool, optional): If True, segmentation masks are used as labels. Defaults to False.\n        use_keypoints (bool, optional): If True, keypoints are used as labels. Defaults to False.\n    Returns:\n        (torch.utils.data.Dataset): A PyTorch dataset object that can be used for training an object detection model.\n    \"\"\"\ndef __init__(self, *args, data=None, use_segments=False, use_keypoints=False, **kwargs):\nself.use_segments = use_segments\nself.use_keypoints = use_keypoints\nself.data = data\nassert not (self.use_segments and self.use_keypoints), 'Can not use both segments and keypoints.'\nsuper().__init__(*args, **kwargs)\ndef cache_labels(self, path=Path('./labels.cache')):\n\"\"\"Cache dataset labels, check images and read shapes.\n        Args:\n            path (Path): path where to save the cache file (default: Path('./labels.cache')).\n        Returns:\n            (dict): labels.\n        \"\"\"\nx = {'labels': []}\nnm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\ndesc = f'{self.prefix}Scanning {path.parent / path.stem}...'\ntotal = len(self.im_files)\nnkpt, ndim = self.data.get('kpt_shape', (0, 0))\nif self.use_keypoints and (nkpt &lt;= 0 or ndim not in (2, 3)):\nraise ValueError(\"'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of \"\n\"keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'\")\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(func=verify_image_label,\niterable=zip(self.im_files, self.label_files, repeat(self.prefix),\nrepeat(self.use_keypoints), repeat(len(self.data['names'])), repeat(nkpt),\nrepeat(ndim)))\npbar = tqdm(results, desc=desc, total=total, bar_format=TQDM_BAR_FORMAT)\nfor im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:\nnm += nm_f\nnf += nf_f\nne += ne_f\nnc += nc_f\nif im_file:\nx['labels'].append(\ndict(\nim_file=im_file,\nshape=shape,\ncls=lb[:, 0:1],  # n, 1\nbboxes=lb[:, 1:],  # n, 4\nsegments=segments,\nkeypoints=keypoint,\nnormalized=True,\nbbox_format='xywh'))\nif msg:\nmsgs.append(msg)\npbar.desc = f'{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt'\npbar.close()\nif msgs:\nLOGGER.info('\\n'.join(msgs))\nif nf == 0:\nLOGGER.warning(f'{self.prefix}WARNING \u26a0\ufe0f No labels found in {path}. {HELP_URL}')\nx['hash'] = get_hash(self.label_files + self.im_files)\nx['results'] = nf, nm, ne, nc, len(self.im_files)\nx['msgs'] = msgs  # warnings\nsave_dataset_cache_file(self.prefix, path, x)\nreturn x\ndef get_labels(self):\n\"\"\"Returns dictionary of labels for YOLO training.\"\"\"\nself.label_files = img2label_paths(self.im_files)\ncache_path = Path(self.label_files[0]).parent.with_suffix('.cache')\ntry:\ncache, exists = load_dataset_cache_file(cache_path), True  # attempt to load a *.cache file\nassert cache['version'] == DATASET_CACHE_VERSION  # matches current version\nassert cache['hash'] == get_hash(self.label_files + self.im_files)  # identical hash\nexcept (FileNotFoundError, AssertionError, AttributeError):\ncache, exists = self.cache_labels(cache_path), False  # run cache ops\n# Display cache\nnf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupt, total\nif exists and LOCAL_RANK in (-1, 0):\nd = f'Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt'\ntqdm(None, desc=self.prefix + d, total=n, initial=n, bar_format=TQDM_BAR_FORMAT)  # display results\nif cache['msgs']:\nLOGGER.info('\\n'.join(cache['msgs']))  # display warnings\nif nf == 0:  # number of labels found\nraise FileNotFoundError(f'{self.prefix}No labels found in {cache_path}, can not start training. {HELP_URL}')\n# Read cache\n[cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items\nlabels = cache['labels']\nassert len(labels), f'No valid labels found, please check your dataset. {HELP_URL}'\nself.im_files = [lb['im_file'] for lb in labels]  # update im_files\n# Check if the dataset is all boxes or all segments\nlengths = ((len(lb['cls']), len(lb['bboxes']), len(lb['segments'])) for lb in labels)\nlen_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\nif len_segments and len_boxes != len_segments:\nLOGGER.warning(\nf'WARNING \u26a0\ufe0f Box and segment counts should be equal, but got len(segments) = {len_segments}, '\nf'len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. '\n'To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.')\nfor lb in labels:\nlb['segments'] = []\nif len_cls == 0:\nraise ValueError(f'All labels empty in {cache_path}, can not start training without labels. {HELP_URL}')\nreturn labels\n# TODO: use hyp config to set all these augmentations\ndef build_transforms(self, hyp=None):\n\"\"\"Builds and appends transforms to the list.\"\"\"\nif self.augment:\nhyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\nhyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\ntransforms = v8_transforms(self, self.imgsz, hyp)\nelse:\ntransforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])\ntransforms.append(\nFormat(bbox_format='xywh',\nnormalize=True,\nreturn_mask=self.use_segments,\nreturn_keypoint=self.use_keypoints,\nbatch_idx=True,\nmask_ratio=hyp.mask_ratio,\nmask_overlap=hyp.overlap_mask))\nreturn transforms\ndef close_mosaic(self, hyp):\n\"\"\"Sets mosaic, copy_paste and mixup options to 0.0 and builds transformations.\"\"\"\nhyp.mosaic = 0.0  # set mosaic ratio=0.0\nhyp.copy_paste = 0.0  # keep the same behavior as previous v8 close-mosaic\nhyp.mixup = 0.0  # keep the same behavior as previous v8 close-mosaic\nself.transforms = self.build_transforms(hyp)\ndef update_labels_info(self, label):\n\"\"\"custom your label format here.\"\"\"\n# NOTE: cls is not with bboxes now, classification and semantic segmentation need an independent cls label\n# we can make it also support classification and semantic segmentation by add or remove some dict keys there.\nbboxes = label.pop('bboxes')\nsegments = label.pop('segments')\nkeypoints = label.pop('keypoints', None)\nbbox_format = label.pop('bbox_format')\nnormalized = label.pop('normalized')\nlabel['instances'] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)\nreturn label\n@staticmethod\ndef collate_fn(batch):\n\"\"\"Collates data samples into batches.\"\"\"\nnew_batch = {}\nkeys = batch[0].keys()\nvalues = list(zip(*[list(b.values()) for b in batch]))\nfor i, k in enumerate(keys):\nvalue = values[i]\nif k == 'img':\nvalue = torch.stack(value, 0)\nif k in ['masks', 'keypoints', 'bboxes', 'cls']:\nvalue = torch.cat(value, 0)\nnew_batch[k] = value\nnew_batch['batch_idx'] = list(new_batch['batch_idx'])\nfor i in range(len(new_batch['batch_idx'])):\nnew_batch['batch_idx'][i] += i  # add target image index for build_targets()\nnew_batch['batch_idx'] = torch.cat(new_batch['batch_idx'], 0)\nreturn new_batch\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.build_transforms","title":"<code>build_transforms(hyp=None)</code>","text":"<p>Builds and appends transforms to the list.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def build_transforms(self, hyp=None):\n\"\"\"Builds and appends transforms to the list.\"\"\"\nif self.augment:\nhyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\nhyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\ntransforms = v8_transforms(self, self.imgsz, hyp)\nelse:\ntransforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])\ntransforms.append(\nFormat(bbox_format='xywh',\nnormalize=True,\nreturn_mask=self.use_segments,\nreturn_keypoint=self.use_keypoints,\nbatch_idx=True,\nmask_ratio=hyp.mask_ratio,\nmask_overlap=hyp.overlap_mask))\nreturn transforms\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.cache_labels","title":"<code>cache_labels(path=Path('./labels.cache'))</code>","text":"<p>Cache dataset labels, check images and read shapes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>path where to save the cache file (default: Path('./labels.cache')).</p> <code>Path('./labels.cache')</code> <p>Returns:</p> Type Description <code>dict</code> <p>labels.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def cache_labels(self, path=Path('./labels.cache')):\n\"\"\"Cache dataset labels, check images and read shapes.\n    Args:\n        path (Path): path where to save the cache file (default: Path('./labels.cache')).\n    Returns:\n        (dict): labels.\n    \"\"\"\nx = {'labels': []}\nnm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\ndesc = f'{self.prefix}Scanning {path.parent / path.stem}...'\ntotal = len(self.im_files)\nnkpt, ndim = self.data.get('kpt_shape', (0, 0))\nif self.use_keypoints and (nkpt &lt;= 0 or ndim not in (2, 3)):\nraise ValueError(\"'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of \"\n\"keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'\")\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(func=verify_image_label,\niterable=zip(self.im_files, self.label_files, repeat(self.prefix),\nrepeat(self.use_keypoints), repeat(len(self.data['names'])), repeat(nkpt),\nrepeat(ndim)))\npbar = tqdm(results, desc=desc, total=total, bar_format=TQDM_BAR_FORMAT)\nfor im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:\nnm += nm_f\nnf += nf_f\nne += ne_f\nnc += nc_f\nif im_file:\nx['labels'].append(\ndict(\nim_file=im_file,\nshape=shape,\ncls=lb[:, 0:1],  # n, 1\nbboxes=lb[:, 1:],  # n, 4\nsegments=segments,\nkeypoints=keypoint,\nnormalized=True,\nbbox_format='xywh'))\nif msg:\nmsgs.append(msg)\npbar.desc = f'{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt'\npbar.close()\nif msgs:\nLOGGER.info('\\n'.join(msgs))\nif nf == 0:\nLOGGER.warning(f'{self.prefix}WARNING \u26a0\ufe0f No labels found in {path}. {HELP_URL}')\nx['hash'] = get_hash(self.label_files + self.im_files)\nx['results'] = nf, nm, ne, nc, len(self.im_files)\nx['msgs'] = msgs  # warnings\nsave_dataset_cache_file(self.prefix, path, x)\nreturn x\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.close_mosaic","title":"<code>close_mosaic(hyp)</code>","text":"<p>Sets mosaic, copy_paste and mixup options to 0.0 and builds transformations.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def close_mosaic(self, hyp):\n\"\"\"Sets mosaic, copy_paste and mixup options to 0.0 and builds transformations.\"\"\"\nhyp.mosaic = 0.0  # set mosaic ratio=0.0\nhyp.copy_paste = 0.0  # keep the same behavior as previous v8 close-mosaic\nhyp.mixup = 0.0  # keep the same behavior as previous v8 close-mosaic\nself.transforms = self.build_transforms(hyp)\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.collate_fn","title":"<code>collate_fn(batch)</code>  <code>staticmethod</code>","text":"<p>Collates data samples into batches.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>@staticmethod\ndef collate_fn(batch):\n\"\"\"Collates data samples into batches.\"\"\"\nnew_batch = {}\nkeys = batch[0].keys()\nvalues = list(zip(*[list(b.values()) for b in batch]))\nfor i, k in enumerate(keys):\nvalue = values[i]\nif k == 'img':\nvalue = torch.stack(value, 0)\nif k in ['masks', 'keypoints', 'bboxes', 'cls']:\nvalue = torch.cat(value, 0)\nnew_batch[k] = value\nnew_batch['batch_idx'] = list(new_batch['batch_idx'])\nfor i in range(len(new_batch['batch_idx'])):\nnew_batch['batch_idx'][i] += i  # add target image index for build_targets()\nnew_batch['batch_idx'] = torch.cat(new_batch['batch_idx'], 0)\nreturn new_batch\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.get_labels","title":"<code>get_labels()</code>","text":"<p>Returns dictionary of labels for YOLO training.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def get_labels(self):\n\"\"\"Returns dictionary of labels for YOLO training.\"\"\"\nself.label_files = img2label_paths(self.im_files)\ncache_path = Path(self.label_files[0]).parent.with_suffix('.cache')\ntry:\ncache, exists = load_dataset_cache_file(cache_path), True  # attempt to load a *.cache file\nassert cache['version'] == DATASET_CACHE_VERSION  # matches current version\nassert cache['hash'] == get_hash(self.label_files + self.im_files)  # identical hash\nexcept (FileNotFoundError, AssertionError, AttributeError):\ncache, exists = self.cache_labels(cache_path), False  # run cache ops\n# Display cache\nnf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupt, total\nif exists and LOCAL_RANK in (-1, 0):\nd = f'Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt'\ntqdm(None, desc=self.prefix + d, total=n, initial=n, bar_format=TQDM_BAR_FORMAT)  # display results\nif cache['msgs']:\nLOGGER.info('\\n'.join(cache['msgs']))  # display warnings\nif nf == 0:  # number of labels found\nraise FileNotFoundError(f'{self.prefix}No labels found in {cache_path}, can not start training. {HELP_URL}')\n# Read cache\n[cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items\nlabels = cache['labels']\nassert len(labels), f'No valid labels found, please check your dataset. {HELP_URL}'\nself.im_files = [lb['im_file'] for lb in labels]  # update im_files\n# Check if the dataset is all boxes or all segments\nlengths = ((len(lb['cls']), len(lb['bboxes']), len(lb['segments'])) for lb in labels)\nlen_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\nif len_segments and len_boxes != len_segments:\nLOGGER.warning(\nf'WARNING \u26a0\ufe0f Box and segment counts should be equal, but got len(segments) = {len_segments}, '\nf'len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. '\n'To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.')\nfor lb in labels:\nlb['segments'] = []\nif len_cls == 0:\nraise ValueError(f'All labels empty in {cache_path}, can not start training without labels. {HELP_URL}')\nreturn labels\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.YOLODataset.update_labels_info","title":"<code>update_labels_info(label)</code>","text":"<p>custom your label format here.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def update_labels_info(self, label):\n\"\"\"custom your label format here.\"\"\"\n# NOTE: cls is not with bboxes now, classification and semantic segmentation need an independent cls label\n# we can make it also support classification and semantic segmentation by add or remove some dict keys there.\nbboxes = label.pop('bboxes')\nsegments = label.pop('segments')\nkeypoints = label.pop('keypoints', None)\nbbox_format = label.pop('bbox_format')\nnormalized = label.pop('normalized')\nlabel['instances'] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)\nreturn label\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.ClassificationDataset","title":"<code>ultralytics.data.dataset.ClassificationDataset</code>","text":"<p>             Bases: <code>ImageFolder</code></p> <p>YOLO Classification Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Dataset path.</p> required <p>Attributes:</p> Name Type Description <code>cache_ram</code> <code>bool</code> <p>True if images should be cached in RAM, False otherwise.</p> <code>cache_disk</code> <code>bool</code> <p>True if images should be cached on disk, False otherwise.</p> <code>samples</code> <code>list</code> <p>List of samples containing file, index, npy, and im.</p> <code>torch_transforms</code> <code>callable</code> <p>torchvision transforms applied to the dataset.</p> <code>album_transforms</code> <code>callable</code> <p>Albumentations transforms applied to the dataset if augment is True.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>class ClassificationDataset(torchvision.datasets.ImageFolder):\n\"\"\"\n    YOLO Classification Dataset.\n    Args:\n        root (str): Dataset path.\n    Attributes:\n        cache_ram (bool): True if images should be cached in RAM, False otherwise.\n        cache_disk (bool): True if images should be cached on disk, False otherwise.\n        samples (list): List of samples containing file, index, npy, and im.\n        torch_transforms (callable): torchvision transforms applied to the dataset.\n        album_transforms (callable, optional): Albumentations transforms applied to the dataset if augment is True.\n    \"\"\"\ndef __init__(self, root, args, augment=False, cache=False, prefix=''):\n\"\"\"\n        Initialize YOLO object with root, image size, augmentations, and cache settings.\n        Args:\n            root (str): Dataset path.\n            args (Namespace): Argument parser containing dataset related settings.\n            augment (bool, optional): True if dataset should be augmented, False otherwise. Defaults to False.\n            cache (bool | str | optional): Cache setting, can be True, False, 'ram' or 'disk'. Defaults to False.\n        \"\"\"\nsuper().__init__(root=root)\nif augment and args.fraction &lt; 1.0:  # reduce training fraction\nself.samples = self.samples[:round(len(self.samples) * args.fraction)]\nself.prefix = colorstr(f'{prefix}: ') if prefix else ''\nself.cache_ram = cache is True or cache == 'ram'\nself.cache_disk = cache == 'disk'\nself.samples = self.verify_images()  # filter out bad images\nself.samples = [list(x) + [Path(x[0]).with_suffix('.npy'), None] for x in self.samples]  # file, index, npy, im\nself.torch_transforms = classify_transforms(args.imgsz)\nself.album_transforms = classify_albumentations(\naugment=augment,\nsize=args.imgsz,\nscale=(1.0 - args.scale, 1.0),  # (0.08, 1.0)\nhflip=args.fliplr,\nvflip=args.flipud,\nhsv_h=args.hsv_h,  # HSV-Hue augmentation (fraction)\nhsv_s=args.hsv_s,  # HSV-Saturation augmentation (fraction)\nhsv_v=args.hsv_v,  # HSV-Value augmentation (fraction)\nmean=(0.0, 0.0, 0.0),  # IMAGENET_MEAN\nstd=(1.0, 1.0, 1.0),  # IMAGENET_STD\nauto_aug=False) if augment else None\ndef __getitem__(self, i):\n\"\"\"Returns subset of data and targets corresponding to given indices.\"\"\"\nf, j, fn, im = self.samples[i]  # filename, index, filename.with_suffix('.npy'), image\nif self.cache_ram and im is None:\nim = self.samples[i][3] = cv2.imread(f)\nelif self.cache_disk:\nif not fn.exists():  # load npy\nnp.save(fn.as_posix(), cv2.imread(f))\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif self.album_transforms:\nsample = self.album_transforms(image=cv2.cvtColor(im, cv2.COLOR_BGR2RGB))['image']\nelse:\nsample = self.torch_transforms(im)\nreturn {'img': sample, 'cls': j}\ndef __len__(self) -&gt; int:\nreturn len(self.samples)\ndef verify_images(self):\n\"\"\"Verify all images in dataset.\"\"\"\ndesc = f'{self.prefix}Scanning {self.root}...'\npath = Path(self.root).with_suffix('.cache')  # *.cache file path\nwith contextlib.suppress(FileNotFoundError, AssertionError, AttributeError):\ncache = load_dataset_cache_file(path)  # attempt to load a *.cache file\nassert cache['version'] == DATASET_CACHE_VERSION  # matches current version\nassert cache['hash'] == get_hash([x[0] for x in self.samples])  # identical hash\nnf, nc, n, samples = cache.pop('results')  # found, missing, empty, corrupt, total\nif LOCAL_RANK in (-1, 0):\nd = f'{desc} {nf} images, {nc} corrupt'\ntqdm(None, desc=d, total=n, initial=n, bar_format=TQDM_BAR_FORMAT)\nif cache['msgs']:\nLOGGER.info('\\n'.join(cache['msgs']))  # display warnings\nreturn samples\n# Run scan if *.cache retrieval failed\nnf, nc, msgs, samples, x = 0, 0, [], [], {}\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(func=verify_image, iterable=zip(self.samples, repeat(self.prefix)))\npbar = tqdm(results, desc=desc, total=len(self.samples), bar_format=TQDM_BAR_FORMAT)\nfor sample, nf_f, nc_f, msg in pbar:\nif nf_f:\nsamples.append(sample)\nif msg:\nmsgs.append(msg)\nnf += nf_f\nnc += nc_f\npbar.desc = f'{desc} {nf} images, {nc} corrupt'\npbar.close()\nif msgs:\nLOGGER.info('\\n'.join(msgs))\nx['hash'] = get_hash([x[0] for x in self.samples])\nx['results'] = nf, nc, len(samples), samples\nx['msgs'] = msgs  # warnings\nsave_dataset_cache_file(self.prefix, path, x)\nreturn samples\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.ClassificationDataset.__getitem__","title":"<code>__getitem__(i)</code>","text":"<p>Returns subset of data and targets corresponding to given indices.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def __getitem__(self, i):\n\"\"\"Returns subset of data and targets corresponding to given indices.\"\"\"\nf, j, fn, im = self.samples[i]  # filename, index, filename.with_suffix('.npy'), image\nif self.cache_ram and im is None:\nim = self.samples[i][3] = cv2.imread(f)\nelif self.cache_disk:\nif not fn.exists():  # load npy\nnp.save(fn.as_posix(), cv2.imread(f))\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif self.album_transforms:\nsample = self.album_transforms(image=cv2.cvtColor(im, cv2.COLOR_BGR2RGB))['image']\nelse:\nsample = self.torch_transforms(im)\nreturn {'img': sample, 'cls': j}\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.ClassificationDataset.__init__","title":"<code>__init__(root, args, augment=False, cache=False, prefix='')</code>","text":"<p>Initialize YOLO object with root, image size, augmentations, and cache settings.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Dataset path.</p> required <code>args</code> <code>Namespace</code> <p>Argument parser containing dataset related settings.</p> required <code>augment</code> <code>bool</code> <p>True if dataset should be augmented, False otherwise. Defaults to False.</p> <code>False</code> <code>cache</code> <code>bool | str | optional</code> <p>Cache setting, can be True, False, 'ram' or 'disk'. Defaults to False.</p> <code>False</code> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def __init__(self, root, args, augment=False, cache=False, prefix=''):\n\"\"\"\n    Initialize YOLO object with root, image size, augmentations, and cache settings.\n    Args:\n        root (str): Dataset path.\n        args (Namespace): Argument parser containing dataset related settings.\n        augment (bool, optional): True if dataset should be augmented, False otherwise. Defaults to False.\n        cache (bool | str | optional): Cache setting, can be True, False, 'ram' or 'disk'. Defaults to False.\n    \"\"\"\nsuper().__init__(root=root)\nif augment and args.fraction &lt; 1.0:  # reduce training fraction\nself.samples = self.samples[:round(len(self.samples) * args.fraction)]\nself.prefix = colorstr(f'{prefix}: ') if prefix else ''\nself.cache_ram = cache is True or cache == 'ram'\nself.cache_disk = cache == 'disk'\nself.samples = self.verify_images()  # filter out bad images\nself.samples = [list(x) + [Path(x[0]).with_suffix('.npy'), None] for x in self.samples]  # file, index, npy, im\nself.torch_transforms = classify_transforms(args.imgsz)\nself.album_transforms = classify_albumentations(\naugment=augment,\nsize=args.imgsz,\nscale=(1.0 - args.scale, 1.0),  # (0.08, 1.0)\nhflip=args.fliplr,\nvflip=args.flipud,\nhsv_h=args.hsv_h,  # HSV-Hue augmentation (fraction)\nhsv_s=args.hsv_s,  # HSV-Saturation augmentation (fraction)\nhsv_v=args.hsv_v,  # HSV-Value augmentation (fraction)\nmean=(0.0, 0.0, 0.0),  # IMAGENET_MEAN\nstd=(1.0, 1.0, 1.0),  # IMAGENET_STD\nauto_aug=False) if augment else None\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.ClassificationDataset.verify_images","title":"<code>verify_images()</code>","text":"<p>Verify all images in dataset.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def verify_images(self):\n\"\"\"Verify all images in dataset.\"\"\"\ndesc = f'{self.prefix}Scanning {self.root}...'\npath = Path(self.root).with_suffix('.cache')  # *.cache file path\nwith contextlib.suppress(FileNotFoundError, AssertionError, AttributeError):\ncache = load_dataset_cache_file(path)  # attempt to load a *.cache file\nassert cache['version'] == DATASET_CACHE_VERSION  # matches current version\nassert cache['hash'] == get_hash([x[0] for x in self.samples])  # identical hash\nnf, nc, n, samples = cache.pop('results')  # found, missing, empty, corrupt, total\nif LOCAL_RANK in (-1, 0):\nd = f'{desc} {nf} images, {nc} corrupt'\ntqdm(None, desc=d, total=n, initial=n, bar_format=TQDM_BAR_FORMAT)\nif cache['msgs']:\nLOGGER.info('\\n'.join(cache['msgs']))  # display warnings\nreturn samples\n# Run scan if *.cache retrieval failed\nnf, nc, msgs, samples, x = 0, 0, [], [], {}\nwith ThreadPool(NUM_THREADS) as pool:\nresults = pool.imap(func=verify_image, iterable=zip(self.samples, repeat(self.prefix)))\npbar = tqdm(results, desc=desc, total=len(self.samples), bar_format=TQDM_BAR_FORMAT)\nfor sample, nf_f, nc_f, msg in pbar:\nif nf_f:\nsamples.append(sample)\nif msg:\nmsgs.append(msg)\nnf += nf_f\nnc += nc_f\npbar.desc = f'{desc} {nf} images, {nc} corrupt'\npbar.close()\nif msgs:\nLOGGER.info('\\n'.join(msgs))\nx['hash'] = get_hash([x[0] for x in self.samples])\nx['results'] = nf, nc, len(samples), samples\nx['msgs'] = msgs  # warnings\nsave_dataset_cache_file(self.prefix, path, x)\nreturn samples\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.SemanticDataset","title":"<code>ultralytics.data.dataset.SemanticDataset</code>","text":"<p>             Bases: <code>BaseDataset</code></p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>class SemanticDataset(BaseDataset):\ndef __init__(self):\n\"\"\"Initialize a SemanticDataset object.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"reference/data/dataset/#ultralytics.data.dataset.SemanticDataset.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a SemanticDataset object.</p> Source code in <code>ultralytics/data/dataset.py</code> <pre><code>def __init__(self):\n\"\"\"Initialize a SemanticDataset object.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"reference/data/loaders/","title":"Reference for <code>ultralytics/data/loaders.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/loaders.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.SourceTypes","title":"<code>ultralytics.data.loaders.SourceTypes</code>  <code>dataclass</code>","text":"Source code in <code>ultralytics/data/loaders.py</code> <pre><code>@dataclass\nclass SourceTypes:\nwebcam: bool = False\nscreenshot: bool = False\nfrom_img: bool = False\ntensor: bool = False\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams","title":"<code>ultralytics.data.loaders.LoadStreams</code>","text":"<p>YOLOv8 streamloader, i.e. <code>yolo predict source='rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams</code>.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>class LoadStreams:\n\"\"\"YOLOv8 streamloader, i.e. `yolo predict source='rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`.\"\"\"\ndef __init__(self, sources='file.streams', imgsz=640, vid_stride=1):\n\"\"\"Initialize instance variables and check for consistent input stream shapes.\"\"\"\ntorch.backends.cudnn.benchmark = True  # faster for fixed-size inference\nself.running = True  # running flag for Thread\nself.mode = 'stream'\nself.imgsz = imgsz\nself.vid_stride = vid_stride  # video frame-rate stride\nsources = Path(sources).read_text().rsplit() if os.path.isfile(sources) else [sources]\nn = len(sources)\nself.sources = [ops.clean_str(x) for x in sources]  # clean source names for later\nself.imgs, self.fps, self.frames, self.threads, self.shape = [[]] * n, [0] * n, [0] * n, [None] * n, [None] * n\nself.caps = [None] * n  # video capture objects\nfor i, s in enumerate(sources):  # index, source\n# Start thread to read frames from video stream\nst = f'{i + 1}/{n}: {s}... '\nif urlparse(s).hostname in ('www.youtube.com', 'youtube.com', 'youtu.be'):  # if source is YouTube video\n# YouTube format i.e. 'https://www.youtube.com/watch?v=Zgi9g1ksQHc' or 'https://youtu.be/Zgi9g1ksQHc'\ns = get_best_youtube_url(s)\ns = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\nif s == 0 and (is_colab() or is_kaggle()):\nraise NotImplementedError(\"'source=0' webcam not supported in Colab and Kaggle notebooks. \"\n\"Try running 'source=0' in a local environment.\")\nself.caps[i] = cv2.VideoCapture(s)  # store video capture object\nif not self.caps[i].isOpened():\nraise ConnectionError(f'{st}Failed to open {s}')\nw = int(self.caps[i].get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(self.caps[i].get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = self.caps[i].get(cv2.CAP_PROP_FPS)  # warning: may return 0 or nan\nself.frames[i] = max(int(self.caps[i].get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float(\n'inf')  # infinite stream fallback\nself.fps[i] = max((fps if math.isfinite(fps) else 0) % 100, 0) or 30  # 30 FPS fallback\nsuccess, im = self.caps[i].read()  # guarantee first frame\nif not success or im is None:\nraise ConnectionError(f'{st}Failed to read images from {s}')\nself.imgs[i].append(im)\nself.shape[i] = im.shape\nself.threads[i] = Thread(target=self.update, args=([i, self.caps[i], s]), daemon=True)\nLOGGER.info(f'{st}Success \u2705 ({self.frames[i]} frames of shape {w}x{h} at {self.fps[i]:.2f} FPS)')\nself.threads[i].start()\nLOGGER.info('')  # newline\n# Check for common shapes\nself.bs = self.__len__()\ndef update(self, i, cap, stream):\n\"\"\"Read stream `i` frames in daemon thread.\"\"\"\nn, f = 0, self.frames[i]  # frame number, frame array\nwhile self.running and cap.isOpened() and n &lt; (f - 1):\n# Only read a new frame if the buffer is empty\nif not self.imgs[i]:\nn += 1\ncap.grab()  # .read() = .grab() followed by .retrieve()\nif n % self.vid_stride == 0:\nsuccess, im = cap.retrieve()\nif not success:\nim = np.zeros(self.shape[i], dtype=np.uint8)\nLOGGER.warning('WARNING \u26a0\ufe0f Video stream unresponsive, please check your IP camera connection.')\ncap.open(stream)  # re-open stream if signal was lost\nself.imgs[i].append(im)  # add image to buffer\nelse:\ntime.sleep(0.01)  # wait until the buffer is empty\ndef close(self):\n\"\"\"Close stream loader and release resources.\"\"\"\nself.running = False  # stop flag for Thread\nfor thread in self.threads:\nif thread.is_alive():\nthread.join(timeout=5)  # Add timeout\nfor cap in self.caps:  # Iterate through the stored VideoCapture objects\ntry:\ncap.release()  # release video capture\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f Could not release VideoCapture object: {e}')\ncv2.destroyAllWindows()\ndef __iter__(self):\n\"\"\"Iterates through YOLO image feed and re-opens unresponsive streams.\"\"\"\nself.count = -1\nreturn self\ndef __next__(self):\n\"\"\"Returns source paths, transformed and original images for processing.\"\"\"\nself.count += 1\n# Wait until a frame is available in each buffer\nwhile not all(self.imgs):\nif not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\ncv2.destroyAllWindows()\nraise StopIteration\ntime.sleep(1 / min(self.fps))\n# Get and remove the next frame from imgs buffer\nreturn self.sources, [x.pop(0) for x in self.imgs], None, ''\ndef __len__(self):\n\"\"\"Return the length of the sources object.\"\"\"\nreturn len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.__init__","title":"<code>__init__(sources='file.streams', imgsz=640, vid_stride=1)</code>","text":"<p>Initialize instance variables and check for consistent input stream shapes.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __init__(self, sources='file.streams', imgsz=640, vid_stride=1):\n\"\"\"Initialize instance variables and check for consistent input stream shapes.\"\"\"\ntorch.backends.cudnn.benchmark = True  # faster for fixed-size inference\nself.running = True  # running flag for Thread\nself.mode = 'stream'\nself.imgsz = imgsz\nself.vid_stride = vid_stride  # video frame-rate stride\nsources = Path(sources).read_text().rsplit() if os.path.isfile(sources) else [sources]\nn = len(sources)\nself.sources = [ops.clean_str(x) for x in sources]  # clean source names for later\nself.imgs, self.fps, self.frames, self.threads, self.shape = [[]] * n, [0] * n, [0] * n, [None] * n, [None] * n\nself.caps = [None] * n  # video capture objects\nfor i, s in enumerate(sources):  # index, source\n# Start thread to read frames from video stream\nst = f'{i + 1}/{n}: {s}... '\nif urlparse(s).hostname in ('www.youtube.com', 'youtube.com', 'youtu.be'):  # if source is YouTube video\n# YouTube format i.e. 'https://www.youtube.com/watch?v=Zgi9g1ksQHc' or 'https://youtu.be/Zgi9g1ksQHc'\ns = get_best_youtube_url(s)\ns = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\nif s == 0 and (is_colab() or is_kaggle()):\nraise NotImplementedError(\"'source=0' webcam not supported in Colab and Kaggle notebooks. \"\n\"Try running 'source=0' in a local environment.\")\nself.caps[i] = cv2.VideoCapture(s)  # store video capture object\nif not self.caps[i].isOpened():\nraise ConnectionError(f'{st}Failed to open {s}')\nw = int(self.caps[i].get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(self.caps[i].get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = self.caps[i].get(cv2.CAP_PROP_FPS)  # warning: may return 0 or nan\nself.frames[i] = max(int(self.caps[i].get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float(\n'inf')  # infinite stream fallback\nself.fps[i] = max((fps if math.isfinite(fps) else 0) % 100, 0) or 30  # 30 FPS fallback\nsuccess, im = self.caps[i].read()  # guarantee first frame\nif not success or im is None:\nraise ConnectionError(f'{st}Failed to read images from {s}')\nself.imgs[i].append(im)\nself.shape[i] = im.shape\nself.threads[i] = Thread(target=self.update, args=([i, self.caps[i], s]), daemon=True)\nLOGGER.info(f'{st}Success \u2705 ({self.frames[i]} frames of shape {w}x{h} at {self.fps[i]:.2f} FPS)')\nself.threads[i].start()\nLOGGER.info('')  # newline\n# Check for common shapes\nself.bs = self.__len__()\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates through YOLO image feed and re-opens unresponsive streams.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterates through YOLO image feed and re-opens unresponsive streams.\"\"\"\nself.count = -1\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the sources object.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __len__(self):\n\"\"\"Return the length of the sources object.\"\"\"\nreturn len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.__next__","title":"<code>__next__()</code>","text":"<p>Returns source paths, transformed and original images for processing.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __next__(self):\n\"\"\"Returns source paths, transformed and original images for processing.\"\"\"\nself.count += 1\n# Wait until a frame is available in each buffer\nwhile not all(self.imgs):\nif not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\ncv2.destroyAllWindows()\nraise StopIteration\ntime.sleep(1 / min(self.fps))\n# Get and remove the next frame from imgs buffer\nreturn self.sources, [x.pop(0) for x in self.imgs], None, ''\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.close","title":"<code>close()</code>","text":"<p>Close stream loader and release resources.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def close(self):\n\"\"\"Close stream loader and release resources.\"\"\"\nself.running = False  # stop flag for Thread\nfor thread in self.threads:\nif thread.is_alive():\nthread.join(timeout=5)  # Add timeout\nfor cap in self.caps:  # Iterate through the stored VideoCapture objects\ntry:\ncap.release()  # release video capture\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f Could not release VideoCapture object: {e}')\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadStreams.update","title":"<code>update(i, cap, stream)</code>","text":"<p>Read stream <code>i</code> frames in daemon thread.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def update(self, i, cap, stream):\n\"\"\"Read stream `i` frames in daemon thread.\"\"\"\nn, f = 0, self.frames[i]  # frame number, frame array\nwhile self.running and cap.isOpened() and n &lt; (f - 1):\n# Only read a new frame if the buffer is empty\nif not self.imgs[i]:\nn += 1\ncap.grab()  # .read() = .grab() followed by .retrieve()\nif n % self.vid_stride == 0:\nsuccess, im = cap.retrieve()\nif not success:\nim = np.zeros(self.shape[i], dtype=np.uint8)\nLOGGER.warning('WARNING \u26a0\ufe0f Video stream unresponsive, please check your IP camera connection.')\ncap.open(stream)  # re-open stream if signal was lost\nself.imgs[i].append(im)  # add image to buffer\nelse:\ntime.sleep(0.01)  # wait until the buffer is empty\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadScreenshots","title":"<code>ultralytics.data.loaders.LoadScreenshots</code>","text":"<p>YOLOv8 screenshot dataloader, i.e. <code>yolo predict source=screen</code>.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>class LoadScreenshots:\n\"\"\"YOLOv8 screenshot dataloader, i.e. `yolo predict source=screen`.\"\"\"\ndef __init__(self, source, imgsz=640):\n\"\"\"source = [screen_number left top width height] (pixels).\"\"\"\ncheck_requirements('mss')\nimport mss  # noqa\nsource, *params = source.split()\nself.screen, left, top, width, height = 0, None, None, None, None  # default to full screen 0\nif len(params) == 1:\nself.screen = int(params[0])\nelif len(params) == 4:\nleft, top, width, height = (int(x) for x in params)\nelif len(params) == 5:\nself.screen, left, top, width, height = (int(x) for x in params)\nself.imgsz = imgsz\nself.mode = 'stream'\nself.frame = 0\nself.sct = mss.mss()\nself.bs = 1\n# Parse monitor shape\nmonitor = self.sct.monitors[self.screen]\nself.top = monitor['top'] if top is None else (monitor['top'] + top)\nself.left = monitor['left'] if left is None else (monitor['left'] + left)\nself.width = width or monitor['width']\nself.height = height or monitor['height']\nself.monitor = {'left': self.left, 'top': self.top, 'width': self.width, 'height': self.height}\ndef __iter__(self):\n\"\"\"Returns an iterator of the object.\"\"\"\nreturn self\ndef __next__(self):\n\"\"\"mss screen capture: get raw pixels from the screen as np array.\"\"\"\nim0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR\ns = f'screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: '\nself.frame += 1\nreturn [str(self.screen)], [im0], None, s  # screen, img, vid_cap, string\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadScreenshots.__init__","title":"<code>__init__(source, imgsz=640)</code>","text":"<p>source = [screen_number left top width height] (pixels).</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __init__(self, source, imgsz=640):\n\"\"\"source = [screen_number left top width height] (pixels).\"\"\"\ncheck_requirements('mss')\nimport mss  # noqa\nsource, *params = source.split()\nself.screen, left, top, width, height = 0, None, None, None, None  # default to full screen 0\nif len(params) == 1:\nself.screen = int(params[0])\nelif len(params) == 4:\nleft, top, width, height = (int(x) for x in params)\nelif len(params) == 5:\nself.screen, left, top, width, height = (int(x) for x in params)\nself.imgsz = imgsz\nself.mode = 'stream'\nself.frame = 0\nself.sct = mss.mss()\nself.bs = 1\n# Parse monitor shape\nmonitor = self.sct.monitors[self.screen]\nself.top = monitor['top'] if top is None else (monitor['top'] + top)\nself.left = monitor['left'] if left is None else (monitor['left'] + left)\nself.width = width or monitor['width']\nself.height = height or monitor['height']\nself.monitor = {'left': self.left, 'top': self.top, 'width': self.width, 'height': self.height}\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadScreenshots.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator of the object.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __iter__(self):\n\"\"\"Returns an iterator of the object.\"\"\"\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadScreenshots.__next__","title":"<code>__next__()</code>","text":"<p>mss screen capture: get raw pixels from the screen as np array.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __next__(self):\n\"\"\"mss screen capture: get raw pixels from the screen as np array.\"\"\"\nim0 = np.asarray(self.sct.grab(self.monitor))[:, :, :3]  # BGRA to BGR\ns = f'screen {self.screen} (LTWH): {self.left},{self.top},{self.width},{self.height}: '\nself.frame += 1\nreturn [str(self.screen)], [im0], None, s  # screen, img, vid_cap, string\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadImages","title":"<code>ultralytics.data.loaders.LoadImages</code>","text":"<p>YOLOv8 image/video dataloader, i.e. <code>yolo predict source=image.jpg/vid.mp4</code>.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>class LoadImages:\n\"\"\"YOLOv8 image/video dataloader, i.e. `yolo predict source=image.jpg/vid.mp4`.\"\"\"\ndef __init__(self, path, imgsz=640, vid_stride=1):\n\"\"\"Initialize the Dataloader and raise FileNotFoundError if file not found.\"\"\"\nparent = None\nif isinstance(path, str) and Path(path).suffix == '.txt':  # *.txt file with img/vid/dir on each line\nparent = Path(path).parent\npath = Path(path).read_text().splitlines()  # list of sources\nfiles = []\nfor p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\na = str(Path(p).absolute())  # do not use .resolve() https://github.com/ultralytics/ultralytics/issues/2912\nif '*' in a:\nfiles.extend(sorted(glob.glob(a, recursive=True)))  # glob\nelif os.path.isdir(a):\nfiles.extend(sorted(glob.glob(os.path.join(a, '*.*'))))  # dir\nelif os.path.isfile(a):\nfiles.append(a)  # files (absolute or relative to CWD)\nelif parent and (parent / p).is_file():\nfiles.append(str((parent / p).absolute()))  # files (relative to *.txt file parent)\nelse:\nraise FileNotFoundError(f'{p} does not exist')\nimages = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\nvideos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\nni, nv = len(images), len(videos)\nself.imgsz = imgsz\nself.files = images + videos\nself.nf = ni + nv  # number of files\nself.video_flag = [False] * ni + [True] * nv\nself.mode = 'image'\nself.vid_stride = vid_stride  # video frame-rate stride\nself.bs = 1\nif any(videos):\nself._new_video(videos[0])  # new video\nelse:\nself.cap = None\nif self.nf == 0:\nraise FileNotFoundError(f'No images or videos found in {p}. '\nf'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}')\ndef __iter__(self):\n\"\"\"Returns an iterator object for VideoStream or ImageFolder.\"\"\"\nself.count = 0\nreturn self\ndef __next__(self):\n\"\"\"Return next image, path and metadata from dataset.\"\"\"\nif self.count == self.nf:\nraise StopIteration\npath = self.files[self.count]\nif self.video_flag[self.count]:\n# Read video\nself.mode = 'video'\nfor _ in range(self.vid_stride):\nself.cap.grab()\nsuccess, im0 = self.cap.retrieve()\nwhile not success:\nself.count += 1\nself.cap.release()\nif self.count == self.nf:  # last video\nraise StopIteration\npath = self.files[self.count]\nself._new_video(path)\nsuccess, im0 = self.cap.read()\nself.frame += 1\n# im0 = self._cv2_rotate(im0)  # for use if cv2 autorotation is False\ns = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '\nelse:\n# Read image\nself.count += 1\nim0 = cv2.imread(path)  # BGR\nif im0 is None:\nraise FileNotFoundError(f'Image Not Found {path}')\ns = f'image {self.count}/{self.nf} {path}: '\nreturn [path], [im0], self.cap, s\ndef _new_video(self, path):\n\"\"\"Create a new video capture object.\"\"\"\nself.frame = 0\nself.cap = cv2.VideoCapture(path)\nself.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)\ndef __len__(self):\n\"\"\"Returns the number of files in the object.\"\"\"\nreturn self.nf  # number of files\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadImages.__init__","title":"<code>__init__(path, imgsz=640, vid_stride=1)</code>","text":"<p>Initialize the Dataloader and raise FileNotFoundError if file not found.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __init__(self, path, imgsz=640, vid_stride=1):\n\"\"\"Initialize the Dataloader and raise FileNotFoundError if file not found.\"\"\"\nparent = None\nif isinstance(path, str) and Path(path).suffix == '.txt':  # *.txt file with img/vid/dir on each line\nparent = Path(path).parent\npath = Path(path).read_text().splitlines()  # list of sources\nfiles = []\nfor p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\na = str(Path(p).absolute())  # do not use .resolve() https://github.com/ultralytics/ultralytics/issues/2912\nif '*' in a:\nfiles.extend(sorted(glob.glob(a, recursive=True)))  # glob\nelif os.path.isdir(a):\nfiles.extend(sorted(glob.glob(os.path.join(a, '*.*'))))  # dir\nelif os.path.isfile(a):\nfiles.append(a)  # files (absolute or relative to CWD)\nelif parent and (parent / p).is_file():\nfiles.append(str((parent / p).absolute()))  # files (relative to *.txt file parent)\nelse:\nraise FileNotFoundError(f'{p} does not exist')\nimages = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\nvideos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\nni, nv = len(images), len(videos)\nself.imgsz = imgsz\nself.files = images + videos\nself.nf = ni + nv  # number of files\nself.video_flag = [False] * ni + [True] * nv\nself.mode = 'image'\nself.vid_stride = vid_stride  # video frame-rate stride\nself.bs = 1\nif any(videos):\nself._new_video(videos[0])  # new video\nelse:\nself.cap = None\nif self.nf == 0:\nraise FileNotFoundError(f'No images or videos found in {p}. '\nf'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}')\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadImages.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator object for VideoStream or ImageFolder.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __iter__(self):\n\"\"\"Returns an iterator object for VideoStream or ImageFolder.\"\"\"\nself.count = 0\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadImages.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of files in the object.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the number of files in the object.\"\"\"\nreturn self.nf  # number of files\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadImages.__next__","title":"<code>__next__()</code>","text":"<p>Return next image, path and metadata from dataset.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __next__(self):\n\"\"\"Return next image, path and metadata from dataset.\"\"\"\nif self.count == self.nf:\nraise StopIteration\npath = self.files[self.count]\nif self.video_flag[self.count]:\n# Read video\nself.mode = 'video'\nfor _ in range(self.vid_stride):\nself.cap.grab()\nsuccess, im0 = self.cap.retrieve()\nwhile not success:\nself.count += 1\nself.cap.release()\nif self.count == self.nf:  # last video\nraise StopIteration\npath = self.files[self.count]\nself._new_video(path)\nsuccess, im0 = self.cap.read()\nself.frame += 1\n# im0 = self._cv2_rotate(im0)  # for use if cv2 autorotation is False\ns = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '\nelse:\n# Read image\nself.count += 1\nim0 = cv2.imread(path)  # BGR\nif im0 is None:\nraise FileNotFoundError(f'Image Not Found {path}')\ns = f'image {self.count}/{self.nf} {path}: '\nreturn [path], [im0], self.cap, s\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadPilAndNumpy","title":"<code>ultralytics.data.loaders.LoadPilAndNumpy</code>","text":"Source code in <code>ultralytics/data/loaders.py</code> <pre><code>class LoadPilAndNumpy:\ndef __init__(self, im0, imgsz=640):\n\"\"\"Initialize PIL and Numpy Dataloader.\"\"\"\nif not isinstance(im0, list):\nim0 = [im0]\nself.paths = [getattr(im, 'filename', f'image{i}.jpg') for i, im in enumerate(im0)]\nself.im0 = [self._single_check(im) for im in im0]\nself.imgsz = imgsz\nself.mode = 'image'\n# Generate fake paths\nself.bs = len(self.im0)\n@staticmethod\ndef _single_check(im):\n\"\"\"Validate and format an image to numpy array.\"\"\"\nassert isinstance(im, (Image.Image, np.ndarray)), f'Expected PIL/np.ndarray image type, but got {type(im)}'\nif isinstance(im, Image.Image):\nif im.mode != 'RGB':\nim = im.convert('RGB')\nim = np.asarray(im)[:, :, ::-1]\nim = np.ascontiguousarray(im)  # contiguous\nreturn im\ndef __len__(self):\n\"\"\"Returns the length of the 'im0' attribute.\"\"\"\nreturn len(self.im0)\ndef __next__(self):\n\"\"\"Returns batch paths, images, processed images, None, ''.\"\"\"\nif self.count == 1:  # loop only once as it's batch inference\nraise StopIteration\nself.count += 1\nreturn self.paths, self.im0, None, ''\ndef __iter__(self):\n\"\"\"Enables iteration for class LoadPilAndNumpy.\"\"\"\nself.count = 0\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadPilAndNumpy.__init__","title":"<code>__init__(im0, imgsz=640)</code>","text":"<p>Initialize PIL and Numpy Dataloader.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __init__(self, im0, imgsz=640):\n\"\"\"Initialize PIL and Numpy Dataloader.\"\"\"\nif not isinstance(im0, list):\nim0 = [im0]\nself.paths = [getattr(im, 'filename', f'image{i}.jpg') for i, im in enumerate(im0)]\nself.im0 = [self._single_check(im) for im in im0]\nself.imgsz = imgsz\nself.mode = 'image'\n# Generate fake paths\nself.bs = len(self.im0)\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadPilAndNumpy.__iter__","title":"<code>__iter__()</code>","text":"<p>Enables iteration for class LoadPilAndNumpy.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __iter__(self):\n\"\"\"Enables iteration for class LoadPilAndNumpy.\"\"\"\nself.count = 0\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadPilAndNumpy.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the 'im0' attribute.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the length of the 'im0' attribute.\"\"\"\nreturn len(self.im0)\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadPilAndNumpy.__next__","title":"<code>__next__()</code>","text":"<p>Returns batch paths, images, processed images, None, ''.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __next__(self):\n\"\"\"Returns batch paths, images, processed images, None, ''.\"\"\"\nif self.count == 1:  # loop only once as it's batch inference\nraise StopIteration\nself.count += 1\nreturn self.paths, self.im0, None, ''\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadTensor","title":"<code>ultralytics.data.loaders.LoadTensor</code>","text":"Source code in <code>ultralytics/data/loaders.py</code> <pre><code>class LoadTensor:\ndef __init__(self, im0) -&gt; None:\nself.im0 = self._single_check(im0)\nself.bs = self.im0.shape[0]\nself.mode = 'image'\nself.paths = [getattr(im, 'filename', f'image{i}.jpg') for i, im in enumerate(im0)]\n@staticmethod\ndef _single_check(im, stride=32):\n\"\"\"Validate and format an image to torch.Tensor.\"\"\"\ns = f'WARNING \u26a0\ufe0f torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) ' \\\n            f'divisible by stride {stride}. Input shape{tuple(im.shape)} is incompatible.'\nif len(im.shape) != 4:\nif len(im.shape) != 3:\nraise ValueError(s)\nLOGGER.warning(s)\nim = im.unsqueeze(0)\nif im.shape[2] % stride or im.shape[3] % stride:\nraise ValueError(s)\nif im.max() &gt; 1.0:\nLOGGER.warning(f'WARNING \u26a0\ufe0f torch.Tensor inputs should be normalized 0.0-1.0 but max value is {im.max()}. '\nf'Dividing input by 255.')\nim = im.float() / 255.0\nreturn im\ndef __iter__(self):\n\"\"\"Returns an iterator object.\"\"\"\nself.count = 0\nreturn self\ndef __next__(self):\n\"\"\"Return next item in the iterator.\"\"\"\nif self.count == 1:\nraise StopIteration\nself.count += 1\nreturn self.paths, self.im0, None, ''\ndef __len__(self):\n\"\"\"Returns the batch size.\"\"\"\nreturn self.bs\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadTensor.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator object.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __iter__(self):\n\"\"\"Returns an iterator object.\"\"\"\nself.count = 0\nreturn self\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadTensor.__len__","title":"<code>__len__()</code>","text":"<p>Returns the batch size.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the batch size.\"\"\"\nreturn self.bs\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.LoadTensor.__next__","title":"<code>__next__()</code>","text":"<p>Return next item in the iterator.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def __next__(self):\n\"\"\"Return next item in the iterator.\"\"\"\nif self.count == 1:\nraise StopIteration\nself.count += 1\nreturn self.paths, self.im0, None, ''\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.autocast_list","title":"<code>ultralytics.data.loaders.autocast_list(source)</code>","text":"<p>Merges a list of source of different types into a list of numpy arrays or PIL images</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def autocast_list(source):\n\"\"\"\n    Merges a list of source of different types into a list of numpy arrays or PIL images\n    \"\"\"\nfiles = []\nfor im in source:\nif isinstance(im, (str, Path)):  # filename or uri\nfiles.append(Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im))\nelif isinstance(im, (Image.Image, np.ndarray)):  # PIL or np Image\nfiles.append(im)\nelse:\nraise TypeError(f'type {type(im).__name__} is not a supported Ultralytics prediction source type. \\n'\nf'See https://docs.ultralytics.com/modes/predict for supported source types.')\nreturn files\n</code></pre>"},{"location":"reference/data/loaders/#ultralytics.data.loaders.get_best_youtube_url","title":"<code>ultralytics.data.loaders.get_best_youtube_url(url, use_pafy=False)</code>","text":"<p>Retrieves the URL of the best quality MP4 video stream from a given YouTube video.</p> <p>This function uses the pafy or yt_dlp library to extract the video info from YouTube. It then finds the highest quality MP4 format that has video codec but no audio codec, and returns the URL of this video stream.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the YouTube video.</p> required <code>use_pafy</code> <code>bool</code> <p>Use the pafy package, default=True, otherwise use yt_dlp package.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The URL of the best quality MP4 video stream, or None if no suitable stream is found.</p> Source code in <code>ultralytics/data/loaders.py</code> <pre><code>def get_best_youtube_url(url, use_pafy=False):\n\"\"\"\n    Retrieves the URL of the best quality MP4 video stream from a given YouTube video.\n    This function uses the pafy or yt_dlp library to extract the video info from YouTube. It then finds the highest\n    quality MP4 format that has video codec but no audio codec, and returns the URL of this video stream.\n    Args:\n        url (str): The URL of the YouTube video.\n        use_pafy (bool): Use the pafy package, default=True, otherwise use yt_dlp package.\n    Returns:\n        (str): The URL of the best quality MP4 video stream, or None if no suitable stream is found.\n    \"\"\"\nif use_pafy:\ncheck_requirements(('pafy', 'youtube_dl==2020.12.2'))\nimport pafy  # noqa\nreturn pafy.new(url).getbestvideo(preftype='mp4').url\nelse:\ncheck_requirements('yt-dlp')\nimport yt_dlp\nwith yt_dlp.YoutubeDL({'quiet': True}) as ydl:\ninfo_dict = ydl.extract_info(url, download=False)  # extract info\nfor f in reversed(info_dict.get('formats', [])):  # reversed because best is usually last\n# Find a format with video codec, no audio, *.mp4 extension at least 1920x1080 size\ngood_size = (f.get('width') or 0) &gt;= 1920 or (f.get('height') or 0) &gt;= 1080\nif good_size and f['vcodec'] != 'none' and f['acodec'] == 'none' and f['ext'] == 'mp4':\nreturn f.get('url')\n</code></pre>"},{"location":"reference/data/utils/","title":"Reference for <code>ultralytics/data/utils.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/utils.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/data/utils/#ultralytics.data.utils.HUBDatasetStats","title":"<code>ultralytics.data.utils.HUBDatasetStats</code>","text":"<p>A class for generating HUB dataset JSON and <code>-hub</code> dataset directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco128.yaml'.</p> <code>'coco128.yaml'</code> <code>task</code> <code>str</code> <p>Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.</p> <code>'detect'</code> <code>autodownload</code> <code>bool</code> <p>Attempt to download dataset if not found locally. Default is False.</p> <code>False</code> Example <pre><code>from ultralytics.data.utils import HUBDatasetStats\nstats = HUBDatasetStats('path/to/coco8.zip', task='detect')  # detect dataset\nstats = HUBDatasetStats('path/to/coco8-seg.zip', task='segment')  # segment dataset\nstats = HUBDatasetStats('path/to/coco8-pose.zip', task='pose')  # pose dataset\nstats.get_json(save=False)\nstats.process_images()\n</code></pre> Source code in <code>ultralytics/data/utils.py</code> <pre><code>class HUBDatasetStats:\n\"\"\"\n    A class for generating HUB dataset JSON and `-hub` dataset directory.\n    Args:\n        path (str): Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco128.yaml'.\n        task (str): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.\n        autodownload (bool): Attempt to download dataset if not found locally. Default is False.\n    Example:\n        ```python\n        from ultralytics.data.utils import HUBDatasetStats\n        stats = HUBDatasetStats('path/to/coco8.zip', task='detect')  # detect dataset\n        stats = HUBDatasetStats('path/to/coco8-seg.zip', task='segment')  # segment dataset\n        stats = HUBDatasetStats('path/to/coco8-pose.zip', task='pose')  # pose dataset\n        stats.get_json(save=False)\n        stats.process_images()\n        ```\n    \"\"\"\ndef __init__(self, path='coco128.yaml', task='detect', autodownload=False):\n\"\"\"Initialize class.\"\"\"\npath = Path(path).resolve()\nLOGGER.info(f'Starting HUB dataset checks for {path}....')\nzipped, data_dir, yaml_path = self._unzip(path)\ntry:\n# data = yaml_load(check_yaml(yaml_path))  # data dict\ndata = check_det_dataset(yaml_path, autodownload)  # data dict\nif zipped:\ndata['path'] = data_dir\nexcept Exception as e:\nraise Exception('error/HUB/dataset_stats/yaml_load') from e\nself.hub_dir = Path(str(data['path']) + '-hub')\nself.im_dir = self.hub_dir / 'images'\nself.im_dir.mkdir(parents=True, exist_ok=True)  # makes /images\nself.stats = {'nc': len(data['names']), 'names': list(data['names'].values())}  # statistics dictionary\nself.data = data\nself.task = task  # detect, segment, pose, classify\n@staticmethod\ndef _find_yaml(dir):\n\"\"\"Return data.yaml file.\"\"\"\nfiles = list(dir.glob('*.yaml')) or list(dir.rglob('*.yaml'))  # try root level first and then recursive\nassert files, f\"No *.yaml file found in '{dir.resolve()}'\"\nif len(files) &gt; 1:\nfiles = [f for f in files if f.stem == dir.stem]  # prefer *.yaml files that match dir name\nassert len(files) == 1, f\"Expected 1 *.yaml file in '{dir.resolve()}', but found {len(files)}.\\n{files}\"\nreturn files[0]\ndef _unzip(self, path):\n\"\"\"Unzip data.zip.\"\"\"\nif not str(path).endswith('.zip'):  # path is data.yaml\nreturn False, None, path\nunzip_dir = unzip_file(path, path=path.parent)\nassert unzip_dir.is_dir(), f'Error unzipping {path}, {unzip_dir} not found. ' \\\n                                   f'path/to/abc.zip MUST unzip to path/to/abc/'\nreturn True, str(unzip_dir), self._find_yaml(unzip_dir)  # zipped, data_dir, yaml_path\ndef _hub_ops(self, f):\n\"\"\"Saves a compressed image for HUB previews.\"\"\"\ncompress_one_image(f, self.im_dir / Path(f).name)  # save to dataset-hub\ndef get_json(self, save=False, verbose=False):\n\"\"\"Return dataset JSON for Ultralytics HUB.\"\"\"\nfrom ultralytics.data import YOLODataset  # ClassificationDataset\ndef _round(labels):\n\"\"\"Update labels to integer class and 4 decimal place floats.\"\"\"\nif self.task == 'detect':\ncoordinates = labels['bboxes']\nelif self.task == 'segment':\ncoordinates = [x.flatten() for x in labels['segments']]\nelif self.task == 'pose':\nn = labels['keypoints'].shape[0]\ncoordinates = np.concatenate((labels['bboxes'], labels['keypoints'].reshape(n, -1)), 1)\nelse:\nraise ValueError('Undefined dataset task.')\nzipped = zip(labels['cls'], coordinates)\nreturn [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]\nfor split in 'train', 'val', 'test':\nif self.data.get(split) is None:\nself.stats[split] = None  # i.e. no test set\ncontinue\ndataset = YOLODataset(img_path=self.data[split],\ndata=self.data,\nuse_segments=self.task == 'segment',\nuse_keypoints=self.task == 'pose')\nx = np.array([\nnp.bincount(label['cls'].astype(int).flatten(), minlength=self.data['nc'])\nfor label in tqdm(dataset.labels, total=len(dataset), desc='Statistics')])  # shape(128x80)\nself.stats[split] = {\n'instance_stats': {\n'total': int(x.sum()),\n'per_class': x.sum(0).tolist()},\n'image_stats': {\n'total': len(dataset),\n'unlabelled': int(np.all(x == 0, 1).sum()),\n'per_class': (x &gt; 0).sum(0).tolist()},\n'labels': [{\nPath(k).name: _round(v)} for k, v in zip(dataset.im_files, dataset.labels)]}\n# Save, print and return\nif save:\nstats_path = self.hub_dir / 'stats.json'\nLOGGER.info(f'Saving {stats_path.resolve()}...')\nwith open(stats_path, 'w') as f:\njson.dump(self.stats, f)  # save stats.json\nif verbose:\nLOGGER.info(json.dumps(self.stats, indent=2, sort_keys=False))\nreturn self.stats\ndef process_images(self):\n\"\"\"Compress images for Ultralytics HUB.\"\"\"\nfrom ultralytics.data import YOLODataset  # ClassificationDataset\nfor split in 'train', 'val', 'test':\nif self.data.get(split) is None:\ncontinue\ndataset = YOLODataset(img_path=self.data[split], data=self.data)\nwith ThreadPool(NUM_THREADS) as pool:\nfor _ in tqdm(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f'{split} images'):\npass\nLOGGER.info(f'Done. All images saved to {self.im_dir}')\nreturn self.im_dir\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.HUBDatasetStats.__init__","title":"<code>__init__(path='coco128.yaml', task='detect', autodownload=False)</code>","text":"<p>Initialize class.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def __init__(self, path='coco128.yaml', task='detect', autodownload=False):\n\"\"\"Initialize class.\"\"\"\npath = Path(path).resolve()\nLOGGER.info(f'Starting HUB dataset checks for {path}....')\nzipped, data_dir, yaml_path = self._unzip(path)\ntry:\n# data = yaml_load(check_yaml(yaml_path))  # data dict\ndata = check_det_dataset(yaml_path, autodownload)  # data dict\nif zipped:\ndata['path'] = data_dir\nexcept Exception as e:\nraise Exception('error/HUB/dataset_stats/yaml_load') from e\nself.hub_dir = Path(str(data['path']) + '-hub')\nself.im_dir = self.hub_dir / 'images'\nself.im_dir.mkdir(parents=True, exist_ok=True)  # makes /images\nself.stats = {'nc': len(data['names']), 'names': list(data['names'].values())}  # statistics dictionary\nself.data = data\nself.task = task  # detect, segment, pose, classify\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.HUBDatasetStats.get_json","title":"<code>get_json(save=False, verbose=False)</code>","text":"<p>Return dataset JSON for Ultralytics HUB.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def get_json(self, save=False, verbose=False):\n\"\"\"Return dataset JSON for Ultralytics HUB.\"\"\"\nfrom ultralytics.data import YOLODataset  # ClassificationDataset\ndef _round(labels):\n\"\"\"Update labels to integer class and 4 decimal place floats.\"\"\"\nif self.task == 'detect':\ncoordinates = labels['bboxes']\nelif self.task == 'segment':\ncoordinates = [x.flatten() for x in labels['segments']]\nelif self.task == 'pose':\nn = labels['keypoints'].shape[0]\ncoordinates = np.concatenate((labels['bboxes'], labels['keypoints'].reshape(n, -1)), 1)\nelse:\nraise ValueError('Undefined dataset task.')\nzipped = zip(labels['cls'], coordinates)\nreturn [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]\nfor split in 'train', 'val', 'test':\nif self.data.get(split) is None:\nself.stats[split] = None  # i.e. no test set\ncontinue\ndataset = YOLODataset(img_path=self.data[split],\ndata=self.data,\nuse_segments=self.task == 'segment',\nuse_keypoints=self.task == 'pose')\nx = np.array([\nnp.bincount(label['cls'].astype(int).flatten(), minlength=self.data['nc'])\nfor label in tqdm(dataset.labels, total=len(dataset), desc='Statistics')])  # shape(128x80)\nself.stats[split] = {\n'instance_stats': {\n'total': int(x.sum()),\n'per_class': x.sum(0).tolist()},\n'image_stats': {\n'total': len(dataset),\n'unlabelled': int(np.all(x == 0, 1).sum()),\n'per_class': (x &gt; 0).sum(0).tolist()},\n'labels': [{\nPath(k).name: _round(v)} for k, v in zip(dataset.im_files, dataset.labels)]}\n# Save, print and return\nif save:\nstats_path = self.hub_dir / 'stats.json'\nLOGGER.info(f'Saving {stats_path.resolve()}...')\nwith open(stats_path, 'w') as f:\njson.dump(self.stats, f)  # save stats.json\nif verbose:\nLOGGER.info(json.dumps(self.stats, indent=2, sort_keys=False))\nreturn self.stats\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.HUBDatasetStats.process_images","title":"<code>process_images()</code>","text":"<p>Compress images for Ultralytics HUB.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def process_images(self):\n\"\"\"Compress images for Ultralytics HUB.\"\"\"\nfrom ultralytics.data import YOLODataset  # ClassificationDataset\nfor split in 'train', 'val', 'test':\nif self.data.get(split) is None:\ncontinue\ndataset = YOLODataset(img_path=self.data[split], data=self.data)\nwith ThreadPool(NUM_THREADS) as pool:\nfor _ in tqdm(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f'{split} images'):\npass\nLOGGER.info(f'Done. All images saved to {self.im_dir}')\nreturn self.im_dir\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.img2label_paths","title":"<code>ultralytics.data.utils.img2label_paths(img_paths)</code>","text":"<p>Define label paths as a function of image paths.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def img2label_paths(img_paths):\n\"\"\"Define label paths as a function of image paths.\"\"\"\nsa, sb = f'{os.sep}images{os.sep}', f'{os.sep}labels{os.sep}'  # /images/, /labels/ substrings\nreturn [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.get_hash","title":"<code>ultralytics.data.utils.get_hash(paths)</code>","text":"<p>Returns a single hash value of a list of paths (files or dirs).</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def get_hash(paths):\n\"\"\"Returns a single hash value of a list of paths (files or dirs).\"\"\"\nsize = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\nh = hashlib.sha256(str(size).encode())  # hash sizes\nh.update(''.join(paths).encode())  # hash paths\nreturn h.hexdigest()  # return hash\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.exif_size","title":"<code>ultralytics.data.utils.exif_size(img)</code>","text":"<p>Returns exif-corrected PIL size.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def exif_size(img: Image.Image):\n\"\"\"Returns exif-corrected PIL size.\"\"\"\ns = img.size  # (width, height)\nif img.format == 'JPEG':  # only support JPEG images\nwith contextlib.suppress(Exception):\nexif = img.getexif()\nif exif:\nrotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274\nif rotation in [6, 8]:  # rotation 270 or 90\ns = s[1], s[0]\nreturn s\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.verify_image_label","title":"<code>ultralytics.data.utils.verify_image_label(args)</code>","text":"<p>Verify one image-label pair.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def verify_image_label(args):\n\"\"\"Verify one image-label pair.\"\"\"\nim_file, lb_file, prefix, keypoint, num_cls, nkpt, ndim = args\n# Number (missing, found, empty, corrupt), message, segments, keypoints\nnm, nf, ne, nc, msg, segments, keypoints = 0, 0, 0, 0, '', [], None\ntry:\n# Verify images\nim = Image.open(im_file)\nim.verify()  # PIL verify\nshape = exif_size(im)  # image size\nshape = (shape[1], shape[0])  # hw\nassert (shape[0] &gt; 9) &amp; (shape[1] &gt; 9), f'image size {shape} &lt;10 pixels'\nassert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'\nif im.format.lower() in ('jpg', 'jpeg'):\nwith open(im_file, 'rb') as f:\nf.seek(-2, 2)\nif f.read() != b'\\xff\\xd9':  # corrupt JPEG\nImageOps.exif_transpose(Image.open(im_file)).save(im_file, 'JPEG', subsampling=0, quality=100)\nmsg = f'{prefix}WARNING \u26a0\ufe0f {im_file}: corrupt JPEG restored and saved'\n# Verify labels\nif os.path.isfile(lb_file):\nnf = 1  # label found\nwith open(lb_file) as f:\nlb = [x.split() for x in f.read().strip().splitlines() if len(x)]\nif any(len(x) &gt; 6 for x in lb) and (not keypoint):  # is segment\nclasses = np.array([x[0] for x in lb], dtype=np.float32)\nsegments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (cls, xy1...)\nlb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\nlb = np.array(lb, dtype=np.float32)\nnl = len(lb)\nif nl:\nif keypoint:\nassert lb.shape[1] == (5 + nkpt * ndim), f'labels require {(5 + nkpt * ndim)} columns each'\nassert (lb[:, 5::ndim] &lt;= 1).all(), 'non-normalized or out of bounds coordinate labels'\nassert (lb[:, 6::ndim] &lt;= 1).all(), 'non-normalized or out of bounds coordinate labels'\nelse:\nassert lb.shape[1] == 5, f'labels require 5 columns, {lb.shape[1]} columns detected'\nassert (lb[:, 1:] &lt;= 1).all(), \\\n                        f'non-normalized or out of bounds coordinates {lb[:, 1:][lb[:, 1:] &gt; 1]}'\nassert (lb &gt;= 0).all(), f'negative label values {lb[lb &lt; 0]}'\n# All labels\nmax_cls = int(lb[:, 0].max())  # max label count\nassert max_cls &lt;= num_cls, \\\n                    f'Label class {max_cls} exceeds dataset class count {num_cls}. ' \\\n                    f'Possible class labels are 0-{num_cls - 1}'\n_, i = np.unique(lb, axis=0, return_index=True)\nif len(i) &lt; nl:  # duplicate row check\nlb = lb[i]  # remove duplicates\nif segments:\nsegments = [segments[x] for x in i]\nmsg = f'{prefix}WARNING \u26a0\ufe0f {im_file}: {nl - len(i)} duplicate labels removed'\nelse:\nne = 1  # label empty\nlb = np.zeros((0, (5 + nkpt * ndim)), dtype=np.float32) if keypoint else np.zeros(\n(0, 5), dtype=np.float32)\nelse:\nnm = 1  # label missing\nlb = np.zeros((0, (5 + nkpt * ndim)), dtype=np.float32) if keypoint else np.zeros((0, 5), dtype=np.float32)\nif keypoint:\nkeypoints = lb[:, 5:].reshape(-1, nkpt, ndim)\nif ndim == 2:\nkpt_mask = np.ones(keypoints.shape[:2], dtype=np.float32)\nkpt_mask = np.where(keypoints[..., 0] &lt; 0, 0.0, kpt_mask)\nkpt_mask = np.where(keypoints[..., 1] &lt; 0, 0.0, kpt_mask)\nkeypoints = np.concatenate([keypoints, kpt_mask[..., None]], axis=-1)  # (nl, nkpt, 3)\nlb = lb[:, :5]\nreturn im_file, lb, shape, segments, keypoints, nm, nf, ne, nc, msg\nexcept Exception as e:\nnc = 1\nmsg = f'{prefix}WARNING \u26a0\ufe0f {im_file}: ignoring corrupt image/label: {e}'\nreturn [None, None, None, None, None, nm, nf, ne, nc, msg]\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.polygon2mask","title":"<code>ultralytics.data.utils.polygon2mask(imgsz, polygons, color=1, downsample_ratio=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>imgsz</code> <code>tuple</code> <p>The image size.</p> required <code>polygons</code> <code>list[numpy.ndarray]</code> <p>[N, M], N is the number of polygons, M is the number of points(Be divided by 2).</p> required <code>color</code> <code>int</code> <p>color</p> <code>1</code> <code>downsample_ratio</code> <code>int</code> <p>downsample ratio</p> <code>1</code> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def polygon2mask(imgsz, polygons, color=1, downsample_ratio=1):\n\"\"\"\n    Args:\n        imgsz (tuple): The image size.\n        polygons (list[np.ndarray]): [N, M], N is the number of polygons, M is the number of points(Be divided by 2).\n        color (int): color\n        downsample_ratio (int): downsample ratio\n    \"\"\"\nmask = np.zeros(imgsz, dtype=np.uint8)\npolygons = np.asarray(polygons, dtype=np.int32)\npolygons = polygons.reshape((polygons.shape[0], -1, 2))\ncv2.fillPoly(mask, polygons, color=color)\nnh, nw = (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio)\n# NOTE: fillPoly first then resize is trying to keep the same way of loss calculation when mask-ratio=1.\nreturn cv2.resize(mask, (nw, nh))\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.polygons2masks","title":"<code>ultralytics.data.utils.polygons2masks(imgsz, polygons, color, downsample_ratio=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>imgsz</code> <code>tuple</code> <p>The image size.</p> required <code>polygons</code> <code>list[numpy.ndarray]</code> <p>each polygon is [N, M], N is number of polygons, M is number of points (M % 2 = 0)</p> required <code>color</code> <code>int</code> <p>color</p> required <code>downsample_ratio</code> <code>int</code> <p>downsample ratio</p> <code>1</code> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def polygons2masks(imgsz, polygons, color, downsample_ratio=1):\n\"\"\"\n    Args:\n        imgsz (tuple): The image size.\n        polygons (list[np.ndarray]): each polygon is [N, M], N is number of polygons, M is number of points (M % 2 = 0)\n        color (int): color\n        downsample_ratio (int): downsample ratio\n    \"\"\"\nreturn np.array([polygon2mask(imgsz, [x.reshape(-1)], color, downsample_ratio) for x in polygons])\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.polygons2masks_overlap","title":"<code>ultralytics.data.utils.polygons2masks_overlap(imgsz, segments, downsample_ratio=1)</code>","text":"<p>Return a (640, 640) overlap mask.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def polygons2masks_overlap(imgsz, segments, downsample_ratio=1):\n\"\"\"Return a (640, 640) overlap mask.\"\"\"\nmasks = np.zeros((imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio),\ndtype=np.int32 if len(segments) &gt; 255 else np.uint8)\nareas = []\nms = []\nfor si in range(len(segments)):\nmask = polygon2mask(imgsz, [segments[si].reshape(-1)], downsample_ratio=downsample_ratio, color=1)\nms.append(mask)\nareas.append(mask.sum())\nareas = np.asarray(areas)\nindex = np.argsort(-areas)\nms = np.array(ms)[index]\nfor i in range(len(segments)):\nmask = ms[i] * (i + 1)\nmasks = masks + mask\nmasks = np.clip(masks, a_min=0, a_max=i + 1)\nreturn masks, index\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.check_det_dataset","title":"<code>ultralytics.data.utils.check_det_dataset(dataset, autodownload=True)</code>","text":"<p>Download, verify, and/or unzip a dataset if not found locally.</p> <p>This function checks the availability of a specified dataset, and if not found, it has the option to download and unzip the dataset. It then reads and parses the accompanying YAML data, ensuring key requirements are met and also resolves paths related to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Path to the dataset or dataset descriptor (like a YAML file).</p> required <code>autodownload</code> <code>bool</code> <p>Whether to automatically download the dataset if not found. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Parsed dataset information and paths.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def check_det_dataset(dataset, autodownload=True):\n\"\"\"\n    Download, verify, and/or unzip a dataset if not found locally.\n    This function checks the availability of a specified dataset, and if not found, it has the option to download and\n    unzip the dataset. It then reads and parses the accompanying YAML data, ensuring key requirements are met and also\n    resolves paths related to the dataset.\n    Args:\n        dataset (str): Path to the dataset or dataset descriptor (like a YAML file).\n        autodownload (bool, optional): Whether to automatically download the dataset if not found. Defaults to True.\n    Returns:\n        (dict): Parsed dataset information and paths.\n    \"\"\"\ndata = check_file(dataset)\n# Download (optional)\nextract_dir = ''\nif isinstance(data, (str, Path)) and (zipfile.is_zipfile(data) or is_tarfile(data)):\nnew_dir = safe_download(data, dir=DATASETS_DIR, unzip=True, delete=False, curl=False)\ndata = next((DATASETS_DIR / new_dir).rglob('*.yaml'))\nextract_dir, autodownload = data.parent, False\n# Read YAML (optional)\nif isinstance(data, (str, Path)):\ndata = yaml_load(data, append_filename=True)  # dictionary\n# Checks\nfor k in 'train', 'val':\nif k not in data:\nif k == 'val' and 'validation' in data:\nLOGGER.info(\"WARNING \u26a0\ufe0f renaming data YAML 'validation' key to 'val' to match YOLO format.\")\ndata['val'] = data.pop('validation')  # replace 'validation' key with 'val' key\nelse:\nraise SyntaxError(\nemojis(f\"{dataset} '{k}:' key missing \u274c.\\n'train' and 'val' are required in all data YAMLs.\"))\nif 'names' not in data and 'nc' not in data:\nraise SyntaxError(emojis(f\"{dataset} key missing \u274c.\\n either 'names' or 'nc' are required in all data YAMLs.\"))\nif 'names' in data and 'nc' in data and len(data['names']) != data['nc']:\nraise SyntaxError(emojis(f\"{dataset} 'names' length {len(data['names'])} and 'nc: {data['nc']}' must match.\"))\nif 'names' not in data:\ndata['names'] = [f'class_{i}' for i in range(data['nc'])]\nelse:\ndata['nc'] = len(data['names'])\ndata['names'] = check_class_names(data['names'])\n# Resolve paths\npath = Path(extract_dir or data.get('path') or Path(data.get('yaml_file', '')).parent)  # dataset root\nif not path.is_absolute():\npath = (DATASETS_DIR / path).resolve()\ndata['path'] = path  # download scripts\nfor k in 'train', 'val', 'test':\nif data.get(k):  # prepend path\nif isinstance(data[k], str):\nx = (path / data[k]).resolve()\nif not x.exists() and data[k].startswith('../'):\nx = (path / data[k][3:]).resolve()\ndata[k] = str(x)\nelse:\ndata[k] = [str((path / x).resolve()) for x in data[k]]\n# Parse YAML\ntrain, val, test, s = (data.get(x) for x in ('train', 'val', 'test', 'download'))\nif val:\nval = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\nif not all(x.exists() for x in val):\nname = clean_url(dataset)  # dataset name with URL auth stripped\nm = f\"\\nDataset '{name}' images not found \u26a0\ufe0f, missing path '{[x for x in val if not x.exists()][0]}'\"\nif s and autodownload:\nLOGGER.warning(m)\nelse:\nm += f\"\\nNote dataset download directory is '{DATASETS_DIR}'. You can update this in '{SETTINGS_YAML}'\"\nraise FileNotFoundError(m)\nt = time.time()\nr = None  # success\nif s.startswith('http') and s.endswith('.zip'):  # URL\nsafe_download(url=s, dir=DATASETS_DIR, delete=True)\nelif s.startswith('bash '):  # bash script\nLOGGER.info(f'Running {s} ...')\nr = os.system(s)\nelse:  # python script\nexec(s, {'yaml': data})\ndt = f'({round(time.time() - t, 1)}s)'\ns = f\"success \u2705 {dt}, saved to {colorstr('bold', DATASETS_DIR)}\" if r in (0, None) else f'failure {dt} \u274c'\nLOGGER.info(f'Dataset download {s}\\n')\ncheck_font('Arial.ttf' if is_ascii(data['names']) else 'Arial.Unicode.ttf')  # download fonts\nreturn data  # dictionary\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.check_cls_dataset","title":"<code>ultralytics.data.utils.check_cls_dataset(dataset, split='')</code>","text":"<p>Checks a classification dataset such as Imagenet.</p> <p>This function accepts a <code>dataset</code> name and attempts to retrieve the corresponding dataset information. If the dataset is not found locally, it attempts to download the dataset from the internet and save it locally.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>The name of the dataset.</p> required <code>split</code> <code>str</code> <p>The split of the dataset. Either 'val', 'test', or ''. Defaults to ''.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: - 'train' (Path): The directory path containing the training set of the dataset. - 'val' (Path): The directory path containing the validation set of the dataset. - 'test' (Path): The directory path containing the test set of the dataset. - 'nc' (int): The number of classes in the dataset. - 'names' (dict): A dictionary of class names in the dataset.</p> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def check_cls_dataset(dataset: str, split=''):\n\"\"\"\n    Checks a classification dataset such as Imagenet.\n    This function accepts a `dataset` name and attempts to retrieve the corresponding dataset information.\n    If the dataset is not found locally, it attempts to download the dataset from the internet and save it locally.\n    Args:\n        dataset (str): The name of the dataset.\n        split (str, optional): The split of the dataset. Either 'val', 'test', or ''. Defaults to ''.\n    Returns:\n        (dict): A dictionary containing the following keys:\n            - 'train' (Path): The directory path containing the training set of the dataset.\n            - 'val' (Path): The directory path containing the validation set of the dataset.\n            - 'test' (Path): The directory path containing the test set of the dataset.\n            - 'nc' (int): The number of classes in the dataset.\n            - 'names' (dict): A dictionary of class names in the dataset.\n    \"\"\"\ndataset = Path(dataset)\ndata_dir = (dataset if dataset.is_dir() else (DATASETS_DIR / dataset)).resolve()\nif not data_dir.is_dir():\nLOGGER.warning(f'\\nDataset not found \u26a0\ufe0f, missing path {data_dir}, attempting download...')\nt = time.time()\nif str(dataset) == 'imagenet':\nsubprocess.run(f\"bash {ROOT / 'data/scripts/get_imagenet.sh'}\", shell=True, check=True)\nelse:\nurl = f'https://github.com/ultralytics/yolov5/releases/download/v1.0/{dataset}.zip'\ndownload(url, dir=data_dir.parent)\ns = f\"Dataset download success \u2705 ({time.time() - t:.1f}s), saved to {colorstr('bold', data_dir)}\\n\"\nLOGGER.info(s)\ntrain_set = data_dir / 'train'\nval_set = data_dir / 'val' if (data_dir / 'val').exists() else data_dir / 'validation' if (\ndata_dir / 'validation').exists() else None  # data/test or data/val\ntest_set = data_dir / 'test' if (data_dir / 'test').exists() else None  # data/val or data/test\nif split == 'val' and not val_set:\nLOGGER.warning(\"WARNING \u26a0\ufe0f Dataset 'split=val' not found, using 'split=test' instead.\")\nelif split == 'test' and not test_set:\nLOGGER.warning(\"WARNING \u26a0\ufe0f Dataset 'split=test' not found, using 'split=val' instead.\")\nnc = len([x for x in (data_dir / 'train').glob('*') if x.is_dir()])  # number of classes\nnames = [x.name for x in (data_dir / 'train').iterdir() if x.is_dir()]  # class names list\nnames = dict(enumerate(sorted(names)))\n# Print to console\nfor k, v in {'train': train_set, 'val': val_set, 'test': test_set}.items():\nprefix = f'{colorstr(k)} {v}...'\nif v is None:\nLOGGER.info(prefix)\nelse:\nfiles = [path for path in v.rglob('*.*') if path.suffix[1:].lower() in IMG_FORMATS]\nnf = len(files)  # number of files\nnd = len({file.parent for file in files})  # number of directories\nif nf == 0:\nif k == 'train':\nraise FileNotFoundError(emojis(f\"{dataset} '{k}:' no training images found \u274c \"))\nelse:\nLOGGER.warning(f'{prefix} found {nf} images in {nd} classes: WARNING \u26a0\ufe0f no images found')\nelif nd != nc:\nLOGGER.warning(f'{prefix} found {nf} images in {nd} classes: ERROR \u274c\ufe0f requires {nc} classes, not {nd}')\nelse:\nLOGGER.info(f'{prefix} found {nf} images in {nd} classes \u2705 ')\nreturn {'train': train_set, 'val': val_set or test_set, 'test': test_set or val_set, 'nc': nc, 'names': names}\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.compress_one_image","title":"<code>ultralytics.data.utils.compress_one_image(f, f_new=None, max_dim=1920, quality=50)</code>","text":"<p>Compresses a single image file to reduced size while preserving its aspect ratio and quality using either the Python Imaging Library (PIL) or OpenCV library. If the input image is smaller than the maximum dimension, it will not be resized.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>The path to the input image file.</p> required <code>f_new</code> <code>str</code> <p>The path to the output image file. If not specified, the input file will be overwritten.</p> <code>None</code> <code>max_dim</code> <code>int</code> <p>The maximum dimension (width or height) of the output image. Default is 1920 pixels.</p> <code>1920</code> <code>quality</code> <code>int</code> <p>The image compression quality as a percentage. Default is 50%.</p> <code>50</code> Example <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfor f in Path('path/to/dataset').rglob('*.jpg'):\ncompress_one_image(f)\n</code></pre> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def compress_one_image(f, f_new=None, max_dim=1920, quality=50):\n\"\"\"\n    Compresses a single image file to reduced size while preserving its aspect ratio and quality using either the\n    Python Imaging Library (PIL) or OpenCV library. If the input image is smaller than the maximum dimension, it will\n    not be resized.\n    Args:\n        f (str): The path to the input image file.\n        f_new (str, optional): The path to the output image file. If not specified, the input file will be overwritten.\n        max_dim (int, optional): The maximum dimension (width or height) of the output image. Default is 1920 pixels.\n        quality (int, optional): The image compression quality as a percentage. Default is 50%.\n    Example:\n        ```python\n        from pathlib import Path\n        from ultralytics.data.utils import compress_one_image\n        for f in Path('path/to/dataset').rglob('*.jpg'):\n            compress_one_image(f)\n        ```\n    \"\"\"\ntry:  # use PIL\nim = Image.open(f)\nr = max_dim / max(im.height, im.width)  # ratio\nif r &lt; 1.0:  # image too large\nim = im.resize((int(im.width * r), int(im.height * r)))\nim.save(f_new or f, 'JPEG', quality=quality, optimize=True)  # save\nexcept Exception as e:  # use OpenCV\nLOGGER.info(f'WARNING \u26a0\ufe0f HUB ops PIL failure {f}: {e}')\nim = cv2.imread(f)\nim_height, im_width = im.shape[:2]\nr = max_dim / max(im_height, im_width)  # ratio\nif r &lt; 1.0:  # image too large\nim = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)\ncv2.imwrite(str(f_new or f), im)\n</code></pre>"},{"location":"reference/data/utils/#ultralytics.data.utils.autosplit","title":"<code>ultralytics.data.utils.autosplit(path=DATASETS_DIR / 'coco8/images', weights=(0.9, 0.1, 0.0), annotated_only=False)</code>","text":"<p>Automatically split a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to images directory. Defaults to DATASETS_DIR / 'coco8/images'.</p> <code>DATASETS_DIR / 'coco8/images'</code> <code>weights</code> <code>list | tuple</code> <p>Train, validation, and test split fractions. Defaults to (0.9, 0.1, 0.0).</p> <code>(0.9, 0.1, 0.0)</code> <code>annotated_only</code> <code>bool</code> <p>If True, only images with an associated txt file are used. Defaults to False.</p> <code>False</code> Example <pre><code>from ultralytics.data.utils import autosplit\nautosplit()\n</code></pre> Source code in <code>ultralytics/data/utils.py</code> <pre><code>def autosplit(path=DATASETS_DIR / 'coco8/images', weights=(0.9, 0.1, 0.0), annotated_only=False):\n\"\"\"\n    Automatically split a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.\n    Args:\n        path (Path, optional): Path to images directory. Defaults to DATASETS_DIR / 'coco8/images'.\n        weights (list | tuple, optional): Train, validation, and test split fractions. Defaults to (0.9, 0.1, 0.0).\n        annotated_only (bool, optional): If True, only images with an associated txt file are used. Defaults to False.\n    Example:\n        ```python\n        from ultralytics.data.utils import autosplit\n        autosplit()\n        ```\n    \"\"\"\npath = Path(path)  # images dir\nfiles = sorted(x for x in path.rglob('*.*') if x.suffix[1:].lower() in IMG_FORMATS)  # image files only\nn = len(files)  # number of files\nrandom.seed(0)  # for reproducibility\nindices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\ntxt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\nfor x in txt:\nif (path.parent / x).exists():\n(path.parent / x).unlink()  # remove existing\nLOGGER.info(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\nfor i, img in tqdm(zip(indices, files), total=n):\nif not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\nwith open(path.parent / txt[i], 'a') as f:\nf.write(f'./{img.relative_to(path.parent).as_posix()}' + '\\n')  # add image to txt file\n</code></pre>"},{"location":"reference/engine/exporter/","title":"Reference for <code>ultralytics/engine/exporter.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/exporter.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter","title":"<code>ultralytics.engine.exporter.Exporter</code>","text":"<p>A class for exporting a model.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the exporter.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>class Exporter:\n\"\"\"\n    A class for exporting a model.\n    Attributes:\n        args (SimpleNamespace): Configuration for the exporter.\n        save_dir (Path): Directory to save results.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n        Initializes the Exporter class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n            _callbacks (list, optional): List of callback functions. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\ncallbacks.add_integration_callbacks(self)\n@smart_inference_mode()\ndef __call__(self, model=None):\n\"\"\"Returns list of exported files/dirs after running callbacks.\"\"\"\nself.run_callbacks('on_export_start')\nt = time.time()\nformat = self.args.format.lower()  # to lowercase\nif format in ('tensorrt', 'trt'):  # 'engine' aliases\nformat = 'engine'\nif format in ('mlmodel', 'mlpackage', 'mlprogram', 'apple', 'ios'):  # 'coreml' aliases\nformat = 'coreml'\nfmts = tuple(export_formats()['Argument'][1:])  # available export formats\nflags = [x == format for x in fmts]\nif sum(flags) != 1:\nraise ValueError(f\"Invalid export format='{format}'. Valid formats are {fmts}\")\njit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, ncnn = flags  # export booleans\n# Load PyTorch model\nself.device = select_device('cpu' if self.args.device is None else self.args.device)\n# Checks\nmodel.names = check_class_names(model.names)\nif self.args.half and onnx and self.device.type == 'cpu':\nLOGGER.warning('WARNING \u26a0\ufe0f half=True only compatible with GPU export, i.e. use device=0')\nself.args.half = False\nassert not self.args.dynamic, 'half=True not compatible with dynamic=True, i.e. use only one.'\nself.imgsz = check_imgsz(self.args.imgsz, stride=model.stride, min_dim=2)  # check image size\nif self.args.optimize:\nassert not ncnn, \"optimize=True not compatible with format='ncnn', i.e. use optimize=False\"\nassert self.device.type == 'cpu', \"optimize=True not compatible with cuda devices, i.e. use device='cpu'\"\nif edgetpu and not LINUX:\nraise SystemError('Edge TPU export only supported on Linux. See https://coral.ai/docs/edgetpu/compiler/')\n# Input\nim = torch.zeros(self.args.batch, 3, *self.imgsz).to(self.device)\nfile = Path(\ngetattr(model, 'pt_path', None) or getattr(model, 'yaml_file', None) or model.yaml.get('yaml_file', ''))\nif file.suffix in ('.yaml', '.yml'):\nfile = Path(file.name)\n# Update model\nmodel = deepcopy(model).to(self.device)\nfor p in model.parameters():\np.requires_grad = False\nmodel.eval()\nmodel.float()\nmodel = model.fuse()\nfor k, m in model.named_modules():\nif isinstance(m, (Detect, RTDETRDecoder)):  # Segment and Pose use Detect base class\nm.dynamic = self.args.dynamic\nm.export = True\nm.format = self.args.format\nelif isinstance(m, C2f) and not any((saved_model, pb, tflite, edgetpu, tfjs)):\n# EdgeTPU does not support FlexSplitV while split provides cleaner ONNX graph\nm.forward = m.forward_split\ny = None\nfor _ in range(2):\ny = model(im)  # dry runs\nif self.args.half and (engine or onnx) and self.device.type != 'cpu':\nim, model = im.half(), model.half()  # to FP16\n# Filter warnings\nwarnings.filterwarnings('ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\nwarnings.filterwarnings('ignore', category=UserWarning)  # suppress shape prim::Constant missing ONNX warning\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # suppress CoreML np.bool deprecation warning\n# Assign\nself.im = im\nself.model = model\nself.file = file\nself.output_shape = tuple(y.shape) if isinstance(y, torch.Tensor) else \\\n            tuple(tuple(x.shape if isinstance(x, torch.Tensor) else []) for x in y)\nself.pretty_name = Path(self.model.yaml.get('yaml_file', self.file)).stem.replace('yolo', 'YOLO')\ndata = model.args['data'] if hasattr(model, 'args') and isinstance(model.args, dict) else ''\ndescription = f'Ultralytics {self.pretty_name} model {f\"trained on {data}\" if data else \"\"}'\nself.metadata = {\n'description': description,\n'author': 'Ultralytics',\n'license': 'AGPL-3.0 https://ultralytics.com/license',\n'date': datetime.now().isoformat(),\n'version': __version__,\n'stride': int(max(model.stride)),\n'task': model.task,\n'batch': self.args.batch,\n'imgsz': self.imgsz,\n'names': model.names}  # model metadata\nif model.task == 'pose':\nself.metadata['kpt_shape'] = model.model[-1].kpt_shape\nLOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from '{file}' with input shape {tuple(im.shape)} BCHW and \"\nf'output shape(s) {self.output_shape} ({file_size(file):.1f} MB)')\n# Exports\nf = [''] * len(fmts)  # exported filenames\nif jit or ncnn:  # TorchScript\nf[0], _ = self.export_torchscript()\nif engine:  # TensorRT required before ONNX\nf[1], _ = self.export_engine()\nif onnx or xml:  # OpenVINO requires ONNX\nf[2], _ = self.export_onnx()\nif xml:  # OpenVINO\nf[3], _ = self.export_openvino()\nif coreml:  # CoreML\nf[4], _ = self.export_coreml()\nif any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\nself.args.int8 |= edgetpu\nf[5], keras_model = self.export_saved_model()\nif pb or tfjs:  # pb prerequisite to tfjs\nf[6], _ = self.export_pb(keras_model=keras_model)\nif tflite:\nf[7], _ = self.export_tflite(keras_model=keras_model, nms=False, agnostic_nms=self.args.agnostic_nms)\nif edgetpu:\nf[8], _ = self.export_edgetpu(tflite_model=Path(f[5]) / f'{self.file.stem}_full_integer_quant.tflite')\nif tfjs:\nf[9], _ = self.export_tfjs()\nif paddle:  # PaddlePaddle\nf[10], _ = self.export_paddle()\nif ncnn:  # ncnn\nf[11], _ = self.export_ncnn()\n# Finish\nf = [str(x) for x in f if x]  # filter out '' and None\nif any(f):\nf = str(Path(f[-1]))\nsquare = self.imgsz[0] == self.imgsz[1]\ns = '' if square else f\"WARNING \u26a0\ufe0f non-PyTorch val requires square images, 'imgsz={self.imgsz}' will not \" \\\n                                  f\"work. Use export 'imgsz={max(self.imgsz)}' if val is required.\"\nimgsz = self.imgsz[0] if square else str(self.imgsz)[1:-1].replace(' ', '')\npredict_data = f'data={data}' if model.task == 'segment' and format == 'pb' else ''\nLOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\nf\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\nf'\\nPredict:         yolo predict task={model.task} model={f} imgsz={imgsz} {predict_data}'\nf'\\nValidate:        yolo val task={model.task} model={f} imgsz={imgsz} data={data} {s}'\nf'\\nVisualize:       https://netron.app')\nself.run_callbacks('on_export_end')\nreturn f  # return list of exported files/dirs\n@try_export\ndef export_torchscript(self, prefix=colorstr('TorchScript:')):\n\"\"\"YOLOv8 TorchScript model export.\"\"\"\nLOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\nf = self.file.with_suffix('.torchscript')\nts = torch.jit.trace(self.model, self.im, strict=False)\nextra_files = {'config.txt': json.dumps(self.metadata)}  # torch._C.ExtraFilesMap()\nif self.args.optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\nLOGGER.info(f'{prefix} optimizing for mobile...')\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\noptimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\nelse:\nts.save(str(f), _extra_files=extra_files)\nreturn f, None\n@try_export\ndef export_onnx(self, prefix=colorstr('ONNX:')):\n\"\"\"YOLOv8 ONNX export.\"\"\"\nrequirements = ['onnx&gt;=1.12.0']\nif self.args.simplify:\nrequirements += ['onnxsim&gt;=0.4.33', 'onnxruntime-gpu' if torch.cuda.is_available() else 'onnxruntime']\ncheck_requirements(requirements)\nimport onnx  # noqa\nopset_version = self.args.opset or get_latest_opset()\nLOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__} opset {opset_version}...')\nf = str(self.file.with_suffix('.onnx'))\noutput_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']\ndynamic = self.args.dynamic\nif dynamic:\ndynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\nif isinstance(self.model, SegmentationModel):\ndynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)\ndynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\nelif isinstance(self.model, DetectionModel):\ndynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)\ntorch.onnx.export(\nself.model.cpu() if dynamic else self.model,  # dynamic=True only compatible with cpu\nself.im.cpu() if dynamic else self.im,\nf,\nverbose=False,\nopset_version=opset_version,\ndo_constant_folding=True,  # WARNING: DNN inference with torch&gt;=1.12 may require do_constant_folding=False\ninput_names=['images'],\noutput_names=output_names,\ndynamic_axes=dynamic or None)\n# Checks\nmodel_onnx = onnx.load(f)  # load onnx model\n# onnx.checker.check_model(model_onnx)  # check onnx model\n# Simplify\nif self.args.simplify:\ntry:\nimport onnxsim\nLOGGER.info(f'{prefix} simplifying with onnxsim {onnxsim.__version__}...')\n# subprocess.run(f'onnxsim \"{f}\" \"{f}\"', shell=True)\nmodel_onnx, check = onnxsim.simplify(model_onnx)\nassert check, 'Simplified ONNX model could not be validated'\nexcept Exception as e:\nLOGGER.info(f'{prefix} simplifier failure: {e}')\n# Metadata\nfor k, v in self.metadata.items():\nmeta = model_onnx.metadata_props.add()\nmeta.key, meta.value = k, str(v)\nonnx.save(model_onnx, f)\nreturn f, model_onnx\n@try_export\ndef export_openvino(self, prefix=colorstr('OpenVINO:')):\n\"\"\"YOLOv8 OpenVINO export.\"\"\"\ncheck_requirements('openvino-dev&gt;=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nimport openvino.runtime as ov  # noqa\nfrom openvino.tools import mo  # noqa\nLOGGER.info(f'\\n{prefix} starting export with openvino {ov.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_openvino_model{os.sep}')\nf_onnx = self.file.with_suffix('.onnx')\nf_ov = str(Path(f) / self.file.with_suffix('.xml').name)\nov_model = mo.convert_model(f_onnx,\nmodel_name=self.pretty_name,\nframework='onnx',\ncompress_to_fp16=self.args.half)  # export\n# Set RT info\nov_model.set_rt_info('YOLOv8', ['model_info', 'model_type'])\nov_model.set_rt_info(True, ['model_info', 'reverse_input_channels'])\nov_model.set_rt_info(114, ['model_info', 'pad_value'])\nov_model.set_rt_info([255.0], ['model_info', 'scale_values'])\nov_model.set_rt_info(self.args.iou, ['model_info', 'iou_threshold'])\nov_model.set_rt_info([v.replace(' ', '_') for k, v in sorted(self.model.names.items())],\n['model_info', 'labels'])\nif self.model.task != 'classify':\nov_model.set_rt_info('fit_to_window_letterbox', ['model_info', 'resize_type'])\nov.serialize(ov_model, f_ov)  # save\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\n@try_export\ndef export_paddle(self, prefix=colorstr('PaddlePaddle:')):\n\"\"\"YOLOv8 Paddle export.\"\"\"\ncheck_requirements(('paddlepaddle', 'x2paddle'))\nimport x2paddle  # noqa\nfrom x2paddle.convert import pytorch2paddle  # noqa\nLOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_paddle_model{os.sep}')\npytorch2paddle(module=self.model, save_dir=f, jit_type='trace', input_examples=[self.im])  # export\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\n@try_export\ndef export_ncnn(self, prefix=colorstr('ncnn:')):\n\"\"\"\n        YOLOv8 ncnn export using PNNX https://github.com/pnnx/pnnx.\n        \"\"\"\ncheck_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn\nimport ncnn  # noqa\nLOGGER.info(f'\\n{prefix} starting export with ncnn {ncnn.__version__}...')\nf = Path(str(self.file).replace(self.file.suffix, f'_ncnn_model{os.sep}'))\nf_ts = self.file.with_suffix('.torchscript')\npnnx_filename = 'pnnx.exe' if WINDOWS else 'pnnx'\nif Path(pnnx_filename).is_file():\npnnx = pnnx_filename\nelif (ROOT / pnnx_filename).is_file():\npnnx = ROOT / pnnx_filename\nelse:\nLOGGER.warning(\nf'{prefix} WARNING \u26a0\ufe0f PNNX not found. Attempting to download binary file from '\n'https://github.com/pnnx/pnnx/.\\nNote PNNX Binary file must be placed in current working directory '\nf'or in {ROOT}. See PNNX repo for full installation instructions.')\n_, assets = get_github_assets(repo='pnnx/pnnx', retry=True)\nsystem = 'macos' if MACOS else 'ubuntu' if LINUX else 'windows'  # operating system\nasset = [x for x in assets if system in x][0] if assets else \\\n                f'https://github.com/pnnx/pnnx/releases/download/20230816/pnnx-20230816-{system}.zip'  # fallback\nattempt_download_asset(asset, repo='pnnx/pnnx', release='latest')\nunzip_dir = Path(asset).with_suffix('')\npnnx = ROOT / pnnx_filename  # new location\n(unzip_dir / pnnx_filename).rename(pnnx)  # move binary to ROOT\nshutil.rmtree(unzip_dir)  # delete unzip dir\nPath(asset).unlink()  # delete zip\npnnx.chmod(0o777)  # set read, write, and execute permissions for everyone\nuse_ncnn = True\nncnn_args = [\nf'ncnnparam={f / \"model.ncnn.param\"}',\nf'ncnnbin={f / \"model.ncnn.bin\"}',\nf'ncnnpy={f / \"model_ncnn.py\"}', ] if use_ncnn else []\nuse_pnnx = False\npnnx_args = [\nf'pnnxparam={f / \"model.pnnx.param\"}',\nf'pnnxbin={f / \"model.pnnx.bin\"}',\nf'pnnxpy={f / \"model_pnnx.py\"}',\nf'pnnxonnx={f / \"model.pnnx.onnx\"}', ] if use_pnnx else []\ncmd = [\nstr(pnnx),\nstr(f_ts),\n*ncnn_args,\n*pnnx_args,\nf'fp16={int(self.args.half)}',\nf'device={self.device.type}',\nf'inputshape=\"{[self.args.batch, 3, *self.imgsz]}\"', ]\nf.mkdir(exist_ok=True)  # make ncnn_model directory\nLOGGER.info(f\"{prefix} running '{' '.join(cmd)}'\")\nsubprocess.run(cmd, check=True)\nfor f_debug in 'debug.bin', 'debug.param', 'debug2.bin', 'debug2.param':  # remove debug files\nPath(f_debug).unlink(missing_ok=True)\nyaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn str(f), None\n@try_export\ndef export_coreml(self, prefix=colorstr('CoreML:')):\n\"\"\"YOLOv8 CoreML export.\"\"\"\nmlmodel = self.args.format.lower() == 'mlmodel'  # legacy *.mlmodel export format requested\ncheck_requirements('coremltools&gt;=6.0,&lt;=6.2' if mlmodel else 'coremltools&gt;=7.0.b1')\nimport coremltools as ct  # noqa\nLOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\nf = self.file.with_suffix('.mlmodel' if mlmodel else '.mlpackage')\nif f.is_dir():\nshutil.rmtree(f)\nbias = [0.0, 0.0, 0.0]\nscale = 1 / 255\nclassifier_config = None\nif self.model.task == 'classify':\nclassifier_config = ct.ClassifierConfig(list(self.model.names.values())) if self.args.nms else None\nmodel = self.model\nelif self.model.task == 'detect':\nmodel = IOSDetectModel(self.model, self.im) if self.args.nms else self.model\nelse:\nif self.args.nms:\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f 'nms=True' is only available for Detect models like 'yolov8n.pt'.\")\n# TODO CoreML Segment and Pose model pipelining\nmodel = self.model\nts = torch.jit.trace(model.eval(), self.im, strict=False)  # TorchScript model\nct_model = ct.convert(ts,\ninputs=[ct.ImageType('image', shape=self.im.shape, scale=scale, bias=bias)],\nclassifier_config=classifier_config,\nconvert_to='neuralnetwork' if mlmodel else 'mlprogram')\nbits, mode = (8, 'kmeans') if self.args.int8 else (16, 'linear') if self.args.half else (32, None)\nif bits &lt; 32:\nif 'kmeans' in mode:\ncheck_requirements('scikit-learn')  # scikit-learn package required for k-means quantization\nif mlmodel:\nct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\nelse:\nimport coremltools.optimize.coreml as cto\nop_config = cto.OpPalettizerConfig(mode=mode, nbits=bits, weight_threshold=512)\nconfig = cto.OptimizationConfig(global_config=op_config)\nct_model = cto.palettize_weights(ct_model, config=config)\nif self.args.nms and self.model.task == 'detect':\nif mlmodel:\nimport platform\n# coremltools&lt;=6.2 NMS export requires Python&lt;3.11\ncheck_version(platform.python_version(), '&lt;3.11', name='Python ', hard=True)\nweights_dir = None\nelse:\nct_model.save(str(f))  # save otherwise weights_dir does not exist\nweights_dir = str(f / 'Data/com.apple.CoreML/weights')\nct_model = self._pipeline_coreml(ct_model, weights_dir=weights_dir)\nm = self.metadata  # metadata dict\nct_model.short_description = m.pop('description')\nct_model.author = m.pop('author')\nct_model.license = m.pop('license')\nct_model.version = m.pop('version')\nct_model.user_defined_metadata.update({k: str(v) for k, v in m.items()})\ntry:\nct_model.save(str(f))  # save *.mlpackage\nexcept Exception as e:\nLOGGER.warning(\nf'{prefix} WARNING \u26a0\ufe0f CoreML export to *.mlpackage failed ({e}), reverting to *.mlmodel export. '\nf'Known coremltools Python 3.11 and Windows bugs https://github.com/apple/coremltools/issues/1928.')\nf = f.with_suffix('.mlmodel')\nct_model.save(str(f))\nreturn f, ct_model\n@try_export\ndef export_engine(self, prefix=colorstr('TensorRT:')):\n\"\"\"YOLOv8 TensorRT export https://developer.nvidia.com/tensorrt.\"\"\"\nassert self.im.device.type != 'cpu', \"export running on CPU but must be on GPU, i.e. use 'device=0'\"\ntry:\nimport tensorrt as trt  # noqa\nexcept ImportError:\nif LINUX:\ncheck_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\nimport tensorrt as trt  # noqa\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nself.args.simplify = True\nf_onnx, _ = self.export_onnx()\nLOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\nassert Path(f_onnx).exists(), f'failed to export ONNX file: {f_onnx}'\nf = self.file.with_suffix('.engine')  # TensorRT engine file\nlogger = trt.Logger(trt.Logger.INFO)\nif self.args.verbose:\nlogger.min_severity = trt.Logger.Severity.VERBOSE\nbuilder = trt.Builder(logger)\nconfig = builder.create_builder_config()\nconfig.max_workspace_size = self.args.workspace * 1 &lt;&lt; 30\n# config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace &lt;&lt; 30)  # fix TRT 8.4 deprecation notice\nflag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nnetwork = builder.create_network(flag)\nparser = trt.OnnxParser(network, logger)\nif not parser.parse_from_file(f_onnx):\nraise RuntimeError(f'failed to load ONNX file: {f_onnx}')\ninputs = [network.get_input(i) for i in range(network.num_inputs)]\noutputs = [network.get_output(i) for i in range(network.num_outputs)]\nfor inp in inputs:\nLOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\nfor out in outputs:\nLOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\nif self.args.dynamic:\nshape = self.im.shape\nif shape[0] &lt;= 1:\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f 'dynamic=True' model requires max batch size, i.e. 'batch=16'\")\nprofile = builder.create_optimization_profile()\nfor inp in inputs:\nprofile.set_shape(inp.name, (1, *shape[1:]), (max(1, shape[0] // 2), *shape[1:]), shape)\nconfig.add_optimization_profile(profile)\nLOGGER.info(\nf'{prefix} building FP{16 if builder.platform_has_fast_fp16 and self.args.half else 32} engine as {f}')\nif builder.platform_has_fast_fp16 and self.args.half:\nconfig.set_flag(trt.BuilderFlag.FP16)\n# Write file\nwith builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n# Metadata\nmeta = json.dumps(self.metadata)\nt.write(len(meta).to_bytes(4, byteorder='little', signed=True))\nt.write(meta.encode())\n# Model\nt.write(engine.serialize())\nreturn f, None\n@try_export\ndef export_saved_model(self, prefix=colorstr('TensorFlow SavedModel:')):\n\"\"\"YOLOv8 TensorFlow SavedModel export.\"\"\"\ncuda = torch.cuda.is_available()\ntry:\nimport tensorflow as tf  # noqa\nexcept ImportError:\ncheck_requirements(f\"tensorflow{'-macos' if MACOS else '-aarch64' if ARM64 else '' if cuda else '-cpu'}\")\nimport tensorflow as tf  # noqa\ncheck_requirements(\n('onnx', 'onnx2tf&gt;=1.15.4', 'sng4onnx&gt;=1.0.1', 'onnxsim&gt;=0.4.33', 'onnx_graphsurgeon&gt;=0.3.26',\n'tflite_support', 'onnxruntime-gpu' if cuda else 'onnxruntime'),\ncmds='--extra-index-url https://pypi.ngc.nvidia.com')  # onnx_graphsurgeon only on NVIDIA\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = Path(str(self.file).replace(self.file.suffix, '_saved_model'))\nif f.is_dir():\nimport shutil\nshutil.rmtree(f)  # delete output folder\n# Export to ONNX\nself.args.simplify = True\nf_onnx, _ = self.export_onnx()\n# Export to TF\ntmp_file = f / 'tmp_tflite_int8_calibration_images.npy'  # int8 calibration images file\nif self.args.int8:\nverbosity = '--verbosity info'\nif self.args.data:\nimport numpy as np\nfrom ultralytics.data.dataset import YOLODataset\nfrom ultralytics.data.utils import check_det_dataset\n# Generate calibration data for integer quantization\nLOGGER.info(f\"{prefix} collecting INT8 calibration images from 'data={self.args.data}'\")\ndata = check_det_dataset(self.args.data)\ndataset = YOLODataset(data['val'], data=data, imgsz=self.imgsz[0], augment=False)\nimages = []\nn_images = 100  # maximum number of images\nfor n, batch in enumerate(dataset):\nif n &gt;= n_images:\nbreak\nim = batch['img'].permute(1, 2, 0)[None]  # list to nparray, CHW to BHWC\nimages.append(im)\nf.mkdir()\nimages = torch.cat(images, 0).float()\n# mean = images.view(-1, 3).mean(0)  # imagenet mean [123.675, 116.28, 103.53]\n# std = images.view(-1, 3).std(0)  # imagenet std [58.395, 57.12, 57.375]\nnp.save(str(tmp_file), images.numpy())  # BHWC\nint8 = f'-oiqt -qt per-tensor -cind images \"{tmp_file}\" \"[[[[0, 0, 0]]]]\" \"[[[[255, 255, 255]]]]\"'\nelse:\nint8 = '-oiqt -qt per-tensor'\nelse:\nverbosity = '--non_verbose'\nint8 = ''\ncmd = f'onnx2tf -i \"{f_onnx}\" -o \"{f}\" -nuo {verbosity} {int8}'.strip()\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nyaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml\n# Remove/rename TFLite models\nif self.args.int8:\ntmp_file.unlink(missing_ok=True)\nfor file in f.rglob('*_dynamic_range_quant.tflite'):\nfile.rename(file.with_name(file.stem.replace('_dynamic_range_quant', '_int8') + file.suffix))\nfor file in f.rglob('*_integer_quant_with_int16_act.tflite'):\nfile.unlink()  # delete extra fp16 activation TFLite files\n# Add TFLite metadata\nfor file in f.rglob('*.tflite'):\nf.unlink() if 'quant_with_int16_act.tflite' in str(f) else self._add_tflite_metadata(file)\nreturn str(f), tf.saved_model.load(f, tags=None, options=None)  # load saved_model as Keras model\n@try_export\ndef export_pb(self, keras_model, prefix=colorstr('TensorFlow GraphDef:')):\n\"\"\"YOLOv8 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow.\"\"\"\nimport tensorflow as tf  # noqa\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = self.file.with_suffix('.pb')\nm = tf.function(lambda x: keras_model(x))  # full model\nm = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\nfrozen_func = convert_variables_to_constants_v2(m)\nfrozen_func.graph.as_graph_def()\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\nreturn f, None\n@try_export\ndef export_tflite(self, keras_model, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n\"\"\"YOLOv8 TensorFlow Lite export.\"\"\"\nimport tensorflow as tf  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nsaved_model = Path(str(self.file).replace(self.file.suffix, '_saved_model'))\nif self.args.int8:\nf = saved_model / f'{self.file.stem}_int8.tflite'  # fp32 in/out\nelif self.args.half:\nf = saved_model / f'{self.file.stem}_float16.tflite'  # fp32 in/out\nelse:\nf = saved_model / f'{self.file.stem}_float32.tflite'\nreturn str(f), None\n@try_export\ndef export_edgetpu(self, tflite_model='', prefix=colorstr('Edge TPU:')):\n\"\"\"YOLOv8 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/.\"\"\"\nLOGGER.warning(f'{prefix} WARNING \u26a0\ufe0f Edge TPU known bug https://github.com/ultralytics/ultralytics/issues/1185')\ncmd = 'edgetpu_compiler --version'\nhelp_url = 'https://coral.ai/docs/edgetpu/compiler/'\nassert LINUX, f'export only supported on Linux. See {help_url}'\nif subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True).returncode != 0:\nLOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\nsudo = subprocess.run('sudo --version &gt;/dev/null', shell=True).returncode == 0  # sudo installed on system\nfor c in (\n'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\nsubprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\nver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\nLOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\nf = str(tflite_model).replace('.tflite', '_edgetpu.tflite')  # Edge TPU model\ncmd = f'edgetpu_compiler -s -d -k 10 --out_dir \"{Path(f).parent}\" \"{tflite_model}\"'\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nself._add_tflite_metadata(f)\nreturn f, None\n@try_export\ndef export_tfjs(self, prefix=colorstr('TensorFlow.js:')):\n\"\"\"YOLOv8 TensorFlow.js export.\"\"\"\ncheck_requirements('tensorflowjs')\nimport tensorflow as tf\nimport tensorflowjs as tfjs  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\nf = str(self.file).replace(self.file.suffix, '_web_model')  # js dir\nf_pb = str(self.file.with_suffix('.pb'))  # *.pb path\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(f_pb, 'rb') as file:\ngd.ParseFromString(file.read())\noutputs = ','.join(gd_outputs(gd))\nLOGGER.info(f'\\n{prefix} output node names: {outputs}')\nwith spaces_in_path(f_pb) as fpb_, spaces_in_path(f) as f_:  # exporter can not handle spaces in path\ncmd = f'tensorflowjs_converter --input_format=tf_frozen_model --output_node_names={outputs} \"{fpb_}\" \"{f_}\"'\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nif ' ' in str(f):\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f your model may not work correctly with spaces in path '{f}'.\")\n# f_json = Path(f) / 'model.json'  # *.json path\n# with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n#     subst = re.sub(\n#         r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}',\n#         r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n#         r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n#         r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n#         r'\"Identity_3\": {\"name\": \"Identity_3\"}}}',\n#         f_json.read_text(),\n#     )\n#     j.write(subst)\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\ndef _add_tflite_metadata(self, file):\n\"\"\"Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata.\"\"\"\nfrom tflite_support import flatbuffers  # noqa\nfrom tflite_support import metadata as _metadata  # noqa\nfrom tflite_support import metadata_schema_py_generated as _metadata_fb  # noqa\n# Create model info\nmodel_meta = _metadata_fb.ModelMetadataT()\nmodel_meta.name = self.metadata['description']\nmodel_meta.version = self.metadata['version']\nmodel_meta.author = self.metadata['author']\nmodel_meta.license = self.metadata['license']\n# Label file\ntmp_file = Path(file).parent / 'temp_meta.txt'\nwith open(tmp_file, 'w') as f:\nf.write(str(self.metadata))\nlabel_file = _metadata_fb.AssociatedFileT()\nlabel_file.name = tmp_file.name\nlabel_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n# Create input info\ninput_meta = _metadata_fb.TensorMetadataT()\ninput_meta.name = 'image'\ninput_meta.description = 'Input image to be detected.'\ninput_meta.content = _metadata_fb.ContentT()\ninput_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\ninput_meta.content.contentProperties.colorSpace = _metadata_fb.ColorSpaceType.RGB\ninput_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.ImageProperties\n# Create output info\noutput1 = _metadata_fb.TensorMetadataT()\noutput1.name = 'output'\noutput1.description = 'Coordinates of detected objects, class labels, and confidence score'\noutput1.associatedFiles = [label_file]\nif self.model.task == 'segment':\noutput2 = _metadata_fb.TensorMetadataT()\noutput2.name = 'output'\noutput2.description = 'Mask protos'\noutput2.associatedFiles = [label_file]\n# Create subgraph info\nsubgraph = _metadata_fb.SubGraphMetadataT()\nsubgraph.inputTensorMetadata = [input_meta]\nsubgraph.outputTensorMetadata = [output1, output2] if self.model.task == 'segment' else [output1]\nmodel_meta.subgraphMetadata = [subgraph]\nb = flatbuffers.Builder(0)\nb.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\nmetadata_buf = b.Output()\npopulator = _metadata.MetadataPopulator.with_model_file(str(file))\npopulator.load_metadata_buffer(metadata_buf)\npopulator.load_associated_files([str(tmp_file)])\npopulator.populate()\ntmp_file.unlink()\ndef _pipeline_coreml(self, model, weights_dir=None, prefix=colorstr('CoreML Pipeline:')):\n\"\"\"YOLOv8 CoreML pipeline.\"\"\"\nimport coremltools as ct  # noqa\nLOGGER.info(f'{prefix} starting pipeline with coremltools {ct.__version__}...')\nbatch_size, ch, h, w = list(self.im.shape)  # BCHW\n# Output shapes\nspec = model.get_spec()\nout0, out1 = iter(spec.description.output)\nif MACOS:\nfrom PIL import Image\nimg = Image.new('RGB', (w, h))  # w=192, h=320\nout = model.predict({'image': img})\nout0_shape = out[out0.name].shape  # (3780, 80)\nout1_shape = out[out1.name].shape  # (3780, 4)\nelse:  # linux and windows can not run model.predict(), get sizes from PyTorch model output y\nout0_shape = self.output_shape[2], self.output_shape[1] - 4  # (3780, 80)\nout1_shape = self.output_shape[2], 4  # (3780, 4)\n# Checks\nnames = self.metadata['names']\nnx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\nna, nc = out0_shape\n# na, nc = out0.type.multiArrayType.shape  # number anchors, classes\nassert len(names) == nc, f'{len(names)} names found for nc={nc}'  # check\n# Define output shapes (missing)\nout0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\nout1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\n# spec.neuralNetwork.preprocessing[0].featureName = '0'\n# Flexible input shapes\n# from coremltools.models.neural_network import flexible_shape_utils\n# s = [] # shapes\n# s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\n# s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\n# flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\n# r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\n# r.add_height_range((192, 640))\n# r.add_width_range((192, 640))\n# flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\n# Print\n# print(spec.description)\n# Model from spec\nmodel = ct.models.MLModel(spec, weights_dir=weights_dir)\n# 3. Create NMS protobuf\nnms_spec = ct.proto.Model_pb2.Model()\nnms_spec.specificationVersion = 5\nfor i in range(2):\ndecoder_output = model._spec.description.output[i].SerializeToString()\nnms_spec.description.input.add()\nnms_spec.description.input[i].ParseFromString(decoder_output)\nnms_spec.description.output.add()\nnms_spec.description.output[i].ParseFromString(decoder_output)\nnms_spec.description.output[0].name = 'confidence'\nnms_spec.description.output[1].name = 'coordinates'\noutput_sizes = [nc, 4]\nfor i in range(2):\nma_type = nms_spec.description.output[i].type.multiArrayType\nma_type.shapeRange.sizeRanges.add()\nma_type.shapeRange.sizeRanges[0].lowerBound = 0\nma_type.shapeRange.sizeRanges[0].upperBound = -1\nma_type.shapeRange.sizeRanges.add()\nma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\nma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\ndel ma_type.shape[:]\nnms = nms_spec.nonMaximumSuppression\nnms.confidenceInputFeatureName = out0.name  # 1x507x80\nnms.coordinatesInputFeatureName = out1.name  # 1x507x4\nnms.confidenceOutputFeatureName = 'confidence'\nnms.coordinatesOutputFeatureName = 'coordinates'\nnms.iouThresholdInputFeatureName = 'iouThreshold'\nnms.confidenceThresholdInputFeatureName = 'confidenceThreshold'\nnms.iouThreshold = 0.45\nnms.confidenceThreshold = 0.25\nnms.pickTop.perClass = True\nnms.stringClassLabels.vector.extend(names.values())\nnms_model = ct.models.MLModel(nms_spec)\n# 4. Pipeline models together\npipeline = ct.models.pipeline.Pipeline(input_features=[('image', ct.models.datatypes.Array(3, ny, nx)),\n('iouThreshold', ct.models.datatypes.Double()),\n('confidenceThreshold', ct.models.datatypes.Double())],\noutput_features=['confidence', 'coordinates'])\npipeline.add_model(model)\npipeline.add_model(nms_model)\n# Correct datatypes\npipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\npipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\npipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\n# Update metadata\npipeline.spec.specificationVersion = 5\npipeline.spec.description.metadata.userDefined.update({\n'IoU threshold': str(nms.iouThreshold),\n'Confidence threshold': str(nms.confidenceThreshold)})\n# Save the model\nmodel = ct.models.MLModel(pipeline.spec, weights_dir=weights_dir)\nmodel.input_description['image'] = 'Input image'\nmodel.input_description['iouThreshold'] = f'(optional) IOU threshold override (default: {nms.iouThreshold})'\nmodel.input_description['confidenceThreshold'] = \\\n            f'(optional) Confidence threshold override (default: {nms.confidenceThreshold})'\nmodel.output_description['confidence'] = 'Boxes \u00d7 Class confidence (see user-defined metadata \"classes\")'\nmodel.output_description['coordinates'] = 'Boxes \u00d7 [x, y, width, height] (relative to image size)'\nLOGGER.info(f'{prefix} pipeline success')\nreturn model\ndef add_callback(self, event: str, callback):\n\"\"\"\n        Appends the given callback.\n        \"\"\"\nself.callbacks[event].append(callback)\ndef run_callbacks(self, event: str):\n\"\"\"Execute all callbacks for a given event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.__call__","title":"<code>__call__(model=None)</code>","text":"<p>Returns list of exported files/dirs after running callbacks.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@smart_inference_mode()\ndef __call__(self, model=None):\n\"\"\"Returns list of exported files/dirs after running callbacks.\"\"\"\nself.run_callbacks('on_export_start')\nt = time.time()\nformat = self.args.format.lower()  # to lowercase\nif format in ('tensorrt', 'trt'):  # 'engine' aliases\nformat = 'engine'\nif format in ('mlmodel', 'mlpackage', 'mlprogram', 'apple', 'ios'):  # 'coreml' aliases\nformat = 'coreml'\nfmts = tuple(export_formats()['Argument'][1:])  # available export formats\nflags = [x == format for x in fmts]\nif sum(flags) != 1:\nraise ValueError(f\"Invalid export format='{format}'. Valid formats are {fmts}\")\njit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, ncnn = flags  # export booleans\n# Load PyTorch model\nself.device = select_device('cpu' if self.args.device is None else self.args.device)\n# Checks\nmodel.names = check_class_names(model.names)\nif self.args.half and onnx and self.device.type == 'cpu':\nLOGGER.warning('WARNING \u26a0\ufe0f half=True only compatible with GPU export, i.e. use device=0')\nself.args.half = False\nassert not self.args.dynamic, 'half=True not compatible with dynamic=True, i.e. use only one.'\nself.imgsz = check_imgsz(self.args.imgsz, stride=model.stride, min_dim=2)  # check image size\nif self.args.optimize:\nassert not ncnn, \"optimize=True not compatible with format='ncnn', i.e. use optimize=False\"\nassert self.device.type == 'cpu', \"optimize=True not compatible with cuda devices, i.e. use device='cpu'\"\nif edgetpu and not LINUX:\nraise SystemError('Edge TPU export only supported on Linux. See https://coral.ai/docs/edgetpu/compiler/')\n# Input\nim = torch.zeros(self.args.batch, 3, *self.imgsz).to(self.device)\nfile = Path(\ngetattr(model, 'pt_path', None) or getattr(model, 'yaml_file', None) or model.yaml.get('yaml_file', ''))\nif file.suffix in ('.yaml', '.yml'):\nfile = Path(file.name)\n# Update model\nmodel = deepcopy(model).to(self.device)\nfor p in model.parameters():\np.requires_grad = False\nmodel.eval()\nmodel.float()\nmodel = model.fuse()\nfor k, m in model.named_modules():\nif isinstance(m, (Detect, RTDETRDecoder)):  # Segment and Pose use Detect base class\nm.dynamic = self.args.dynamic\nm.export = True\nm.format = self.args.format\nelif isinstance(m, C2f) and not any((saved_model, pb, tflite, edgetpu, tfjs)):\n# EdgeTPU does not support FlexSplitV while split provides cleaner ONNX graph\nm.forward = m.forward_split\ny = None\nfor _ in range(2):\ny = model(im)  # dry runs\nif self.args.half and (engine or onnx) and self.device.type != 'cpu':\nim, model = im.half(), model.half()  # to FP16\n# Filter warnings\nwarnings.filterwarnings('ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\nwarnings.filterwarnings('ignore', category=UserWarning)  # suppress shape prim::Constant missing ONNX warning\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # suppress CoreML np.bool deprecation warning\n# Assign\nself.im = im\nself.model = model\nself.file = file\nself.output_shape = tuple(y.shape) if isinstance(y, torch.Tensor) else \\\n        tuple(tuple(x.shape if isinstance(x, torch.Tensor) else []) for x in y)\nself.pretty_name = Path(self.model.yaml.get('yaml_file', self.file)).stem.replace('yolo', 'YOLO')\ndata = model.args['data'] if hasattr(model, 'args') and isinstance(model.args, dict) else ''\ndescription = f'Ultralytics {self.pretty_name} model {f\"trained on {data}\" if data else \"\"}'\nself.metadata = {\n'description': description,\n'author': 'Ultralytics',\n'license': 'AGPL-3.0 https://ultralytics.com/license',\n'date': datetime.now().isoformat(),\n'version': __version__,\n'stride': int(max(model.stride)),\n'task': model.task,\n'batch': self.args.batch,\n'imgsz': self.imgsz,\n'names': model.names}  # model metadata\nif model.task == 'pose':\nself.metadata['kpt_shape'] = model.model[-1].kpt_shape\nLOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from '{file}' with input shape {tuple(im.shape)} BCHW and \"\nf'output shape(s) {self.output_shape} ({file_size(file):.1f} MB)')\n# Exports\nf = [''] * len(fmts)  # exported filenames\nif jit or ncnn:  # TorchScript\nf[0], _ = self.export_torchscript()\nif engine:  # TensorRT required before ONNX\nf[1], _ = self.export_engine()\nif onnx or xml:  # OpenVINO requires ONNX\nf[2], _ = self.export_onnx()\nif xml:  # OpenVINO\nf[3], _ = self.export_openvino()\nif coreml:  # CoreML\nf[4], _ = self.export_coreml()\nif any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\nself.args.int8 |= edgetpu\nf[5], keras_model = self.export_saved_model()\nif pb or tfjs:  # pb prerequisite to tfjs\nf[6], _ = self.export_pb(keras_model=keras_model)\nif tflite:\nf[7], _ = self.export_tflite(keras_model=keras_model, nms=False, agnostic_nms=self.args.agnostic_nms)\nif edgetpu:\nf[8], _ = self.export_edgetpu(tflite_model=Path(f[5]) / f'{self.file.stem}_full_integer_quant.tflite')\nif tfjs:\nf[9], _ = self.export_tfjs()\nif paddle:  # PaddlePaddle\nf[10], _ = self.export_paddle()\nif ncnn:  # ncnn\nf[11], _ = self.export_ncnn()\n# Finish\nf = [str(x) for x in f if x]  # filter out '' and None\nif any(f):\nf = str(Path(f[-1]))\nsquare = self.imgsz[0] == self.imgsz[1]\ns = '' if square else f\"WARNING \u26a0\ufe0f non-PyTorch val requires square images, 'imgsz={self.imgsz}' will not \" \\\n                              f\"work. Use export 'imgsz={max(self.imgsz)}' if val is required.\"\nimgsz = self.imgsz[0] if square else str(self.imgsz)[1:-1].replace(' ', '')\npredict_data = f'data={data}' if model.task == 'segment' and format == 'pb' else ''\nLOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\nf\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\nf'\\nPredict:         yolo predict task={model.task} model={f} imgsz={imgsz} {predict_data}'\nf'\\nValidate:        yolo val task={model.task} model={f} imgsz={imgsz} data={data} {s}'\nf'\\nVisualize:       https://netron.app')\nself.run_callbacks('on_export_end')\nreturn f  # return list of exported files/dirs\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initializes the Exporter class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> <code>_callbacks</code> <code>list</code> <p>List of callback functions. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n    Initializes the Exporter class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n        _callbacks (list, optional): List of callback functions. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.add_callback","title":"<code>add_callback(event, callback)</code>","text":"<p>Appends the given callback.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def add_callback(self, event: str, callback):\n\"\"\"\n    Appends the given callback.\n    \"\"\"\nself.callbacks[event].append(callback)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_coreml","title":"<code>export_coreml(prefix=colorstr('CoreML:'))</code>","text":"<p>YOLOv8 CoreML export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_coreml(self, prefix=colorstr('CoreML:')):\n\"\"\"YOLOv8 CoreML export.\"\"\"\nmlmodel = self.args.format.lower() == 'mlmodel'  # legacy *.mlmodel export format requested\ncheck_requirements('coremltools&gt;=6.0,&lt;=6.2' if mlmodel else 'coremltools&gt;=7.0.b1')\nimport coremltools as ct  # noqa\nLOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\nf = self.file.with_suffix('.mlmodel' if mlmodel else '.mlpackage')\nif f.is_dir():\nshutil.rmtree(f)\nbias = [0.0, 0.0, 0.0]\nscale = 1 / 255\nclassifier_config = None\nif self.model.task == 'classify':\nclassifier_config = ct.ClassifierConfig(list(self.model.names.values())) if self.args.nms else None\nmodel = self.model\nelif self.model.task == 'detect':\nmodel = IOSDetectModel(self.model, self.im) if self.args.nms else self.model\nelse:\nif self.args.nms:\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f 'nms=True' is only available for Detect models like 'yolov8n.pt'.\")\n# TODO CoreML Segment and Pose model pipelining\nmodel = self.model\nts = torch.jit.trace(model.eval(), self.im, strict=False)  # TorchScript model\nct_model = ct.convert(ts,\ninputs=[ct.ImageType('image', shape=self.im.shape, scale=scale, bias=bias)],\nclassifier_config=classifier_config,\nconvert_to='neuralnetwork' if mlmodel else 'mlprogram')\nbits, mode = (8, 'kmeans') if self.args.int8 else (16, 'linear') if self.args.half else (32, None)\nif bits &lt; 32:\nif 'kmeans' in mode:\ncheck_requirements('scikit-learn')  # scikit-learn package required for k-means quantization\nif mlmodel:\nct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\nelse:\nimport coremltools.optimize.coreml as cto\nop_config = cto.OpPalettizerConfig(mode=mode, nbits=bits, weight_threshold=512)\nconfig = cto.OptimizationConfig(global_config=op_config)\nct_model = cto.palettize_weights(ct_model, config=config)\nif self.args.nms and self.model.task == 'detect':\nif mlmodel:\nimport platform\n# coremltools&lt;=6.2 NMS export requires Python&lt;3.11\ncheck_version(platform.python_version(), '&lt;3.11', name='Python ', hard=True)\nweights_dir = None\nelse:\nct_model.save(str(f))  # save otherwise weights_dir does not exist\nweights_dir = str(f / 'Data/com.apple.CoreML/weights')\nct_model = self._pipeline_coreml(ct_model, weights_dir=weights_dir)\nm = self.metadata  # metadata dict\nct_model.short_description = m.pop('description')\nct_model.author = m.pop('author')\nct_model.license = m.pop('license')\nct_model.version = m.pop('version')\nct_model.user_defined_metadata.update({k: str(v) for k, v in m.items()})\ntry:\nct_model.save(str(f))  # save *.mlpackage\nexcept Exception as e:\nLOGGER.warning(\nf'{prefix} WARNING \u26a0\ufe0f CoreML export to *.mlpackage failed ({e}), reverting to *.mlmodel export. '\nf'Known coremltools Python 3.11 and Windows bugs https://github.com/apple/coremltools/issues/1928.')\nf = f.with_suffix('.mlmodel')\nct_model.save(str(f))\nreturn f, ct_model\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_edgetpu","title":"<code>export_edgetpu(tflite_model='', prefix=colorstr('Edge TPU:'))</code>","text":"<p>YOLOv8 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_edgetpu(self, tflite_model='', prefix=colorstr('Edge TPU:')):\n\"\"\"YOLOv8 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/.\"\"\"\nLOGGER.warning(f'{prefix} WARNING \u26a0\ufe0f Edge TPU known bug https://github.com/ultralytics/ultralytics/issues/1185')\ncmd = 'edgetpu_compiler --version'\nhelp_url = 'https://coral.ai/docs/edgetpu/compiler/'\nassert LINUX, f'export only supported on Linux. See {help_url}'\nif subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True).returncode != 0:\nLOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\nsudo = subprocess.run('sudo --version &gt;/dev/null', shell=True).returncode == 0  # sudo installed on system\nfor c in (\n'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n'sudo apt-get update', 'sudo apt-get install edgetpu-compiler'):\nsubprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\nver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\nLOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\nf = str(tflite_model).replace('.tflite', '_edgetpu.tflite')  # Edge TPU model\ncmd = f'edgetpu_compiler -s -d -k 10 --out_dir \"{Path(f).parent}\" \"{tflite_model}\"'\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nself._add_tflite_metadata(f)\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_engine","title":"<code>export_engine(prefix=colorstr('TensorRT:'))</code>","text":"<p>YOLOv8 TensorRT export https://developer.nvidia.com/tensorrt.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_engine(self, prefix=colorstr('TensorRT:')):\n\"\"\"YOLOv8 TensorRT export https://developer.nvidia.com/tensorrt.\"\"\"\nassert self.im.device.type != 'cpu', \"export running on CPU but must be on GPU, i.e. use 'device=0'\"\ntry:\nimport tensorrt as trt  # noqa\nexcept ImportError:\nif LINUX:\ncheck_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\nimport tensorrt as trt  # noqa\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nself.args.simplify = True\nf_onnx, _ = self.export_onnx()\nLOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\nassert Path(f_onnx).exists(), f'failed to export ONNX file: {f_onnx}'\nf = self.file.with_suffix('.engine')  # TensorRT engine file\nlogger = trt.Logger(trt.Logger.INFO)\nif self.args.verbose:\nlogger.min_severity = trt.Logger.Severity.VERBOSE\nbuilder = trt.Builder(logger)\nconfig = builder.create_builder_config()\nconfig.max_workspace_size = self.args.workspace * 1 &lt;&lt; 30\n# config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace &lt;&lt; 30)  # fix TRT 8.4 deprecation notice\nflag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nnetwork = builder.create_network(flag)\nparser = trt.OnnxParser(network, logger)\nif not parser.parse_from_file(f_onnx):\nraise RuntimeError(f'failed to load ONNX file: {f_onnx}')\ninputs = [network.get_input(i) for i in range(network.num_inputs)]\noutputs = [network.get_output(i) for i in range(network.num_outputs)]\nfor inp in inputs:\nLOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\nfor out in outputs:\nLOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\nif self.args.dynamic:\nshape = self.im.shape\nif shape[0] &lt;= 1:\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f 'dynamic=True' model requires max batch size, i.e. 'batch=16'\")\nprofile = builder.create_optimization_profile()\nfor inp in inputs:\nprofile.set_shape(inp.name, (1, *shape[1:]), (max(1, shape[0] // 2), *shape[1:]), shape)\nconfig.add_optimization_profile(profile)\nLOGGER.info(\nf'{prefix} building FP{16 if builder.platform_has_fast_fp16 and self.args.half else 32} engine as {f}')\nif builder.platform_has_fast_fp16 and self.args.half:\nconfig.set_flag(trt.BuilderFlag.FP16)\n# Write file\nwith builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n# Metadata\nmeta = json.dumps(self.metadata)\nt.write(len(meta).to_bytes(4, byteorder='little', signed=True))\nt.write(meta.encode())\n# Model\nt.write(engine.serialize())\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_ncnn","title":"<code>export_ncnn(prefix=colorstr('ncnn:'))</code>","text":"<p>YOLOv8 ncnn export using PNNX https://github.com/pnnx/pnnx.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_ncnn(self, prefix=colorstr('ncnn:')):\n\"\"\"\n    YOLOv8 ncnn export using PNNX https://github.com/pnnx/pnnx.\n    \"\"\"\ncheck_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn\nimport ncnn  # noqa\nLOGGER.info(f'\\n{prefix} starting export with ncnn {ncnn.__version__}...')\nf = Path(str(self.file).replace(self.file.suffix, f'_ncnn_model{os.sep}'))\nf_ts = self.file.with_suffix('.torchscript')\npnnx_filename = 'pnnx.exe' if WINDOWS else 'pnnx'\nif Path(pnnx_filename).is_file():\npnnx = pnnx_filename\nelif (ROOT / pnnx_filename).is_file():\npnnx = ROOT / pnnx_filename\nelse:\nLOGGER.warning(\nf'{prefix} WARNING \u26a0\ufe0f PNNX not found. Attempting to download binary file from '\n'https://github.com/pnnx/pnnx/.\\nNote PNNX Binary file must be placed in current working directory '\nf'or in {ROOT}. See PNNX repo for full installation instructions.')\n_, assets = get_github_assets(repo='pnnx/pnnx', retry=True)\nsystem = 'macos' if MACOS else 'ubuntu' if LINUX else 'windows'  # operating system\nasset = [x for x in assets if system in x][0] if assets else \\\n            f'https://github.com/pnnx/pnnx/releases/download/20230816/pnnx-20230816-{system}.zip'  # fallback\nattempt_download_asset(asset, repo='pnnx/pnnx', release='latest')\nunzip_dir = Path(asset).with_suffix('')\npnnx = ROOT / pnnx_filename  # new location\n(unzip_dir / pnnx_filename).rename(pnnx)  # move binary to ROOT\nshutil.rmtree(unzip_dir)  # delete unzip dir\nPath(asset).unlink()  # delete zip\npnnx.chmod(0o777)  # set read, write, and execute permissions for everyone\nuse_ncnn = True\nncnn_args = [\nf'ncnnparam={f / \"model.ncnn.param\"}',\nf'ncnnbin={f / \"model.ncnn.bin\"}',\nf'ncnnpy={f / \"model_ncnn.py\"}', ] if use_ncnn else []\nuse_pnnx = False\npnnx_args = [\nf'pnnxparam={f / \"model.pnnx.param\"}',\nf'pnnxbin={f / \"model.pnnx.bin\"}',\nf'pnnxpy={f / \"model_pnnx.py\"}',\nf'pnnxonnx={f / \"model.pnnx.onnx\"}', ] if use_pnnx else []\ncmd = [\nstr(pnnx),\nstr(f_ts),\n*ncnn_args,\n*pnnx_args,\nf'fp16={int(self.args.half)}',\nf'device={self.device.type}',\nf'inputshape=\"{[self.args.batch, 3, *self.imgsz]}\"', ]\nf.mkdir(exist_ok=True)  # make ncnn_model directory\nLOGGER.info(f\"{prefix} running '{' '.join(cmd)}'\")\nsubprocess.run(cmd, check=True)\nfor f_debug in 'debug.bin', 'debug.param', 'debug2.bin', 'debug2.param':  # remove debug files\nPath(f_debug).unlink(missing_ok=True)\nyaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn str(f), None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_onnx","title":"<code>export_onnx(prefix=colorstr('ONNX:'))</code>","text":"<p>YOLOv8 ONNX export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_onnx(self, prefix=colorstr('ONNX:')):\n\"\"\"YOLOv8 ONNX export.\"\"\"\nrequirements = ['onnx&gt;=1.12.0']\nif self.args.simplify:\nrequirements += ['onnxsim&gt;=0.4.33', 'onnxruntime-gpu' if torch.cuda.is_available() else 'onnxruntime']\ncheck_requirements(requirements)\nimport onnx  # noqa\nopset_version = self.args.opset or get_latest_opset()\nLOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__} opset {opset_version}...')\nf = str(self.file.with_suffix('.onnx'))\noutput_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']\ndynamic = self.args.dynamic\nif dynamic:\ndynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\nif isinstance(self.model, SegmentationModel):\ndynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)\ndynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\nelif isinstance(self.model, DetectionModel):\ndynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)\ntorch.onnx.export(\nself.model.cpu() if dynamic else self.model,  # dynamic=True only compatible with cpu\nself.im.cpu() if dynamic else self.im,\nf,\nverbose=False,\nopset_version=opset_version,\ndo_constant_folding=True,  # WARNING: DNN inference with torch&gt;=1.12 may require do_constant_folding=False\ninput_names=['images'],\noutput_names=output_names,\ndynamic_axes=dynamic or None)\n# Checks\nmodel_onnx = onnx.load(f)  # load onnx model\n# onnx.checker.check_model(model_onnx)  # check onnx model\n# Simplify\nif self.args.simplify:\ntry:\nimport onnxsim\nLOGGER.info(f'{prefix} simplifying with onnxsim {onnxsim.__version__}...')\n# subprocess.run(f'onnxsim \"{f}\" \"{f}\"', shell=True)\nmodel_onnx, check = onnxsim.simplify(model_onnx)\nassert check, 'Simplified ONNX model could not be validated'\nexcept Exception as e:\nLOGGER.info(f'{prefix} simplifier failure: {e}')\n# Metadata\nfor k, v in self.metadata.items():\nmeta = model_onnx.metadata_props.add()\nmeta.key, meta.value = k, str(v)\nonnx.save(model_onnx, f)\nreturn f, model_onnx\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_openvino","title":"<code>export_openvino(prefix=colorstr('OpenVINO:'))</code>","text":"<p>YOLOv8 OpenVINO export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_openvino(self, prefix=colorstr('OpenVINO:')):\n\"\"\"YOLOv8 OpenVINO export.\"\"\"\ncheck_requirements('openvino-dev&gt;=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nimport openvino.runtime as ov  # noqa\nfrom openvino.tools import mo  # noqa\nLOGGER.info(f'\\n{prefix} starting export with openvino {ov.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_openvino_model{os.sep}')\nf_onnx = self.file.with_suffix('.onnx')\nf_ov = str(Path(f) / self.file.with_suffix('.xml').name)\nov_model = mo.convert_model(f_onnx,\nmodel_name=self.pretty_name,\nframework='onnx',\ncompress_to_fp16=self.args.half)  # export\n# Set RT info\nov_model.set_rt_info('YOLOv8', ['model_info', 'model_type'])\nov_model.set_rt_info(True, ['model_info', 'reverse_input_channels'])\nov_model.set_rt_info(114, ['model_info', 'pad_value'])\nov_model.set_rt_info([255.0], ['model_info', 'scale_values'])\nov_model.set_rt_info(self.args.iou, ['model_info', 'iou_threshold'])\nov_model.set_rt_info([v.replace(' ', '_') for k, v in sorted(self.model.names.items())],\n['model_info', 'labels'])\nif self.model.task != 'classify':\nov_model.set_rt_info('fit_to_window_letterbox', ['model_info', 'resize_type'])\nov.serialize(ov_model, f_ov)  # save\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_paddle","title":"<code>export_paddle(prefix=colorstr('PaddlePaddle:'))</code>","text":"<p>YOLOv8 Paddle export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_paddle(self, prefix=colorstr('PaddlePaddle:')):\n\"\"\"YOLOv8 Paddle export.\"\"\"\ncheck_requirements(('paddlepaddle', 'x2paddle'))\nimport x2paddle  # noqa\nfrom x2paddle.convert import pytorch2paddle  # noqa\nLOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\nf = str(self.file).replace(self.file.suffix, f'_paddle_model{os.sep}')\npytorch2paddle(module=self.model, save_dir=f, jit_type='trace', input_examples=[self.im])  # export\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_pb","title":"<code>export_pb(keras_model, prefix=colorstr('TensorFlow GraphDef:'))</code>","text":"<p>YOLOv8 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_pb(self, keras_model, prefix=colorstr('TensorFlow GraphDef:')):\n\"\"\"YOLOv8 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow.\"\"\"\nimport tensorflow as tf  # noqa\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = self.file.with_suffix('.pb')\nm = tf.function(lambda x: keras_model(x))  # full model\nm = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\nfrozen_func = convert_variables_to_constants_v2(m)\nfrozen_func.graph.as_graph_def()\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_saved_model","title":"<code>export_saved_model(prefix=colorstr('TensorFlow SavedModel:'))</code>","text":"<p>YOLOv8 TensorFlow SavedModel export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_saved_model(self, prefix=colorstr('TensorFlow SavedModel:')):\n\"\"\"YOLOv8 TensorFlow SavedModel export.\"\"\"\ncuda = torch.cuda.is_available()\ntry:\nimport tensorflow as tf  # noqa\nexcept ImportError:\ncheck_requirements(f\"tensorflow{'-macos' if MACOS else '-aarch64' if ARM64 else '' if cuda else '-cpu'}\")\nimport tensorflow as tf  # noqa\ncheck_requirements(\n('onnx', 'onnx2tf&gt;=1.15.4', 'sng4onnx&gt;=1.0.1', 'onnxsim&gt;=0.4.33', 'onnx_graphsurgeon&gt;=0.3.26',\n'tflite_support', 'onnxruntime-gpu' if cuda else 'onnxruntime'),\ncmds='--extra-index-url https://pypi.ngc.nvidia.com')  # onnx_graphsurgeon only on NVIDIA\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nf = Path(str(self.file).replace(self.file.suffix, '_saved_model'))\nif f.is_dir():\nimport shutil\nshutil.rmtree(f)  # delete output folder\n# Export to ONNX\nself.args.simplify = True\nf_onnx, _ = self.export_onnx()\n# Export to TF\ntmp_file = f / 'tmp_tflite_int8_calibration_images.npy'  # int8 calibration images file\nif self.args.int8:\nverbosity = '--verbosity info'\nif self.args.data:\nimport numpy as np\nfrom ultralytics.data.dataset import YOLODataset\nfrom ultralytics.data.utils import check_det_dataset\n# Generate calibration data for integer quantization\nLOGGER.info(f\"{prefix} collecting INT8 calibration images from 'data={self.args.data}'\")\ndata = check_det_dataset(self.args.data)\ndataset = YOLODataset(data['val'], data=data, imgsz=self.imgsz[0], augment=False)\nimages = []\nn_images = 100  # maximum number of images\nfor n, batch in enumerate(dataset):\nif n &gt;= n_images:\nbreak\nim = batch['img'].permute(1, 2, 0)[None]  # list to nparray, CHW to BHWC\nimages.append(im)\nf.mkdir()\nimages = torch.cat(images, 0).float()\n# mean = images.view(-1, 3).mean(0)  # imagenet mean [123.675, 116.28, 103.53]\n# std = images.view(-1, 3).std(0)  # imagenet std [58.395, 57.12, 57.375]\nnp.save(str(tmp_file), images.numpy())  # BHWC\nint8 = f'-oiqt -qt per-tensor -cind images \"{tmp_file}\" \"[[[[0, 0, 0]]]]\" \"[[[[255, 255, 255]]]]\"'\nelse:\nint8 = '-oiqt -qt per-tensor'\nelse:\nverbosity = '--non_verbose'\nint8 = ''\ncmd = f'onnx2tf -i \"{f_onnx}\" -o \"{f}\" -nuo {verbosity} {int8}'.strip()\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nyaml_save(f / 'metadata.yaml', self.metadata)  # add metadata.yaml\n# Remove/rename TFLite models\nif self.args.int8:\ntmp_file.unlink(missing_ok=True)\nfor file in f.rglob('*_dynamic_range_quant.tflite'):\nfile.rename(file.with_name(file.stem.replace('_dynamic_range_quant', '_int8') + file.suffix))\nfor file in f.rglob('*_integer_quant_with_int16_act.tflite'):\nfile.unlink()  # delete extra fp16 activation TFLite files\n# Add TFLite metadata\nfor file in f.rglob('*.tflite'):\nf.unlink() if 'quant_with_int16_act.tflite' in str(f) else self._add_tflite_metadata(file)\nreturn str(f), tf.saved_model.load(f, tags=None, options=None)  # load saved_model as Keras model\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_tfjs","title":"<code>export_tfjs(prefix=colorstr('TensorFlow.js:'))</code>","text":"<p>YOLOv8 TensorFlow.js export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_tfjs(self, prefix=colorstr('TensorFlow.js:')):\n\"\"\"YOLOv8 TensorFlow.js export.\"\"\"\ncheck_requirements('tensorflowjs')\nimport tensorflow as tf\nimport tensorflowjs as tfjs  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\nf = str(self.file).replace(self.file.suffix, '_web_model')  # js dir\nf_pb = str(self.file.with_suffix('.pb'))  # *.pb path\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(f_pb, 'rb') as file:\ngd.ParseFromString(file.read())\noutputs = ','.join(gd_outputs(gd))\nLOGGER.info(f'\\n{prefix} output node names: {outputs}')\nwith spaces_in_path(f_pb) as fpb_, spaces_in_path(f) as f_:  # exporter can not handle spaces in path\ncmd = f'tensorflowjs_converter --input_format=tf_frozen_model --output_node_names={outputs} \"{fpb_}\" \"{f_}\"'\nLOGGER.info(f\"{prefix} running '{cmd}'\")\nsubprocess.run(cmd, shell=True)\nif ' ' in str(f):\nLOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f your model may not work correctly with spaces in path '{f}'.\")\n# f_json = Path(f) / 'model.json'  # *.json path\n# with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n#     subst = re.sub(\n#         r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n#         r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}',\n#         r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n#         r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n#         r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n#         r'\"Identity_3\": {\"name\": \"Identity_3\"}}}',\n#         f_json.read_text(),\n#     )\n#     j.write(subst)\nyaml_save(Path(f) / 'metadata.yaml', self.metadata)  # add metadata.yaml\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_tflite","title":"<code>export_tflite(keras_model, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:'))</code>","text":"<p>YOLOv8 TensorFlow Lite export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_tflite(self, keras_model, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n\"\"\"YOLOv8 TensorFlow Lite export.\"\"\"\nimport tensorflow as tf  # noqa\nLOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\nsaved_model = Path(str(self.file).replace(self.file.suffix, '_saved_model'))\nif self.args.int8:\nf = saved_model / f'{self.file.stem}_int8.tflite'  # fp32 in/out\nelif self.args.half:\nf = saved_model / f'{self.file.stem}_float16.tflite'  # fp32 in/out\nelse:\nf = saved_model / f'{self.file.stem}_float32.tflite'\nreturn str(f), None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.export_torchscript","title":"<code>export_torchscript(prefix=colorstr('TorchScript:'))</code>","text":"<p>YOLOv8 TorchScript model export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>@try_export\ndef export_torchscript(self, prefix=colorstr('TorchScript:')):\n\"\"\"YOLOv8 TorchScript model export.\"\"\"\nLOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\nf = self.file.with_suffix('.torchscript')\nts = torch.jit.trace(self.model, self.im, strict=False)\nextra_files = {'config.txt': json.dumps(self.metadata)}  # torch._C.ExtraFilesMap()\nif self.args.optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\nLOGGER.info(f'{prefix} optimizing for mobile...')\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\noptimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\nelse:\nts.save(str(f), _extra_files=extra_files)\nreturn f, None\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.Exporter.run_callbacks","title":"<code>run_callbacks(event)</code>","text":"<p>Execute all callbacks for a given event.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def run_callbacks(self, event: str):\n\"\"\"Execute all callbacks for a given event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.IOSDetectModel","title":"<code>ultralytics.engine.exporter.IOSDetectModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>Wrap an Ultralytics YOLO model for Apple iOS CoreML export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>class IOSDetectModel(torch.nn.Module):\n\"\"\"Wrap an Ultralytics YOLO model for Apple iOS CoreML export.\"\"\"\ndef __init__(self, model, im):\n\"\"\"Initialize the IOSDetectModel class with a YOLO model and example image.\"\"\"\nsuper().__init__()\nb, c, h, w = im.shape  # batch, channel, height, width\nself.model = model\nself.nc = len(model.names)  # number of classes\nif w == h:\nself.normalize = 1.0 / w  # scalar\nelse:\nself.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\ndef forward(self, x):\n\"\"\"Normalize predictions of object detection model with input size-dependent factors.\"\"\"\nxywh, cls = self.model(x)[0].transpose(0, 1).split((4, self.nc), 1)\nreturn cls, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.IOSDetectModel.__init__","title":"<code>__init__(model, im)</code>","text":"<p>Initialize the IOSDetectModel class with a YOLO model and example image.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def __init__(self, model, im):\n\"\"\"Initialize the IOSDetectModel class with a YOLO model and example image.\"\"\"\nsuper().__init__()\nb, c, h, w = im.shape  # batch, channel, height, width\nself.model = model\nself.nc = len(model.names)  # number of classes\nif w == h:\nself.normalize = 1.0 / w  # scalar\nelse:\nself.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.IOSDetectModel.forward","title":"<code>forward(x)</code>","text":"<p>Normalize predictions of object detection model with input size-dependent factors.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def forward(self, x):\n\"\"\"Normalize predictions of object detection model with input size-dependent factors.\"\"\"\nxywh, cls = self.model(x)[0].transpose(0, 1).split((4, self.nc), 1)\nreturn cls, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.export_formats","title":"<code>ultralytics.engine.exporter.export_formats()</code>","text":"<p>YOLOv8 export formats.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def export_formats():\n\"\"\"YOLOv8 export formats.\"\"\"\nimport pandas\nx = [\n['PyTorch', '-', '.pt', True, True],\n['TorchScript', 'torchscript', '.torchscript', True, True],\n['ONNX', 'onnx', '.onnx', True, True],\n['OpenVINO', 'openvino', '_openvino_model', True, False],\n['TensorRT', 'engine', '.engine', False, True],\n['CoreML', 'coreml', '.mlpackage', True, False],\n['TensorFlow SavedModel', 'saved_model', '_saved_model', True, True],\n['TensorFlow GraphDef', 'pb', '.pb', True, True],\n['TensorFlow Lite', 'tflite', '.tflite', True, False],\n['TensorFlow Edge TPU', 'edgetpu', '_edgetpu.tflite', True, False],\n['TensorFlow.js', 'tfjs', '_web_model', True, False],\n['PaddlePaddle', 'paddle', '_paddle_model', True, True],\n['ncnn', 'ncnn', '_ncnn_model', True, True], ]\nreturn pandas.DataFrame(x, columns=['Format', 'Argument', 'Suffix', 'CPU', 'GPU'])\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.gd_outputs","title":"<code>ultralytics.engine.exporter.gd_outputs(gd)</code>","text":"<p>TensorFlow GraphDef model output node names.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def gd_outputs(gd):\n\"\"\"TensorFlow GraphDef model output node names.\"\"\"\nname_list, input_list = [], []\nfor node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\nname_list.append(node.name)\ninput_list.extend(node.input)\nreturn sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))\n</code></pre>"},{"location":"reference/engine/exporter/#ultralytics.engine.exporter.try_export","title":"<code>ultralytics.engine.exporter.try_export(inner_func)</code>","text":"<p>YOLOv8 export decorator, i..e @try_export.</p> Source code in <code>ultralytics/engine/exporter.py</code> <pre><code>def try_export(inner_func):\n\"\"\"YOLOv8 export decorator, i..e @try_export.\"\"\"\ninner_args = get_default_args(inner_func)\ndef outer_func(*args, **kwargs):\n\"\"\"Export a model.\"\"\"\nprefix = inner_args['prefix']\ntry:\nwith Profile() as dt:\nf, model = inner_func(*args, **kwargs)\nLOGGER.info(f\"{prefix} export success \u2705 {dt.t:.1f}s, saved as '{f}' ({file_size(f):.1f} MB)\")\nreturn f, model\nexcept Exception as e:\nLOGGER.info(f'{prefix} export failure \u274c {dt.t:.1f}s: {e}')\nraise e\nreturn outer_func\n</code></pre>"},{"location":"reference/engine/model/","title":"Reference for <code>ultralytics/engine/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model","title":"<code>ultralytics.engine.model.Model</code>","text":"<p>A base model class to unify apis for all the models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>(str, Path)</code> <p>Path to the model file to load or create.</p> <code>'yolov8n.pt'</code> <code>task</code> <code>Any</code> <p>Task type for the YOLO model. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>predictor</code> <code>Any</code> <p>The predictor object.</p> <code>model</code> <code>Any</code> <p>The model object.</p> <code>trainer</code> <code>Any</code> <p>The trainer object.</p> <code>task</code> <code>str</code> <p>The type of model task.</p> <code>ckpt</code> <code>Any</code> <p>The checkpoint object if the model loaded from *.pt file.</p> <code>cfg</code> <code>str</code> <p>The model configuration if loaded from *.yaml file.</p> <code>ckpt_path</code> <code>str</code> <p>The checkpoint file path.</p> <code>overrides</code> <code>dict</code> <p>Overrides for the trainer object.</p> <code>metrics</code> <code>Any</code> <p>The data for metrics.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Alias for the predict method.</p> <code>_new</code> <p>str, verbose:bool=True) -&gt; None: Initializes a new model and infers the task type from the model definitions.</p> <code>_load</code> <p>str, task:str='') -&gt; None: Initializes a new model and infers the task type from the model head.</p> <code>_check_is_pytorch_model</code> <p>Raises TypeError if the model is not a PyTorch model.</p> <code>reset</code> <p>Resets the model modules.</p> <code>info</code> <p>bool=False) -&gt; None: Logs the model info.</p> <code>fuse</code> <p>Fuses the model for faster inference.</p> <code>predict</code> <p>Performs prediction using the YOLO model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Results</code> <p>The prediction results.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>class Model:\n\"\"\"\n    A base model class to unify apis for all the models.\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -&gt; None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -&gt; None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -&gt; None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -&gt; None:\n            Resets the model modules.\n        info(verbose:bool=False) -&gt; None:\n            Logs the model info.\n        fuse() -&gt; None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -&gt; List[ultralytics.engine.results.Results]:\n            Performs prediction using the YOLO model.\n    Returns:\n        list(ultralytics.engine.results.Results): The prediction results.\n    \"\"\"\ndef __init__(self, model: Union[str, Path] = 'yolov8n.pt', task=None) -&gt; None:\n\"\"\"\n        Initializes the YOLO model.\n        Args:\n            model (Union[str, Path], optional): Path or name of the model to load or create. Defaults to 'yolov8n.pt'.\n            task (Any, optional): Task type for the YOLO model. Defaults to None.\n        \"\"\"\nself.callbacks = callbacks.get_default_callbacks()\nself.predictor = None  # reuse predictor\nself.model = None  # model object\nself.trainer = None  # trainer object\nself.ckpt = None  # if loaded from *.pt\nself.cfg = None  # if loaded from *.yaml\nself.ckpt_path = None\nself.overrides = {}  # overrides for trainer object\nself.metrics = None  # validation/training metrics\nself.session = None  # HUB session\nself.task = task  # task type\nmodel = str(model).strip()  # strip spaces\n# Check if Ultralytics HUB model from https://hub.ultralytics.com\nif self.is_hub_model(model):\nfrom ultralytics.hub.session import HUBTrainingSession\nself.session = HUBTrainingSession(model)\nmodel = self.session.model_file\n# Load or create new YOLO model\nsuffix = Path(model).suffix\nif not suffix and Path(model).stem in GITHUB_ASSETS_STEMS:\nmodel, suffix = Path(model).with_suffix('.pt'), '.pt'  # add suffix, i.e. yolov8n -&gt; yolov8n.pt\nif suffix in ('.yaml', '.yml'):\nself._new(model, task)\nelse:\nself._load(model, task)\ndef __call__(self, source=None, stream=False, **kwargs):\n\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\nreturn self.predict(source, stream, **kwargs)\n@staticmethod\ndef is_hub_model(model):\n\"\"\"Check if the provided model is a HUB model.\"\"\"\nreturn any((\nmodel.startswith(f'{HUB_WEB_ROOT}/models/'),  # i.e. https://hub.ultralytics.com/models/MODEL_ID\n[len(x) for x in model.split('_')] == [42, 20],  # APIKEY_MODELID\nlen(model) == 20 and not Path(model).exists() and all(x not in model for x in './\\\\')))  # MODELID\ndef _new(self, cfg: str, task=None, model=None, verbose=True):\n\"\"\"\n        Initializes a new model and infers the task type from the model definitions.\n        Args:\n            cfg (str): model configuration file\n            task (str | None): model task\n            model (BaseModel): Customized model.\n            verbose (bool): display model info on load\n        \"\"\"\ncfg_dict = yaml_model_load(cfg)\nself.cfg = cfg\nself.task = task or guess_model_task(cfg_dict)\nmodel = model or self.smart_load('model')\nself.model = model(cfg_dict, verbose=verbose and RANK == -1)  # build model\nself.overrides['model'] = self.cfg\n# Below added to allow export from YAMLs\nargs = {**DEFAULT_CFG_DICT, **self.overrides}  # combine model and default args, preferring model args\nself.model.args = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # attach args to model\nself.model.task = self.task\ndef _load(self, weights: str, task=None):\n\"\"\"\n        Initializes a new model and infers the task type from the model head.\n        Args:\n            weights (str): model checkpoint to be loaded\n            task (str | None): model task\n        \"\"\"\nsuffix = Path(weights).suffix\nif suffix == '.pt':\nself.model, self.ckpt = attempt_load_one_weight(weights)\nself.task = self.model.args['task']\nself.overrides = self.model.args = self._reset_ckpt_args(self.model.args)\nself.ckpt_path = self.model.pt_path\nelse:\nweights = check_file(weights)\nself.model, self.ckpt = weights, None\nself.task = task or guess_model_task(weights)\nself.ckpt_path = weights\nself.overrides['model'] = weights\nself.overrides['task'] = self.task\ndef _check_is_pytorch_model(self):\n\"\"\"\n        Raises TypeError is model is not a PyTorch model\n        \"\"\"\npt_str = isinstance(self.model, (str, Path)) and Path(self.model).suffix == '.pt'\npt_module = isinstance(self.model, nn.Module)\nif not (pt_module or pt_str):\nraise TypeError(f\"model='{self.model}' must be a *.pt PyTorch model, but is a different type. \"\nf'PyTorch models can be used to train, val, predict and export, i.e. '\nf\"'yolo export model=yolov8n.pt', but exported formats like ONNX, TensorRT etc. only \"\nf\"support 'predict' and 'val' modes, i.e. 'yolo predict model=yolov8n.onnx'.\")\n@smart_inference_mode()\ndef reset_weights(self):\n\"\"\"\n        Resets the model modules parameters to randomly initialized values, losing all training information.\n        \"\"\"\nself._check_is_pytorch_model()\nfor m in self.model.modules():\nif hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nfor p in self.model.parameters():\np.requires_grad = True\nreturn self\n@smart_inference_mode()\ndef load(self, weights='yolov8n.pt'):\n\"\"\"\n        Transfers parameters with matching names and shapes from 'weights' to model.\n        \"\"\"\nself._check_is_pytorch_model()\nif isinstance(weights, (str, Path)):\nweights, self.ckpt = attempt_load_one_weight(weights)\nself.model.load(weights)\nreturn self\ndef info(self, detailed=False, verbose=True):\n\"\"\"\n        Logs model info.\n        Args:\n            detailed (bool): Show detailed information about model.\n            verbose (bool): Controls verbosity.\n        \"\"\"\nself._check_is_pytorch_model()\nreturn self.model.info(detailed=detailed, verbose=verbose)\ndef fuse(self):\n\"\"\"Fuse PyTorch Conv2d and BatchNorm2d layers.\"\"\"\nself._check_is_pytorch_model()\nself.model.fuse()\n@smart_inference_mode()\ndef predict(self, source=None, stream=False, predictor=None, **kwargs):\n\"\"\"\n        Perform prediction using the YOLO model.\n        Args:\n            source (str | int | PIL | np.ndarray): The source of the image to make predictions on.\n                          Accepts all source types accepted by the YOLO model.\n            stream (bool): Whether to stream the predictions or not. Defaults to False.\n            predictor (BasePredictor): Customized predictor.\n            **kwargs : Additional keyword arguments passed to the predictor.\n                       Check the 'configuration' section in the documentation for all available options.\n        Returns:\n            (List[ultralytics.engine.results.Results]): The prediction results.\n        \"\"\"\nif source is None:\nsource = ASSETS\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'source' is missing. Using 'source={source}'.\")\nis_cli = (sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')) and any(\nx in sys.argv for x in ('predict', 'track', 'mode=predict', 'mode=track'))\n# Check prompts for SAM/FastSAM\nprompts = kwargs.pop('prompts', None)\noverrides = self.overrides.copy()\noverrides['conf'] = 0.25\noverrides.update(kwargs)  # prefer kwargs\noverrides['mode'] = kwargs.get('mode', 'predict')\nassert overrides['mode'] in ['track', 'predict']\nif not is_cli:\noverrides['save'] = kwargs.get('save', False)  # do not save by default if called in Python\nif not self.predictor:\nself.task = overrides.get('task') or self.task\npredictor = predictor or self.smart_load('predictor')\nself.predictor = predictor(overrides=overrides, _callbacks=self.callbacks)\nself.predictor.setup_model(model=self.model, verbose=is_cli)\nelse:  # only update args if predictor is already setup\nself.predictor.args = get_cfg(self.predictor.args, overrides)\nif 'project' in overrides or 'name' in overrides:\nself.predictor.save_dir = self.predictor.get_save_dir()\n# Set prompts for SAM/FastSAM\nif len and hasattr(self.predictor, 'set_prompts'):\nself.predictor.set_prompts(prompts)\nreturn self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\ndef track(self, source=None, stream=False, persist=False, **kwargs):\n\"\"\"\n        Perform object tracking on the input source using the registered trackers.\n        Args:\n            source (str, optional): The input source for object tracking. Can be a file path or a video stream.\n            stream (bool, optional): Whether the input source is a video stream. Defaults to False.\n            persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n            **kwargs (optional): Additional keyword arguments for the tracking process.\n        Returns:\n            (List[ultralytics.engine.results.Results]): The tracking results.\n        \"\"\"\nif not hasattr(self.predictor, 'trackers'):\nfrom ultralytics.trackers import register_tracker\nregister_tracker(self, persist)\n# ByteTrack-based method needs low confidence predictions as input\nconf = kwargs.get('conf') or 0.1\nkwargs['conf'] = conf\nkwargs['mode'] = 'track'\nreturn self.predict(source=source, stream=stream, **kwargs)\n@smart_inference_mode()\ndef val(self, data=None, validator=None, **kwargs):\n\"\"\"\n        Validate a model on a given dataset.\n        Args:\n            data (str): The dataset to validate on. Accepts all formats accepted by yolo\n            validator (BaseValidator): Customized validator.\n            **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n        \"\"\"\noverrides = self.overrides.copy()\noverrides['rect'] = True  # rect batches as default\noverrides.update(kwargs)\noverrides['mode'] = 'val'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.data = data or args.data\nif 'task' in overrides:\nself.task = args.task\nelse:\nargs.task = self.task\nvalidator = validator or self.smart_load('validator')\nargs.imgsz = check_imgsz(args.imgsz, max_dim=1)\nvalidator = validator(args=args, _callbacks=self.callbacks)\nvalidator(model=self.model)\nself.metrics = validator.metrics\nreturn validator.metrics\n@smart_inference_mode()\ndef benchmark(self, **kwargs):\n\"\"\"\n        Benchmark a model on all export formats.\n        Args:\n            **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n        \"\"\"\nself._check_is_pytorch_model()\nfrom ultralytics.utils.benchmarks import benchmark\noverrides = self.model.args.copy()\noverrides.update(kwargs)\noverrides['mode'] = 'benchmark'\noverrides = {**DEFAULT_CFG_DICT, **overrides}  # fill in missing overrides keys with defaults\nreturn benchmark(\nmodel=self,\ndata=kwargs.get('data'),  # if no 'data' argument passed set data=None for default datasets\nimgsz=overrides['imgsz'],\nhalf=overrides['half'],\nint8=overrides['int8'],\ndevice=overrides['device'],\nverbose=kwargs.get('verbose'))\ndef export(self, **kwargs):\n\"\"\"\n        Export model.\n        Args:\n            **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n        \"\"\"\nself._check_is_pytorch_model()\noverrides = self.overrides.copy()\noverrides.update(kwargs)\noverrides['mode'] = 'export'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nif 'batch' not in kwargs:\noverrides['batch'] = 1  # default to 1 if not modified\nif 'data' not in kwargs:\noverrides['data'] = None  # default to None if not modified (avoid int8 calibration with coco.yaml)\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.task = self.task\nreturn Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\ndef train(self, trainer=None, **kwargs):\n\"\"\"\n        Trains the model on a given dataset.\n        Args:\n            trainer (BaseTrainer, optional): Customized trainer.\n            **kwargs (Any): Any number of arguments representing the training configuration.\n        \"\"\"\nself._check_is_pytorch_model()\nif self.session:  # Ultralytics HUB session\nif any(kwargs):\nLOGGER.warning('WARNING \u26a0\ufe0f using HUB training arguments, ignoring local training arguments.')\nkwargs = self.session.train_args\ncheck_pip_update_available()\noverrides = self.overrides.copy()\nif kwargs.get('cfg'):\nLOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\noverrides = yaml_load(check_yaml(kwargs['cfg']))\noverrides.update(kwargs)\noverrides['mode'] = 'train'\nif not overrides.get('data'):\nraise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\nif overrides.get('resume'):\noverrides['resume'] = self.ckpt_path\nself.task = overrides.get('task') or self.task\ntrainer = trainer or self.smart_load('trainer')\nself.trainer = trainer(overrides=overrides, _callbacks=self.callbacks)\nif not overrides.get('resume'):  # manually set model only if not resuming\nself.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\nself.model = self.trainer.model\nself.trainer.hub_session = self.session  # attach optional HUB session\nself.trainer.train()\n# Update model and cfg after training\nif RANK in (-1, 0):\nself.model, _ = attempt_load_one_weight(str(self.trainer.best))\nself.overrides = self.model.args\nself.metrics = getattr(self.trainer.validator, 'metrics', None)  # TODO: no metrics returned by DDP\ndef to(self, device):\n\"\"\"\n        Sends the model to the given device.\n        Args:\n            device (str): device\n        \"\"\"\nself._check_is_pytorch_model()\nself.model.to(device)\nreturn self\ndef tune(self, *args, **kwargs):\n\"\"\"\n        Runs hyperparameter tuning using Ray Tune. See ultralytics.utils.tuner.run_ray_tune for Args.\n        Returns:\n            (dict): A dictionary containing the results of the hyperparameter search.\n        Raises:\n            ModuleNotFoundError: If Ray Tune is not installed.\n        \"\"\"\nself._check_is_pytorch_model()\nfrom ultralytics.utils.tuner import run_ray_tune\nreturn run_ray_tune(self, *args, **kwargs)\n@property\ndef names(self):\n\"\"\"Returns class names of the loaded model.\"\"\"\nreturn self.model.names if hasattr(self.model, 'names') else None\n@property\ndef device(self):\n\"\"\"Returns device if PyTorch model.\"\"\"\nreturn next(self.model.parameters()).device if isinstance(self.model, nn.Module) else None\n@property\ndef transforms(self):\n\"\"\"Returns transform of the loaded model.\"\"\"\nreturn self.model.transforms if hasattr(self.model, 'transforms') else None\ndef add_callback(self, event: str, func):\n\"\"\"Add a callback.\"\"\"\nself.callbacks[event].append(func)\ndef clear_callback(self, event: str):\n\"\"\"Clear all event callbacks.\"\"\"\nself.callbacks[event] = []\n@staticmethod\ndef _reset_ckpt_args(args):\n\"\"\"Reset arguments when loading a PyTorch model.\"\"\"\ninclude = {'imgsz', 'data', 'task', 'single_cls'}  # only remember these arguments when loading a PyTorch model\nreturn {k: v for k, v in args.items() if k in include}\ndef _reset_callbacks(self):\n\"\"\"Reset all registered callbacks.\"\"\"\nfor event in callbacks.default_callbacks.keys():\nself.callbacks[event] = [callbacks.default_callbacks[event][0]]\ndef __getattr__(self, attr):\n\"\"\"Raises error if object has no requested attribute.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\ndef smart_load(self, key):\n\"\"\"Load model/trainer/validator/predictor.\"\"\"\ntry:\nreturn self.task_map[self.task][key]\nexcept Exception as e:\nname = self.__class__.__name__\nmode = inspect.stack()[1][3]  # get the function name.\nraise NotImplementedError(\nemojis(f'WARNING \u26a0\ufe0f `{name}` model does not support `{mode}` mode for `{self.task}` task yet.')) from e\n@property\ndef task_map(self):\n\"\"\"\n        Map head to model, trainer, validator, and predictor classes.\n        Returns:\n            task_map (dict): The map of model task to mode classes.\n        \"\"\"\nraise NotImplementedError('Please provide task map for your model!')\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.device","title":"<code>device</code>  <code>property</code>","text":"<p>Returns device if PyTorch model.</p>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.names","title":"<code>names</code>  <code>property</code>","text":"<p>Returns class names of the loaded model.</p>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.task_map","title":"<code>task_map</code>  <code>property</code>","text":"<p>Map head to model, trainer, validator, and predictor classes.</p> <p>Returns:</p> Name Type Description <code>task_map</code> <code>dict</code> <p>The map of model task to mode classes.</p>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.transforms","title":"<code>transforms</code>  <code>property</code>","text":"<p>Returns transform of the loaded model.</p>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.__call__","title":"<code>__call__(source=None, stream=False, **kwargs)</code>","text":"<p>Calls the 'predict' function with given arguments to perform object detection.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def __call__(self, source=None, stream=False, **kwargs):\n\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\nreturn self.predict(source, stream, **kwargs)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>Raises error if object has no requested attribute.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def __getattr__(self, attr):\n\"\"\"Raises error if object has no requested attribute.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.__init__","title":"<code>__init__(model='yolov8n.pt', task=None)</code>","text":"<p>Initializes the YOLO model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[str, Path]</code> <p>Path or name of the model to load or create. Defaults to 'yolov8n.pt'.</p> <code>'yolov8n.pt'</code> <code>task</code> <code>Any</code> <p>Task type for the YOLO model. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def __init__(self, model: Union[str, Path] = 'yolov8n.pt', task=None) -&gt; None:\n\"\"\"\n    Initializes the YOLO model.\n    Args:\n        model (Union[str, Path], optional): Path or name of the model to load or create. Defaults to 'yolov8n.pt'.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n    \"\"\"\nself.callbacks = callbacks.get_default_callbacks()\nself.predictor = None  # reuse predictor\nself.model = None  # model object\nself.trainer = None  # trainer object\nself.ckpt = None  # if loaded from *.pt\nself.cfg = None  # if loaded from *.yaml\nself.ckpt_path = None\nself.overrides = {}  # overrides for trainer object\nself.metrics = None  # validation/training metrics\nself.session = None  # HUB session\nself.task = task  # task type\nmodel = str(model).strip()  # strip spaces\n# Check if Ultralytics HUB model from https://hub.ultralytics.com\nif self.is_hub_model(model):\nfrom ultralytics.hub.session import HUBTrainingSession\nself.session = HUBTrainingSession(model)\nmodel = self.session.model_file\n# Load or create new YOLO model\nsuffix = Path(model).suffix\nif not suffix and Path(model).stem in GITHUB_ASSETS_STEMS:\nmodel, suffix = Path(model).with_suffix('.pt'), '.pt'  # add suffix, i.e. yolov8n -&gt; yolov8n.pt\nif suffix in ('.yaml', '.yml'):\nself._new(model, task)\nelse:\nself._load(model, task)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.add_callback","title":"<code>add_callback(event, func)</code>","text":"<p>Add a callback.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def add_callback(self, event: str, func):\n\"\"\"Add a callback.\"\"\"\nself.callbacks[event].append(func)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.benchmark","title":"<code>benchmark(**kwargs)</code>","text":"<p>Benchmark a model on all export formats.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Any other args accepted by the validators. To see all args check 'configuration' section in docs</p> <code>{}</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef benchmark(self, **kwargs):\n\"\"\"\n    Benchmark a model on all export formats.\n    Args:\n        **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n    \"\"\"\nself._check_is_pytorch_model()\nfrom ultralytics.utils.benchmarks import benchmark\noverrides = self.model.args.copy()\noverrides.update(kwargs)\noverrides['mode'] = 'benchmark'\noverrides = {**DEFAULT_CFG_DICT, **overrides}  # fill in missing overrides keys with defaults\nreturn benchmark(\nmodel=self,\ndata=kwargs.get('data'),  # if no 'data' argument passed set data=None for default datasets\nimgsz=overrides['imgsz'],\nhalf=overrides['half'],\nint8=overrides['int8'],\ndevice=overrides['device'],\nverbose=kwargs.get('verbose'))\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.clear_callback","title":"<code>clear_callback(event)</code>","text":"<p>Clear all event callbacks.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def clear_callback(self, event: str):\n\"\"\"Clear all event callbacks.\"\"\"\nself.callbacks[event] = []\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.export","title":"<code>export(**kwargs)</code>","text":"<p>Export model.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Any other args accepted by the predictors. To see all args check 'configuration' section in docs</p> <code>{}</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def export(self, **kwargs):\n\"\"\"\n    Export model.\n    Args:\n        **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n    \"\"\"\nself._check_is_pytorch_model()\noverrides = self.overrides.copy()\noverrides.update(kwargs)\noverrides['mode'] = 'export'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nif 'batch' not in kwargs:\noverrides['batch'] = 1  # default to 1 if not modified\nif 'data' not in kwargs:\noverrides['data'] = None  # default to None if not modified (avoid int8 calibration with coco.yaml)\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.task = self.task\nreturn Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.fuse","title":"<code>fuse()</code>","text":"<p>Fuse PyTorch Conv2d and BatchNorm2d layers.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def fuse(self):\n\"\"\"Fuse PyTorch Conv2d and BatchNorm2d layers.\"\"\"\nself._check_is_pytorch_model()\nself.model.fuse()\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.info","title":"<code>info(detailed=False, verbose=True)</code>","text":"<p>Logs model info.</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>Show detailed information about model.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Controls verbosity.</p> <code>True</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def info(self, detailed=False, verbose=True):\n\"\"\"\n    Logs model info.\n    Args:\n        detailed (bool): Show detailed information about model.\n        verbose (bool): Controls verbosity.\n    \"\"\"\nself._check_is_pytorch_model()\nreturn self.model.info(detailed=detailed, verbose=verbose)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.is_hub_model","title":"<code>is_hub_model(model)</code>  <code>staticmethod</code>","text":"<p>Check if the provided model is a HUB model.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@staticmethod\ndef is_hub_model(model):\n\"\"\"Check if the provided model is a HUB model.\"\"\"\nreturn any((\nmodel.startswith(f'{HUB_WEB_ROOT}/models/'),  # i.e. https://hub.ultralytics.com/models/MODEL_ID\n[len(x) for x in model.split('_')] == [42, 20],  # APIKEY_MODELID\nlen(model) == 20 and not Path(model).exists() and all(x not in model for x in './\\\\')))  # MODELID\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.load","title":"<code>load(weights='yolov8n.pt')</code>","text":"<p>Transfers parameters with matching names and shapes from 'weights' to model.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef load(self, weights='yolov8n.pt'):\n\"\"\"\n    Transfers parameters with matching names and shapes from 'weights' to model.\n    \"\"\"\nself._check_is_pytorch_model()\nif isinstance(weights, (str, Path)):\nweights, self.ckpt = attempt_load_one_weight(weights)\nself.model.load(weights)\nreturn self\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.predict","title":"<code>predict(source=None, stream=False, predictor=None, **kwargs)</code>","text":"<p>Perform prediction using the YOLO model.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | int | PIL | ndarray</code> <p>The source of the image to make predictions on.           Accepts all source types accepted by the YOLO model.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the predictions or not. Defaults to False.</p> <code>False</code> <code>predictor</code> <code>BasePredictor</code> <p>Customized predictor.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the predictor.        Check the 'configuration' section in the documentation for all available options.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ultralytics.engine.results.Results]</code> <p>The prediction results.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef predict(self, source=None, stream=False, predictor=None, **kwargs):\n\"\"\"\n    Perform prediction using the YOLO model.\n    Args:\n        source (str | int | PIL | np.ndarray): The source of the image to make predictions on.\n                      Accepts all source types accepted by the YOLO model.\n        stream (bool): Whether to stream the predictions or not. Defaults to False.\n        predictor (BasePredictor): Customized predictor.\n        **kwargs : Additional keyword arguments passed to the predictor.\n                   Check the 'configuration' section in the documentation for all available options.\n    Returns:\n        (List[ultralytics.engine.results.Results]): The prediction results.\n    \"\"\"\nif source is None:\nsource = ASSETS\nLOGGER.warning(f\"WARNING \u26a0\ufe0f 'source' is missing. Using 'source={source}'.\")\nis_cli = (sys.argv[0].endswith('yolo') or sys.argv[0].endswith('ultralytics')) and any(\nx in sys.argv for x in ('predict', 'track', 'mode=predict', 'mode=track'))\n# Check prompts for SAM/FastSAM\nprompts = kwargs.pop('prompts', None)\noverrides = self.overrides.copy()\noverrides['conf'] = 0.25\noverrides.update(kwargs)  # prefer kwargs\noverrides['mode'] = kwargs.get('mode', 'predict')\nassert overrides['mode'] in ['track', 'predict']\nif not is_cli:\noverrides['save'] = kwargs.get('save', False)  # do not save by default if called in Python\nif not self.predictor:\nself.task = overrides.get('task') or self.task\npredictor = predictor or self.smart_load('predictor')\nself.predictor = predictor(overrides=overrides, _callbacks=self.callbacks)\nself.predictor.setup_model(model=self.model, verbose=is_cli)\nelse:  # only update args if predictor is already setup\nself.predictor.args = get_cfg(self.predictor.args, overrides)\nif 'project' in overrides or 'name' in overrides:\nself.predictor.save_dir = self.predictor.get_save_dir()\n# Set prompts for SAM/FastSAM\nif len and hasattr(self.predictor, 'set_prompts'):\nself.predictor.set_prompts(prompts)\nreturn self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.reset_weights","title":"<code>reset_weights()</code>","text":"<p>Resets the model modules parameters to randomly initialized values, losing all training information.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef reset_weights(self):\n\"\"\"\n    Resets the model modules parameters to randomly initialized values, losing all training information.\n    \"\"\"\nself._check_is_pytorch_model()\nfor m in self.model.modules():\nif hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nfor p in self.model.parameters():\np.requires_grad = True\nreturn self\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.smart_load","title":"<code>smart_load(key)</code>","text":"<p>Load model/trainer/validator/predictor.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def smart_load(self, key):\n\"\"\"Load model/trainer/validator/predictor.\"\"\"\ntry:\nreturn self.task_map[self.task][key]\nexcept Exception as e:\nname = self.__class__.__name__\nmode = inspect.stack()[1][3]  # get the function name.\nraise NotImplementedError(\nemojis(f'WARNING \u26a0\ufe0f `{name}` model does not support `{mode}` mode for `{self.task}` task yet.')) from e\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.to","title":"<code>to(device)</code>","text":"<p>Sends the model to the given device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>device</p> required Source code in <code>ultralytics/engine/model.py</code> <pre><code>def to(self, device):\n\"\"\"\n    Sends the model to the given device.\n    Args:\n        device (str): device\n    \"\"\"\nself._check_is_pytorch_model()\nself.model.to(device)\nreturn self\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.track","title":"<code>track(source=None, stream=False, persist=False, **kwargs)</code>","text":"<p>Perform object tracking on the input source using the registered trackers.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The input source for object tracking. Can be a file path or a video stream.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether the input source is a video stream. Defaults to False.</p> <code>False</code> <code>persist</code> <code>bool</code> <p>Whether to persist the trackers if they already exist. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments for the tracking process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ultralytics.engine.results.Results]</code> <p>The tracking results.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def track(self, source=None, stream=False, persist=False, **kwargs):\n\"\"\"\n    Perform object tracking on the input source using the registered trackers.\n    Args:\n        source (str, optional): The input source for object tracking. Can be a file path or a video stream.\n        stream (bool, optional): Whether the input source is a video stream. Defaults to False.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n        **kwargs (optional): Additional keyword arguments for the tracking process.\n    Returns:\n        (List[ultralytics.engine.results.Results]): The tracking results.\n    \"\"\"\nif not hasattr(self.predictor, 'trackers'):\nfrom ultralytics.trackers import register_tracker\nregister_tracker(self, persist)\n# ByteTrack-based method needs low confidence predictions as input\nconf = kwargs.get('conf') or 0.1\nkwargs['conf'] = conf\nkwargs['mode'] = 'track'\nreturn self.predict(source=source, stream=stream, **kwargs)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.train","title":"<code>train(trainer=None, **kwargs)</code>","text":"<p>Trains the model on a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>BaseTrainer</code> <p>Customized trainer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Any number of arguments representing the training configuration.</p> <code>{}</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def train(self, trainer=None, **kwargs):\n\"\"\"\n    Trains the model on a given dataset.\n    Args:\n        trainer (BaseTrainer, optional): Customized trainer.\n        **kwargs (Any): Any number of arguments representing the training configuration.\n    \"\"\"\nself._check_is_pytorch_model()\nif self.session:  # Ultralytics HUB session\nif any(kwargs):\nLOGGER.warning('WARNING \u26a0\ufe0f using HUB training arguments, ignoring local training arguments.')\nkwargs = self.session.train_args\ncheck_pip_update_available()\noverrides = self.overrides.copy()\nif kwargs.get('cfg'):\nLOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\noverrides = yaml_load(check_yaml(kwargs['cfg']))\noverrides.update(kwargs)\noverrides['mode'] = 'train'\nif not overrides.get('data'):\nraise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\nif overrides.get('resume'):\noverrides['resume'] = self.ckpt_path\nself.task = overrides.get('task') or self.task\ntrainer = trainer or self.smart_load('trainer')\nself.trainer = trainer(overrides=overrides, _callbacks=self.callbacks)\nif not overrides.get('resume'):  # manually set model only if not resuming\nself.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\nself.model = self.trainer.model\nself.trainer.hub_session = self.session  # attach optional HUB session\nself.trainer.train()\n# Update model and cfg after training\nif RANK in (-1, 0):\nself.model, _ = attempt_load_one_weight(str(self.trainer.best))\nself.overrides = self.model.args\nself.metrics = getattr(self.trainer.validator, 'metrics', None)  # TODO: no metrics returned by DDP\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.tune","title":"<code>tune(*args, **kwargs)</code>","text":"<p>Runs hyperparameter tuning using Ray Tune. See ultralytics.utils.tuner.run_ray_tune for Args.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the hyperparameter search.</p> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If Ray Tune is not installed.</p> Source code in <code>ultralytics/engine/model.py</code> <pre><code>def tune(self, *args, **kwargs):\n\"\"\"\n    Runs hyperparameter tuning using Ray Tune. See ultralytics.utils.tuner.run_ray_tune for Args.\n    Returns:\n        (dict): A dictionary containing the results of the hyperparameter search.\n    Raises:\n        ModuleNotFoundError: If Ray Tune is not installed.\n    \"\"\"\nself._check_is_pytorch_model()\nfrom ultralytics.utils.tuner import run_ray_tune\nreturn run_ray_tune(self, *args, **kwargs)\n</code></pre>"},{"location":"reference/engine/model/#ultralytics.engine.model.Model.val","title":"<code>val(data=None, validator=None, **kwargs)</code>","text":"<p>Validate a model on a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The dataset to validate on. Accepts all formats accepted by yolo</p> <code>None</code> <code>validator</code> <code>BaseValidator</code> <p>Customized validator.</p> <code>None</code> <code>**kwargs</code> <p>Any other args accepted by the validators. To see all args check 'configuration' section in docs</p> <code>{}</code> Source code in <code>ultralytics/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef val(self, data=None, validator=None, **kwargs):\n\"\"\"\n    Validate a model on a given dataset.\n    Args:\n        data (str): The dataset to validate on. Accepts all formats accepted by yolo\n        validator (BaseValidator): Customized validator.\n        **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n    \"\"\"\noverrides = self.overrides.copy()\noverrides['rect'] = True  # rect batches as default\noverrides.update(kwargs)\noverrides['mode'] = 'val'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = self.model.args['imgsz']  # use trained imgsz unless custom value is passed\nargs = get_cfg(cfg=DEFAULT_CFG, overrides=overrides)\nargs.data = data or args.data\nif 'task' in overrides:\nself.task = args.task\nelse:\nargs.task = self.task\nvalidator = validator or self.smart_load('validator')\nargs.imgsz = check_imgsz(args.imgsz, max_dim=1)\nvalidator = validator(args=args, _callbacks=self.callbacks)\nvalidator(model=self.model)\nself.metrics = validator.metrics\nreturn validator.metrics\n</code></pre>"},{"location":"reference/engine/predictor/","title":"Reference for <code>ultralytics/engine/predictor.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/predictor.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor","title":"<code>ultralytics.engine.predictor.BasePredictor</code>","text":"<p>BasePredictor</p> <p>A base class for creating predictors.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the predictor.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>done_warmup</code> <code>bool</code> <p>Whether the predictor has finished setup.</p> <code>model</code> <code>Module</code> <p>Model used for prediction.</p> <code>data</code> <code>dict</code> <p>Data configuration.</p> <code>device</code> <code>device</code> <p>Device used for prediction.</p> <code>dataset</code> <code>Dataset</code> <p>Dataset used for prediction.</p> <code>vid_path</code> <code>str</code> <p>Path to video file.</p> <code>vid_writer</code> <code>VideoWriter</code> <p>Video writer for saving video output.</p> <code>data_path</code> <code>str</code> <p>Path to data.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>class BasePredictor:\n\"\"\"\n    BasePredictor\n    A base class for creating predictors.\n    Attributes:\n        args (SimpleNamespace): Configuration for the predictor.\n        save_dir (Path): Directory to save results.\n        done_warmup (bool): Whether the predictor has finished setup.\n        model (nn.Module): Model used for prediction.\n        data (dict): Data configuration.\n        device (torch.device): Device used for prediction.\n        dataset (Dataset): Dataset used for prediction.\n        vid_path (str): Path to video file.\n        vid_writer (cv2.VideoWriter): Video writer for saving video output.\n        data_path (str): Path to data.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n        Initializes the BasePredictor class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.save_dir = self.get_save_dir()\nif self.args.conf is None:\nself.args.conf = 0.25  # default conf=0.25\nself.done_warmup = False\nif self.args.show:\nself.args.show = check_imshow(warn=True)\n# Usable if setup is done\nself.model = None\nself.data = self.args.data  # data_dict\nself.imgsz = None\nself.device = None\nself.dataset = None\nself.vid_path, self.vid_writer = None, None\nself.plotted_img = None\nself.data_path = None\nself.source_type = None\nself.batch = None\nself.results = None\nself.transforms = None\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\nself.txt_path = None\ncallbacks.add_integration_callbacks(self)\ndef get_save_dir(self):\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f'{self.args.mode}'\nreturn increment_path(Path(project) / name, exist_ok=self.args.exist_ok)\ndef preprocess(self, im):\n\"\"\"Prepares input image before inference.\n        Args:\n            im (torch.Tensor | List(np.ndarray)): BCHW for tensor, [(HWC) x B] for list.\n        \"\"\"\nnot_tensor = not isinstance(im, torch.Tensor)\nif not_tensor:\nim = np.stack(self.pre_transform(im))\nim = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\nim = np.ascontiguousarray(im)  # contiguous\nim = torch.from_numpy(im)\nimg = im.to(self.device)\nimg = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\nif not_tensor:\nimg /= 255  # 0 - 255 to 0.0 - 1.0\nreturn img\ndef inference(self, im, *args, **kwargs):\nvisualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,\nmkdir=True) if self.args.visualize and (not self.source_type.tensor) else False\nreturn self.model(im, augment=self.args.augment, visualize=visualize)\ndef pre_transform(self, im):\n\"\"\"\n        Pre-transform input image before inference.\n        Args:\n            im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n        Returns:\n            (list): A list of transformed images.\n        \"\"\"\nsame_shapes = all(x.shape == im[0].shape for x in im)\nauto = same_shapes and self.model.pt\nreturn [LetterBox(self.imgsz, auto=auto, stride=self.model.stride)(image=x) for x in im]\ndef write_results(self, idx, results, batch):\n\"\"\"Write inference results to a file or directory.\"\"\"\np, im, _ = batch\nlog_string = ''\nif len(im.shape) == 3:\nim = im[None]  # expand for batch dim\nif self.source_type.webcam or self.source_type.from_img or self.source_type.tensor:  # batch_size &gt;= 1\nlog_string += f'{idx}: '\nframe = self.dataset.count\nelse:\nframe = getattr(self.dataset, 'frame', 0)\nself.data_path = p\nself.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')\nlog_string += '%gx%g ' % im.shape[2:]  # print string\nresult = results[idx]\nlog_string += result.verbose()\nif self.args.save or self.args.show:  # Add bbox to image\nplot_args = {\n'line_width': self.args.line_width,\n'boxes': self.args.boxes,\n'conf': self.args.show_conf,\n'labels': self.args.show_labels}\nif not self.args.retina_masks:\nplot_args['im_gpu'] = im[idx]\nself.plotted_img = result.plot(**plot_args)\n# Write\nif self.args.save_txt:\nresult.save_txt(f'{self.txt_path}.txt', save_conf=self.args.save_conf)\nif self.args.save_crop:\nresult.save_crop(save_dir=self.save_dir / 'crops',\nfile_name=self.data_path.stem + ('' if self.dataset.mode == 'image' else f'_{frame}'))\nreturn log_string\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions for an image and returns them.\"\"\"\nreturn preds\ndef __call__(self, source=None, model=None, stream=False, *args, **kwargs):\n\"\"\"Performs inference on an image or stream.\"\"\"\nself.stream = stream\nif stream:\nreturn self.stream_inference(source, model, *args, **kwargs)\nelse:\nreturn list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\ndef predict_cli(self, source=None, model=None):\n\"\"\"Method used for CLI prediction. It uses always generator as outputs as not required by CLI mode.\"\"\"\ngen = self.stream_inference(source, model)\nfor _ in gen:  # running CLI inference without accumulating any outputs (do not modify)\npass\ndef setup_source(self, source):\n\"\"\"Sets up source and inference mode.\"\"\"\nself.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)  # check image size\nself.transforms = getattr(self.model.model, 'transforms', classify_transforms(\nself.imgsz[0])) if self.args.task == 'classify' else None\nself.dataset = load_inference_source(source=source, imgsz=self.imgsz, vid_stride=self.args.vid_stride)\nself.source_type = self.dataset.source_type\nif not getattr(self, 'stream', True) and (self.dataset.mode == 'stream' or  # streams\nlen(self.dataset) &gt; 1000 or  # images\nany(getattr(self.dataset, 'video_flag', [False]))):  # videos\nLOGGER.warning(STREAM_WARNING)\nself.vid_path, self.vid_writer = [None] * self.dataset.bs, [None] * self.dataset.bs\n@smart_inference_mode()\ndef stream_inference(self, source=None, model=None, *args, **kwargs):\n\"\"\"Streams real-time inference on camera feed and saves results to file.\"\"\"\nif self.args.verbose:\nLOGGER.info('')\n# Setup model\nif not self.model:\nself.setup_model(model)\n# Setup source every time predict is called\nself.setup_source(source if source is not None else self.args.source)\n# Check if save_dir/ label file exists\nif self.args.save or self.args.save_txt:\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n# Warmup model\nif not self.done_warmup:\nself.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))\nself.done_warmup = True\nself.seen, self.windows, self.batch, profilers = 0, [], None, (ops.Profile(), ops.Profile(), ops.Profile())\nself.run_callbacks('on_predict_start')\nfor batch in self.dataset:\nself.run_callbacks('on_predict_batch_start')\nself.batch = batch\npath, im0s, vid_cap, s = batch\n# Preprocess\nwith profilers[0]:\nim = self.preprocess(im0s)\n# Inference\nwith profilers[1]:\npreds = self.inference(im, *args, **kwargs)\n# Postprocess\nwith profilers[2]:\nself.results = self.postprocess(preds, im, im0s)\nself.run_callbacks('on_predict_postprocess_end')\n# Visualize, save, write results\nn = len(im0s)\nfor i in range(n):\nself.seen += 1\nself.results[i].speed = {\n'preprocess': profilers[0].dt * 1E3 / n,\n'inference': profilers[1].dt * 1E3 / n,\n'postprocess': profilers[2].dt * 1E3 / n}\np, im0 = path[i], None if self.source_type.tensor else im0s[i].copy()\np = Path(p)\nif self.args.verbose or self.args.save or self.args.save_txt or self.args.show:\ns += self.write_results(i, self.results, (p, im, im0))\nif self.args.save or self.args.save_txt:\nself.results[i].save_dir = self.save_dir.__str__()\nif self.args.show and self.plotted_img is not None:\nself.show(p)\nif self.args.save and self.plotted_img is not None:\nself.save_preds(vid_cap, i, str(self.save_dir / p.name))\nself.run_callbacks('on_predict_batch_end')\nyield from self.results\n# Print time (inference-only)\nif self.args.verbose:\nLOGGER.info(f'{s}{profilers[1].dt * 1E3:.1f}ms')\n# Release assets\nif isinstance(self.vid_writer[-1], cv2.VideoWriter):\nself.vid_writer[-1].release()  # release final video writer\n# Print results\nif self.args.verbose and self.seen:\nt = tuple(x.t / self.seen * 1E3 for x in profilers)  # speeds per image\nLOGGER.info(f'Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape '\nf'{(1, 3, *im.shape[2:])}' % t)\nif self.args.save or self.args.save_txt or self.args.save_crop:\nnl = len(list(self.save_dir.glob('labels/*.txt')))  # number of labels\ns = f\"\\n{nl} label{'s' * (nl &gt; 1)} saved to {self.save_dir / 'labels'}\" if self.args.save_txt else ''\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}{s}\")\nself.run_callbacks('on_predict_end')\ndef setup_model(self, model, verbose=True):\n\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\nself.model = AutoBackend(model or self.args.model,\ndevice=select_device(self.args.device, verbose=verbose),\ndnn=self.args.dnn,\ndata=self.args.data,\nfp16=self.args.half,\nfuse=True,\nverbose=verbose)\nself.device = self.model.device  # update device\nself.args.half = self.model.fp16  # update half\nself.model.eval()\ndef show(self, p):\n\"\"\"Display an image in a window using OpenCV imshow().\"\"\"\nim0 = self.plotted_img\nif platform.system() == 'Linux' and p not in self.windows:\nself.windows.append(p)\ncv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\ncv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\ncv2.imshow(str(p), im0)\ncv2.waitKey(500 if self.batch[3].startswith('image') else 1)  # 1 millisecond\ndef save_preds(self, vid_cap, idx, save_path):\n\"\"\"Save video predictions as mp4 at specified path.\"\"\"\nim0 = self.plotted_img\n# Save imgs\nif self.dataset.mode == 'image':\ncv2.imwrite(save_path, im0)\nelse:  # 'video' or 'stream'\nif self.vid_path[idx] != save_path:  # new video\nself.vid_path[idx] = save_path\nif isinstance(self.vid_writer[idx], cv2.VideoWriter):\nself.vid_writer[idx].release()  # release previous video writer\nif vid_cap:  # video\nfps = int(vid_cap.get(cv2.CAP_PROP_FPS))  # integer required, floats produce error in MP4 codec\nw = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nelse:  # stream\nfps, w, h = 30, im0.shape[1], im0.shape[0]\nsuffix = '.mp4' if MACOS else '.avi' if WINDOWS else '.avi'\nfourcc = 'avc1' if MACOS else 'WMV2' if WINDOWS else 'MJPG'\nsave_path = str(Path(save_path).with_suffix(suffix))\nself.vid_writer[idx] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\nself.vid_writer[idx].write(im0)\ndef run_callbacks(self, event: str):\n\"\"\"Runs all registered callbacks for a specific event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\ndef add_callback(self, event: str, func):\n\"\"\"\n        Add callback\n        \"\"\"\nself.callbacks[event].append(func)\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.__call__","title":"<code>__call__(source=None, model=None, stream=False, *args, **kwargs)</code>","text":"<p>Performs inference on an image or stream.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def __call__(self, source=None, model=None, stream=False, *args, **kwargs):\n\"\"\"Performs inference on an image or stream.\"\"\"\nself.stream = stream\nif stream:\nreturn self.stream_inference(source, model, *args, **kwargs)\nelse:\nreturn list(self.stream_inference(source, model, *args, **kwargs))  # merge list of Result into one\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initializes the BasePredictor class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n    Initializes the BasePredictor class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.save_dir = self.get_save_dir()\nif self.args.conf is None:\nself.args.conf = 0.25  # default conf=0.25\nself.done_warmup = False\nif self.args.show:\nself.args.show = check_imshow(warn=True)\n# Usable if setup is done\nself.model = None\nself.data = self.args.data  # data_dict\nself.imgsz = None\nself.device = None\nself.dataset = None\nself.vid_path, self.vid_writer = None, None\nself.plotted_img = None\nself.data_path = None\nself.source_type = None\nself.batch = None\nself.results = None\nself.transforms = None\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\nself.txt_path = None\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.add_callback","title":"<code>add_callback(event, func)</code>","text":"<p>Add callback</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def add_callback(self, event: str, func):\n\"\"\"\n    Add callback\n    \"\"\"\nself.callbacks[event].append(func)\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Post-processes predictions for an image and returns them.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions for an image and returns them.\"\"\"\nreturn preds\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.pre_transform","title":"<code>pre_transform(im)</code>","text":"<p>Pre-transform input image before inference.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>List(np.ndarray</code> <p>(N, 3, h, w) for tensor, [(h, w, 3) x N] for list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of transformed images.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def pre_transform(self, im):\n\"\"\"\n    Pre-transform input image before inference.\n    Args:\n        im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n    Returns:\n        (list): A list of transformed images.\n    \"\"\"\nsame_shapes = all(x.shape == im[0].shape for x in im)\nauto = same_shapes and self.model.pt\nreturn [LetterBox(self.imgsz, auto=auto, stride=self.model.stride)(image=x) for x in im]\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.predict_cli","title":"<code>predict_cli(source=None, model=None)</code>","text":"<p>Method used for CLI prediction. It uses always generator as outputs as not required by CLI mode.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def predict_cli(self, source=None, model=None):\n\"\"\"Method used for CLI prediction. It uses always generator as outputs as not required by CLI mode.\"\"\"\ngen = self.stream_inference(source, model)\nfor _ in gen:  # running CLI inference without accumulating any outputs (do not modify)\npass\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.preprocess","title":"<code>preprocess(im)</code>","text":"<p>Prepares input image before inference.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>torch.Tensor | List(np.ndarray</code> <p>BCHW for tensor, [(HWC) x B] for list.</p> required Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def preprocess(self, im):\n\"\"\"Prepares input image before inference.\n    Args:\n        im (torch.Tensor | List(np.ndarray)): BCHW for tensor, [(HWC) x B] for list.\n    \"\"\"\nnot_tensor = not isinstance(im, torch.Tensor)\nif not_tensor:\nim = np.stack(self.pre_transform(im))\nim = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\nim = np.ascontiguousarray(im)  # contiguous\nim = torch.from_numpy(im)\nimg = im.to(self.device)\nimg = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\nif not_tensor:\nimg /= 255  # 0 - 255 to 0.0 - 1.0\nreturn img\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.run_callbacks","title":"<code>run_callbacks(event)</code>","text":"<p>Runs all registered callbacks for a specific event.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def run_callbacks(self, event: str):\n\"\"\"Runs all registered callbacks for a specific event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.save_preds","title":"<code>save_preds(vid_cap, idx, save_path)</code>","text":"<p>Save video predictions as mp4 at specified path.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def save_preds(self, vid_cap, idx, save_path):\n\"\"\"Save video predictions as mp4 at specified path.\"\"\"\nim0 = self.plotted_img\n# Save imgs\nif self.dataset.mode == 'image':\ncv2.imwrite(save_path, im0)\nelse:  # 'video' or 'stream'\nif self.vid_path[idx] != save_path:  # new video\nself.vid_path[idx] = save_path\nif isinstance(self.vid_writer[idx], cv2.VideoWriter):\nself.vid_writer[idx].release()  # release previous video writer\nif vid_cap:  # video\nfps = int(vid_cap.get(cv2.CAP_PROP_FPS))  # integer required, floats produce error in MP4 codec\nw = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nelse:  # stream\nfps, w, h = 30, im0.shape[1], im0.shape[0]\nsuffix = '.mp4' if MACOS else '.avi' if WINDOWS else '.avi'\nfourcc = 'avc1' if MACOS else 'WMV2' if WINDOWS else 'MJPG'\nsave_path = str(Path(save_path).with_suffix(suffix))\nself.vid_writer[idx] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\nself.vid_writer[idx].write(im0)\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.setup_model","title":"<code>setup_model(model, verbose=True)</code>","text":"<p>Initialize YOLO model with given parameters and set it to evaluation mode.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def setup_model(self, model, verbose=True):\n\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\nself.model = AutoBackend(model or self.args.model,\ndevice=select_device(self.args.device, verbose=verbose),\ndnn=self.args.dnn,\ndata=self.args.data,\nfp16=self.args.half,\nfuse=True,\nverbose=verbose)\nself.device = self.model.device  # update device\nself.args.half = self.model.fp16  # update half\nself.model.eval()\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.setup_source","title":"<code>setup_source(source)</code>","text":"<p>Sets up source and inference mode.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def setup_source(self, source):\n\"\"\"Sets up source and inference mode.\"\"\"\nself.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)  # check image size\nself.transforms = getattr(self.model.model, 'transforms', classify_transforms(\nself.imgsz[0])) if self.args.task == 'classify' else None\nself.dataset = load_inference_source(source=source, imgsz=self.imgsz, vid_stride=self.args.vid_stride)\nself.source_type = self.dataset.source_type\nif not getattr(self, 'stream', True) and (self.dataset.mode == 'stream' or  # streams\nlen(self.dataset) &gt; 1000 or  # images\nany(getattr(self.dataset, 'video_flag', [False]))):  # videos\nLOGGER.warning(STREAM_WARNING)\nself.vid_path, self.vid_writer = [None] * self.dataset.bs, [None] * self.dataset.bs\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.show","title":"<code>show(p)</code>","text":"<p>Display an image in a window using OpenCV imshow().</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def show(self, p):\n\"\"\"Display an image in a window using OpenCV imshow().\"\"\"\nim0 = self.plotted_img\nif platform.system() == 'Linux' and p not in self.windows:\nself.windows.append(p)\ncv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\ncv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\ncv2.imshow(str(p), im0)\ncv2.waitKey(500 if self.batch[3].startswith('image') else 1)  # 1 millisecond\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.stream_inference","title":"<code>stream_inference(source=None, model=None, *args, **kwargs)</code>","text":"<p>Streams real-time inference on camera feed and saves results to file.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>@smart_inference_mode()\ndef stream_inference(self, source=None, model=None, *args, **kwargs):\n\"\"\"Streams real-time inference on camera feed and saves results to file.\"\"\"\nif self.args.verbose:\nLOGGER.info('')\n# Setup model\nif not self.model:\nself.setup_model(model)\n# Setup source every time predict is called\nself.setup_source(source if source is not None else self.args.source)\n# Check if save_dir/ label file exists\nif self.args.save or self.args.save_txt:\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n# Warmup model\nif not self.done_warmup:\nself.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))\nself.done_warmup = True\nself.seen, self.windows, self.batch, profilers = 0, [], None, (ops.Profile(), ops.Profile(), ops.Profile())\nself.run_callbacks('on_predict_start')\nfor batch in self.dataset:\nself.run_callbacks('on_predict_batch_start')\nself.batch = batch\npath, im0s, vid_cap, s = batch\n# Preprocess\nwith profilers[0]:\nim = self.preprocess(im0s)\n# Inference\nwith profilers[1]:\npreds = self.inference(im, *args, **kwargs)\n# Postprocess\nwith profilers[2]:\nself.results = self.postprocess(preds, im, im0s)\nself.run_callbacks('on_predict_postprocess_end')\n# Visualize, save, write results\nn = len(im0s)\nfor i in range(n):\nself.seen += 1\nself.results[i].speed = {\n'preprocess': profilers[0].dt * 1E3 / n,\n'inference': profilers[1].dt * 1E3 / n,\n'postprocess': profilers[2].dt * 1E3 / n}\np, im0 = path[i], None if self.source_type.tensor else im0s[i].copy()\np = Path(p)\nif self.args.verbose or self.args.save or self.args.save_txt or self.args.show:\ns += self.write_results(i, self.results, (p, im, im0))\nif self.args.save or self.args.save_txt:\nself.results[i].save_dir = self.save_dir.__str__()\nif self.args.show and self.plotted_img is not None:\nself.show(p)\nif self.args.save and self.plotted_img is not None:\nself.save_preds(vid_cap, i, str(self.save_dir / p.name))\nself.run_callbacks('on_predict_batch_end')\nyield from self.results\n# Print time (inference-only)\nif self.args.verbose:\nLOGGER.info(f'{s}{profilers[1].dt * 1E3:.1f}ms')\n# Release assets\nif isinstance(self.vid_writer[-1], cv2.VideoWriter):\nself.vid_writer[-1].release()  # release final video writer\n# Print results\nif self.args.verbose and self.seen:\nt = tuple(x.t / self.seen * 1E3 for x in profilers)  # speeds per image\nLOGGER.info(f'Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape '\nf'{(1, 3, *im.shape[2:])}' % t)\nif self.args.save or self.args.save_txt or self.args.save_crop:\nnl = len(list(self.save_dir.glob('labels/*.txt')))  # number of labels\ns = f\"\\n{nl} label{'s' * (nl &gt; 1)} saved to {self.save_dir / 'labels'}\" if self.args.save_txt else ''\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}{s}\")\nself.run_callbacks('on_predict_end')\n</code></pre>"},{"location":"reference/engine/predictor/#ultralytics.engine.predictor.BasePredictor.write_results","title":"<code>write_results(idx, results, batch)</code>","text":"<p>Write inference results to a file or directory.</p> Source code in <code>ultralytics/engine/predictor.py</code> <pre><code>def write_results(self, idx, results, batch):\n\"\"\"Write inference results to a file or directory.\"\"\"\np, im, _ = batch\nlog_string = ''\nif len(im.shape) == 3:\nim = im[None]  # expand for batch dim\nif self.source_type.webcam or self.source_type.from_img or self.source_type.tensor:  # batch_size &gt;= 1\nlog_string += f'{idx}: '\nframe = self.dataset.count\nelse:\nframe = getattr(self.dataset, 'frame', 0)\nself.data_path = p\nself.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')\nlog_string += '%gx%g ' % im.shape[2:]  # print string\nresult = results[idx]\nlog_string += result.verbose()\nif self.args.save or self.args.show:  # Add bbox to image\nplot_args = {\n'line_width': self.args.line_width,\n'boxes': self.args.boxes,\n'conf': self.args.show_conf,\n'labels': self.args.show_labels}\nif not self.args.retina_masks:\nplot_args['im_gpu'] = im[idx]\nself.plotted_img = result.plot(**plot_args)\n# Write\nif self.args.save_txt:\nresult.save_txt(f'{self.txt_path}.txt', save_conf=self.args.save_conf)\nif self.args.save_crop:\nresult.save_crop(save_dir=self.save_dir / 'crops',\nfile_name=self.data_path.stem + ('' if self.dataset.mode == 'image' else f'_{frame}'))\nreturn log_string\n</code></pre>"},{"location":"reference/engine/results/","title":"Reference for <code>ultralytics/engine/results.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/results.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor","title":"<code>ultralytics.engine.results.BaseTensor</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>Base tensor class with additional methods for easy manipulation and device handling.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class BaseTensor(SimpleClass):\n\"\"\"\n    Base tensor class with additional methods for easy manipulation and device handling.\n    \"\"\"\ndef __init__(self, data, orig_shape) -&gt; None:\n\"\"\"Initialize BaseTensor with data and original shape.\n        Args:\n            data (torch.Tensor | np.ndarray): Predictions, such as bboxes, masks and keypoints.\n            orig_shape (tuple): Original shape of image.\n        \"\"\"\nassert isinstance(data, (torch.Tensor, np.ndarray))\nself.data = data\nself.orig_shape = orig_shape\n@property\ndef shape(self):\n\"\"\"Return the shape of the data tensor.\"\"\"\nreturn self.data.shape\ndef cpu(self):\n\"\"\"Return a copy of the tensor on CPU memory.\"\"\"\nreturn self if isinstance(self.data, np.ndarray) else self.__class__(self.data.cpu(), self.orig_shape)\ndef numpy(self):\n\"\"\"Return a copy of the tensor as a numpy array.\"\"\"\nreturn self if isinstance(self.data, np.ndarray) else self.__class__(self.data.numpy(), self.orig_shape)\ndef cuda(self):\n\"\"\"Return a copy of the tensor on GPU memory.\"\"\"\nreturn self.__class__(torch.as_tensor(self.data).cuda(), self.orig_shape)\ndef to(self, *args, **kwargs):\n\"\"\"Return a copy of the tensor with the specified device and dtype.\"\"\"\nreturn self.__class__(torch.as_tensor(self.data).to(*args, **kwargs), self.orig_shape)\ndef __len__(self):  # override len(results)\n\"\"\"Return the length of the data tensor.\"\"\"\nreturn len(self.data)\ndef __getitem__(self, idx):\n\"\"\"Return a BaseTensor with the specified index of the data tensor.\"\"\"\nreturn self.__class__(self.data[idx], self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Return the shape of the data tensor.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return a BaseTensor with the specified index of the data tensor.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __getitem__(self, idx):\n\"\"\"Return a BaseTensor with the specified index of the data tensor.\"\"\"\nreturn self.__class__(self.data[idx], self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.__init__","title":"<code>__init__(data, orig_shape)</code>","text":"<p>Initialize BaseTensor with data and original shape.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor | ndarray</code> <p>Predictions, such as bboxes, masks and keypoints.</p> required <code>orig_shape</code> <code>tuple</code> <p>Original shape of image.</p> required Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __init__(self, data, orig_shape) -&gt; None:\n\"\"\"Initialize BaseTensor with data and original shape.\n    Args:\n        data (torch.Tensor | np.ndarray): Predictions, such as bboxes, masks and keypoints.\n        orig_shape (tuple): Original shape of image.\n    \"\"\"\nassert isinstance(data, (torch.Tensor, np.ndarray))\nself.data = data\nself.orig_shape = orig_shape\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the data tensor.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __len__(self):  # override len(results)\n\"\"\"Return the length of the data tensor.\"\"\"\nreturn len(self.data)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.cpu","title":"<code>cpu()</code>","text":"<p>Return a copy of the tensor on CPU memory.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def cpu(self):\n\"\"\"Return a copy of the tensor on CPU memory.\"\"\"\nreturn self if isinstance(self.data, np.ndarray) else self.__class__(self.data.cpu(), self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.cuda","title":"<code>cuda()</code>","text":"<p>Return a copy of the tensor on GPU memory.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def cuda(self):\n\"\"\"Return a copy of the tensor on GPU memory.\"\"\"\nreturn self.__class__(torch.as_tensor(self.data).cuda(), self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.numpy","title":"<code>numpy()</code>","text":"<p>Return a copy of the tensor as a numpy array.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def numpy(self):\n\"\"\"Return a copy of the tensor as a numpy array.\"\"\"\nreturn self if isinstance(self.data, np.ndarray) else self.__class__(self.data.numpy(), self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.BaseTensor.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Return a copy of the tensor with the specified device and dtype.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def to(self, *args, **kwargs):\n\"\"\"Return a copy of the tensor with the specified device and dtype.\"\"\"\nreturn self.__class__(torch.as_tensor(self.data).to(*args, **kwargs), self.orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results","title":"<code>ultralytics.engine.results.Results</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>A class for storing and manipulating inference results.</p> <p>Parameters:</p> Name Type Description Default <code>orig_img</code> <code>ndarray</code> <p>The original image as a numpy array.</p> required <code>path</code> <code>str</code> <p>The path to the image file.</p> required <code>names</code> <code>dict</code> <p>A dictionary of class names.</p> required <code>boxes</code> <code>tensor</code> <p>A 2D tensor of bounding box coordinates for each detection.</p> <code>None</code> <code>masks</code> <code>tensor</code> <p>A 3D tensor of detection masks, where each mask is a binary image.</p> <code>None</code> <code>probs</code> <code>tensor</code> <p>A 1D tensor of probabilities of each class for classification task.</p> <code>None</code> <code>keypoints</code> <code>List[List[float]]</code> <p>A list of detected keypoints for each object.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>orig_img</code> <code>ndarray</code> <p>The original image as a numpy array.</p> <code>orig_shape</code> <code>tuple</code> <p>The original image shape in (height, width) format.</p> <code>boxes</code> <code>Boxes</code> <p>A Boxes object containing the detection bounding boxes.</p> <code>masks</code> <code>Masks</code> <p>A Masks object containing the detection masks.</p> <code>probs</code> <code>Probs</code> <p>A Probs object containing probabilities of each class for classification task.</p> <code>keypoints</code> <code>Keypoints</code> <p>A Keypoints object containing detected keypoints for each object.</p> <code>speed</code> <code>dict</code> <p>A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.</p> <code>names</code> <code>dict</code> <p>A dictionary of class names.</p> <code>path</code> <code>str</code> <p>The path to the image file.</p> <code>_keys</code> <code>tuple</code> <p>A tuple of attribute names for non-empty attributes.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class Results(SimpleClass):\n\"\"\"\n    A class for storing and manipulating inference results.\n    Args:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        path (str): The path to the image file.\n        names (dict): A dictionary of class names.\n        boxes (torch.tensor, optional): A 2D tensor of bounding box coordinates for each detection.\n        masks (torch.tensor, optional): A 3D tensor of detection masks, where each mask is a binary image.\n        probs (torch.tensor, optional): A 1D tensor of probabilities of each class for classification task.\n        keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n    Attributes:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        orig_shape (tuple): The original image shape in (height, width) format.\n        boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n        masks (Masks, optional): A Masks object containing the detection masks.\n        probs (Probs, optional): A Probs object containing probabilities of each class for classification task.\n        keypoints (Keypoints, optional): A Keypoints object containing detected keypoints for each object.\n        speed (dict): A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.\n        names (dict): A dictionary of class names.\n        path (str): The path to the image file.\n        _keys (tuple): A tuple of attribute names for non-empty attributes.\n    \"\"\"\ndef __init__(self, orig_img, path, names, boxes=None, masks=None, probs=None, keypoints=None) -&gt; None:\n\"\"\"Initialize the Results class.\"\"\"\nself.orig_img = orig_img\nself.orig_shape = orig_img.shape[:2]\nself.boxes = Boxes(boxes, self.orig_shape) if boxes is not None else None  # native size boxes\nself.masks = Masks(masks, self.orig_shape) if masks is not None else None  # native size or imgsz masks\nself.probs = Probs(probs) if probs is not None else None\nself.keypoints = Keypoints(keypoints, self.orig_shape) if keypoints is not None else None\nself.speed = {'preprocess': None, 'inference': None, 'postprocess': None}  # milliseconds per image\nself.names = names\nself.path = path\nself.save_dir = None\nself._keys = ('boxes', 'masks', 'probs', 'keypoints')\ndef __getitem__(self, idx):\n\"\"\"Return a Results object for the specified index.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k)[idx])\nreturn r\ndef __len__(self):\n\"\"\"Return the number of detections in the Results object.\"\"\"\nfor k in self.keys:\nreturn len(getattr(self, k))\ndef update(self, boxes=None, masks=None, probs=None):\n\"\"\"Update the boxes, masks, and probs attributes of the Results object.\"\"\"\nif boxes is not None:\nops.clip_boxes(boxes, self.orig_shape)  # clip boxes\nself.boxes = Boxes(boxes, self.orig_shape)\nif masks is not None:\nself.masks = Masks(masks, self.orig_shape)\nif probs is not None:\nself.probs = probs\ndef cpu(self):\n\"\"\"Return a copy of the Results object with all tensors on CPU memory.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).cpu())\nreturn r\ndef numpy(self):\n\"\"\"Return a copy of the Results object with all tensors as numpy arrays.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).numpy())\nreturn r\ndef cuda(self):\n\"\"\"Return a copy of the Results object with all tensors on GPU memory.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).cuda())\nreturn r\ndef to(self, *args, **kwargs):\n\"\"\"Return a copy of the Results object with tensors on the specified device and dtype.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).to(*args, **kwargs))\nreturn r\ndef new(self):\n\"\"\"Return a new Results object with the same image, path, and names.\"\"\"\nreturn Results(orig_img=self.orig_img, path=self.path, names=self.names)\n@property\ndef keys(self):\n\"\"\"Return a list of non-empty attribute names.\"\"\"\nreturn [k for k in self._keys if getattr(self, k) is not None]\ndef plot(\nself,\nconf=True,\nline_width=None,\nfont_size=None,\nfont='Arial.ttf',\npil=False,\nimg=None,\nim_gpu=None,\nkpt_radius=5,\nkpt_line=True,\nlabels=True,\nboxes=True,\nmasks=True,\nprobs=True,\n**kwargs  # deprecated args TODO: remove support in 8.2\n):\n\"\"\"\n        Plots the detection results on an input RGB image. Accepts a numpy array (cv2) or a PIL Image.\n        Args:\n            conf (bool): Whether to plot the detection confidence score.\n            line_width (float, optional): The line width of the bounding boxes. If None, it is scaled to the image size.\n            font_size (float, optional): The font size of the text. If None, it is scaled to the image size.\n            font (str): The font to use for the text.\n            pil (bool): Whether to return the image as a PIL Image.\n            img (numpy.ndarray): Plot to another image. if not, plot to original image.\n            im_gpu (torch.Tensor): Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting.\n            kpt_radius (int, optional): Radius of the drawn keypoints. Default is 5.\n            kpt_line (bool): Whether to draw lines connecting keypoints.\n            labels (bool): Whether to plot the label of bounding boxes.\n            boxes (bool): Whether to plot the bounding boxes.\n            masks (bool): Whether to plot the masks.\n            probs (bool): Whether to plot classification probability\n        Returns:\n            (numpy.ndarray): A numpy array of the annotated image.\n        Example:\n            ```python\n            from PIL import Image\n            from ultralytics import YOLO\n            model = YOLO('yolov8n.pt')\n            results = model('bus.jpg')  # results list\n            for r in results:\n                im_array = r.plot()  # plot a BGR numpy array of predictions\n                im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n                im.show()  # show image\n                im.save('results.jpg')  # save image\n            ```\n        \"\"\"\nif img is None and isinstance(self.orig_img, torch.Tensor):\nimg = (self.orig_img[0].detach().permute(1, 2, 0).cpu().contiguous() * 255).to(torch.uint8).numpy()\n# Deprecation warn TODO: remove in 8.2\nif 'show_conf' in kwargs:\ndeprecation_warn('show_conf', 'conf')\nconf = kwargs['show_conf']\nassert isinstance(conf, bool), '`show_conf` should be of boolean type, i.e, show_conf=True/False'\nif 'line_thickness' in kwargs:\ndeprecation_warn('line_thickness', 'line_width')\nline_width = kwargs['line_thickness']\nassert isinstance(line_width, int), '`line_width` should be of int type, i.e, line_width=3'\nnames = self.names\npred_boxes, show_boxes = self.boxes, boxes\npred_masks, show_masks = self.masks, masks\npred_probs, show_probs = self.probs, probs\nannotator = Annotator(\ndeepcopy(self.orig_img if img is None else img),\nline_width,\nfont_size,\nfont,\npil or (pred_probs is not None and show_probs),  # Classify tasks default to pil=True\nexample=names)\n# Plot Segment results\nif pred_masks and show_masks:\nif im_gpu is None:\nimg = LetterBox(pred_masks.shape[1:])(image=annotator.result())\nim_gpu = torch.as_tensor(img, dtype=torch.float16, device=pred_masks.data.device).permute(\n2, 0, 1).flip(0).contiguous() / 255\nidx = pred_boxes.cls if pred_boxes else range(len(pred_masks))\nannotator.masks(pred_masks.data, colors=[colors(x, True) for x in idx], im_gpu=im_gpu)\n# Plot Detect results\nif pred_boxes and show_boxes:\nfor d in reversed(pred_boxes):\nc, conf, id = int(d.cls), float(d.conf) if conf else None, None if d.id is None else int(d.id.item())\nname = ('' if id is None else f'id:{id} ') + names[c]\nlabel = (f'{name} {conf:.2f}' if conf else name) if labels else None\nannotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))\n# Plot Classify results\nif pred_probs is not None and show_probs:\ntext = ',\\n'.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)\nx = round(self.orig_shape[0] * 0.03)\nannotator.text([x, x], text, txt_color=(255, 255, 255))  # TODO: allow setting colors\n# Plot Pose results\nif self.keypoints is not None:\nfor k in reversed(self.keypoints.data):\nannotator.kpts(k, self.orig_shape, radius=kpt_radius, kpt_line=kpt_line)\nreturn annotator.result()\ndef verbose(self):\n\"\"\"\n        Return log string for each task.\n        \"\"\"\nlog_string = ''\nprobs = self.probs\nboxes = self.boxes\nif len(self) == 0:\nreturn log_string if probs is not None else f'{log_string}(no detections), '\nif probs is not None:\nlog_string += f\"{', '.join(f'{self.names[j]} {probs.data[j]:.2f}' for j in probs.top5)}, \"\nif boxes:\nfor c in boxes.cls.unique():\nn = (boxes.cls == c).sum()  # detections per class\nlog_string += f\"{n} {self.names[int(c)]}{'s' * (n &gt; 1)}, \"\nreturn log_string\ndef save_txt(self, txt_file, save_conf=False):\n\"\"\"\n        Save predictions into txt file.\n        Args:\n            txt_file (str): txt file path.\n            save_conf (bool): save confidence score or not.\n        \"\"\"\nboxes = self.boxes\nmasks = self.masks\nprobs = self.probs\nkpts = self.keypoints\ntexts = []\nif probs is not None:\n# Classify\n[texts.append(f'{probs.data[j]:.2f} {self.names[j]}') for j in probs.top5]\nelif boxes:\n# Detect/segment/pose\nfor j, d in enumerate(boxes):\nc, conf, id = int(d.cls), float(d.conf), None if d.id is None else int(d.id.item())\nline = (c, *d.xywhn.view(-1))\nif masks:\nseg = masks[j].xyn[0].copy().reshape(-1)  # reversed mask.xyn, (n,2) to (n*2)\nline = (c, *seg)\nif kpts is not None:\nkpt = torch.cat((kpts[j].xyn, kpts[j].conf[..., None]), 2) if kpts[j].has_visible else kpts[j].xyn\nline += (*kpt.reshape(-1).tolist(), )\nline += (conf, ) * save_conf + (() if id is None else (id, ))\ntexts.append(('%g ' * len(line)).rstrip() % line)\nif texts:\nPath(txt_file).parent.mkdir(parents=True, exist_ok=True)  # make directory\nwith open(txt_file, 'a') as f:\nf.writelines(text + '\\n' for text in texts)\ndef save_crop(self, save_dir, file_name=Path('im.jpg')):\n\"\"\"\n        Save cropped predictions to `save_dir/cls/file_name.jpg`.\n        Args:\n            save_dir (str | pathlib.Path): Save path.\n            file_name (str | pathlib.Path): File name.\n        \"\"\"\nif self.probs is not None:\nLOGGER.warning('WARNING \u26a0\ufe0f Classify task do not support `save_crop`.')\nreturn\nif isinstance(save_dir, str):\nsave_dir = Path(save_dir)\nif isinstance(file_name, str):\nfile_name = Path(file_name)\nfor d in self.boxes:\nsave_one_box(d.xyxy,\nself.orig_img.copy(),\nfile=save_dir / self.names[int(d.cls)] / f'{file_name.stem}.jpg',\nBGR=True)\ndef tojson(self, normalize=False):\n\"\"\"Convert the object to JSON format.\"\"\"\nif self.probs is not None:\nLOGGER.warning('Warning: Classify task do not support `tojson` yet.')\nreturn\nimport json\n# Create list of detection dictionaries\nresults = []\ndata = self.boxes.data.cpu().tolist()\nh, w = self.orig_shape if normalize else (1, 1)\nfor i, row in enumerate(data):  # xyxy, track_id if tracking, conf, class_id\nbox = {'x1': row[0] / w, 'y1': row[1] / h, 'x2': row[2] / w, 'y2': row[3] / h}\nconf = row[-2]\nclass_id = int(row[-1])\nname = self.names[class_id]\nresult = {'name': name, 'class': class_id, 'confidence': conf, 'box': box}\nif self.boxes.is_track:\nresult['track_id'] = int(row[-3])  # track ID\nif self.masks:\nx, y = self.masks.xy[i][:, 0], self.masks.xy[i][:, 1]  # numpy array\nresult['segments'] = {'x': (x / w).tolist(), 'y': (y / h).tolist()}\nif self.keypoints is not None:\nx, y, visible = self.keypoints[i].data[0].cpu().unbind(dim=1)  # torch Tensor\nresult['keypoints'] = {'x': (x / w).tolist(), 'y': (y / h).tolist(), 'visible': visible.tolist()}\nresults.append(result)\n# Convert detections to JSON\nreturn json.dumps(results, indent=2)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Return a list of non-empty attribute names.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return a Results object for the specified index.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __getitem__(self, idx):\n\"\"\"Return a Results object for the specified index.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k)[idx])\nreturn r\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.__init__","title":"<code>__init__(orig_img, path, names, boxes=None, masks=None, probs=None, keypoints=None)</code>","text":"<p>Initialize the Results class.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __init__(self, orig_img, path, names, boxes=None, masks=None, probs=None, keypoints=None) -&gt; None:\n\"\"\"Initialize the Results class.\"\"\"\nself.orig_img = orig_img\nself.orig_shape = orig_img.shape[:2]\nself.boxes = Boxes(boxes, self.orig_shape) if boxes is not None else None  # native size boxes\nself.masks = Masks(masks, self.orig_shape) if masks is not None else None  # native size or imgsz masks\nself.probs = Probs(probs) if probs is not None else None\nself.keypoints = Keypoints(keypoints, self.orig_shape) if keypoints is not None else None\nself.speed = {'preprocess': None, 'inference': None, 'postprocess': None}  # milliseconds per image\nself.names = names\nself.path = path\nself.save_dir = None\nself._keys = ('boxes', 'masks', 'probs', 'keypoints')\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of detections in the Results object.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __len__(self):\n\"\"\"Return the number of detections in the Results object.\"\"\"\nfor k in self.keys:\nreturn len(getattr(self, k))\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.cpu","title":"<code>cpu()</code>","text":"<p>Return a copy of the Results object with all tensors on CPU memory.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def cpu(self):\n\"\"\"Return a copy of the Results object with all tensors on CPU memory.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).cpu())\nreturn r\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.cuda","title":"<code>cuda()</code>","text":"<p>Return a copy of the Results object with all tensors on GPU memory.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def cuda(self):\n\"\"\"Return a copy of the Results object with all tensors on GPU memory.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).cuda())\nreturn r\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.new","title":"<code>new()</code>","text":"<p>Return a new Results object with the same image, path, and names.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def new(self):\n\"\"\"Return a new Results object with the same image, path, and names.\"\"\"\nreturn Results(orig_img=self.orig_img, path=self.path, names=self.names)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.numpy","title":"<code>numpy()</code>","text":"<p>Return a copy of the Results object with all tensors as numpy arrays.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def numpy(self):\n\"\"\"Return a copy of the Results object with all tensors as numpy arrays.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).numpy())\nreturn r\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.plot","title":"<code>plot(conf=True, line_width=None, font_size=None, font='Arial.ttf', pil=False, img=None, im_gpu=None, kpt_radius=5, kpt_line=True, labels=True, boxes=True, masks=True, probs=True, **kwargs)</code>","text":"<p>Plots the detection results on an input RGB image. Accepts a numpy array (cv2) or a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>conf</code> <code>bool</code> <p>Whether to plot the detection confidence score.</p> <code>True</code> <code>line_width</code> <code>float</code> <p>The line width of the bounding boxes. If None, it is scaled to the image size.</p> <code>None</code> <code>font_size</code> <code>float</code> <p>The font size of the text. If None, it is scaled to the image size.</p> <code>None</code> <code>font</code> <code>str</code> <p>The font to use for the text.</p> <code>'Arial.ttf'</code> <code>pil</code> <code>bool</code> <p>Whether to return the image as a PIL Image.</p> <code>False</code> <code>img</code> <code>ndarray</code> <p>Plot to another image. if not, plot to original image.</p> <code>None</code> <code>im_gpu</code> <code>Tensor</code> <p>Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting.</p> <code>None</code> <code>kpt_radius</code> <code>int</code> <p>Radius of the drawn keypoints. Default is 5.</p> <code>5</code> <code>kpt_line</code> <code>bool</code> <p>Whether to draw lines connecting keypoints.</p> <code>True</code> <code>labels</code> <code>bool</code> <p>Whether to plot the label of bounding boxes.</p> <code>True</code> <code>boxes</code> <code>bool</code> <p>Whether to plot the bounding boxes.</p> <code>True</code> <code>masks</code> <code>bool</code> <p>Whether to plot the masks.</p> <code>True</code> <code>probs</code> <code>bool</code> <p>Whether to plot classification probability</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of the annotated image.</p> Example <pre><code>from PIL import Image\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\nresults = model('bus.jpg')  # results list\nfor r in results:\nim_array = r.plot()  # plot a BGR numpy array of predictions\nim = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\nim.show()  # show image\nim.save('results.jpg')  # save image\n</code></pre> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def plot(\nself,\nconf=True,\nline_width=None,\nfont_size=None,\nfont='Arial.ttf',\npil=False,\nimg=None,\nim_gpu=None,\nkpt_radius=5,\nkpt_line=True,\nlabels=True,\nboxes=True,\nmasks=True,\nprobs=True,\n**kwargs  # deprecated args TODO: remove support in 8.2\n):\n\"\"\"\n    Plots the detection results on an input RGB image. Accepts a numpy array (cv2) or a PIL Image.\n    Args:\n        conf (bool): Whether to plot the detection confidence score.\n        line_width (float, optional): The line width of the bounding boxes. If None, it is scaled to the image size.\n        font_size (float, optional): The font size of the text. If None, it is scaled to the image size.\n        font (str): The font to use for the text.\n        pil (bool): Whether to return the image as a PIL Image.\n        img (numpy.ndarray): Plot to another image. if not, plot to original image.\n        im_gpu (torch.Tensor): Normalized image in gpu with shape (1, 3, 640, 640), for faster mask plotting.\n        kpt_radius (int, optional): Radius of the drawn keypoints. Default is 5.\n        kpt_line (bool): Whether to draw lines connecting keypoints.\n        labels (bool): Whether to plot the label of bounding boxes.\n        boxes (bool): Whether to plot the bounding boxes.\n        masks (bool): Whether to plot the masks.\n        probs (bool): Whether to plot classification probability\n    Returns:\n        (numpy.ndarray): A numpy array of the annotated image.\n    Example:\n        ```python\n        from PIL import Image\n        from ultralytics import YOLO\n        model = YOLO('yolov8n.pt')\n        results = model('bus.jpg')  # results list\n        for r in results:\n            im_array = r.plot()  # plot a BGR numpy array of predictions\n            im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n            im.show()  # show image\n            im.save('results.jpg')  # save image\n        ```\n    \"\"\"\nif img is None and isinstance(self.orig_img, torch.Tensor):\nimg = (self.orig_img[0].detach().permute(1, 2, 0).cpu().contiguous() * 255).to(torch.uint8).numpy()\n# Deprecation warn TODO: remove in 8.2\nif 'show_conf' in kwargs:\ndeprecation_warn('show_conf', 'conf')\nconf = kwargs['show_conf']\nassert isinstance(conf, bool), '`show_conf` should be of boolean type, i.e, show_conf=True/False'\nif 'line_thickness' in kwargs:\ndeprecation_warn('line_thickness', 'line_width')\nline_width = kwargs['line_thickness']\nassert isinstance(line_width, int), '`line_width` should be of int type, i.e, line_width=3'\nnames = self.names\npred_boxes, show_boxes = self.boxes, boxes\npred_masks, show_masks = self.masks, masks\npred_probs, show_probs = self.probs, probs\nannotator = Annotator(\ndeepcopy(self.orig_img if img is None else img),\nline_width,\nfont_size,\nfont,\npil or (pred_probs is not None and show_probs),  # Classify tasks default to pil=True\nexample=names)\n# Plot Segment results\nif pred_masks and show_masks:\nif im_gpu is None:\nimg = LetterBox(pred_masks.shape[1:])(image=annotator.result())\nim_gpu = torch.as_tensor(img, dtype=torch.float16, device=pred_masks.data.device).permute(\n2, 0, 1).flip(0).contiguous() / 255\nidx = pred_boxes.cls if pred_boxes else range(len(pred_masks))\nannotator.masks(pred_masks.data, colors=[colors(x, True) for x in idx], im_gpu=im_gpu)\n# Plot Detect results\nif pred_boxes and show_boxes:\nfor d in reversed(pred_boxes):\nc, conf, id = int(d.cls), float(d.conf) if conf else None, None if d.id is None else int(d.id.item())\nname = ('' if id is None else f'id:{id} ') + names[c]\nlabel = (f'{name} {conf:.2f}' if conf else name) if labels else None\nannotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))\n# Plot Classify results\nif pred_probs is not None and show_probs:\ntext = ',\\n'.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)\nx = round(self.orig_shape[0] * 0.03)\nannotator.text([x, x], text, txt_color=(255, 255, 255))  # TODO: allow setting colors\n# Plot Pose results\nif self.keypoints is not None:\nfor k in reversed(self.keypoints.data):\nannotator.kpts(k, self.orig_shape, radius=kpt_radius, kpt_line=kpt_line)\nreturn annotator.result()\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.save_crop","title":"<code>save_crop(save_dir, file_name=Path('im.jpg'))</code>","text":"<p>Save cropped predictions to <code>save_dir/cls/file_name.jpg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str | Path</code> <p>Save path.</p> required <code>file_name</code> <code>str | Path</code> <p>File name.</p> <code>Path('im.jpg')</code> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def save_crop(self, save_dir, file_name=Path('im.jpg')):\n\"\"\"\n    Save cropped predictions to `save_dir/cls/file_name.jpg`.\n    Args:\n        save_dir (str | pathlib.Path): Save path.\n        file_name (str | pathlib.Path): File name.\n    \"\"\"\nif self.probs is not None:\nLOGGER.warning('WARNING \u26a0\ufe0f Classify task do not support `save_crop`.')\nreturn\nif isinstance(save_dir, str):\nsave_dir = Path(save_dir)\nif isinstance(file_name, str):\nfile_name = Path(file_name)\nfor d in self.boxes:\nsave_one_box(d.xyxy,\nself.orig_img.copy(),\nfile=save_dir / self.names[int(d.cls)] / f'{file_name.stem}.jpg',\nBGR=True)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.save_txt","title":"<code>save_txt(txt_file, save_conf=False)</code>","text":"<p>Save predictions into txt file.</p> <p>Parameters:</p> Name Type Description Default <code>txt_file</code> <code>str</code> <p>txt file path.</p> required <code>save_conf</code> <code>bool</code> <p>save confidence score or not.</p> <code>False</code> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def save_txt(self, txt_file, save_conf=False):\n\"\"\"\n    Save predictions into txt file.\n    Args:\n        txt_file (str): txt file path.\n        save_conf (bool): save confidence score or not.\n    \"\"\"\nboxes = self.boxes\nmasks = self.masks\nprobs = self.probs\nkpts = self.keypoints\ntexts = []\nif probs is not None:\n# Classify\n[texts.append(f'{probs.data[j]:.2f} {self.names[j]}') for j in probs.top5]\nelif boxes:\n# Detect/segment/pose\nfor j, d in enumerate(boxes):\nc, conf, id = int(d.cls), float(d.conf), None if d.id is None else int(d.id.item())\nline = (c, *d.xywhn.view(-1))\nif masks:\nseg = masks[j].xyn[0].copy().reshape(-1)  # reversed mask.xyn, (n,2) to (n*2)\nline = (c, *seg)\nif kpts is not None:\nkpt = torch.cat((kpts[j].xyn, kpts[j].conf[..., None]), 2) if kpts[j].has_visible else kpts[j].xyn\nline += (*kpt.reshape(-1).tolist(), )\nline += (conf, ) * save_conf + (() if id is None else (id, ))\ntexts.append(('%g ' * len(line)).rstrip() % line)\nif texts:\nPath(txt_file).parent.mkdir(parents=True, exist_ok=True)  # make directory\nwith open(txt_file, 'a') as f:\nf.writelines(text + '\\n' for text in texts)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Return a copy of the Results object with tensors on the specified device and dtype.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def to(self, *args, **kwargs):\n\"\"\"Return a copy of the Results object with tensors on the specified device and dtype.\"\"\"\nr = self.new()\nfor k in self.keys:\nsetattr(r, k, getattr(self, k).to(*args, **kwargs))\nreturn r\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.tojson","title":"<code>tojson(normalize=False)</code>","text":"<p>Convert the object to JSON format.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def tojson(self, normalize=False):\n\"\"\"Convert the object to JSON format.\"\"\"\nif self.probs is not None:\nLOGGER.warning('Warning: Classify task do not support `tojson` yet.')\nreturn\nimport json\n# Create list of detection dictionaries\nresults = []\ndata = self.boxes.data.cpu().tolist()\nh, w = self.orig_shape if normalize else (1, 1)\nfor i, row in enumerate(data):  # xyxy, track_id if tracking, conf, class_id\nbox = {'x1': row[0] / w, 'y1': row[1] / h, 'x2': row[2] / w, 'y2': row[3] / h}\nconf = row[-2]\nclass_id = int(row[-1])\nname = self.names[class_id]\nresult = {'name': name, 'class': class_id, 'confidence': conf, 'box': box}\nif self.boxes.is_track:\nresult['track_id'] = int(row[-3])  # track ID\nif self.masks:\nx, y = self.masks.xy[i][:, 0], self.masks.xy[i][:, 1]  # numpy array\nresult['segments'] = {'x': (x / w).tolist(), 'y': (y / h).tolist()}\nif self.keypoints is not None:\nx, y, visible = self.keypoints[i].data[0].cpu().unbind(dim=1)  # torch Tensor\nresult['keypoints'] = {'x': (x / w).tolist(), 'y': (y / h).tolist(), 'visible': visible.tolist()}\nresults.append(result)\n# Convert detections to JSON\nreturn json.dumps(results, indent=2)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.update","title":"<code>update(boxes=None, masks=None, probs=None)</code>","text":"<p>Update the boxes, masks, and probs attributes of the Results object.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def update(self, boxes=None, masks=None, probs=None):\n\"\"\"Update the boxes, masks, and probs attributes of the Results object.\"\"\"\nif boxes is not None:\nops.clip_boxes(boxes, self.orig_shape)  # clip boxes\nself.boxes = Boxes(boxes, self.orig_shape)\nif masks is not None:\nself.masks = Masks(masks, self.orig_shape)\nif probs is not None:\nself.probs = probs\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Results.verbose","title":"<code>verbose()</code>","text":"<p>Return log string for each task.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def verbose(self):\n\"\"\"\n    Return log string for each task.\n    \"\"\"\nlog_string = ''\nprobs = self.probs\nboxes = self.boxes\nif len(self) == 0:\nreturn log_string if probs is not None else f'{log_string}(no detections), '\nif probs is not None:\nlog_string += f\"{', '.join(f'{self.names[j]} {probs.data[j]:.2f}' for j in probs.top5)}, \"\nif boxes:\nfor c in boxes.cls.unique():\nn = (boxes.cls == c).sum()  # detections per class\nlog_string += f\"{n} {self.names[int(c)]}{'s' * (n &gt; 1)}, \"\nreturn log_string\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes","title":"<code>ultralytics.engine.results.Boxes</code>","text":"<p>             Bases: <code>BaseTensor</code></p> <p>A class for storing and manipulating detection boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>Tensor | ndarray</code> <p>A tensor or numpy array containing the detection boxes, with shape (num_boxes, 6) or (num_boxes, 7). The last two columns contain confidence and class values. If present, the third last column contains track IDs.</p> required <code>orig_shape</code> <code>tuple</code> <p>Original image size, in the format (height, width).</p> required <p>Attributes:</p> Name Type Description <code>xyxy</code> <code>Tensor | ndarray</code> <p>The boxes in xyxy format.</p> <code>conf</code> <code>Tensor | ndarray</code> <p>The confidence values of the boxes.</p> <code>cls</code> <code>Tensor | ndarray</code> <p>The class values of the boxes.</p> <code>id</code> <code>Tensor | ndarray</code> <p>The track IDs of the boxes (if available).</p> <code>xywh</code> <code>Tensor | ndarray</code> <p>The boxes in xywh format.</p> <code>xyxyn</code> <code>Tensor | ndarray</code> <p>The boxes in xyxy format normalized by original image size.</p> <code>xywhn</code> <code>Tensor | ndarray</code> <p>The boxes in xywh format normalized by original image size.</p> <code>data</code> <code>Tensor</code> <p>The raw bboxes tensor (alias for <code>boxes</code>).</p> <p>Methods:</p> Name Description <code>cpu</code> <p>Move the object to CPU memory.</p> <code>numpy</code> <p>Convert the object to a numpy array.</p> <code>cuda</code> <p>Move the object to CUDA memory.</p> <code>to</code> <p>Move the object to the specified device.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class Boxes(BaseTensor):\n\"\"\"\n    A class for storing and manipulating detection boxes.\n    Args:\n        boxes (torch.Tensor | numpy.ndarray): A tensor or numpy array containing the detection boxes,\n            with shape (num_boxes, 6) or (num_boxes, 7). The last two columns contain confidence and class values.\n            If present, the third last column contains track IDs.\n        orig_shape (tuple): Original image size, in the format (height, width).\n    Attributes:\n        xyxy (torch.Tensor | numpy.ndarray): The boxes in xyxy format.\n        conf (torch.Tensor | numpy.ndarray): The confidence values of the boxes.\n        cls (torch.Tensor | numpy.ndarray): The class values of the boxes.\n        id (torch.Tensor | numpy.ndarray): The track IDs of the boxes (if available).\n        xywh (torch.Tensor | numpy.ndarray): The boxes in xywh format.\n        xyxyn (torch.Tensor | numpy.ndarray): The boxes in xyxy format normalized by original image size.\n        xywhn (torch.Tensor | numpy.ndarray): The boxes in xywh format normalized by original image size.\n        data (torch.Tensor): The raw bboxes tensor (alias for `boxes`).\n    Methods:\n        cpu(): Move the object to CPU memory.\n        numpy(): Convert the object to a numpy array.\n        cuda(): Move the object to CUDA memory.\n        to(*args, **kwargs): Move the object to the specified device.\n    \"\"\"\ndef __init__(self, boxes, orig_shape) -&gt; None:\n\"\"\"Initialize the Boxes class.\"\"\"\nif boxes.ndim == 1:\nboxes = boxes[None, :]\nn = boxes.shape[-1]\nassert n in (6, 7), f'expected `n` in [6, 7], but got {n}'  # xyxy, track_id, conf, cls\nsuper().__init__(boxes, orig_shape)\nself.is_track = n == 7\nself.orig_shape = orig_shape\n@property\ndef xyxy(self):\n\"\"\"Return the boxes in xyxy format.\"\"\"\nreturn self.data[:, :4]\n@property\ndef conf(self):\n\"\"\"Return the confidence values of the boxes.\"\"\"\nreturn self.data[:, -2]\n@property\ndef cls(self):\n\"\"\"Return the class values of the boxes.\"\"\"\nreturn self.data[:, -1]\n@property\ndef id(self):\n\"\"\"Return the track IDs of the boxes (if available).\"\"\"\nreturn self.data[:, -3] if self.is_track else None\n@property\n@lru_cache(maxsize=2)  # maxsize 1 should suffice\ndef xywh(self):\n\"\"\"Return the boxes in xywh format.\"\"\"\nreturn ops.xyxy2xywh(self.xyxy)\n@property\n@lru_cache(maxsize=2)\ndef xyxyn(self):\n\"\"\"Return the boxes in xyxy format normalized by original image size.\"\"\"\nxyxy = self.xyxy.clone() if isinstance(self.xyxy, torch.Tensor) else np.copy(self.xyxy)\nxyxy[..., [0, 2]] /= self.orig_shape[1]\nxyxy[..., [1, 3]] /= self.orig_shape[0]\nreturn xyxy\n@property\n@lru_cache(maxsize=2)\ndef xywhn(self):\n\"\"\"Return the boxes in xywh format normalized by original image size.\"\"\"\nxywh = ops.xyxy2xywh(self.xyxy)\nxywh[..., [0, 2]] /= self.orig_shape[1]\nxywh[..., [1, 3]] /= self.orig_shape[0]\nreturn xywh\n@property\ndef boxes(self):\n\"\"\"Return the raw bboxes tensor (deprecated).\"\"\"\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\")\nreturn self.data\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.boxes","title":"<code>boxes</code>  <code>property</code>","text":"<p>Return the raw bboxes tensor (deprecated).</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.cls","title":"<code>cls</code>  <code>property</code>","text":"<p>Return the class values of the boxes.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.conf","title":"<code>conf</code>  <code>property</code>","text":"<p>Return the confidence values of the boxes.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.id","title":"<code>id</code>  <code>property</code>","text":"<p>Return the track IDs of the boxes (if available).</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.xywh","title":"<code>xywh</code>  <code>cached</code> <code>property</code>","text":"<p>Return the boxes in xywh format.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.xywhn","title":"<code>xywhn</code>  <code>cached</code> <code>property</code>","text":"<p>Return the boxes in xywh format normalized by original image size.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.xyxy","title":"<code>xyxy</code>  <code>property</code>","text":"<p>Return the boxes in xyxy format.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.xyxyn","title":"<code>xyxyn</code>  <code>cached</code> <code>property</code>","text":"<p>Return the boxes in xyxy format normalized by original image size.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Boxes.__init__","title":"<code>__init__(boxes, orig_shape)</code>","text":"<p>Initialize the Boxes class.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __init__(self, boxes, orig_shape) -&gt; None:\n\"\"\"Initialize the Boxes class.\"\"\"\nif boxes.ndim == 1:\nboxes = boxes[None, :]\nn = boxes.shape[-1]\nassert n in (6, 7), f'expected `n` in [6, 7], but got {n}'  # xyxy, track_id, conf, cls\nsuper().__init__(boxes, orig_shape)\nself.is_track = n == 7\nself.orig_shape = orig_shape\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks","title":"<code>ultralytics.engine.results.Masks</code>","text":"<p>             Bases: <code>BaseTensor</code></p> <p>A class for storing and manipulating detection masks.</p> <p>Attributes:</p> Name Type Description <code>segments</code> <code>list</code> <p>Deprecated property for segments (normalized).</p> <code>xy</code> <code>list</code> <p>A list of segments in pixel coordinates.</p> <code>xyn</code> <code>list</code> <p>A list of normalized segments.</p> <p>Methods:</p> Name Description <code>cpu</code> <p>Returns the masks tensor on CPU memory.</p> <code>numpy</code> <p>Returns the masks tensor as a numpy array.</p> <code>cuda</code> <p>Returns the masks tensor on GPU memory.</p> <code>to</code> <p>Returns the masks tensor with the specified device and dtype.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class Masks(BaseTensor):\n\"\"\"\n    A class for storing and manipulating detection masks.\n    Attributes:\n        segments (list): Deprecated property for segments (normalized).\n        xy (list): A list of segments in pixel coordinates.\n        xyn (list): A list of normalized segments.\n    Methods:\n        cpu(): Returns the masks tensor on CPU memory.\n        numpy(): Returns the masks tensor as a numpy array.\n        cuda(): Returns the masks tensor on GPU memory.\n        to(device, dtype): Returns the masks tensor with the specified device and dtype.\n    \"\"\"\ndef __init__(self, masks, orig_shape) -&gt; None:\n\"\"\"Initialize the Masks class with the given masks tensor and original image shape.\"\"\"\nif masks.ndim == 2:\nmasks = masks[None, :]\nsuper().__init__(masks, orig_shape)\n@property\n@lru_cache(maxsize=1)\ndef segments(self):\n\"\"\"Return segments (normalized). Deprecated; use xyn property instead.\"\"\"\nLOGGER.warning(\n\"WARNING \u26a0\ufe0f 'Masks.segments' is deprecated. Use 'Masks.xyn' for segments (normalized) and 'Masks.xy' for segments (pixels) instead.\"\n)\nreturn self.xyn\n@property\n@lru_cache(maxsize=1)\ndef xyn(self):\n\"\"\"Return normalized segments.\"\"\"\nreturn [\nops.scale_coords(self.data.shape[1:], x, self.orig_shape, normalize=True)\nfor x in ops.masks2segments(self.data)]\n@property\n@lru_cache(maxsize=1)\ndef xy(self):\n\"\"\"Return segments in pixel coordinates.\"\"\"\nreturn [\nops.scale_coords(self.data.shape[1:], x, self.orig_shape, normalize=False)\nfor x in ops.masks2segments(self.data)]\n@property\ndef masks(self):\n\"\"\"Return the raw masks tensor. Deprecated; use data attribute instead.\"\"\"\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'Masks.masks' is deprecated. Use 'Masks.data' instead.\")\nreturn self.data\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks.masks","title":"<code>masks</code>  <code>property</code>","text":"<p>Return the raw masks tensor. Deprecated; use data attribute instead.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks.segments","title":"<code>segments</code>  <code>cached</code> <code>property</code>","text":"<p>Return segments (normalized). Deprecated; use xyn property instead.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks.xy","title":"<code>xy</code>  <code>cached</code> <code>property</code>","text":"<p>Return segments in pixel coordinates.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks.xyn","title":"<code>xyn</code>  <code>cached</code> <code>property</code>","text":"<p>Return normalized segments.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Masks.__init__","title":"<code>__init__(masks, orig_shape)</code>","text":"<p>Initialize the Masks class with the given masks tensor and original image shape.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __init__(self, masks, orig_shape) -&gt; None:\n\"\"\"Initialize the Masks class with the given masks tensor and original image shape.\"\"\"\nif masks.ndim == 2:\nmasks = masks[None, :]\nsuper().__init__(masks, orig_shape)\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Keypoints","title":"<code>ultralytics.engine.results.Keypoints</code>","text":"<p>             Bases: <code>BaseTensor</code></p> <p>A class for storing and manipulating detection keypoints.</p> <p>Attributes:</p> Name Type Description <code>xy</code> <code>Tensor</code> <p>A collection of keypoints containing x, y coordinates for each detection.</p> <code>xyn</code> <code>Tensor</code> <p>A normalized version of xy with coordinates in the range [0, 1].</p> <code>conf</code> <code>Tensor</code> <p>Confidence values associated with keypoints if available, otherwise None.</p> <p>Methods:</p> Name Description <code>cpu</code> <p>Returns a copy of the keypoints tensor on CPU memory.</p> <code>numpy</code> <p>Returns a copy of the keypoints tensor as a numpy array.</p> <code>cuda</code> <p>Returns a copy of the keypoints tensor on GPU memory.</p> <code>to</code> <p>Returns a copy of the keypoints tensor with the specified device and dtype.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class Keypoints(BaseTensor):\n\"\"\"\n    A class for storing and manipulating detection keypoints.\n    Attributes:\n        xy (torch.Tensor): A collection of keypoints containing x, y coordinates for each detection.\n        xyn (torch.Tensor): A normalized version of xy with coordinates in the range [0, 1].\n        conf (torch.Tensor): Confidence values associated with keypoints if available, otherwise None.\n    Methods:\n        cpu(): Returns a copy of the keypoints tensor on CPU memory.\n        numpy(): Returns a copy of the keypoints tensor as a numpy array.\n        cuda(): Returns a copy of the keypoints tensor on GPU memory.\n        to(device, dtype): Returns a copy of the keypoints tensor with the specified device and dtype.\n    \"\"\"\ndef __init__(self, keypoints, orig_shape) -&gt; None:\n\"\"\"Initializes the Keypoints object with detection keypoints and original image size.\"\"\"\nif keypoints.ndim == 2:\nkeypoints = keypoints[None, :]\nsuper().__init__(keypoints, orig_shape)\nself.has_visible = self.data.shape[-1] == 3\n@property\n@lru_cache(maxsize=1)\ndef xy(self):\n\"\"\"Returns x, y coordinates of keypoints.\"\"\"\nreturn self.data[..., :2]\n@property\n@lru_cache(maxsize=1)\ndef xyn(self):\n\"\"\"Returns normalized x, y coordinates of keypoints.\"\"\"\nxy = self.xy.clone() if isinstance(self.xy, torch.Tensor) else np.copy(self.xy)\nxy[..., 0] /= self.orig_shape[1]\nxy[..., 1] /= self.orig_shape[0]\nreturn xy\n@property\n@lru_cache(maxsize=1)\ndef conf(self):\n\"\"\"Returns confidence values of keypoints if available, else None.\"\"\"\nreturn self.data[..., 2] if self.has_visible else None\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Keypoints.conf","title":"<code>conf</code>  <code>cached</code> <code>property</code>","text":"<p>Returns confidence values of keypoints if available, else None.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Keypoints.xy","title":"<code>xy</code>  <code>cached</code> <code>property</code>","text":"<p>Returns x, y coordinates of keypoints.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Keypoints.xyn","title":"<code>xyn</code>  <code>cached</code> <code>property</code>","text":"<p>Returns normalized x, y coordinates of keypoints.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Keypoints.__init__","title":"<code>__init__(keypoints, orig_shape)</code>","text":"<p>Initializes the Keypoints object with detection keypoints and original image size.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>def __init__(self, keypoints, orig_shape) -&gt; None:\n\"\"\"Initializes the Keypoints object with detection keypoints and original image size.\"\"\"\nif keypoints.ndim == 2:\nkeypoints = keypoints[None, :]\nsuper().__init__(keypoints, orig_shape)\nself.has_visible = self.data.shape[-1] == 3\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Probs","title":"<code>ultralytics.engine.results.Probs</code>","text":"<p>             Bases: <code>BaseTensor</code></p> <p>A class for storing and manipulating classification predictions.</p> <p>Attributes:</p> Name Type Description <code>top1</code> <code>int</code> <p>Index of the top 1 class.</p> <code>top5</code> <code>list[int]</code> <p>Indices of the top 5 classes.</p> <code>top1conf</code> <code>Tensor</code> <p>Confidence of the top 1 class.</p> <code>top5conf</code> <code>Tensor</code> <p>Confidences of the top 5 classes.</p> <p>Methods:</p> Name Description <code>cpu</code> <p>Returns a copy of the probs tensor on CPU memory.</p> <code>numpy</code> <p>Returns a copy of the probs tensor as a numpy array.</p> <code>cuda</code> <p>Returns a copy of the probs tensor on GPU memory.</p> <code>to</code> <p>Returns a copy of the probs tensor with the specified device and dtype.</p> Source code in <code>ultralytics/engine/results.py</code> <pre><code>class Probs(BaseTensor):\n\"\"\"\n    A class for storing and manipulating classification predictions.\n    Attributes:\n        top1 (int): Index of the top 1 class.\n        top5 (list[int]): Indices of the top 5 classes.\n        top1conf (torch.Tensor): Confidence of the top 1 class.\n        top5conf (torch.Tensor): Confidences of the top 5 classes.\n    Methods:\n        cpu(): Returns a copy of the probs tensor on CPU memory.\n        numpy(): Returns a copy of the probs tensor as a numpy array.\n        cuda(): Returns a copy of the probs tensor on GPU memory.\n        to(): Returns a copy of the probs tensor with the specified device and dtype.\n    \"\"\"\ndef __init__(self, probs, orig_shape=None) -&gt; None:\nsuper().__init__(probs, orig_shape)\n@property\n@lru_cache(maxsize=1)\ndef top1(self):\n\"\"\"Return the index of top 1.\"\"\"\nreturn int(self.data.argmax())\n@property\n@lru_cache(maxsize=1)\ndef top5(self):\n\"\"\"Return the indices of top 5.\"\"\"\nreturn (-self.data).argsort(0)[:5].tolist()  # this way works with both torch and numpy.\n@property\n@lru_cache(maxsize=1)\ndef top1conf(self):\n\"\"\"Return the confidence of top 1.\"\"\"\nreturn self.data[self.top1]\n@property\n@lru_cache(maxsize=1)\ndef top5conf(self):\n\"\"\"Return the confidences of top 5.\"\"\"\nreturn self.data[self.top5]\n</code></pre>"},{"location":"reference/engine/results/#ultralytics.engine.results.Probs.top1","title":"<code>top1</code>  <code>cached</code> <code>property</code>","text":"<p>Return the index of top 1.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Probs.top1conf","title":"<code>top1conf</code>  <code>cached</code> <code>property</code>","text":"<p>Return the confidence of top 1.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Probs.top5","title":"<code>top5</code>  <code>cached</code> <code>property</code>","text":"<p>Return the indices of top 5.</p>"},{"location":"reference/engine/results/#ultralytics.engine.results.Probs.top5conf","title":"<code>top5conf</code>  <code>cached</code> <code>property</code>","text":"<p>Return the confidences of top 5.</p>"},{"location":"reference/engine/trainer/","title":"Reference for <code>ultralytics/engine/trainer.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/trainer.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer","title":"<code>ultralytics.engine.trainer.BaseTrainer</code>","text":"<p>BaseTrainer</p> <p>A base class for creating trainers.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the trainer.</p> <code>check_resume</code> <code>method</code> <p>Method to check if training should be resumed from a saved checkpoint.</p> <code>validator</code> <code>BaseValidator</code> <p>Validator instance.</p> <code>model</code> <code>Module</code> <p>Model instance.</p> <code>callbacks</code> <code>defaultdict</code> <p>Dictionary of callbacks.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>wdir</code> <code>Path</code> <p>Directory to save weights.</p> <code>last</code> <code>Path</code> <p>Path to the last checkpoint.</p> <code>best</code> <code>Path</code> <p>Path to the best checkpoint.</p> <code>save_period</code> <code>int</code> <p>Save checkpoint every x epochs (disabled if &lt; 1).</p> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>epochs</code> <code>int</code> <p>Number of epochs to train for.</p> <code>start_epoch</code> <code>int</code> <p>Starting epoch for training.</p> <code>device</code> <code>device</code> <p>Device to use for training.</p> <code>amp</code> <code>bool</code> <p>Flag to enable AMP (Automatic Mixed Precision).</p> <code>scaler</code> <code>GradScaler</code> <p>Gradient scaler for AMP.</p> <code>data</code> <code>str</code> <p>Path to data.</p> <code>trainset</code> <code>Dataset</code> <p>Training dataset.</p> <code>testset</code> <code>Dataset</code> <p>Testing dataset.</p> <code>ema</code> <code>Module</code> <p>EMA (Exponential Moving Average) of the model.</p> <code>lf</code> <code>Module</code> <p>Loss function.</p> <code>scheduler</code> <code>_LRScheduler</code> <p>Learning rate scheduler.</p> <code>best_fitness</code> <code>float</code> <p>The best fitness value achieved.</p> <code>fitness</code> <code>float</code> <p>Current fitness value.</p> <code>loss</code> <code>float</code> <p>Current loss value.</p> <code>tloss</code> <code>float</code> <p>Total loss value.</p> <code>loss_names</code> <code>list</code> <p>List of loss names.</p> <code>csv</code> <code>Path</code> <p>Path to results CSV file.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>class BaseTrainer:\n\"\"\"\n    BaseTrainer\n    A base class for creating trainers.\n    Attributes:\n        args (SimpleNamespace): Configuration for the trainer.\n        check_resume (method): Method to check if training should be resumed from a saved checkpoint.\n        validator (BaseValidator): Validator instance.\n        model (nn.Module): Model instance.\n        callbacks (defaultdict): Dictionary of callbacks.\n        save_dir (Path): Directory to save results.\n        wdir (Path): Directory to save weights.\n        last (Path): Path to the last checkpoint.\n        best (Path): Path to the best checkpoint.\n        save_period (int): Save checkpoint every x epochs (disabled if &lt; 1).\n        batch_size (int): Batch size for training.\n        epochs (int): Number of epochs to train for.\n        start_epoch (int): Starting epoch for training.\n        device (torch.device): Device to use for training.\n        amp (bool): Flag to enable AMP (Automatic Mixed Precision).\n        scaler (amp.GradScaler): Gradient scaler for AMP.\n        data (str): Path to data.\n        trainset (torch.utils.data.Dataset): Training dataset.\n        testset (torch.utils.data.Dataset): Testing dataset.\n        ema (nn.Module): EMA (Exponential Moving Average) of the model.\n        lf (nn.Module): Loss function.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n        best_fitness (float): The best fitness value achieved.\n        fitness (float): Current fitness value.\n        loss (float): Current loss value.\n        tloss (float): Total loss value.\n        loss_names (list): List of loss names.\n        csv (Path): Path to results CSV file.\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n        Initializes the BaseTrainer class.\n        Args:\n            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.check_resume(overrides)\nself.device = select_device(self.args.device, self.args.batch)\nself.validator = None\nself.model = None\nself.metrics = None\nself.plots = {}\ninit_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n# Dirs\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f'{self.args.mode}'\nif hasattr(self.args, 'save_dir'):\nself.save_dir = Path(self.args.save_dir)\nelse:\nself.save_dir = Path(\nincrement_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in (-1, 0) else True))\nself.wdir = self.save_dir / 'weights'  # weights dir\nif RANK in (-1, 0):\nself.wdir.mkdir(parents=True, exist_ok=True)  # make dir\nself.args.save_dir = str(self.save_dir)\nyaml_save(self.save_dir / 'args.yaml', vars(self.args))  # save run args\nself.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\nself.save_period = self.args.save_period\nself.batch_size = self.args.batch\nself.epochs = self.args.epochs\nself.start_epoch = 0\nif RANK == -1:\nprint_args(vars(self.args))\n# Device\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n# Model and Dataset\nself.model = self.args.model\ntry:\nif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelif self.args.data.split('.')[-1] in ('yaml', 'yml') or self.args.task in ('detect', 'segment'):\nself.data = check_det_dataset(self.args.data)\nif 'yaml_file' in self.data:\nself.args.data = self.data['yaml_file']  # for validating 'yolo train data=url.zip' usage\nexcept Exception as e:\nraise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error \u274c {e}\")) from e\nself.trainset, self.testset = self.get_dataset(self.data)\nself.ema = None\n# Optimization utils init\nself.lf = None\nself.scheduler = None\n# Epoch level metrics\nself.best_fitness = None\nself.fitness = None\nself.loss = None\nself.tloss = None\nself.loss_names = ['Loss']\nself.csv = self.save_dir / 'results.csv'\nself.plot_idx = [0, 1, 2]\n# Callbacks\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\nif RANK in (-1, 0):\ncallbacks.add_integration_callbacks(self)\ndef add_callback(self, event: str, callback):\n\"\"\"\n        Appends the given callback.\n        \"\"\"\nself.callbacks[event].append(callback)\ndef set_callback(self, event: str, callback):\n\"\"\"\n        Overrides the existing callbacks with the given callback.\n        \"\"\"\nself.callbacks[event] = [callback]\ndef run_callbacks(self, event: str):\n\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\ndef train(self):\n\"\"\"Allow device='', device=None on Multi-GPU systems to default to device=0.\"\"\"\nif isinstance(self.args.device, int) or self.args.device:  # i.e. device=0 or device=[0,1,2,3]\nworld_size = torch.cuda.device_count()\nelif torch.cuda.is_available():  # i.e. device=None or device=''\nworld_size = 1  # default to device 0\nelse:  # i.e. device='cpu' or 'mps'\nworld_size = 0\n# Run subprocess if DDP training, else train normally\nif world_size &gt; 1 and 'LOCAL_RANK' not in os.environ:\n# Argument checks\nif self.args.rect:\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'rect=True' is incompatible with Multi-GPU training, setting rect=False\")\nself.args.rect = False\n# Command\ncmd, file = generate_ddp_command(world_size, self)\ntry:\nLOGGER.info(f'DDP command: {cmd}')\nsubprocess.run(cmd, check=True)\nexcept Exception as e:\nraise e\nfinally:\nddp_cleanup(self, str(file))\nelse:\nself._do_train(world_size)\ndef _setup_ddp(self, world_size):\n\"\"\"Initializes and sets the DistributedDataParallel parameters for training.\"\"\"\ntorch.cuda.set_device(RANK)\nself.device = torch.device('cuda', RANK)\nLOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')\nos.environ['NCCL_BLOCKING_WAIT'] = '1'  # set to enforce timeout\ndist.init_process_group(\n'nccl' if dist.is_nccl_available() else 'gloo',\ntimeout=timedelta(seconds=10800),  # 3 hours\nrank=RANK,\nworld_size=world_size)\ndef _setup_train(self, world_size):\n\"\"\"\n        Builds dataloaders and optimizer on correct rank process.\n        \"\"\"\n# Model\nself.run_callbacks('on_pretrain_routine_start')\nckpt = self.setup_model()\nself.model = self.model.to(self.device)\nself.set_model_attributes()\n# Freeze layers\nfreeze_list = self.args.freeze if isinstance(\nself.args.freeze, list) else range(self.args.freeze) if isinstance(self.args.freeze, int) else []\nalways_freeze_names = ['.dfl']  # always freeze these layers\nfreeze_layer_names = [f'model.{x}.' for x in freeze_list] + always_freeze_names\nfor k, v in self.model.named_parameters():\n# v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)\nif any(x in k for x in freeze_layer_names):\nLOGGER.info(f\"Freezing layer '{k}'\")\nv.requires_grad = False\nelif not v.requires_grad:\nLOGGER.info(f\"WARNING \u26a0\ufe0f setting 'requires_grad=True' for frozen layer '{k}'. \"\n'See ultralytics.engine.trainer for customization of frozen layers.')\nv.requires_grad = True\n# Check AMP\nself.amp = torch.tensor(self.args.amp).to(self.device)  # True or False\nif self.amp and RANK in (-1, 0):  # Single-GPU and DDP\ncallbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them\nself.amp = torch.tensor(check_amp(self.model), device=self.device)\ncallbacks.default_callbacks = callbacks_backup  # restore callbacks\nif RANK &gt; -1 and world_size &gt; 1:  # DDP\ndist.broadcast(self.amp, src=0)  # broadcast the tensor from rank 0 to all other ranks (returns None)\nself.amp = bool(self.amp)  # as boolean\nself.scaler = amp.GradScaler(enabled=self.amp)\nif world_size &gt; 1:\nself.model = DDP(self.model, device_ids=[RANK])\n# Check imgsz\ngs = max(int(self.model.stride.max() if hasattr(self.model, 'stride') else 32), 32)  # grid size (max stride)\nself.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)\n# Batch size\nif self.batch_size == -1:\nif RANK == -1:  # single-GPU only, estimate best batch size\nself.args.batch = self.batch_size = check_train_batch_size(self.model, self.args.imgsz, self.amp)\nelse:\nSyntaxError('batch=-1 to use AutoBatch is only available in Single-GPU training. '\n'Please pass a valid batch size value for Multi-GPU DDP training, i.e. batch=16')\n# Dataloaders\nbatch_size = self.batch_size // max(world_size, 1)\nself.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=RANK, mode='train')\nif RANK in (-1, 0):\nself.test_loader = self.get_dataloader(self.testset, batch_size=batch_size * 2, rank=-1, mode='val')\nself.validator = self.get_validator()\nmetric_keys = self.validator.metrics.keys + self.label_loss_items(prefix='val')\nself.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # TODO: init metrics for plot_results()?\nself.ema = ModelEMA(self.model)\nif self.args.plots:\nself.plot_training_labels()\n# Optimizer\nself.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing\nweight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay\niterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs\nself.optimizer = self.build_optimizer(model=self.model,\nname=self.args.optimizer,\nlr=self.args.lr0,\nmomentum=self.args.momentum,\ndecay=weight_decay,\niterations=iterations)\n# Scheduler\nif self.args.cos_lr:\nself.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1-&gt;hyp['lrf']\nelse:\nself.lf = lambda x: (1 - x / self.epochs) * (1.0 - self.args.lrf) + self.args.lrf  # linear\nself.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)\nself.stopper, self.stop = EarlyStopping(patience=self.args.patience), False\nself.resume_training(ckpt)\nself.scheduler.last_epoch = self.start_epoch - 1  # do not move\nself.run_callbacks('on_pretrain_routine_end')\ndef _do_train(self, world_size=1):\n\"\"\"Train completed, evaluate and plot if specified by arguments.\"\"\"\nif world_size &gt; 1:\nself._setup_ddp(world_size)\nself._setup_train(world_size)\nself.epoch_time = None\nself.epoch_time_start = time.time()\nself.train_time_start = time.time()\nnb = len(self.train_loader)  # number of batches\nnw = max(round(self.args.warmup_epochs *\nnb), 100) if self.args.warmup_epochs &gt; 0 else -1  # number of warmup iterations\nlast_opt_step = -1\nself.run_callbacks('on_train_start')\nLOGGER.info(f'Image sizes {self.args.imgsz} train, {self.args.imgsz} val\\n'\nf'Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\\n'\nf\"Logging results to {colorstr('bold', self.save_dir)}\\n\"\nf'Starting training for {self.epochs} epochs...')\nif self.args.close_mosaic:\nbase_idx = (self.epochs - self.args.close_mosaic) * nb\nself.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])\nepoch = self.epochs  # predefine for resume fully trained model edge cases\nfor epoch in range(self.start_epoch, self.epochs):\nself.epoch = epoch\nself.run_callbacks('on_train_epoch_start')\nself.model.train()\nif RANK != -1:\nself.train_loader.sampler.set_epoch(epoch)\npbar = enumerate(self.train_loader)\n# Update dataloader attributes (optional)\nif epoch == (self.epochs - self.args.close_mosaic):\nLOGGER.info('Closing dataloader mosaic')\nif hasattr(self.train_loader.dataset, 'mosaic'):\nself.train_loader.dataset.mosaic = False\nif hasattr(self.train_loader.dataset, 'close_mosaic'):\nself.train_loader.dataset.close_mosaic(hyp=self.args)\nself.train_loader.reset()\nif RANK in (-1, 0):\nLOGGER.info(self.progress_string())\npbar = tqdm(enumerate(self.train_loader), total=nb, bar_format=TQDM_BAR_FORMAT)\nself.tloss = None\nself.optimizer.zero_grad()\nfor i, batch in pbar:\nself.run_callbacks('on_train_batch_start')\n# Warmup\nni = i + nb * epoch\nif ni &lt;= nw:\nxi = [0, nw]  # x interp\nself.accumulate = max(1, np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round())\nfor j, x in enumerate(self.optimizer.param_groups):\n# Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\nx['lr'] = np.interp(\nni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x['initial_lr'] * self.lf(epoch)])\nif 'momentum' in x:\nx['momentum'] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])\n# Forward\nwith torch.cuda.amp.autocast(self.amp):\nbatch = self.preprocess_batch(batch)\nself.loss, self.loss_items = self.model(batch)\nif RANK != -1:\nself.loss *= world_size\nself.tloss = (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None \\\n                        else self.loss_items\n# Backward\nself.scaler.scale(self.loss).backward()\n# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\nif ni - last_opt_step &gt;= self.accumulate:\nself.optimizer_step()\nlast_opt_step = ni\n# Log\nmem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\nloss_len = self.tloss.shape[0] if len(self.tloss.size()) else 1\nlosses = self.tloss if loss_len &gt; 1 else torch.unsqueeze(self.tloss, 0)\nif RANK in (-1, 0):\npbar.set_description(\n('%11s' * 2 + '%11.4g' * (2 + loss_len)) %\n(f'{epoch + 1}/{self.epochs}', mem, *losses, batch['cls'].shape[0], batch['img'].shape[-1]))\nself.run_callbacks('on_batch_end')\nif self.args.plots and ni in self.plot_idx:\nself.plot_training_samples(batch, ni)\nself.run_callbacks('on_train_batch_end')\nself.lr = {f'lr/pg{ir}': x['lr'] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers\nwith warnings.catch_warnings():\nwarnings.simplefilter('ignore')  # suppress 'Detected lr_scheduler.step() before optimizer.step()'\nself.scheduler.step()\nself.run_callbacks('on_train_epoch_end')\nif RANK in (-1, 0):\n# Validation\nself.ema.update_attr(self.model, include=['yaml', 'nc', 'args', 'names', 'stride', 'class_weights'])\nfinal_epoch = (epoch + 1 == self.epochs) or self.stopper.possible_stop\nif self.args.val or final_epoch:\nself.metrics, self.fitness = self.validate()\nself.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\nself.stop = self.stopper(epoch + 1, self.fitness)\n# Save model\nif self.args.save or (epoch + 1 == self.epochs):\nself.save_model()\nself.run_callbacks('on_model_save')\ntnow = time.time()\nself.epoch_time = tnow - self.epoch_time_start\nself.epoch_time_start = tnow\nself.run_callbacks('on_fit_epoch_end')\ntorch.cuda.empty_cache()  # clears GPU vRAM at end of epoch, can help with out of memory errors\n# Early Stopping\nif RANK != -1:  # if DDP training\nbroadcast_list = [self.stop if RANK == 0 else None]\ndist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\nif RANK != 0:\nself.stop = broadcast_list[0]\nif self.stop:\nbreak  # must break all DDP ranks\nif RANK in (-1, 0):\n# Do final val with best.pt\nLOGGER.info(f'\\n{epoch - self.start_epoch + 1} epochs completed in '\nf'{(time.time() - self.train_time_start) / 3600:.3f} hours.')\nself.final_eval()\nif self.args.plots:\nself.plot_metrics()\nself.run_callbacks('on_train_end')\ntorch.cuda.empty_cache()\nself.run_callbacks('teardown')\ndef save_model(self):\n\"\"\"Save model checkpoints based on various conditions.\"\"\"\nckpt = {\n'epoch': self.epoch,\n'best_fitness': self.best_fitness,\n'model': deepcopy(de_parallel(self.model)).half(),\n'ema': deepcopy(self.ema.ema).half(),\n'updates': self.ema.updates,\n'optimizer': self.optimizer.state_dict(),\n'train_args': vars(self.args),  # save as dict\n'date': datetime.now().isoformat(),\n'version': __version__}\n# Use dill (if exists) to serialize the lambda functions where pickle does not do this\ntry:\nimport dill as pickle\nexcept ImportError:\nimport pickle\n# Save last, best and delete\ntorch.save(ckpt, self.last, pickle_module=pickle)\nif self.best_fitness == self.fitness:\ntorch.save(ckpt, self.best, pickle_module=pickle)\nif (self.epoch &gt; 0) and (self.save_period &gt; 0) and (self.epoch % self.save_period == 0):\ntorch.save(ckpt, self.wdir / f'epoch{self.epoch}.pt', pickle_module=pickle)\ndel ckpt\n@staticmethod\ndef get_dataset(data):\n\"\"\"\n        Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n        \"\"\"\nreturn data['train'], data.get('val') or data.get('test')\ndef setup_model(self):\n\"\"\"\n        load/create/download model for any task.\n        \"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, weights = self.model, None\nckpt = None\nif str(model).endswith('.pt'):\nweights, ckpt = attempt_load_one_weight(model)\ncfg = ckpt['model'].yaml\nelse:\ncfg = model\nself.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)\nreturn ckpt\ndef optimizer_step(self):\n\"\"\"Perform a single step of the training optimizer with gradient clipping and EMA update.\"\"\"\nself.scaler.unscale_(self.optimizer)  # unscale gradients\ntorch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\nself.scaler.step(self.optimizer)\nself.scaler.update()\nself.optimizer.zero_grad()\nif self.ema:\nself.ema.update(self.model)\ndef preprocess_batch(self, batch):\n\"\"\"\n        Allows custom preprocessing model inputs and ground truths depending on task type.\n        \"\"\"\nreturn batch\ndef validate(self):\n\"\"\"\n        Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n        \"\"\"\nmetrics = self.validator(self)\nfitness = metrics.pop('fitness', -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\nif not self.best_fitness or self.best_fitness &lt; fitness:\nself.best_fitness = fitness\nreturn metrics, fitness\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Get model and raise NotImplementedError for loading cfg files.\"\"\"\nraise NotImplementedError(\"This task trainer doesn't support loading cfg files\")\ndef get_validator(self):\n\"\"\"Returns a NotImplementedError when the get_validator function is called.\"\"\"\nraise NotImplementedError('get_validator function not implemented in trainer')\ndef get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"\n        Returns dataloader derived from torch.data.Dataloader.\n        \"\"\"\nraise NotImplementedError('get_dataloader function not implemented in trainer')\ndef build_dataset(self, img_path, mode='train', batch=None):\n\"\"\"Build dataset\"\"\"\nraise NotImplementedError('build_dataset function not implemented in trainer')\ndef label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n        Returns a loss dict with labelled training loss items tensor\n        \"\"\"\n# Not needed for classification but necessary for segmentation &amp; detection\nreturn {'loss': loss_items} if loss_items is not None else ['loss']\ndef set_model_attributes(self):\n\"\"\"\n        To set or update model parameters before training.\n        \"\"\"\nself.model.names = self.data['names']\ndef build_targets(self, preds, targets):\n\"\"\"Builds target tensors for training YOLO model.\"\"\"\npass\ndef progress_string(self):\n\"\"\"Returns a string describing training progress.\"\"\"\nreturn ''\n# TODO: may need to put these following functions into callback\ndef plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples during YOLOv5 training.\"\"\"\npass\ndef plot_training_labels(self):\n\"\"\"Plots training labels for YOLO model.\"\"\"\npass\ndef save_metrics(self, metrics):\n\"\"\"Saves training metrics to a CSV file.\"\"\"\nkeys, vals = list(metrics.keys()), list(metrics.values())\nn = len(metrics) + 1  # number of cols\ns = '' if self.csv.exists() else (('%23s,' * n % tuple(['epoch'] + keys)).rstrip(',') + '\\n')  # header\nwith open(self.csv, 'a') as f:\nf.write(s + ('%23.5g,' * n % tuple([self.epoch] + vals)).rstrip(',') + '\\n')\ndef plot_metrics(self):\n\"\"\"Plot and display metrics visually.\"\"\"\npass\ndef on_plot(self, name, data=None):\n\"\"\"Registers plots (e.g. to be consumed in callbacks)\"\"\"\npath = Path(name)\nself.plots[path] = {'data': data, 'timestamp': time.time()}\ndef final_eval(self):\n\"\"\"Performs final evaluation and validation for object detection YOLO model.\"\"\"\nfor f in self.last, self.best:\nif f.exists():\nstrip_optimizer(f)  # strip optimizers\nif f is self.best:\nLOGGER.info(f'\\nValidating {f}...')\nself.metrics = self.validator(model=f)\nself.metrics.pop('fitness', None)\nself.run_callbacks('on_fit_epoch_end')\ndef check_resume(self, overrides):\n\"\"\"Check if resume checkpoint exists and update arguments accordingly.\"\"\"\nresume = self.args.resume\nif resume:\ntry:\nexists = isinstance(resume, (str, Path)) and Path(resume).exists()\nlast = Path(check_file(resume) if exists else get_latest_run())\n# Check that resume data YAML exists, otherwise strip to force re-download of dataset\nckpt_args = attempt_load_weights(last).args\nif not Path(ckpt_args['data']).exists():\nckpt_args['data'] = self.args.data\nresume = True\nself.args = get_cfg(ckpt_args)\nself.args.model = str(last)  # reinstate model\nfor k in 'imgsz', 'batch':  # allow arg updates to reduce memory on resume if crashed due to CUDA OOM\nif k in overrides:\nsetattr(self.args, k, overrides[k])\nexcept Exception as e:\nraise FileNotFoundError('Resume checkpoint not found. Please pass a valid checkpoint to resume from, '\n\"i.e. 'yolo train resume model=path/to/last.pt'\") from e\nself.resume = resume\ndef resume_training(self, ckpt):\n\"\"\"Resume YOLO training from given epoch and best fitness.\"\"\"\nif ckpt is None:\nreturn\nbest_fitness = 0.0\nstart_epoch = ckpt['epoch'] + 1\nif ckpt['optimizer'] is not None:\nself.optimizer.load_state_dict(ckpt['optimizer'])  # optimizer\nbest_fitness = ckpt['best_fitness']\nif self.ema and ckpt.get('ema'):\nself.ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA\nself.ema.updates = ckpt['updates']\nif self.resume:\nassert start_epoch &gt; 0, \\\n                f'{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\\n' \\\n                f\"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'\"\nLOGGER.info(\nf'Resuming training from {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs')\nif self.epochs &lt; start_epoch:\nLOGGER.info(\nf\"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.\")\nself.epochs += ckpt['epoch']  # finetune additional epochs\nself.best_fitness = best_fitness\nself.start_epoch = start_epoch\nif start_epoch &gt; (self.epochs - self.args.close_mosaic):\nLOGGER.info('Closing dataloader mosaic')\nif hasattr(self.train_loader.dataset, 'mosaic'):\nself.train_loader.dataset.mosaic = False\nif hasattr(self.train_loader.dataset, 'close_mosaic'):\nself.train_loader.dataset.close_mosaic(hyp=self.args)\ndef build_optimizer(self, model, name='auto', lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):\n\"\"\"\n        Constructs an optimizer for the given model, based on the specified optimizer name, learning rate,\n        momentum, weight decay, and number of iterations.\n        Args:\n            model (torch.nn.Module): The model for which to build an optimizer.\n            name (str, optional): The name of the optimizer to use. If 'auto', the optimizer is selected\n                based on the number of iterations. Default: 'auto'.\n            lr (float, optional): The learning rate for the optimizer. Default: 0.001.\n            momentum (float, optional): The momentum factor for the optimizer. Default: 0.9.\n            decay (float, optional): The weight decay for the optimizer. Default: 1e-5.\n            iterations (float, optional): The number of iterations, which determines the optimizer if\n                name is 'auto'. Default: 1e5.\n        Returns:\n            (torch.optim.Optimizer): The constructed optimizer.\n        \"\"\"\ng = [], [], []  # optimizer parameter groups\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nif name == 'auto':\nnc = getattr(model, 'nc', 10)  # number of classes\nlr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 fit equation to 6 decimal places\nname, lr, momentum = ('SGD', 0.01, 0.9) if iterations &gt; 10000 else ('AdamW', lr_fit, 0.9)\nself.args.warmup_bias_lr = 0.0  # no higher than 0.01 for Adam\nfor module_name, module in model.named_modules():\nfor param_name, param in module.named_parameters(recurse=False):\nfullname = f'{module_name}.{param_name}' if module_name else param_name\nif 'bias' in fullname:  # bias (no decay)\ng[2].append(param)\nelif isinstance(module, bn):  # weight (no decay)\ng[1].append(param)\nelse:  # weight (with decay)\ng[0].append(param)\nif name in ('Adam', 'Adamax', 'AdamW', 'NAdam', 'RAdam'):\noptimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\nelif name == 'RMSProp':\noptimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)\nelif name == 'SGD':\noptimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\nelse:\nraise NotImplementedError(\nf\"Optimizer '{name}' not found in list of available optimizers \"\nf'[Adam, AdamW, NAdam, RAdam, RMSProp, SGD, auto].'\n'To request support for addition optimizers please visit https://github.com/ultralytics/ultralytics.')\noptimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\noptimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\nLOGGER.info(\nf\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups \"\nf'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)')\nreturn optimizer\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initializes the BaseTrainer class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str</code> <p>Path to a configuration file. Defaults to DEFAULT_CFG.</p> <code>DEFAULT_CFG</code> <code>overrides</code> <code>dict</code> <p>Configuration overrides. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"\n    Initializes the BaseTrainer class.\n    Args:\n        cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\nself.args = get_cfg(cfg, overrides)\nself.check_resume(overrides)\nself.device = select_device(self.args.device, self.args.batch)\nself.validator = None\nself.model = None\nself.metrics = None\nself.plots = {}\ninit_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n# Dirs\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f'{self.args.mode}'\nif hasattr(self.args, 'save_dir'):\nself.save_dir = Path(self.args.save_dir)\nelse:\nself.save_dir = Path(\nincrement_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in (-1, 0) else True))\nself.wdir = self.save_dir / 'weights'  # weights dir\nif RANK in (-1, 0):\nself.wdir.mkdir(parents=True, exist_ok=True)  # make dir\nself.args.save_dir = str(self.save_dir)\nyaml_save(self.save_dir / 'args.yaml', vars(self.args))  # save run args\nself.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\nself.save_period = self.args.save_period\nself.batch_size = self.args.batch\nself.epochs = self.args.epochs\nself.start_epoch = 0\nif RANK == -1:\nprint_args(vars(self.args))\n# Device\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n# Model and Dataset\nself.model = self.args.model\ntry:\nif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data)\nelif self.args.data.split('.')[-1] in ('yaml', 'yml') or self.args.task in ('detect', 'segment'):\nself.data = check_det_dataset(self.args.data)\nif 'yaml_file' in self.data:\nself.args.data = self.data['yaml_file']  # for validating 'yolo train data=url.zip' usage\nexcept Exception as e:\nraise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error \u274c {e}\")) from e\nself.trainset, self.testset = self.get_dataset(self.data)\nself.ema = None\n# Optimization utils init\nself.lf = None\nself.scheduler = None\n# Epoch level metrics\nself.best_fitness = None\nself.fitness = None\nself.loss = None\nself.tloss = None\nself.loss_names = ['Loss']\nself.csv = self.save_dir / 'results.csv'\nself.plot_idx = [0, 1, 2]\n# Callbacks\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\nif RANK in (-1, 0):\ncallbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.add_callback","title":"<code>add_callback(event, callback)</code>","text":"<p>Appends the given callback.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def add_callback(self, event: str, callback):\n\"\"\"\n    Appends the given callback.\n    \"\"\"\nself.callbacks[event].append(callback)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.build_dataset","title":"<code>build_dataset(img_path, mode='train', batch=None)</code>","text":"<p>Build dataset</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def build_dataset(self, img_path, mode='train', batch=None):\n\"\"\"Build dataset\"\"\"\nraise NotImplementedError('build_dataset function not implemented in trainer')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.build_optimizer","title":"<code>build_optimizer(model, name='auto', lr=0.001, momentum=0.9, decay=1e-05, iterations=100000.0)</code>","text":"<p>Constructs an optimizer for the given model, based on the specified optimizer name, learning rate, momentum, weight decay, and number of iterations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model for which to build an optimizer.</p> required <code>name</code> <code>str</code> <p>The name of the optimizer to use. If 'auto', the optimizer is selected based on the number of iterations. Default: 'auto'.</p> <code>'auto'</code> <code>lr</code> <code>float</code> <p>The learning rate for the optimizer. Default: 0.001.</p> <code>0.001</code> <code>momentum</code> <code>float</code> <p>The momentum factor for the optimizer. Default: 0.9.</p> <code>0.9</code> <code>decay</code> <code>float</code> <p>The weight decay for the optimizer. Default: 1e-5.</p> <code>1e-05</code> <code>iterations</code> <code>float</code> <p>The number of iterations, which determines the optimizer if name is 'auto'. Default: 1e5.</p> <code>100000.0</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The constructed optimizer.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def build_optimizer(self, model, name='auto', lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):\n\"\"\"\n    Constructs an optimizer for the given model, based on the specified optimizer name, learning rate,\n    momentum, weight decay, and number of iterations.\n    Args:\n        model (torch.nn.Module): The model for which to build an optimizer.\n        name (str, optional): The name of the optimizer to use. If 'auto', the optimizer is selected\n            based on the number of iterations. Default: 'auto'.\n        lr (float, optional): The learning rate for the optimizer. Default: 0.001.\n        momentum (float, optional): The momentum factor for the optimizer. Default: 0.9.\n        decay (float, optional): The weight decay for the optimizer. Default: 1e-5.\n        iterations (float, optional): The number of iterations, which determines the optimizer if\n            name is 'auto'. Default: 1e5.\n    Returns:\n        (torch.optim.Optimizer): The constructed optimizer.\n    \"\"\"\ng = [], [], []  # optimizer parameter groups\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nif name == 'auto':\nnc = getattr(model, 'nc', 10)  # number of classes\nlr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 fit equation to 6 decimal places\nname, lr, momentum = ('SGD', 0.01, 0.9) if iterations &gt; 10000 else ('AdamW', lr_fit, 0.9)\nself.args.warmup_bias_lr = 0.0  # no higher than 0.01 for Adam\nfor module_name, module in model.named_modules():\nfor param_name, param in module.named_parameters(recurse=False):\nfullname = f'{module_name}.{param_name}' if module_name else param_name\nif 'bias' in fullname:  # bias (no decay)\ng[2].append(param)\nelif isinstance(module, bn):  # weight (no decay)\ng[1].append(param)\nelse:  # weight (with decay)\ng[0].append(param)\nif name in ('Adam', 'Adamax', 'AdamW', 'NAdam', 'RAdam'):\noptimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\nelif name == 'RMSProp':\noptimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)\nelif name == 'SGD':\noptimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\nelse:\nraise NotImplementedError(\nf\"Optimizer '{name}' not found in list of available optimizers \"\nf'[Adam, AdamW, NAdam, RAdam, RMSProp, SGD, auto].'\n'To request support for addition optimizers please visit https://github.com/ultralytics/ultralytics.')\noptimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\noptimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\nLOGGER.info(\nf\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups \"\nf'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)')\nreturn optimizer\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.build_targets","title":"<code>build_targets(preds, targets)</code>","text":"<p>Builds target tensors for training YOLO model.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def build_targets(self, preds, targets):\n\"\"\"Builds target tensors for training YOLO model.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.check_resume","title":"<code>check_resume(overrides)</code>","text":"<p>Check if resume checkpoint exists and update arguments accordingly.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def check_resume(self, overrides):\n\"\"\"Check if resume checkpoint exists and update arguments accordingly.\"\"\"\nresume = self.args.resume\nif resume:\ntry:\nexists = isinstance(resume, (str, Path)) and Path(resume).exists()\nlast = Path(check_file(resume) if exists else get_latest_run())\n# Check that resume data YAML exists, otherwise strip to force re-download of dataset\nckpt_args = attempt_load_weights(last).args\nif not Path(ckpt_args['data']).exists():\nckpt_args['data'] = self.args.data\nresume = True\nself.args = get_cfg(ckpt_args)\nself.args.model = str(last)  # reinstate model\nfor k in 'imgsz', 'batch':  # allow arg updates to reduce memory on resume if crashed due to CUDA OOM\nif k in overrides:\nsetattr(self.args, k, overrides[k])\nexcept Exception as e:\nraise FileNotFoundError('Resume checkpoint not found. Please pass a valid checkpoint to resume from, '\n\"i.e. 'yolo train resume model=path/to/last.pt'\") from e\nself.resume = resume\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.final_eval","title":"<code>final_eval()</code>","text":"<p>Performs final evaluation and validation for object detection YOLO model.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def final_eval(self):\n\"\"\"Performs final evaluation and validation for object detection YOLO model.\"\"\"\nfor f in self.last, self.best:\nif f.exists():\nstrip_optimizer(f)  # strip optimizers\nif f is self.best:\nLOGGER.info(f'\\nValidating {f}...')\nself.metrics = self.validator(model=f)\nself.metrics.pop('fitness', None)\nself.run_callbacks('on_fit_epoch_end')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size=16, rank=0, mode='train')</code>","text":"<p>Returns dataloader derived from torch.data.Dataloader.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"\n    Returns dataloader derived from torch.data.Dataloader.\n    \"\"\"\nraise NotImplementedError('get_dataloader function not implemented in trainer')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.get_dataset","title":"<code>get_dataset(data)</code>  <code>staticmethod</code>","text":"<p>Get train, val path from data dict if it exists. Returns None if data format is not recognized.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>@staticmethod\ndef get_dataset(data):\n\"\"\"\n    Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n    \"\"\"\nreturn data['train'], data.get('val') or data.get('test')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Get model and raise NotImplementedError for loading cfg files.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Get model and raise NotImplementedError for loading cfg files.\"\"\"\nraise NotImplementedError(\"This task trainer doesn't support loading cfg files\")\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Returns a NotImplementedError when the get_validator function is called.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def get_validator(self):\n\"\"\"Returns a NotImplementedError when the get_validator function is called.\"\"\"\nraise NotImplementedError('get_validator function not implemented in trainer')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.label_loss_items","title":"<code>label_loss_items(loss_items=None, prefix='train')</code>","text":"<p>Returns a loss dict with labelled training loss items tensor</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n    Returns a loss dict with labelled training loss items tensor\n    \"\"\"\n# Not needed for classification but necessary for segmentation &amp; detection\nreturn {'loss': loss_items} if loss_items is not None else ['loss']\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.on_plot","title":"<code>on_plot(name, data=None)</code>","text":"<p>Registers plots (e.g. to be consumed in callbacks)</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def on_plot(self, name, data=None):\n\"\"\"Registers plots (e.g. to be consumed in callbacks)\"\"\"\npath = Path(name)\nself.plots[path] = {'data': data, 'timestamp': time.time()}\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.optimizer_step","title":"<code>optimizer_step()</code>","text":"<p>Perform a single step of the training optimizer with gradient clipping and EMA update.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def optimizer_step(self):\n\"\"\"Perform a single step of the training optimizer with gradient clipping and EMA update.\"\"\"\nself.scaler.unscale_(self.optimizer)  # unscale gradients\ntorch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\nself.scaler.step(self.optimizer)\nself.scaler.update()\nself.optimizer.zero_grad()\nif self.ema:\nself.ema.update(self.model)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.plot_metrics","title":"<code>plot_metrics()</code>","text":"<p>Plot and display metrics visually.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def plot_metrics(self):\n\"\"\"Plot and display metrics visually.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.plot_training_labels","title":"<code>plot_training_labels()</code>","text":"<p>Plots training labels for YOLO model.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def plot_training_labels(self):\n\"\"\"Plots training labels for YOLO model.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.plot_training_samples","title":"<code>plot_training_samples(batch, ni)</code>","text":"<p>Plots training samples during YOLOv5 training.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples during YOLOv5 training.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Allows custom preprocessing model inputs and ground truths depending on task type.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def preprocess_batch(self, batch):\n\"\"\"\n    Allows custom preprocessing model inputs and ground truths depending on task type.\n    \"\"\"\nreturn batch\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.progress_string","title":"<code>progress_string()</code>","text":"<p>Returns a string describing training progress.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def progress_string(self):\n\"\"\"Returns a string describing training progress.\"\"\"\nreturn ''\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.resume_training","title":"<code>resume_training(ckpt)</code>","text":"<p>Resume YOLO training from given epoch and best fitness.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def resume_training(self, ckpt):\n\"\"\"Resume YOLO training from given epoch and best fitness.\"\"\"\nif ckpt is None:\nreturn\nbest_fitness = 0.0\nstart_epoch = ckpt['epoch'] + 1\nif ckpt['optimizer'] is not None:\nself.optimizer.load_state_dict(ckpt['optimizer'])  # optimizer\nbest_fitness = ckpt['best_fitness']\nif self.ema and ckpt.get('ema'):\nself.ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA\nself.ema.updates = ckpt['updates']\nif self.resume:\nassert start_epoch &gt; 0, \\\n            f'{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\\n' \\\n            f\"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'\"\nLOGGER.info(\nf'Resuming training from {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs')\nif self.epochs &lt; start_epoch:\nLOGGER.info(\nf\"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.\")\nself.epochs += ckpt['epoch']  # finetune additional epochs\nself.best_fitness = best_fitness\nself.start_epoch = start_epoch\nif start_epoch &gt; (self.epochs - self.args.close_mosaic):\nLOGGER.info('Closing dataloader mosaic')\nif hasattr(self.train_loader.dataset, 'mosaic'):\nself.train_loader.dataset.mosaic = False\nif hasattr(self.train_loader.dataset, 'close_mosaic'):\nself.train_loader.dataset.close_mosaic(hyp=self.args)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.run_callbacks","title":"<code>run_callbacks(event)</code>","text":"<p>Run all existing callbacks associated with a particular event.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def run_callbacks(self, event: str):\n\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.save_metrics","title":"<code>save_metrics(metrics)</code>","text":"<p>Saves training metrics to a CSV file.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def save_metrics(self, metrics):\n\"\"\"Saves training metrics to a CSV file.\"\"\"\nkeys, vals = list(metrics.keys()), list(metrics.values())\nn = len(metrics) + 1  # number of cols\ns = '' if self.csv.exists() else (('%23s,' * n % tuple(['epoch'] + keys)).rstrip(',') + '\\n')  # header\nwith open(self.csv, 'a') as f:\nf.write(s + ('%23.5g,' * n % tuple([self.epoch] + vals)).rstrip(',') + '\\n')\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.save_model","title":"<code>save_model()</code>","text":"<p>Save model checkpoints based on various conditions.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def save_model(self):\n\"\"\"Save model checkpoints based on various conditions.\"\"\"\nckpt = {\n'epoch': self.epoch,\n'best_fitness': self.best_fitness,\n'model': deepcopy(de_parallel(self.model)).half(),\n'ema': deepcopy(self.ema.ema).half(),\n'updates': self.ema.updates,\n'optimizer': self.optimizer.state_dict(),\n'train_args': vars(self.args),  # save as dict\n'date': datetime.now().isoformat(),\n'version': __version__}\n# Use dill (if exists) to serialize the lambda functions where pickle does not do this\ntry:\nimport dill as pickle\nexcept ImportError:\nimport pickle\n# Save last, best and delete\ntorch.save(ckpt, self.last, pickle_module=pickle)\nif self.best_fitness == self.fitness:\ntorch.save(ckpt, self.best, pickle_module=pickle)\nif (self.epoch &gt; 0) and (self.save_period &gt; 0) and (self.epoch % self.save_period == 0):\ntorch.save(ckpt, self.wdir / f'epoch{self.epoch}.pt', pickle_module=pickle)\ndel ckpt\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.set_callback","title":"<code>set_callback(event, callback)</code>","text":"<p>Overrides the existing callbacks with the given callback.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def set_callback(self, event: str, callback):\n\"\"\"\n    Overrides the existing callbacks with the given callback.\n    \"\"\"\nself.callbacks[event] = [callback]\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>To set or update model parameters before training.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def set_model_attributes(self):\n\"\"\"\n    To set or update model parameters before training.\n    \"\"\"\nself.model.names = self.data['names']\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.setup_model","title":"<code>setup_model()</code>","text":"<p>load/create/download model for any task.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def setup_model(self):\n\"\"\"\n    load/create/download model for any task.\n    \"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, weights = self.model, None\nckpt = None\nif str(model).endswith('.pt'):\nweights, ckpt = attempt_load_one_weight(model)\ncfg = ckpt['model'].yaml\nelse:\ncfg = model\nself.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)\nreturn ckpt\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.train","title":"<code>train()</code>","text":"<p>Allow device='', device=None on Multi-GPU systems to default to device=0.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def train(self):\n\"\"\"Allow device='', device=None on Multi-GPU systems to default to device=0.\"\"\"\nif isinstance(self.args.device, int) or self.args.device:  # i.e. device=0 or device=[0,1,2,3]\nworld_size = torch.cuda.device_count()\nelif torch.cuda.is_available():  # i.e. device=None or device=''\nworld_size = 1  # default to device 0\nelse:  # i.e. device='cpu' or 'mps'\nworld_size = 0\n# Run subprocess if DDP training, else train normally\nif world_size &gt; 1 and 'LOCAL_RANK' not in os.environ:\n# Argument checks\nif self.args.rect:\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'rect=True' is incompatible with Multi-GPU training, setting rect=False\")\nself.args.rect = False\n# Command\ncmd, file = generate_ddp_command(world_size, self)\ntry:\nLOGGER.info(f'DDP command: {cmd}')\nsubprocess.run(cmd, check=True)\nexcept Exception as e:\nraise e\nfinally:\nddp_cleanup(self, str(file))\nelse:\nself._do_train(world_size)\n</code></pre>"},{"location":"reference/engine/trainer/#ultralytics.engine.trainer.BaseTrainer.validate","title":"<code>validate()</code>","text":"<p>Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.</p> Source code in <code>ultralytics/engine/trainer.py</code> <pre><code>def validate(self):\n\"\"\"\n    Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n    \"\"\"\nmetrics = self.validator(self)\nfitness = metrics.pop('fitness', -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\nif not self.best_fitness or self.best_fitness &lt; fitness:\nself.best_fitness = fitness\nreturn metrics, fitness\n</code></pre>"},{"location":"reference/engine/validator/","title":"Reference for <code>ultralytics/engine/validator.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/validator.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator","title":"<code>ultralytics.engine.validator.BaseValidator</code>","text":"<p>BaseValidator</p> <p>A base class for creating validators.</p> <p>Attributes:</p> Name Type Description <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the validator.</p> <code>dataloader</code> <code>DataLoader</code> <p>Dataloader to use for validation.</p> <code>pbar</code> <code>tqdm</code> <p>Progress bar to update during validation.</p> <code>model</code> <code>Module</code> <p>Model to validate.</p> <code>data</code> <code>dict</code> <p>Data dictionary.</p> <code>device</code> <code>device</code> <p>Device to use for validation.</p> <code>batch_i</code> <code>int</code> <p>Current batch index.</p> <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>names</code> <code>dict</code> <p>Class names.</p> <code>seen</code> <p>Records the number of images seen so far during validation.</p> <code>stats</code> <p>Placeholder for statistics during validation.</p> <code>confusion_matrix</code> <p>Placeholder for a confusion matrix.</p> <code>nc</code> <p>Number of classes.</p> <code>iouv</code> <p>(torch.Tensor): IoU thresholds from 0.50 to 0.95 in spaces of 0.05.</p> <code>jdict</code> <code>dict</code> <p>Dictionary to store JSON validation results.</p> <code>speed</code> <code>dict</code> <p>Dictionary with keys 'preprocess', 'inference', 'loss', 'postprocess' and their respective           batch processing times in milliseconds.</p> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>plots</code> <code>dict</code> <p>Dictionary to store plots for visualization.</p> <code>callbacks</code> <code>dict</code> <p>Dictionary to store various callback functions.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>class BaseValidator:\n\"\"\"\n    BaseValidator\n    A base class for creating validators.\n    Attributes:\n        args (SimpleNamespace): Configuration for the validator.\n        dataloader (DataLoader): Dataloader to use for validation.\n        pbar (tqdm): Progress bar to update during validation.\n        model (nn.Module): Model to validate.\n        data (dict): Data dictionary.\n        device (torch.device): Device to use for validation.\n        batch_i (int): Current batch index.\n        training (bool): Whether the model is in training mode.\n        names (dict): Class names.\n        seen: Records the number of images seen so far during validation.\n        stats: Placeholder for statistics during validation.\n        confusion_matrix: Placeholder for a confusion matrix.\n        nc: Number of classes.\n        iouv: (torch.Tensor): IoU thresholds from 0.50 to 0.95 in spaces of 0.05.\n        jdict (dict): Dictionary to store JSON validation results.\n        speed (dict): Dictionary with keys 'preprocess', 'inference', 'loss', 'postprocess' and their respective\n                      batch processing times in milliseconds.\n        save_dir (Path): Directory to save results.\n        plots (dict): Dictionary to store plots for visualization.\n        callbacks (dict): Dictionary to store various callback functions.\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"\n        Initializes a BaseValidator instance.\n        Args:\n            dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n            save_dir (Path): Directory to save results.\n            pbar (tqdm.tqdm): Progress bar for displaying progress.\n            args (SimpleNamespace): Configuration for the validator.\n            _callbacks (dict): Dictionary to store various callback functions.\n        \"\"\"\nself.args = get_cfg(overrides=args)\nself.dataloader = dataloader\nself.pbar = pbar\nself.model = None\nself.data = None\nself.device = None\nself.batch_i = None\nself.training = True\nself.names = None\nself.seen = None\nself.stats = None\nself.confusion_matrix = None\nself.nc = None\nself.iouv = None\nself.jdict = None\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f'{self.args.mode}'\nself.save_dir = save_dir or increment_path(Path(project) / name,\nexist_ok=self.args.exist_ok if RANK in (-1, 0) else True)\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\nif self.args.conf is None:\nself.args.conf = 0.001  # default conf=0.001\nself.plots = {}\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\n@smart_inference_mode()\ndef __call__(self, trainer=None, model=None):\n\"\"\"\n        Supports validation of a pre-trained model if passed or a model being trained\n        if trainer is passed (trainer gets priority).\n        \"\"\"\nself.training = trainer is not None\naugment = self.args.augment and (not self.training)\nif self.training:\nself.device = trainer.device\nself.data = trainer.data\nmodel = trainer.ema.ema or trainer.model\nself.args.half = self.device.type != 'cpu'  # force FP16 val during training\nmodel = model.half() if self.args.half else model.float()\nself.model = model\nself.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\nself.args.plots = trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)\nmodel.eval()\nelse:\ncallbacks.add_integration_callbacks(self)\nself.run_callbacks('on_val_start')\nmodel = AutoBackend(model or self.args.model,\ndevice=select_device(self.args.device, self.args.batch),\ndnn=self.args.dnn,\ndata=self.args.data,\nfp16=self.args.half)\nself.model = model\nself.device = model.device  # update device\nself.args.half = model.fp16  # update half\nstride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\nimgsz = check_imgsz(self.args.imgsz, stride=stride)\nif engine:\nself.args.batch = model.batch_size\nelif not pt and not jit:\nself.args.batch = 1  # export.py models default to batch-size 1\nLOGGER.info(f'Forcing batch=1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\nif isinstance(self.args.data, str) and self.args.data.split('.')[-1] in ('yaml', 'yml'):\nself.data = check_det_dataset(self.args.data)\nelif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data, split=self.args.split)\nelse:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' for task={self.args.task} not found \u274c\"))\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\nif not pt:\nself.args.rect = False\nself.dataloader = self.dataloader or self.get_dataloader(self.data.get(self.args.split), self.args.batch)\nmodel.eval()\nmodel.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\ndt = Profile(), Profile(), Profile(), Profile()\nn_batches = len(self.dataloader)\ndesc = self.get_desc()\n# NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n# which may affect classification task since this arg is in yolov5/classify/val.py.\n# bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\nbar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\nself.init_metrics(de_parallel(model))\nself.jdict = []  # empty before each val\nfor batch_i, batch in enumerate(bar):\nself.run_callbacks('on_val_batch_start')\nself.batch_i = batch_i\n# Preprocess\nwith dt[0]:\nbatch = self.preprocess(batch)\n# Inference\nwith dt[1]:\npreds = model(batch['img'], augment=augment)\n# Loss\nwith dt[2]:\nif self.training:\nself.loss += model.loss(batch, preds)[1]\n# Postprocess\nwith dt[3]:\npreds = self.postprocess(preds)\nself.update_metrics(preds, batch)\nif self.args.plots and batch_i &lt; 3:\nself.plot_val_samples(batch, batch_i)\nself.plot_predictions(batch, preds, batch_i)\nself.run_callbacks('on_val_batch_end')\nstats = self.get_stats()\nself.check_stats(stats)\nself.speed = dict(zip(self.speed.keys(), (x.t / len(self.dataloader.dataset) * 1E3 for x in dt)))\nself.finalize_metrics()\nself.print_results()\nself.run_callbacks('on_val_end')\nif self.training:\nmodel.float()\nresults = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix='val')}\nreturn {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\nelse:\nLOGGER.info('Speed: %.1fms preprocess, %.1fms inference, %.1fms loss, %.1fms postprocess per image' %\ntuple(self.speed.values()))\nif self.args.save_json and self.jdict:\nwith open(str(self.save_dir / 'predictions.json'), 'w') as f:\nLOGGER.info(f'Saving {f.name}...')\njson.dump(self.jdict, f)  # flatten and save\nstats = self.eval_json(stats)  # update stats\nif self.args.plots or self.args.save_json:\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\nreturn stats\ndef match_predictions(self, pred_classes, true_classes, iou):\n\"\"\"\n        Matches predictions to ground truth objects (pred_classes, true_classes) using IoU.\n        Args:\n            pred_classes (torch.Tensor): Predicted class indices of shape(N,).\n            true_classes (torch.Tensor): Target class indices of shape(M,).\n            iou (torch.Tensor): IoU thresholds from 0.50 to 0.95 in space of 0.05.\n        Returns:\n            (torch.Tensor): Correct tensor of shape(N,10) for 10 IoU thresholds.\n        \"\"\"\ncorrect = np.zeros((pred_classes.shape[0], self.iouv.shape[0])).astype(bool)\ncorrect_class = true_classes[:, None] == pred_classes\nfor i, iouv in enumerate(self.iouv):\nx = torch.nonzero(iou.ge(iouv) &amp; correct_class)  # IoU &gt; threshold and classes match\nif x.shape[0]:\n# Concatenate [label, detect, iou]\nmatches = torch.cat((x, iou[x[:, 0], x[:, 1]].unsqueeze(1)), 1).cpu().numpy()\nif x.shape[0] &gt; 1:\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n# matches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 0], return_index=True)[1]]\ncorrect[matches[:, 1].astype(int), i] = True\nreturn torch.tensor(correct, dtype=torch.bool, device=pred_classes.device)\ndef add_callback(self, event: str, callback):\n\"\"\"Appends the given callback.\"\"\"\nself.callbacks[event].append(callback)\ndef run_callbacks(self, event: str):\n\"\"\"Runs all callbacks associated with a specified event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\ndef get_dataloader(self, dataset_path, batch_size):\n\"\"\"Get data loader from dataset path and batch size.\"\"\"\nraise NotImplementedError('get_dataloader function not implemented for this validator')\ndef build_dataset(self, img_path):\n\"\"\"Build dataset\"\"\"\nraise NotImplementedError('build_dataset function not implemented in validator')\ndef preprocess(self, batch):\n\"\"\"Preprocesses an input batch.\"\"\"\nreturn batch\ndef postprocess(self, preds):\n\"\"\"Describes and summarizes the purpose of 'postprocess()' but no details mentioned.\"\"\"\nreturn preds\ndef init_metrics(self, model):\n\"\"\"Initialize performance metrics for the YOLO model.\"\"\"\npass\ndef update_metrics(self, preds, batch):\n\"\"\"Updates metrics based on predictions and batch.\"\"\"\npass\ndef finalize_metrics(self, *args, **kwargs):\n\"\"\"Finalizes and returns all metrics.\"\"\"\npass\ndef get_stats(self):\n\"\"\"Returns statistics about the model's performance.\"\"\"\nreturn {}\ndef check_stats(self, stats):\n\"\"\"Checks statistics.\"\"\"\npass\ndef print_results(self):\n\"\"\"Prints the results of the model's predictions.\"\"\"\npass\ndef get_desc(self):\n\"\"\"Get description of the YOLO model.\"\"\"\npass\n@property\ndef metric_keys(self):\n\"\"\"Returns the metric keys used in YOLO training/validation.\"\"\"\nreturn []\ndef on_plot(self, name, data=None):\n\"\"\"Registers plots (e.g. to be consumed in callbacks)\"\"\"\npath = Path(name)\nself.plots[path] = {'data': data, 'timestamp': time.time()}\n# TODO: may need to put these following functions into callback\ndef plot_val_samples(self, batch, ni):\n\"\"\"Plots validation samples during training.\"\"\"\npass\ndef plot_predictions(self, batch, preds, ni):\n\"\"\"Plots YOLO model predictions on batch images.\"\"\"\npass\ndef pred_to_json(self, preds, batch):\n\"\"\"Convert predictions to JSON format.\"\"\"\npass\ndef eval_json(self, stats):\n\"\"\"Evaluate and return JSON format of prediction statistics.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.metric_keys","title":"<code>metric_keys</code>  <code>property</code>","text":"<p>Returns the metric keys used in YOLO training/validation.</p>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.__call__","title":"<code>__call__(trainer=None, model=None)</code>","text":"<p>Supports validation of a pre-trained model if passed or a model being trained if trainer is passed (trainer gets priority).</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>@smart_inference_mode()\ndef __call__(self, trainer=None, model=None):\n\"\"\"\n    Supports validation of a pre-trained model if passed or a model being trained\n    if trainer is passed (trainer gets priority).\n    \"\"\"\nself.training = trainer is not None\naugment = self.args.augment and (not self.training)\nif self.training:\nself.device = trainer.device\nself.data = trainer.data\nmodel = trainer.ema.ema or trainer.model\nself.args.half = self.device.type != 'cpu'  # force FP16 val during training\nmodel = model.half() if self.args.half else model.float()\nself.model = model\nself.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\nself.args.plots = trainer.stopper.possible_stop or (trainer.epoch == trainer.epochs - 1)\nmodel.eval()\nelse:\ncallbacks.add_integration_callbacks(self)\nself.run_callbacks('on_val_start')\nmodel = AutoBackend(model or self.args.model,\ndevice=select_device(self.args.device, self.args.batch),\ndnn=self.args.dnn,\ndata=self.args.data,\nfp16=self.args.half)\nself.model = model\nself.device = model.device  # update device\nself.args.half = model.fp16  # update half\nstride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\nimgsz = check_imgsz(self.args.imgsz, stride=stride)\nif engine:\nself.args.batch = model.batch_size\nelif not pt and not jit:\nself.args.batch = 1  # export.py models default to batch-size 1\nLOGGER.info(f'Forcing batch=1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\nif isinstance(self.args.data, str) and self.args.data.split('.')[-1] in ('yaml', 'yml'):\nself.data = check_det_dataset(self.args.data)\nelif self.args.task == 'classify':\nself.data = check_cls_dataset(self.args.data, split=self.args.split)\nelse:\nraise FileNotFoundError(emojis(f\"Dataset '{self.args.data}' for task={self.args.task} not found \u274c\"))\nif self.device.type == 'cpu':\nself.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\nif not pt:\nself.args.rect = False\nself.dataloader = self.dataloader or self.get_dataloader(self.data.get(self.args.split), self.args.batch)\nmodel.eval()\nmodel.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\ndt = Profile(), Profile(), Profile(), Profile()\nn_batches = len(self.dataloader)\ndesc = self.get_desc()\n# NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n# which may affect classification task since this arg is in yolov5/classify/val.py.\n# bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\nbar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\nself.init_metrics(de_parallel(model))\nself.jdict = []  # empty before each val\nfor batch_i, batch in enumerate(bar):\nself.run_callbacks('on_val_batch_start')\nself.batch_i = batch_i\n# Preprocess\nwith dt[0]:\nbatch = self.preprocess(batch)\n# Inference\nwith dt[1]:\npreds = model(batch['img'], augment=augment)\n# Loss\nwith dt[2]:\nif self.training:\nself.loss += model.loss(batch, preds)[1]\n# Postprocess\nwith dt[3]:\npreds = self.postprocess(preds)\nself.update_metrics(preds, batch)\nif self.args.plots and batch_i &lt; 3:\nself.plot_val_samples(batch, batch_i)\nself.plot_predictions(batch, preds, batch_i)\nself.run_callbacks('on_val_batch_end')\nstats = self.get_stats()\nself.check_stats(stats)\nself.speed = dict(zip(self.speed.keys(), (x.t / len(self.dataloader.dataset) * 1E3 for x in dt)))\nself.finalize_metrics()\nself.print_results()\nself.run_callbacks('on_val_end')\nif self.training:\nmodel.float()\nresults = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix='val')}\nreturn {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\nelse:\nLOGGER.info('Speed: %.1fms preprocess, %.1fms inference, %.1fms loss, %.1fms postprocess per image' %\ntuple(self.speed.values()))\nif self.args.save_json and self.jdict:\nwith open(str(self.save_dir / 'predictions.json'), 'w') as f:\nLOGGER.info(f'Saving {f.name}...')\njson.dump(self.jdict, f)  # flatten and save\nstats = self.eval_json(stats)  # update stats\nif self.args.plots or self.args.save_json:\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\nreturn stats\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initializes a BaseValidator instance.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>Dataloader to be used for validation.</p> <code>None</code> <code>save_dir</code> <code>Path</code> <p>Directory to save results.</p> <code>None</code> <code>pbar</code> <code>tqdm</code> <p>Progress bar for displaying progress.</p> <code>None</code> <code>args</code> <code>SimpleNamespace</code> <p>Configuration for the validator.</p> <code>None</code> <code>_callbacks</code> <code>dict</code> <p>Dictionary to store various callback functions.</p> <code>None</code> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"\n    Initializes a BaseValidator instance.\n    Args:\n        dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n        save_dir (Path): Directory to save results.\n        pbar (tqdm.tqdm): Progress bar for displaying progress.\n        args (SimpleNamespace): Configuration for the validator.\n        _callbacks (dict): Dictionary to store various callback functions.\n    \"\"\"\nself.args = get_cfg(overrides=args)\nself.dataloader = dataloader\nself.pbar = pbar\nself.model = None\nself.data = None\nself.device = None\nself.batch_i = None\nself.training = True\nself.names = None\nself.seen = None\nself.stats = None\nself.confusion_matrix = None\nself.nc = None\nself.iouv = None\nself.jdict = None\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\nproject = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\nname = self.args.name or f'{self.args.mode}'\nself.save_dir = save_dir or increment_path(Path(project) / name,\nexist_ok=self.args.exist_ok if RANK in (-1, 0) else True)\n(self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\nif self.args.conf is None:\nself.args.conf = 0.001  # default conf=0.001\nself.plots = {}\nself.callbacks = _callbacks or callbacks.get_default_callbacks()\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.add_callback","title":"<code>add_callback(event, callback)</code>","text":"<p>Appends the given callback.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def add_callback(self, event: str, callback):\n\"\"\"Appends the given callback.\"\"\"\nself.callbacks[event].append(callback)\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.build_dataset","title":"<code>build_dataset(img_path)</code>","text":"<p>Build dataset</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def build_dataset(self, img_path):\n\"\"\"Build dataset\"\"\"\nraise NotImplementedError('build_dataset function not implemented in validator')\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.check_stats","title":"<code>check_stats(stats)</code>","text":"<p>Checks statistics.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def check_stats(self, stats):\n\"\"\"Checks statistics.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.eval_json","title":"<code>eval_json(stats)</code>","text":"<p>Evaluate and return JSON format of prediction statistics.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def eval_json(self, stats):\n\"\"\"Evaluate and return JSON format of prediction statistics.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.finalize_metrics","title":"<code>finalize_metrics(*args, **kwargs)</code>","text":"<p>Finalizes and returns all metrics.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def finalize_metrics(self, *args, **kwargs):\n\"\"\"Finalizes and returns all metrics.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size)</code>","text":"<p>Get data loader from dataset path and batch size.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size):\n\"\"\"Get data loader from dataset path and batch size.\"\"\"\nraise NotImplementedError('get_dataloader function not implemented for this validator')\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.get_desc","title":"<code>get_desc()</code>","text":"<p>Get description of the YOLO model.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def get_desc(self):\n\"\"\"Get description of the YOLO model.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.get_stats","title":"<code>get_stats()</code>","text":"<p>Returns statistics about the model's performance.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def get_stats(self):\n\"\"\"Returns statistics about the model's performance.\"\"\"\nreturn {}\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.init_metrics","title":"<code>init_metrics(model)</code>","text":"<p>Initialize performance metrics for the YOLO model.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def init_metrics(self, model):\n\"\"\"Initialize performance metrics for the YOLO model.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.match_predictions","title":"<code>match_predictions(pred_classes, true_classes, iou)</code>","text":"<p>Matches predictions to ground truth objects (pred_classes, true_classes) using IoU.</p> <p>Parameters:</p> Name Type Description Default <code>pred_classes</code> <code>Tensor</code> <p>Predicted class indices of shape(N,).</p> required <code>true_classes</code> <code>Tensor</code> <p>Target class indices of shape(M,).</p> required <code>iou</code> <code>Tensor</code> <p>IoU thresholds from 0.50 to 0.95 in space of 0.05.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Correct tensor of shape(N,10) for 10 IoU thresholds.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def match_predictions(self, pred_classes, true_classes, iou):\n\"\"\"\n    Matches predictions to ground truth objects (pred_classes, true_classes) using IoU.\n    Args:\n        pred_classes (torch.Tensor): Predicted class indices of shape(N,).\n        true_classes (torch.Tensor): Target class indices of shape(M,).\n        iou (torch.Tensor): IoU thresholds from 0.50 to 0.95 in space of 0.05.\n    Returns:\n        (torch.Tensor): Correct tensor of shape(N,10) for 10 IoU thresholds.\n    \"\"\"\ncorrect = np.zeros((pred_classes.shape[0], self.iouv.shape[0])).astype(bool)\ncorrect_class = true_classes[:, None] == pred_classes\nfor i, iouv in enumerate(self.iouv):\nx = torch.nonzero(iou.ge(iouv) &amp; correct_class)  # IoU &gt; threshold and classes match\nif x.shape[0]:\n# Concatenate [label, detect, iou]\nmatches = torch.cat((x, iou[x[:, 0], x[:, 1]].unsqueeze(1)), 1).cpu().numpy()\nif x.shape[0] &gt; 1:\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n# matches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 0], return_index=True)[1]]\ncorrect[matches[:, 1].astype(int), i] = True\nreturn torch.tensor(correct, dtype=torch.bool, device=pred_classes.device)\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.on_plot","title":"<code>on_plot(name, data=None)</code>","text":"<p>Registers plots (e.g. to be consumed in callbacks)</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def on_plot(self, name, data=None):\n\"\"\"Registers plots (e.g. to be consumed in callbacks)\"\"\"\npath = Path(name)\nself.plots[path] = {'data': data, 'timestamp': time.time()}\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.plot_predictions","title":"<code>plot_predictions(batch, preds, ni)</code>","text":"<p>Plots YOLO model predictions on batch images.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def plot_predictions(self, batch, preds, ni):\n\"\"\"Plots YOLO model predictions on batch images.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.plot_val_samples","title":"<code>plot_val_samples(batch, ni)</code>","text":"<p>Plots validation samples during training.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def plot_val_samples(self, batch, ni):\n\"\"\"Plots validation samples during training.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.postprocess","title":"<code>postprocess(preds)</code>","text":"<p>Describes and summarizes the purpose of 'postprocess()' but no details mentioned.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def postprocess(self, preds):\n\"\"\"Describes and summarizes the purpose of 'postprocess()' but no details mentioned.\"\"\"\nreturn preds\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.pred_to_json","title":"<code>pred_to_json(preds, batch)</code>","text":"<p>Convert predictions to JSON format.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def pred_to_json(self, preds, batch):\n\"\"\"Convert predictions to JSON format.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.preprocess","title":"<code>preprocess(batch)</code>","text":"<p>Preprocesses an input batch.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def preprocess(self, batch):\n\"\"\"Preprocesses an input batch.\"\"\"\nreturn batch\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.print_results","title":"<code>print_results()</code>","text":"<p>Prints the results of the model's predictions.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def print_results(self):\n\"\"\"Prints the results of the model's predictions.\"\"\"\npass\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.run_callbacks","title":"<code>run_callbacks(event)</code>","text":"<p>Runs all callbacks associated with a specified event.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def run_callbacks(self, event: str):\n\"\"\"Runs all callbacks associated with a specified event.\"\"\"\nfor callback in self.callbacks.get(event, []):\ncallback(self)\n</code></pre>"},{"location":"reference/engine/validator/#ultralytics.engine.validator.BaseValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Updates metrics based on predictions and batch.</p> Source code in <code>ultralytics/engine/validator.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Updates metrics based on predictions and batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/hub/__init__/","title":"Reference for <code>ultralytics/hub/__init__.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/hub/init.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/hub/__init__/#ultralytics.hub.login","title":"<code>ultralytics.hub.login(api_key='')</code>","text":"<p>Log in to the Ultralytics HUB API using the provided API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>May be an API key or a combination API key and model ID, i.e. key_id</p> <code>''</code> Example <pre><code>from ultralytics import hub\nhub.login('API_KEY')\n</code></pre> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def login(api_key=''):\n\"\"\"\n    Log in to the Ultralytics HUB API using the provided API key.\n    Args:\n        api_key (str, optional): May be an API key or a combination API key and model ID, i.e. key_id\n    Example:\n        ```python\n        from ultralytics import hub\n        hub.login('API_KEY')\n        ```\n    \"\"\"\nAuth(api_key, verbose=True)\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.logout","title":"<code>ultralytics.hub.logout()</code>","text":"<p>Log out of Ultralytics HUB by removing the API key from the settings file. To log in again, use 'yolo hub login'.</p> Example <pre><code>from ultralytics import hub\nhub.logout()\n</code></pre> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def logout():\n\"\"\"\n    Log out of Ultralytics HUB by removing the API key from the settings file. To log in again, use 'yolo hub login'.\n    Example:\n        ```python\n        from ultralytics import hub\n        hub.logout()\n        ```\n    \"\"\"\nSETTINGS['api_key'] = ''\nSETTINGS.save()\nLOGGER.info(f\"{PREFIX}logged out \u2705. To log in again, use 'yolo hub login'.\")\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.reset_model","title":"<code>ultralytics.hub.reset_model(model_id='')</code>","text":"<p>Reset a trained model to an untrained state.</p> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def reset_model(model_id=''):\n\"\"\"Reset a trained model to an untrained state.\"\"\"\nr = requests.post(f'{HUB_API_ROOT}/model-reset', json={'apiKey': Auth().api_key, 'modelId': model_id})\nif r.status_code == 200:\nLOGGER.info(f'{PREFIX}Model reset successfully')\nreturn\nLOGGER.warning(f'{PREFIX}Model reset failure {r.status_code} {r.reason}')\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.export_fmts_hub","title":"<code>ultralytics.hub.export_fmts_hub()</code>","text":"<p>Returns a list of HUB-supported export formats.</p> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def export_fmts_hub():\n\"\"\"Returns a list of HUB-supported export formats.\"\"\"\nfrom ultralytics.engine.exporter import export_formats\nreturn list(export_formats()['Argument'][1:]) + ['ultralytics_tflite', 'ultralytics_coreml']\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.export_model","title":"<code>ultralytics.hub.export_model(model_id='', format='torchscript')</code>","text":"<p>Export a model to all formats.</p> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def export_model(model_id='', format='torchscript'):\n\"\"\"Export a model to all formats.\"\"\"\nassert format in export_fmts_hub(), f\"Unsupported export format '{format}', valid formats are {export_fmts_hub()}\"\nr = requests.post(f'{HUB_API_ROOT}/v1/models/{model_id}/export',\njson={'format': format},\nheaders={'x-api-key': Auth().api_key})\nassert r.status_code == 200, f'{PREFIX}{format} export failure {r.status_code} {r.reason}'\nLOGGER.info(f'{PREFIX}{format} export started \u2705')\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.get_export","title":"<code>ultralytics.hub.get_export(model_id='', format='torchscript')</code>","text":"<p>Get an exported model dictionary with download URL.</p> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def get_export(model_id='', format='torchscript'):\n\"\"\"Get an exported model dictionary with download URL.\"\"\"\nassert format in export_fmts_hub(), f\"Unsupported export format '{format}', valid formats are {export_fmts_hub()}\"\nr = requests.post(f'{HUB_API_ROOT}/get-export',\njson={\n'apiKey': Auth().api_key,\n'modelId': model_id,\n'format': format})\nassert r.status_code == 200, f'{PREFIX}{format} get_export failure {r.status_code} {r.reason}'\nreturn r.json()\n</code></pre>"},{"location":"reference/hub/__init__/#ultralytics.hub.check_dataset","title":"<code>ultralytics.hub.check_dataset(path='', task='detect')</code>","text":"<p>Function for error-checking HUB dataset Zip file before upload. It checks a dataset for errors before it is uploaded to the HUB. Usage examples are given below.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to data.zip (with data.yaml inside data.zip). Defaults to ''.</p> <code>''</code> <code>task</code> <code>str</code> <p>Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Defaults to 'detect'.</p> <code>'detect'</code> Example <pre><code>from ultralytics.hub import check_dataset\ncheck_dataset('path/to/coco8.zip', task='detect')  # detect dataset\ncheck_dataset('path/to/coco8-seg.zip', task='segment')  # segment dataset\ncheck_dataset('path/to/coco8-pose.zip', task='pose')  # pose dataset\n</code></pre> Source code in <code>ultralytics/hub/__init__.py</code> <pre><code>def check_dataset(path='', task='detect'):\n\"\"\"\n    Function for error-checking HUB dataset Zip file before upload. It checks a dataset for errors before it is\n    uploaded to the HUB. Usage examples are given below.\n    Args:\n        path (str, optional): Path to data.zip (with data.yaml inside data.zip). Defaults to ''.\n        task (str, optional): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Defaults to 'detect'.\n    Example:\n        ```python\n        from ultralytics.hub import check_dataset\n        check_dataset('path/to/coco8.zip', task='detect')  # detect dataset\n        check_dataset('path/to/coco8-seg.zip', task='segment')  # segment dataset\n        check_dataset('path/to/coco8-pose.zip', task='pose')  # pose dataset\n        ```\n    \"\"\"\nHUBDatasetStats(path=path, task=task).get_json()\nLOGGER.info(f'Checks completed correctly \u2705. Upload this dataset to {HUB_WEB_ROOT}/datasets/.')\n</code></pre>"},{"location":"reference/hub/auth/","title":"Reference for <code>ultralytics/hub/auth.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/hub/auth.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth","title":"<code>ultralytics.hub.auth.Auth</code>","text":"Source code in <code>ultralytics/hub/auth.py</code> <pre><code>class Auth:\nid_token = api_key = model_key = False\ndef __init__(self, api_key='', verbose=False):\n\"\"\"\n        Initialize the Auth class with an optional API key.\n        Args:\n            api_key (str, optional): May be an API key or a combination API key and model ID, i.e. key_id\n        \"\"\"\n# Split the input API key in case it contains a combined key_model and keep only the API key part\napi_key = api_key.split('_')[0]\n# Set API key attribute as value passed or SETTINGS API key if none passed\nself.api_key = api_key or SETTINGS.get('api_key', '')\n# If an API key is provided\nif self.api_key:\n# If the provided API key matches the API key in the SETTINGS\nif self.api_key == SETTINGS.get('api_key'):\n# Log that the user is already logged in\nif verbose:\nLOGGER.info(f'{PREFIX}Authenticated \u2705')\nreturn\nelse:\n# Attempt to authenticate with the provided API key\nsuccess = self.authenticate()\n# If the API key is not provided and the environment is a Google Colab notebook\nelif is_colab():\n# Attempt to authenticate using browser cookies\nsuccess = self.auth_with_cookies()\nelse:\n# Request an API key\nsuccess = self.request_api_key()\n# Update SETTINGS with the new API key after successful authentication\nif success:\nSETTINGS.update({'api_key': self.api_key})\n# Log that the new login was successful\nif verbose:\nLOGGER.info(f'{PREFIX}New authentication successful \u2705')\nelif verbose:\nLOGGER.info(f'{PREFIX}Retrieve API key from {API_KEY_URL}')\ndef request_api_key(self, max_attempts=3):\n\"\"\"\n        Prompt the user to input their API key. Returns the model ID.\n        \"\"\"\nimport getpass\nfor attempts in range(max_attempts):\nLOGGER.info(f'{PREFIX}Login. Attempt {attempts + 1} of {max_attempts}')\ninput_key = getpass.getpass(f'Enter API key from {API_KEY_URL} ')\nself.api_key = input_key.split('_')[0]  # remove model id if present\nif self.authenticate():\nreturn True\nraise ConnectionError(emojis(f'{PREFIX}Failed to authenticate \u274c'))\ndef authenticate(self) -&gt; bool:\n\"\"\"\n        Attempt to authenticate with the server using either id_token or API key.\n        Returns:\n            bool: True if authentication is successful, False otherwise.\n        \"\"\"\ntry:\nif header := self.get_auth_header():\nr = requests.post(f'{HUB_API_ROOT}/v1/auth', headers=header)\nif not r.json().get('success', False):\nraise ConnectionError('Unable to authenticate.')\nreturn True\nraise ConnectionError('User has not authenticated locally.')\nexcept ConnectionError:\nself.id_token = self.api_key = False  # reset invalid\nLOGGER.warning(f'{PREFIX}Invalid API key \u26a0\ufe0f')\nreturn False\ndef auth_with_cookies(self) -&gt; bool:\n\"\"\"\n        Attempt to fetch authentication via cookies and set id_token.\n        User must be logged in to HUB and running in a supported browser.\n        Returns:\n            bool: True if authentication is successful, False otherwise.\n        \"\"\"\nif not is_colab():\nreturn False  # Currently only works with Colab\ntry:\nauthn = request_with_credentials(f'{HUB_API_ROOT}/v1/auth/auto')\nif authn.get('success', False):\nself.id_token = authn.get('data', {}).get('idToken', None)\nself.authenticate()\nreturn True\nraise ConnectionError('Unable to fetch browser authentication details.')\nexcept ConnectionError:\nself.id_token = False  # reset invalid\nreturn False\ndef get_auth_header(self):\n\"\"\"\n        Get the authentication header for making API requests.\n        Returns:\n            (dict): The authentication header if id_token or API key is set, None otherwise.\n        \"\"\"\nif self.id_token:\nreturn {'authorization': f'Bearer {self.id_token}'}\nelif self.api_key:\nreturn {'x-api-key': self.api_key}\n</code></pre>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth.__init__","title":"<code>__init__(api_key='', verbose=False)</code>","text":"<p>Initialize the Auth class with an optional API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>May be an API key or a combination API key and model ID, i.e. key_id</p> <code>''</code> Source code in <code>ultralytics/hub/auth.py</code> <pre><code>def __init__(self, api_key='', verbose=False):\n\"\"\"\n    Initialize the Auth class with an optional API key.\n    Args:\n        api_key (str, optional): May be an API key or a combination API key and model ID, i.e. key_id\n    \"\"\"\n# Split the input API key in case it contains a combined key_model and keep only the API key part\napi_key = api_key.split('_')[0]\n# Set API key attribute as value passed or SETTINGS API key if none passed\nself.api_key = api_key or SETTINGS.get('api_key', '')\n# If an API key is provided\nif self.api_key:\n# If the provided API key matches the API key in the SETTINGS\nif self.api_key == SETTINGS.get('api_key'):\n# Log that the user is already logged in\nif verbose:\nLOGGER.info(f'{PREFIX}Authenticated \u2705')\nreturn\nelse:\n# Attempt to authenticate with the provided API key\nsuccess = self.authenticate()\n# If the API key is not provided and the environment is a Google Colab notebook\nelif is_colab():\n# Attempt to authenticate using browser cookies\nsuccess = self.auth_with_cookies()\nelse:\n# Request an API key\nsuccess = self.request_api_key()\n# Update SETTINGS with the new API key after successful authentication\nif success:\nSETTINGS.update({'api_key': self.api_key})\n# Log that the new login was successful\nif verbose:\nLOGGER.info(f'{PREFIX}New authentication successful \u2705')\nelif verbose:\nLOGGER.info(f'{PREFIX}Retrieve API key from {API_KEY_URL}')\n</code></pre>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth.auth_with_cookies","title":"<code>auth_with_cookies()</code>","text":"<p>Attempt to fetch authentication via cookies and set id_token. User must be logged in to HUB and running in a supported browser.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if authentication is successful, False otherwise.</p> Source code in <code>ultralytics/hub/auth.py</code> <pre><code>def auth_with_cookies(self) -&gt; bool:\n\"\"\"\n    Attempt to fetch authentication via cookies and set id_token.\n    User must be logged in to HUB and running in a supported browser.\n    Returns:\n        bool: True if authentication is successful, False otherwise.\n    \"\"\"\nif not is_colab():\nreturn False  # Currently only works with Colab\ntry:\nauthn = request_with_credentials(f'{HUB_API_ROOT}/v1/auth/auto')\nif authn.get('success', False):\nself.id_token = authn.get('data', {}).get('idToken', None)\nself.authenticate()\nreturn True\nraise ConnectionError('Unable to fetch browser authentication details.')\nexcept ConnectionError:\nself.id_token = False  # reset invalid\nreturn False\n</code></pre>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth.authenticate","title":"<code>authenticate()</code>","text":"<p>Attempt to authenticate with the server using either id_token or API key.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if authentication is successful, False otherwise.</p> Source code in <code>ultralytics/hub/auth.py</code> <pre><code>def authenticate(self) -&gt; bool:\n\"\"\"\n    Attempt to authenticate with the server using either id_token or API key.\n    Returns:\n        bool: True if authentication is successful, False otherwise.\n    \"\"\"\ntry:\nif header := self.get_auth_header():\nr = requests.post(f'{HUB_API_ROOT}/v1/auth', headers=header)\nif not r.json().get('success', False):\nraise ConnectionError('Unable to authenticate.')\nreturn True\nraise ConnectionError('User has not authenticated locally.')\nexcept ConnectionError:\nself.id_token = self.api_key = False  # reset invalid\nLOGGER.warning(f'{PREFIX}Invalid API key \u26a0\ufe0f')\nreturn False\n</code></pre>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth.get_auth_header","title":"<code>get_auth_header()</code>","text":"<p>Get the authentication header for making API requests.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The authentication header if id_token or API key is set, None otherwise.</p> Source code in <code>ultralytics/hub/auth.py</code> <pre><code>def get_auth_header(self):\n\"\"\"\n    Get the authentication header for making API requests.\n    Returns:\n        (dict): The authentication header if id_token or API key is set, None otherwise.\n    \"\"\"\nif self.id_token:\nreturn {'authorization': f'Bearer {self.id_token}'}\nelif self.api_key:\nreturn {'x-api-key': self.api_key}\n</code></pre>"},{"location":"reference/hub/auth/#ultralytics.hub.auth.Auth.request_api_key","title":"<code>request_api_key(max_attempts=3)</code>","text":"<p>Prompt the user to input their API key. Returns the model ID.</p> Source code in <code>ultralytics/hub/auth.py</code> <pre><code>def request_api_key(self, max_attempts=3):\n\"\"\"\n    Prompt the user to input their API key. Returns the model ID.\n    \"\"\"\nimport getpass\nfor attempts in range(max_attempts):\nLOGGER.info(f'{PREFIX}Login. Attempt {attempts + 1} of {max_attempts}')\ninput_key = getpass.getpass(f'Enter API key from {API_KEY_URL} ')\nself.api_key = input_key.split('_')[0]  # remove model id if present\nif self.authenticate():\nreturn True\nraise ConnectionError(emojis(f'{PREFIX}Failed to authenticate \u274c'))\n</code></pre>"},{"location":"reference/hub/session/","title":"Reference for <code>ultralytics/hub/session.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/hub/session.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/hub/session/#ultralytics.hub.session.HUBTrainingSession","title":"<code>ultralytics.hub.session.HUBTrainingSession</code>","text":"<p>HUB training session for Ultralytics HUB YOLO models. Handles model initialization, heartbeats, and checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Model identifier used to initialize the HUB training session.</p> required <p>Attributes:</p> Name Type Description <code>agent_id</code> <code>str</code> <p>Identifier for the instance communicating with the server.</p> <code>model_id</code> <code>str</code> <p>Identifier for the YOLOv5 model being trained.</p> <code>model_url</code> <code>str</code> <p>URL for the model in Ultralytics HUB.</p> <code>api_url</code> <code>str</code> <p>API URL for the model in Ultralytics HUB.</p> <code>auth_header</code> <code>dict</code> <p>Authentication header for the Ultralytics HUB API requests.</p> <code>rate_limits</code> <code>dict</code> <p>Rate limits for different API calls (in seconds).</p> <code>timers</code> <code>dict</code> <p>Timers for rate limiting.</p> <code>metrics_queue</code> <code>dict</code> <p>Queue for the model's metrics.</p> <code>model</code> <code>dict</code> <p>Model data fetched from Ultralytics HUB.</p> <code>alive</code> <code>bool</code> <p>Indicates if the heartbeat loop is active.</p> Source code in <code>ultralytics/hub/session.py</code> <pre><code>class HUBTrainingSession:\n\"\"\"\n    HUB training session for Ultralytics HUB YOLO models. Handles model initialization, heartbeats, and checkpointing.\n    Args:\n        url (str): Model identifier used to initialize the HUB training session.\n    Attributes:\n        agent_id (str): Identifier for the instance communicating with the server.\n        model_id (str): Identifier for the YOLOv5 model being trained.\n        model_url (str): URL for the model in Ultralytics HUB.\n        api_url (str): API URL for the model in Ultralytics HUB.\n        auth_header (dict): Authentication header for the Ultralytics HUB API requests.\n        rate_limits (dict): Rate limits for different API calls (in seconds).\n        timers (dict): Timers for rate limiting.\n        metrics_queue (dict): Queue for the model's metrics.\n        model (dict): Model data fetched from Ultralytics HUB.\n        alive (bool): Indicates if the heartbeat loop is active.\n    \"\"\"\ndef __init__(self, url):\n\"\"\"\n        Initialize the HUBTrainingSession with the provided model identifier.\n        Args:\n            url (str): Model identifier used to initialize the HUB training session.\n                         It can be a URL string or a model key with specific format.\n        Raises:\n            ValueError: If the provided model identifier is invalid.\n            ConnectionError: If connecting with global API key is not supported.\n        \"\"\"\nfrom ultralytics.hub.auth import Auth\n# Parse input\nif url.startswith(f'{HUB_WEB_ROOT}/models/'):\nurl = url.split(f'{HUB_WEB_ROOT}/models/')[-1]\nif [len(x) for x in url.split('_')] == [42, 20]:\nkey, model_id = url.split('_')\nelif len(url) == 20:\nkey, model_id = '', url\nelse:\nraise HUBModelError(f\"model='{url}' not found. Check format is correct, i.e. \"\nf\"model='{HUB_WEB_ROOT}/models/MODEL_ID' and try again.\")\n# Authorize\nauth = Auth(key)\nself.agent_id = None  # identifies which instance is communicating with server\nself.model_id = model_id\nself.model_url = f'{HUB_WEB_ROOT}/models/{model_id}'\nself.api_url = f'{HUB_API_ROOT}/v1/models/{model_id}'\nself.auth_header = auth.get_auth_header()\nself.rate_limits = {'metrics': 3.0, 'ckpt': 900.0, 'heartbeat': 300.0}  # rate limits (seconds)\nself.timers = {}  # rate limit timers (seconds)\nself.metrics_queue = {}  # metrics queue\nself.model = self._get_model()\nself.alive = True\nself._start_heartbeat()  # start heartbeats\nself._register_signal_handlers()\nLOGGER.info(f'{PREFIX}View model at {self.model_url} \ud83d\ude80')\ndef _register_signal_handlers(self):\n\"\"\"Register signal handlers for SIGTERM and SIGINT signals to gracefully handle termination.\"\"\"\nsignal.signal(signal.SIGTERM, self._handle_signal)\nsignal.signal(signal.SIGINT, self._handle_signal)\ndef _handle_signal(self, signum, frame):\n\"\"\"\n        Handle kill signals and prevent heartbeats from being sent on Colab after termination.\n        This method does not use frame, it is included as it is passed by signal.\n        \"\"\"\nif self.alive is True:\nLOGGER.info(f'{PREFIX}Kill signal received! \u274c')\nself._stop_heartbeat()\nsys.exit(signum)\ndef _stop_heartbeat(self):\n\"\"\"Terminate the heartbeat loop.\"\"\"\nself.alive = False\ndef upload_metrics(self):\n\"\"\"Upload model metrics to Ultralytics HUB.\"\"\"\npayload = {'metrics': self.metrics_queue.copy(), 'type': 'metrics'}\nsmart_request('post', self.api_url, json=payload, headers=self.auth_header, code=2)\ndef _get_model(self):\n\"\"\"Fetch and return model data from Ultralytics HUB.\"\"\"\napi_url = f'{HUB_API_ROOT}/v1/models/{self.model_id}'\ntry:\nresponse = smart_request('get', api_url, headers=self.auth_header, thread=False, code=0)\ndata = response.json().get('data', None)\nif data.get('status', None) == 'trained':\nraise ValueError(emojis(f'Model is already trained and uploaded to {self.model_url} \ud83d\ude80'))\nif not data.get('data', None):\nraise ValueError('Dataset may still be processing. Please wait a minute and try again.')  # RF fix\nself.model_id = data['id']\nif data['status'] == 'new':  # new model to start training\nself.train_args = {\n# TODO: deprecate 'batch_size' key for 'batch' in 3Q23\n'batch': data['batch' if ('batch' in data) else 'batch_size'],\n'epochs': data['epochs'],\n'imgsz': data['imgsz'],\n'patience': data['patience'],\n'device': data['device'],\n'cache': data['cache'],\n'data': data['data']}\nself.model_file = data.get('cfg') or data.get('weights')  # cfg for pretrained=False\nself.model_file = checks.check_yolov5u_filename(self.model_file, verbose=False)  # YOLOv5-&gt;YOLOv5u\nelif data['status'] == 'training':  # existing model to resume training\nself.train_args = {'data': data['data'], 'resume': True}\nself.model_file = data['resume']\nreturn data\nexcept requests.exceptions.ConnectionError as e:\nraise ConnectionRefusedError('ERROR: The HUB server is not online. Please try again later.') from e\nexcept Exception:\nraise\ndef upload_model(self, epoch, weights, is_best=False, map=0.0, final=False):\n\"\"\"\n        Upload a model checkpoint to Ultralytics HUB.\n        Args:\n            epoch (int): The current training epoch.\n            weights (str): Path to the model weights file.\n            is_best (bool): Indicates if the current model is the best one so far.\n            map (float): Mean average precision of the model.\n            final (bool): Indicates if the model is the final model after training.\n        \"\"\"\nif Path(weights).is_file():\nwith open(weights, 'rb') as f:\nfile = f.read()\nelse:\nLOGGER.warning(f'{PREFIX}WARNING \u26a0\ufe0f Model upload issue. Missing model {weights}.')\nfile = None\nurl = f'{self.api_url}/upload'\n# url = 'http://httpbin.org/post'  # for debug\ndata = {'epoch': epoch}\nif final:\ndata.update({'type': 'final', 'map': map})\nsmart_request('post',\nurl,\ndata=data,\nfiles={'best.pt': file},\nheaders=self.auth_header,\nretry=10,\ntimeout=3600,\nthread=False,\nprogress=True,\ncode=4)\nelse:\ndata.update({'type': 'epoch', 'isBest': bool(is_best)})\nsmart_request('post', url, data=data, files={'last.pt': file}, headers=self.auth_header, code=3)\n@threaded\ndef _start_heartbeat(self):\n\"\"\"Begin a threaded heartbeat loop to report the agent's status to Ultralytics HUB.\"\"\"\nwhile self.alive:\nr = smart_request('post',\nf'{HUB_API_ROOT}/v1/agent/heartbeat/models/{self.model_id}',\njson={\n'agent': AGENT_NAME,\n'agentId': self.agent_id},\nheaders=self.auth_header,\nretry=0,\ncode=5,\nthread=False)  # already in a thread\nself.agent_id = r.json().get('data', {}).get('agentId', None)\nsleep(self.rate_limits['heartbeat'])\n</code></pre>"},{"location":"reference/hub/session/#ultralytics.hub.session.HUBTrainingSession.__init__","title":"<code>__init__(url)</code>","text":"<p>Initialize the HUBTrainingSession with the provided model identifier.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Model identifier used to initialize the HUB training session.          It can be a URL string or a model key with specific format.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided model identifier is invalid.</p> <code>ConnectionError</code> <p>If connecting with global API key is not supported.</p> Source code in <code>ultralytics/hub/session.py</code> <pre><code>def __init__(self, url):\n\"\"\"\n    Initialize the HUBTrainingSession with the provided model identifier.\n    Args:\n        url (str): Model identifier used to initialize the HUB training session.\n                     It can be a URL string or a model key with specific format.\n    Raises:\n        ValueError: If the provided model identifier is invalid.\n        ConnectionError: If connecting with global API key is not supported.\n    \"\"\"\nfrom ultralytics.hub.auth import Auth\n# Parse input\nif url.startswith(f'{HUB_WEB_ROOT}/models/'):\nurl = url.split(f'{HUB_WEB_ROOT}/models/')[-1]\nif [len(x) for x in url.split('_')] == [42, 20]:\nkey, model_id = url.split('_')\nelif len(url) == 20:\nkey, model_id = '', url\nelse:\nraise HUBModelError(f\"model='{url}' not found. Check format is correct, i.e. \"\nf\"model='{HUB_WEB_ROOT}/models/MODEL_ID' and try again.\")\n# Authorize\nauth = Auth(key)\nself.agent_id = None  # identifies which instance is communicating with server\nself.model_id = model_id\nself.model_url = f'{HUB_WEB_ROOT}/models/{model_id}'\nself.api_url = f'{HUB_API_ROOT}/v1/models/{model_id}'\nself.auth_header = auth.get_auth_header()\nself.rate_limits = {'metrics': 3.0, 'ckpt': 900.0, 'heartbeat': 300.0}  # rate limits (seconds)\nself.timers = {}  # rate limit timers (seconds)\nself.metrics_queue = {}  # metrics queue\nself.model = self._get_model()\nself.alive = True\nself._start_heartbeat()  # start heartbeats\nself._register_signal_handlers()\nLOGGER.info(f'{PREFIX}View model at {self.model_url} \ud83d\ude80')\n</code></pre>"},{"location":"reference/hub/session/#ultralytics.hub.session.HUBTrainingSession.upload_metrics","title":"<code>upload_metrics()</code>","text":"<p>Upload model metrics to Ultralytics HUB.</p> Source code in <code>ultralytics/hub/session.py</code> <pre><code>def upload_metrics(self):\n\"\"\"Upload model metrics to Ultralytics HUB.\"\"\"\npayload = {'metrics': self.metrics_queue.copy(), 'type': 'metrics'}\nsmart_request('post', self.api_url, json=payload, headers=self.auth_header, code=2)\n</code></pre>"},{"location":"reference/hub/session/#ultralytics.hub.session.HUBTrainingSession.upload_model","title":"<code>upload_model(epoch, weights, is_best=False, map=0.0, final=False)</code>","text":"<p>Upload a model checkpoint to Ultralytics HUB.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current training epoch.</p> required <code>weights</code> <code>str</code> <p>Path to the model weights file.</p> required <code>is_best</code> <code>bool</code> <p>Indicates if the current model is the best one so far.</p> <code>False</code> <code>map</code> <code>float</code> <p>Mean average precision of the model.</p> <code>0.0</code> <code>final</code> <code>bool</code> <p>Indicates if the model is the final model after training.</p> <code>False</code> Source code in <code>ultralytics/hub/session.py</code> <pre><code>def upload_model(self, epoch, weights, is_best=False, map=0.0, final=False):\n\"\"\"\n    Upload a model checkpoint to Ultralytics HUB.\n    Args:\n        epoch (int): The current training epoch.\n        weights (str): Path to the model weights file.\n        is_best (bool): Indicates if the current model is the best one so far.\n        map (float): Mean average precision of the model.\n        final (bool): Indicates if the model is the final model after training.\n    \"\"\"\nif Path(weights).is_file():\nwith open(weights, 'rb') as f:\nfile = f.read()\nelse:\nLOGGER.warning(f'{PREFIX}WARNING \u26a0\ufe0f Model upload issue. Missing model {weights}.')\nfile = None\nurl = f'{self.api_url}/upload'\n# url = 'http://httpbin.org/post'  # for debug\ndata = {'epoch': epoch}\nif final:\ndata.update({'type': 'final', 'map': map})\nsmart_request('post',\nurl,\ndata=data,\nfiles={'best.pt': file},\nheaders=self.auth_header,\nretry=10,\ntimeout=3600,\nthread=False,\nprogress=True,\ncode=4)\nelse:\ndata.update({'type': 'epoch', 'isBest': bool(is_best)})\nsmart_request('post', url, data=data, files={'last.pt': file}, headers=self.auth_header, code=3)\n</code></pre>"},{"location":"reference/hub/utils/","title":"Reference for <code>ultralytics/hub/utils.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/hub/utils.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.Events","title":"<code>ultralytics.hub.utils.Events</code>","text":"<p>A class for collecting anonymous event analytics. Event analytics are enabled when sync=True in settings and disabled when sync=False. Run 'yolo settings' to see and update settings YAML file.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL to send anonymous events.</p> <code>rate_limit</code> <code>float</code> <p>The rate limit in seconds for sending events.</p> <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata about the environment.</p> <code>enabled</code> <code>bool</code> <p>A flag to enable or disable Events based on certain conditions.</p> Source code in <code>ultralytics/hub/utils.py</code> <pre><code>class Events:\n\"\"\"\n    A class for collecting anonymous event analytics. Event analytics are enabled when sync=True in settings and\n    disabled when sync=False. Run 'yolo settings' to see and update settings YAML file.\n    Attributes:\n        url (str): The URL to send anonymous events.\n        rate_limit (float): The rate limit in seconds for sending events.\n        metadata (dict): A dictionary containing metadata about the environment.\n        enabled (bool): A flag to enable or disable Events based on certain conditions.\n    \"\"\"\nurl = 'https://www.google-analytics.com/mp/collect?measurement_id=G-X8NCJYTQXM&amp;api_secret=QLQrATrNSwGRFRLE-cbHJw'\ndef __init__(self):\n\"\"\"\n        Initializes the Events object with default values for events, rate_limit, and metadata.\n        \"\"\"\nself.events = []  # events list\nself.rate_limit = 60.0  # rate limit (seconds)\nself.t = 0.0  # rate limit timer (seconds)\nself.metadata = {\n'cli': Path(sys.argv[0]).name == 'yolo',\n'install': 'git' if is_git_dir() else 'pip' if is_pip_package() else 'other',\n'python': '.'.join(platform.python_version_tuple()[:2]),  # i.e. 3.10\n'version': __version__,\n'env': ENVIRONMENT,\n'session_id': round(random.random() * 1E15),\n'engagement_time_msec': 1000}\nself.enabled = \\\n            SETTINGS['sync'] and \\\n            RANK in (-1, 0) and \\\n            not TESTS_RUNNING and \\\n            ONLINE and \\\n            (is_pip_package() or get_git_origin_url() == 'https://github.com/ultralytics/ultralytics.git')\ndef __call__(self, cfg):\n\"\"\"\n        Attempts to add a new event to the events list and send events if the rate limit is reached.\n        Args:\n            cfg (IterableSimpleNamespace): The configuration object containing mode and task information.\n        \"\"\"\nif not self.enabled:\n# Events disabled, do nothing\nreturn\n# Attempt to add to events\nif len(self.events) &lt; 25:  # Events list limited to 25 events (drop any events past this)\nparams = {\n**self.metadata, 'task': cfg.task,\n'model': cfg.model if cfg.model in GITHUB_ASSETS_NAMES else 'custom'}\nif cfg.mode == 'export':\nparams['format'] = cfg.format\nself.events.append({'name': cfg.mode, 'params': params})\n# Check rate limit\nt = time.time()\nif (t - self.t) &lt; self.rate_limit:\n# Time is under rate limiter, wait to send\nreturn\n# Time is over rate limiter, send now\ndata = {'client_id': SETTINGS['uuid'], 'events': self.events}  # SHA-256 anonymized UUID hash and events list\n# POST equivalent to requests.post(self.url, json=data)\nsmart_request('post', self.url, json=data, retry=0, verbose=False)\n# Reset events and rate limit timer\nself.events = []\nself.t = t\n</code></pre>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.Events.__call__","title":"<code>__call__(cfg)</code>","text":"<p>Attempts to add a new event to the events list and send events if the rate limit is reached.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>IterableSimpleNamespace</code> <p>The configuration object containing mode and task information.</p> required Source code in <code>ultralytics/hub/utils.py</code> <pre><code>def __call__(self, cfg):\n\"\"\"\n    Attempts to add a new event to the events list and send events if the rate limit is reached.\n    Args:\n        cfg (IterableSimpleNamespace): The configuration object containing mode and task information.\n    \"\"\"\nif not self.enabled:\n# Events disabled, do nothing\nreturn\n# Attempt to add to events\nif len(self.events) &lt; 25:  # Events list limited to 25 events (drop any events past this)\nparams = {\n**self.metadata, 'task': cfg.task,\n'model': cfg.model if cfg.model in GITHUB_ASSETS_NAMES else 'custom'}\nif cfg.mode == 'export':\nparams['format'] = cfg.format\nself.events.append({'name': cfg.mode, 'params': params})\n# Check rate limit\nt = time.time()\nif (t - self.t) &lt; self.rate_limit:\n# Time is under rate limiter, wait to send\nreturn\n# Time is over rate limiter, send now\ndata = {'client_id': SETTINGS['uuid'], 'events': self.events}  # SHA-256 anonymized UUID hash and events list\n# POST equivalent to requests.post(self.url, json=data)\nsmart_request('post', self.url, json=data, retry=0, verbose=False)\n# Reset events and rate limit timer\nself.events = []\nself.t = t\n</code></pre>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.Events.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Events object with default values for events, rate_limit, and metadata.</p> Source code in <code>ultralytics/hub/utils.py</code> <pre><code>def __init__(self):\n\"\"\"\n    Initializes the Events object with default values for events, rate_limit, and metadata.\n    \"\"\"\nself.events = []  # events list\nself.rate_limit = 60.0  # rate limit (seconds)\nself.t = 0.0  # rate limit timer (seconds)\nself.metadata = {\n'cli': Path(sys.argv[0]).name == 'yolo',\n'install': 'git' if is_git_dir() else 'pip' if is_pip_package() else 'other',\n'python': '.'.join(platform.python_version_tuple()[:2]),  # i.e. 3.10\n'version': __version__,\n'env': ENVIRONMENT,\n'session_id': round(random.random() * 1E15),\n'engagement_time_msec': 1000}\nself.enabled = \\\n        SETTINGS['sync'] and \\\n        RANK in (-1, 0) and \\\n        not TESTS_RUNNING and \\\n        ONLINE and \\\n        (is_pip_package() or get_git_origin_url() == 'https://github.com/ultralytics/ultralytics.git')\n</code></pre>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.request_with_credentials","title":"<code>ultralytics.hub.utils.request_with_credentials(url)</code>","text":"<p>Make an AJAX request with cookies attached in a Google Colab environment.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to make the request to.</p> required <p>Returns:</p> Type Description <code>any</code> <p>The response data from the AJAX request.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the function is not run in a Google Colab environment.</p> Source code in <code>ultralytics/hub/utils.py</code> <pre><code>def request_with_credentials(url: str) -&gt; any:\n\"\"\"\n    Make an AJAX request with cookies attached in a Google Colab environment.\n    Args:\n        url (str): The URL to make the request to.\n    Returns:\n        (any): The response data from the AJAX request.\n    Raises:\n        OSError: If the function is not run in a Google Colab environment.\n    \"\"\"\nif not is_colab():\nraise OSError('request_with_credentials() must run in a Colab environment')\nfrom google.colab import output  # noqa\nfrom IPython import display  # noqa\ndisplay.display(\ndisplay.Javascript(\"\"\"\n            window._hub_tmp = new Promise((resolve, reject) =&gt; {\n                const timeout = setTimeout(() =&gt; reject(\"Failed authenticating existing browser session\"), 5000)\n                fetch(\"%s\", {\n                    method: 'POST',\n                    credentials: 'include'\n                })\n                    .then((response) =&gt; resolve(response.json()))\n                    .then((json) =&gt; {\n                    clearTimeout(timeout);\n                    }).catch((err) =&gt; {\n                    clearTimeout(timeout);\n                    reject(err);\n                });\n            });\n            \"\"\" % url))\nreturn output.eval_js('_hub_tmp')\n</code></pre>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.requests_with_progress","title":"<code>ultralytics.hub.utils.requests_with_progress(method, url, **kwargs)</code>","text":"<p>Make an HTTP request using the specified method and URL, with an optional progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The HTTP method to use (e.g. 'GET', 'POST').</p> required <code>url</code> <code>str</code> <p>The URL to send the request to.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the underlying <code>requests.request</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>The response object from the HTTP request.</p> Note <p>If 'progress' is set to True, the progress bar will display the download progress for responses with a known content length.</p> Source code in <code>ultralytics/hub/utils.py</code> <pre><code>def requests_with_progress(method, url, **kwargs):\n\"\"\"\n    Make an HTTP request using the specified method and URL, with an optional progress bar.\n    Args:\n        method (str): The HTTP method to use (e.g. 'GET', 'POST').\n        url (str): The URL to send the request to.\n        **kwargs (dict): Additional keyword arguments to pass to the underlying `requests.request` function.\n    Returns:\n        (requests.Response): The response object from the HTTP request.\n    Note:\n        If 'progress' is set to True, the progress bar will display the download progress\n        for responses with a known content length.\n    \"\"\"\nprogress = kwargs.pop('progress', False)\nif not progress:\nreturn requests.request(method, url, **kwargs)\nresponse = requests.request(method, url, stream=True, **kwargs)\ntotal = int(response.headers.get('content-length', 0))  # total size\ntry:\npbar = tqdm(total=total, unit='B', unit_scale=True, unit_divisor=1024, bar_format=TQDM_BAR_FORMAT)\nfor data in response.iter_content(chunk_size=1024):\npbar.update(len(data))\npbar.close()\nexcept requests.exceptions.ChunkedEncodingError:  # avoid 'Connection broken: IncompleteRead' warnings\nresponse.close()\nreturn response\n</code></pre>"},{"location":"reference/hub/utils/#ultralytics.hub.utils.smart_request","title":"<code>ultralytics.hub.utils.smart_request(method, url, retry=3, timeout=30, thread=True, code=-1, verbose=True, progress=False, **kwargs)</code>","text":"<p>Makes an HTTP request using the 'requests' library, with exponential backoff retries up to a specified timeout.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The HTTP method to use for the request. Choices are 'post' and 'get'.</p> required <code>url</code> <code>str</code> <p>The URL to make the request to.</p> required <code>retry</code> <code>int</code> <p>Number of retries to attempt before giving up. Default is 3.</p> <code>3</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds after which the function will give up retrying. Default is 30.</p> <code>30</code> <code>thread</code> <code>bool</code> <p>Whether to execute the request in a separate daemon thread. Default is True.</p> <code>True</code> <code>code</code> <code>int</code> <p>An identifier for the request, used for logging purposes. Default is -1.</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>A flag to determine whether to print out to console or not. Default is True.</p> <code>True</code> <code>progress</code> <code>bool</code> <p>Whether to show a progress bar during the request. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the requests function specified in method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>The HTTP response object. If the request is executed in a separate thread, returns None.</p> Source code in <code>ultralytics/hub/utils.py</code> <pre><code>def smart_request(method, url, retry=3, timeout=30, thread=True, code=-1, verbose=True, progress=False, **kwargs):\n\"\"\"\n    Makes an HTTP request using the 'requests' library, with exponential backoff retries up to a specified timeout.\n    Args:\n        method (str): The HTTP method to use for the request. Choices are 'post' and 'get'.\n        url (str): The URL to make the request to.\n        retry (int, optional): Number of retries to attempt before giving up. Default is 3.\n        timeout (int, optional): Timeout in seconds after which the function will give up retrying. Default is 30.\n        thread (bool, optional): Whether to execute the request in a separate daemon thread. Default is True.\n        code (int, optional): An identifier for the request, used for logging purposes. Default is -1.\n        verbose (bool, optional): A flag to determine whether to print out to console or not. Default is True.\n        progress (bool, optional): Whether to show a progress bar during the request. Default is False.\n        **kwargs (dict): Keyword arguments to be passed to the requests function specified in method.\n    Returns:\n        (requests.Response): The HTTP response object. If the request is executed in a separate thread, returns None.\n    \"\"\"\nretry_codes = (408, 500)  # retry only these codes\n@TryExcept(verbose=verbose)\ndef func(func_method, func_url, **func_kwargs):\n\"\"\"Make HTTP requests with retries and timeouts, with optional progress tracking.\"\"\"\nr = None  # response\nt0 = time.time()  # initial time for timer\nfor i in range(retry + 1):\nif (time.time() - t0) &gt; timeout:\nbreak\nr = requests_with_progress(func_method, func_url, **func_kwargs)  # i.e. get(url, data, json, files)\nif r.status_code &lt; 300:  # return codes in the 2xx range are generally considered \"good\" or \"successful\"\nbreak\ntry:\nm = r.json().get('message', 'No JSON message.')\nexcept AttributeError:\nm = 'Unable to read JSON.'\nif i == 0:\nif r.status_code in retry_codes:\nm += f' Retrying {retry}x for {timeout}s.' if retry else ''\nelif r.status_code == 429:  # rate limit\nh = r.headers  # response headers\nm = f\"Rate limit reached ({h['X-RateLimit-Remaining']}/{h['X-RateLimit-Limit']}). \" \\\n                        f\"Please retry after {h['Retry-After']}s.\"\nif verbose:\nLOGGER.warning(f'{PREFIX}{m} {HELP_MSG} ({r.status_code} #{code})')\nif r.status_code not in retry_codes:\nreturn r\ntime.sleep(2 ** i)  # exponential standoff\nreturn r\nargs = method, url\nkwargs['progress'] = progress\nif thread:\nthreading.Thread(target=func, args=args, kwargs=kwargs, daemon=True).start()\nelse:\nreturn func(*args, **kwargs)\n</code></pre>"},{"location":"reference/models/fastsam/model/","title":"Reference for <code>ultralytics/models/fastsam/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/fastsam/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/fastsam/model/#ultralytics.models.fastsam.model.FastSAM","title":"<code>ultralytics.models.fastsam.model.FastSAM</code>","text":"<p>             Bases: <code>Model</code></p> <p>FastSAM model interface.</p> Example <pre><code>from ultralytics import FastSAM\nmodel = FastSAM('last.pt')\nresults = model.predict('ultralytics/assets/bus.jpg')\n</code></pre> Source code in <code>ultralytics/models/fastsam/model.py</code> <pre><code>class FastSAM(Model):\n\"\"\"\n    FastSAM model interface.\n    Example:\n        ```python\n        from ultralytics import FastSAM\n        model = FastSAM('last.pt')\n        results = model.predict('ultralytics/assets/bus.jpg')\n        ```\n    \"\"\"\ndef __init__(self, model='FastSAM-x.pt'):\n\"\"\"Call the __init__ method of the parent class (YOLO) with the updated default model\"\"\"\nif str(model) == 'FastSAM.pt':\nmodel = 'FastSAM-x.pt'\nassert Path(model).suffix not in ('.yaml', '.yml'), 'FastSAM models only support pre-trained models.'\nsuper().__init__(model=model, task='segment')\n@property\ndef task_map(self):\nreturn {'segment': {'predictor': FastSAMPredictor, 'validator': FastSAMValidator}}\n</code></pre>"},{"location":"reference/models/fastsam/model/#ultralytics.models.fastsam.model.FastSAM.__init__","title":"<code>__init__(model='FastSAM-x.pt')</code>","text":"<p>Call the init method of the parent class (YOLO) with the updated default model</p> Source code in <code>ultralytics/models/fastsam/model.py</code> <pre><code>def __init__(self, model='FastSAM-x.pt'):\n\"\"\"Call the __init__ method of the parent class (YOLO) with the updated default model\"\"\"\nif str(model) == 'FastSAM.pt':\nmodel = 'FastSAM-x.pt'\nassert Path(model).suffix not in ('.yaml', '.yml'), 'FastSAM models only support pre-trained models.'\nsuper().__init__(model=model, task='segment')\n</code></pre>"},{"location":"reference/models/fastsam/predict/","title":"Reference for <code>ultralytics/models/fastsam/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/fastsam/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/fastsam/predict/#ultralytics.models.fastsam.predict.FastSAMPredictor","title":"<code>ultralytics.models.fastsam.predict.FastSAMPredictor</code>","text":"<p>             Bases: <code>DetectionPredictor</code></p> Source code in <code>ultralytics/models/fastsam/predict.py</code> <pre><code>class FastSAMPredictor(DetectionPredictor):\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\nsuper().__init__(cfg, overrides, _callbacks)\nself.args.task = 'segment'\ndef postprocess(self, preds, img, orig_imgs):\np = ops.non_max_suppression(preds[0],\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nnc=len(self.model.names),\nclasses=self.args.classes)\nfull_box = torch.zeros_like(p[0][0])\nfull_box[2], full_box[3], full_box[4], full_box[6:] = img.shape[3], img.shape[2], 1.0, 1.0\nfull_box = full_box.view(1, -1)\ncritical_iou_index = bbox_iou(full_box[0][:4], p[0][:, :4], iou_thres=0.9, image_shape=img.shape[2:])\nif critical_iou_index.numel() != 0:\nfull_box[0][4] = p[0][critical_iou_index][:, 4]\nfull_box[0][6:] = p[0][critical_iou_index][:, 6:]\np[0][critical_iou_index] = full_box\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nproto = preds[1][-1] if len(preds[1]) == 3 else preds[1]  # second output is len 3 if pt, but only 1 if exported\nfor i, pred in enumerate(p):\norig_img = orig_imgs[i] if is_list else orig_imgs\nimg_path = self.batch[0][i]\nif not len(pred):  # save empty boxes\nmasks = None\nelif self.args.retina_masks:\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nmasks = ops.process_mask_native(proto[i], pred[:, 6:], pred[:, :4], orig_img.shape[:2])  # HWC\nelse:\nmasks = ops.process_mask(proto[i], pred[:, 6:], pred[:, :4], img.shape[2:], upsample=True)  # HWC\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6], masks=masks))\nreturn results\n</code></pre>"},{"location":"reference/models/fastsam/prompt/","title":"Reference for <code>ultralytics/models/fastsam/prompt.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/fastsam/prompt.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/fastsam/prompt/#ultralytics.models.fastsam.prompt.FastSAMPrompt","title":"<code>ultralytics.models.fastsam.prompt.FastSAMPrompt</code>","text":"Source code in <code>ultralytics/models/fastsam/prompt.py</code> <pre><code>class FastSAMPrompt:\ndef __init__(self, img_path, results, device='cuda') -&gt; None:\n# self.img_path = img_path\nself.device = device\nself.results = results\nself.img_path = str(img_path)\nself.ori_img = cv2.imread(self.img_path)\n# Import and assign clip\ntry:\nimport clip  # for linear_assignment\nexcept ImportError:\nfrom ultralytics.utils.checks import check_requirements\ncheck_requirements('git+https://github.com/openai/CLIP.git')\nimport clip\nself.clip = clip\n@staticmethod\ndef _segment_image(image, bbox):\nimage_array = np.array(image)\nsegmented_image_array = np.zeros_like(image_array)\nx1, y1, x2, y2 = bbox\nsegmented_image_array[y1:y2, x1:x2] = image_array[y1:y2, x1:x2]\nsegmented_image = Image.fromarray(segmented_image_array)\nblack_image = Image.new('RGB', image.size, (255, 255, 255))\n# transparency_mask = np.zeros_like((), dtype=np.uint8)\ntransparency_mask = np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)\ntransparency_mask[y1:y2, x1:x2] = 255\ntransparency_mask_image = Image.fromarray(transparency_mask, mode='L')\nblack_image.paste(segmented_image, mask=transparency_mask_image)\nreturn black_image\n@staticmethod\ndef _format_results(result, filter=0):\nannotations = []\nn = len(result.masks.data)\nfor i in range(n):\nmask = result.masks.data[i] == 1.0\nif torch.sum(mask) &lt; filter:\ncontinue\nannotation = {\n'id': i,\n'segmentation': mask.cpu().numpy(),\n'bbox': result.boxes.data[i],\n'score': result.boxes.conf[i]}\nannotation['area'] = annotation['segmentation'].sum()\nannotations.append(annotation)\nreturn annotations\n@staticmethod\ndef filter_masks(annotations):  # filter the overlap mask\nannotations.sort(key=lambda x: x['area'], reverse=True)\nto_remove = set()\nfor i in range(len(annotations)):\na = annotations[i]\nfor j in range(i + 1, len(annotations)):\nb = annotations[j]\nif i != j and j not in to_remove and b['area'] &lt; a['area'] and \\\n                        (a['segmentation'] &amp; b['segmentation']).sum() / b['segmentation'].sum() &gt; 0.8:\nto_remove.add(j)\nreturn [a for i, a in enumerate(annotations) if i not in to_remove], to_remove\n@staticmethod\ndef _get_bbox_from_mask(mask):\nmask = mask.astype(np.uint8)\ncontours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nx1, y1, w, h = cv2.boundingRect(contours[0])\nx2, y2 = x1 + w, y1 + h\nif len(contours) &gt; 1:\nfor b in contours:\nx_t, y_t, w_t, h_t = cv2.boundingRect(b)\n# \u5c06\u591a\u4e2abbox\u5408\u5e76\u6210\u4e00\u4e2a\nx1 = min(x1, x_t)\ny1 = min(y1, y_t)\nx2 = max(x2, x_t + w_t)\ny2 = max(y2, y_t + h_t)\nreturn [x1, y1, x2, y2]\ndef plot(self,\nannotations,\noutput,\nbbox=None,\npoints=None,\npoint_label=None,\nmask_random_color=True,\nbetter_quality=True,\nretina=False,\nwith_countouers=True):\nif isinstance(annotations[0], dict):\nannotations = [annotation['segmentation'] for annotation in annotations]\nif isinstance(annotations, torch.Tensor):\nannotations = annotations.cpu().numpy()\nresult_name = os.path.basename(self.img_path)\nimage = self.ori_img\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\noriginal_h = image.shape[0]\noriginal_w = image.shape[1]\n# for macOS only\n# plt.switch_backend('TkAgg')\nfig = plt.figure(figsize=(original_w / 100, original_h / 100))\n# Add subplot with no margin.\nplt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\nplt.margins(0, 0)\nplt.gca().xaxis.set_major_locator(plt.NullLocator())\nplt.gca().yaxis.set_major_locator(plt.NullLocator())\nplt.imshow(image)\nif better_quality:\nfor i, mask in enumerate(annotations):\nmask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))\nannotations[i] = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8))\nself.fast_show_mask(\nannotations,\nplt.gca(),\nrandom_color=mask_random_color,\nbbox=bbox,\npoints=points,\npointlabel=point_label,\nretinamask=retina,\ntarget_height=original_h,\ntarget_width=original_w,\n)\nif with_countouers:\ncontour_all = []\ntemp = np.zeros((original_h, original_w, 1))\nfor i, mask in enumerate(annotations):\nif isinstance(mask, dict):\nmask = mask['segmentation']\nannotation = mask.astype(np.uint8)\nif not retina:\nannotation = cv2.resize(\nannotation,\n(original_w, original_h),\ninterpolation=cv2.INTER_NEAREST,\n)\ncontours, hierarchy = cv2.findContours(annotation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncontour_all.extend(iter(contours))\ncv2.drawContours(temp, contour_all, -1, (255, 255, 255), 2)\ncolor = np.array([0 / 255, 0 / 255, 1.0, 0.8])\ncontour_mask = temp / 255 * color.reshape(1, 1, -1)\nplt.imshow(contour_mask)\nsave_path = Path(output) / result_name\nsave_path.parent.mkdir(exist_ok=True, parents=True)\nplt.axis('off')\nfig.savefig(save_path)\nLOGGER.info(f'Saved to {save_path.absolute()}')\n#   CPU post process\n@staticmethod\ndef fast_show_mask(\nannotation,\nax,\nrandom_color=False,\nbbox=None,\npoints=None,\npointlabel=None,\nretinamask=True,\ntarget_height=960,\ntarget_width=960,\n):\nn, h, w = annotation.shape  # batch, height, width\nareas = np.sum(annotation, axis=(1, 2))\nannotation = annotation[np.argsort(areas)]\nindex = (annotation != 0).argmax(axis=0)\nif random_color:\ncolor = np.random.random((n, 1, 1, 3))\nelse:\ncolor = np.ones((n, 1, 1, 3)) * np.array([30 / 255, 144 / 255, 1.0])\ntransparency = np.ones((n, 1, 1, 1)) * 0.6\nvisual = np.concatenate([color, transparency], axis=-1)\nmask_image = np.expand_dims(annotation, -1) * visual\nshow = np.zeros((h, w, 4))\nh_indices, w_indices = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\nindices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\nshow[h_indices, w_indices, :] = mask_image[indices]\nif bbox is not None:\nx1, y1, x2, y2 = bbox\nax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='b', linewidth=1))\n# Draw point\nif points is not None:\nplt.scatter(\n[point[0] for i, point in enumerate(points) if pointlabel[i] == 1],\n[point[1] for i, point in enumerate(points) if pointlabel[i] == 1],\ns=20,\nc='y',\n)\nplt.scatter(\n[point[0] for i, point in enumerate(points) if pointlabel[i] == 0],\n[point[1] for i, point in enumerate(points) if pointlabel[i] == 0],\ns=20,\nc='m',\n)\nif not retinamask:\nshow = cv2.resize(show, (target_width, target_height), interpolation=cv2.INTER_NEAREST)\nax.imshow(show)\n@torch.no_grad()\ndef retrieve(self, model, preprocess, elements, search_text: str, device) -&gt; int:\npreprocessed_images = [preprocess(image).to(device) for image in elements]\ntokenized_text = self.clip.tokenize([search_text]).to(device)\nstacked_images = torch.stack(preprocessed_images)\nimage_features = model.encode_image(stacked_images)\ntext_features = model.encode_text(tokenized_text)\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nprobs = 100.0 * image_features @ text_features.T\nreturn probs[:, 0].softmax(dim=0)\ndef _crop_image(self, format_results):\nimage = Image.fromarray(cv2.cvtColor(self.ori_img, cv2.COLOR_BGR2RGB))\nori_w, ori_h = image.size\nannotations = format_results\nmask_h, mask_w = annotations[0]['segmentation'].shape\nif ori_w != mask_w or ori_h != mask_h:\nimage = image.resize((mask_w, mask_h))\ncropped_boxes = []\ncropped_images = []\nnot_crop = []\nfilter_id = []\n# annotations, _ = filter_masks(annotations)\n# filter_id = list(_)\nfor _, mask in enumerate(annotations):\nif np.sum(mask['segmentation']) &lt;= 100:\nfilter_id.append(_)\ncontinue\nbbox = self._get_bbox_from_mask(mask['segmentation'])  # mask \u7684 bbox\ncropped_boxes.append(self._segment_image(image, bbox))  # \u4fdd\u5b58\u88c1\u526a\u7684\u56fe\u7247\n# cropped_boxes.append(segment_image(image,mask[\"segmentation\"]))\ncropped_images.append(bbox)  # \u4fdd\u5b58\u88c1\u526a\u7684\u56fe\u7247\u7684bbox\nreturn cropped_boxes, cropped_images, not_crop, filter_id, annotations\ndef box_prompt(self, bbox):\nassert (bbox[2] != 0 and bbox[3] != 0)\nmasks = self.results[0].masks.data\ntarget_height = self.ori_img.shape[0]\ntarget_width = self.ori_img.shape[1]\nh = masks.shape[1]\nw = masks.shape[2]\nif h != target_height or w != target_width:\nbbox = [\nint(bbox[0] * w / target_width),\nint(bbox[1] * h / target_height),\nint(bbox[2] * w / target_width),\nint(bbox[3] * h / target_height), ]\nbbox[0] = max(round(bbox[0]), 0)\nbbox[1] = max(round(bbox[1]), 0)\nbbox[2] = min(round(bbox[2]), w)\nbbox[3] = min(round(bbox[3]), h)\n# IoUs = torch.zeros(len(masks), dtype=torch.float32)\nbbox_area = (bbox[3] - bbox[1]) * (bbox[2] - bbox[0])\nmasks_area = torch.sum(masks[:, bbox[1]:bbox[3], bbox[0]:bbox[2]], dim=(1, 2))\norig_masks_area = torch.sum(masks, dim=(1, 2))\nunion = bbox_area + orig_masks_area - masks_area\nIoUs = masks_area / union\nmax_iou_index = torch.argmax(IoUs)\nreturn np.array([masks[max_iou_index].cpu().numpy()])\ndef point_prompt(self, points, pointlabel):  # numpy \u5904\u7406\nmasks = self._format_results(self.results[0], 0)\ntarget_height = self.ori_img.shape[0]\ntarget_width = self.ori_img.shape[1]\nh = masks[0]['segmentation'].shape[0]\nw = masks[0]['segmentation'].shape[1]\nif h != target_height or w != target_width:\npoints = [[int(point[0] * w / target_width), int(point[1] * h / target_height)] for point in points]\nonemask = np.zeros((h, w))\nfor i, annotation in enumerate(masks):\nmask = annotation['segmentation'] if isinstance(annotation, dict) else annotation\nfor i, point in enumerate(points):\nif mask[point[1], point[0]] == 1 and pointlabel[i] == 1:\nonemask += mask\nif mask[point[1], point[0]] == 1 and pointlabel[i] == 0:\nonemask -= mask\nonemask = onemask &gt;= 1\nreturn np.array([onemask])\ndef text_prompt(self, text):\nformat_results = self._format_results(self.results[0], 0)\ncropped_boxes, cropped_images, not_crop, filter_id, annotations = self._crop_image(format_results)\nclip_model, preprocess = self.clip.load('ViT-B/32', device=self.device)\nscores = self.retrieve(clip_model, preprocess, cropped_boxes, text, device=self.device)\nmax_idx = scores.argsort()\nmax_idx = max_idx[-1]\nmax_idx += sum(np.array(filter_id) &lt;= int(max_idx))\nreturn np.array([annotations[max_idx]['segmentation']])\ndef everything_prompt(self):\nreturn self.results[0].masks.data\n</code></pre>"},{"location":"reference/models/fastsam/prompt/#ultralytics.models.fastsam.prompt.FastSAMPrompt.__init__","title":"<code>__init__(img_path, results, device='cuda')</code>","text":"Source code in <code>ultralytics/models/fastsam/prompt.py</code> <pre><code>def __init__(self, img_path, results, device='cuda') -&gt; None:\n# self.img_path = img_path\nself.device = device\nself.results = results\nself.img_path = str(img_path)\nself.ori_img = cv2.imread(self.img_path)\n# Import and assign clip\ntry:\nimport clip  # for linear_assignment\nexcept ImportError:\nfrom ultralytics.utils.checks import check_requirements\ncheck_requirements('git+https://github.com/openai/CLIP.git')\nimport clip\nself.clip = clip\n</code></pre>"},{"location":"reference/models/fastsam/utils/","title":"Reference for <code>ultralytics/models/fastsam/utils.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/fastsam/utils.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/models/fastsam/utils/#ultralytics.models.fastsam.utils.adjust_bboxes_to_image_border","title":"<code>ultralytics.models.fastsam.utils.adjust_bboxes_to_image_border(boxes, image_shape, threshold=20)</code>","text":"<p>Adjust bounding boxes to stick to image border if they are within a certain threshold.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>Tensor</code> <p>(n, 4)</p> required <code>image_shape</code> <code>tuple</code> <p>(height, width)</p> required <code>threshold</code> <code>int</code> <p>pixel threshold</p> <code>20</code> <p>Returns:</p> Name Type Description <code>adjusted_boxes</code> <code>Tensor</code> <p>adjusted bounding boxes</p> Source code in <code>ultralytics/models/fastsam/utils.py</code> <pre><code>def adjust_bboxes_to_image_border(boxes, image_shape, threshold=20):\n\"\"\"\n    Adjust bounding boxes to stick to image border if they are within a certain threshold.\n    Args:\n        boxes (torch.Tensor): (n, 4)\n        image_shape (tuple): (height, width)\n        threshold (int): pixel threshold\n    Returns:\n        adjusted_boxes (torch.Tensor): adjusted bounding boxes\n    \"\"\"\n# Image dimensions\nh, w = image_shape\n# Adjust boxes\nboxes[boxes[:, 0] &lt; threshold, 0] = 0  # x1\nboxes[boxes[:, 1] &lt; threshold, 1] = 0  # y1\nboxes[boxes[:, 2] &gt; w - threshold, 2] = w  # x2\nboxes[boxes[:, 3] &gt; h - threshold, 3] = h  # y2\nreturn boxes\n</code></pre>"},{"location":"reference/models/fastsam/utils/#ultralytics.models.fastsam.utils.bbox_iou","title":"<code>ultralytics.models.fastsam.utils.bbox_iou(box1, boxes, iou_thres=0.9, image_shape=(640, 640), raw_output=False)</code>","text":"<p>Compute the Intersection-Over-Union of a bounding box with respect to an array of other bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>box1</code> <code>Tensor</code> <p>(4, )</p> required <code>boxes</code> <code>Tensor</code> <p>(n, 4)</p> required <code>iou_thres</code> <code>float</code> <p>IoU threshold</p> <code>0.9</code> <code>image_shape</code> <code>tuple</code> <p>(height, width)</p> <code>(640, 640)</code> <code>raw_output</code> <code>bool</code> <p>If True, return the raw IoU values instead of the indices</p> <code>False</code> <p>Returns:</p> Name Type Description <code>high_iou_indices</code> <code>Tensor</code> <p>Indices of boxes with IoU &gt; thres</p> Source code in <code>ultralytics/models/fastsam/utils.py</code> <pre><code>def bbox_iou(box1, boxes, iou_thres=0.9, image_shape=(640, 640), raw_output=False):\n\"\"\"\n    Compute the Intersection-Over-Union of a bounding box with respect to an array of other bounding boxes.\n    Args:\n        box1 (torch.Tensor): (4, )\n        boxes (torch.Tensor): (n, 4)\n        iou_thres (float): IoU threshold\n        image_shape (tuple): (height, width)\n        raw_output (bool): If True, return the raw IoU values instead of the indices\n    Returns:\n        high_iou_indices (torch.Tensor): Indices of boxes with IoU &gt; thres\n    \"\"\"\nboxes = adjust_bboxes_to_image_border(boxes, image_shape)\n# obtain coordinates for intersections\nx1 = torch.max(box1[0], boxes[:, 0])\ny1 = torch.max(box1[1], boxes[:, 1])\nx2 = torch.min(box1[2], boxes[:, 2])\ny2 = torch.min(box1[3], boxes[:, 3])\n# compute the area of intersection\nintersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n# compute the area of both individual boxes\nbox1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\nbox2_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n# compute the area of union\nunion = box1_area + box2_area - intersection\n# compute the IoU\niou = intersection / union  # Should be shape (n, )\nif raw_output:\nreturn 0 if iou.numel() == 0 else iou\n# return indices of boxes with IoU &gt; thres\nreturn torch.nonzero(iou &gt; iou_thres).flatten()\n</code></pre>"},{"location":"reference/models/fastsam/val/","title":"Reference for <code>ultralytics/models/fastsam/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/fastsam/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/fastsam/val/#ultralytics.models.fastsam.val.FastSAMValidator","title":"<code>ultralytics.models.fastsam.val.FastSAMValidator</code>","text":"<p>             Bases: <code>SegmentationValidator</code></p> Source code in <code>ultralytics/models/fastsam/val.py</code> <pre><code>class FastSAMValidator(SegmentationValidator):\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.args.task = 'segment'\nself.args.plots = False  # disable ConfusionMatrix and other plots to avoid errors\nself.metrics = SegmentMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/fastsam/val/#ultralytics.models.fastsam.val.FastSAMValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.</p> Source code in <code>ultralytics/models/fastsam/val.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.args.task = 'segment'\nself.args.plots = False  # disable ConfusionMatrix and other plots to avoid errors\nself.metrics = SegmentMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/nas/model/","title":"Reference for <code>ultralytics/models/nas/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/nas/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/nas/model/#ultralytics.models.nas.model.NAS","title":"<code>ultralytics.models.nas.model.NAS</code>","text":"<p>             Bases: <code>Model</code></p> Source code in <code>ultralytics/models/nas/model.py</code> <pre><code>class NAS(Model):\ndef __init__(self, model='yolo_nas_s.pt') -&gt; None:\nassert Path(model).suffix not in ('.yaml', '.yml'), 'YOLO-NAS models only support pre-trained models.'\nsuper().__init__(model, task='detect')\n@smart_inference_mode()\ndef _load(self, weights: str, task: str):\n# Load or create new NAS model\nimport super_gradients\nsuffix = Path(weights).suffix\nif suffix == '.pt':\nself.model = torch.load(weights)\nelif suffix == '':\nself.model = super_gradients.training.models.get(weights, pretrained_weights='coco')\n# Standardize model\nself.model.fuse = lambda verbose=True: self.model\nself.model.stride = torch.tensor([32])\nself.model.names = dict(enumerate(self.model._class_names))\nself.model.is_fused = lambda: False  # for info()\nself.model.yaml = {}  # for info()\nself.model.pt_path = weights  # for export()\nself.model.task = 'detect'  # for export()\ndef info(self, detailed=False, verbose=True):\n\"\"\"\n        Logs model info.\n        Args:\n            detailed (bool): Show detailed information about model.\n            verbose (bool): Controls verbosity.\n        \"\"\"\nreturn model_info(self.model, detailed=detailed, verbose=verbose, imgsz=640)\n@property\ndef task_map(self):\nreturn {'detect': {'predictor': NASPredictor, 'validator': NASValidator}}\n</code></pre>"},{"location":"reference/models/nas/model/#ultralytics.models.nas.model.NAS.info","title":"<code>info(detailed=False, verbose=True)</code>","text":"<p>Logs model info.</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>Show detailed information about model.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Controls verbosity.</p> <code>True</code> Source code in <code>ultralytics/models/nas/model.py</code> <pre><code>def info(self, detailed=False, verbose=True):\n\"\"\"\n    Logs model info.\n    Args:\n        detailed (bool): Show detailed information about model.\n        verbose (bool): Controls verbosity.\n    \"\"\"\nreturn model_info(self.model, detailed=detailed, verbose=verbose, imgsz=640)\n</code></pre>"},{"location":"reference/models/nas/predict/","title":"Reference for <code>ultralytics/models/nas/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/nas/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/nas/predict/#ultralytics.models.nas.predict.NASPredictor","title":"<code>ultralytics.models.nas.predict.NASPredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> Source code in <code>ultralytics/models/nas/predict.py</code> <pre><code>class NASPredictor(BasePredictor):\ndef postprocess(self, preds_in, img, orig_imgs):\n\"\"\"Postprocess predictions and returns a list of Results objects.\"\"\"\n# Cat boxes and class scores\nboxes = ops.xyxy2xywh(preds_in[0][0])\npreds = torch.cat((boxes, preds_in[0][1]), -1).permute(0, 2, 1)\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/nas/predict/#ultralytics.models.nas.predict.NASPredictor.postprocess","title":"<code>postprocess(preds_in, img, orig_imgs)</code>","text":"<p>Postprocess predictions and returns a list of Results objects.</p> Source code in <code>ultralytics/models/nas/predict.py</code> <pre><code>def postprocess(self, preds_in, img, orig_imgs):\n\"\"\"Postprocess predictions and returns a list of Results objects.\"\"\"\n# Cat boxes and class scores\nboxes = ops.xyxy2xywh(preds_in[0][0])\npreds = torch.cat((boxes, preds_in[0][1]), -1).permute(0, 2, 1)\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/nas/val/","title":"Reference for <code>ultralytics/models/nas/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/nas/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/nas/val/#ultralytics.models.nas.val.NASValidator","title":"<code>ultralytics.models.nas.val.NASValidator</code>","text":"<p>             Bases: <code>DetectionValidator</code></p> Source code in <code>ultralytics/models/nas/val.py</code> <pre><code>class NASValidator(DetectionValidator):\ndef postprocess(self, preds_in):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nboxes = ops.xyxy2xywh(preds_in[0][0])\npreds = torch.cat((boxes, preds_in[0][1]), -1).permute(0, 2, 1)\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=False,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nmax_time_img=0.5)\n</code></pre>"},{"location":"reference/models/nas/val/#ultralytics.models.nas.val.NASValidator.postprocess","title":"<code>postprocess(preds_in)</code>","text":"<p>Apply Non-maximum suppression to prediction outputs.</p> Source code in <code>ultralytics/models/nas/val.py</code> <pre><code>def postprocess(self, preds_in):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nboxes = ops.xyxy2xywh(preds_in[0][0])\npreds = torch.cat((boxes, preds_in[0][1]), -1).permute(0, 2, 1)\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=False,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nmax_time_img=0.5)\n</code></pre>"},{"location":"reference/models/rtdetr/model/","title":"Reference for <code>ultralytics/models/rtdetr/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/rtdetr/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/rtdetr/model/#ultralytics.models.rtdetr.model.RTDETR","title":"<code>ultralytics.models.rtdetr.model.RTDETR</code>","text":"<p>             Bases: <code>Model</code></p> <p>RTDETR model interface.</p> Source code in <code>ultralytics/models/rtdetr/model.py</code> <pre><code>class RTDETR(Model):\n\"\"\"\n    RTDETR model interface.\n    \"\"\"\ndef __init__(self, model='rtdetr-l.pt') -&gt; None:\nif model and model.split('.')[-1] not in ('pt', 'yaml', 'yml'):\nraise NotImplementedError('RT-DETR only supports creating from *.pt file or *.yaml file.')\nsuper().__init__(model=model, task='detect')\n@property\ndef task_map(self):\nreturn {\n'detect': {\n'predictor': RTDETRPredictor,\n'validator': RTDETRValidator,\n'trainer': RTDETRTrainer,\n'model': RTDETRDetectionModel}}\n</code></pre>"},{"location":"reference/models/rtdetr/predict/","title":"Reference for <code>ultralytics/models/rtdetr/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/rtdetr/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/rtdetr/predict/#ultralytics.models.rtdetr.predict.RTDETRPredictor","title":"<code>ultralytics.models.rtdetr.predict.RTDETRPredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> <p>A class extending the BasePredictor class for prediction based on an RT-DETR detection model.</p> Example <pre><code>from ultralytics.utils import ASSETS\nfrom ultralytics.models.rtdetr import RTDETRPredictor\nargs = dict(model='rtdetr-l.pt', source=ASSETS)\npredictor = RTDETRPredictor(overrides=args)\npredictor.predict_cli()\n</code></pre> Source code in <code>ultralytics/models/rtdetr/predict.py</code> <pre><code>class RTDETRPredictor(BasePredictor):\n\"\"\"\n    A class extending the BasePredictor class for prediction based on an RT-DETR detection model.\n    Example:\n        ```python\n        from ultralytics.utils import ASSETS\n        from ultralytics.models.rtdetr import RTDETRPredictor\n        args = dict(model='rtdetr-l.pt', source=ASSETS)\n        predictor = RTDETRPredictor(overrides=args)\n        predictor.predict_cli()\n        ```\n    \"\"\"\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Postprocess predictions and returns a list of Results objects.\"\"\"\nnd = preds[0].shape[-1]\nbboxes, scores = preds[0].split((4, nd - 4), dim=-1)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, bbox in enumerate(bboxes):  # (300, 4)\nbbox = ops.xywh2xyxy(bbox)\nscore, cls = scores[i].max(-1, keepdim=True)  # (300, 1)\nidx = score.squeeze(-1) &gt; self.args.conf  # (300, )\nif self.args.classes is not None:\nidx = (cls == torch.tensor(self.args.classes, device=cls.device)).any(1) &amp; idx\npred = torch.cat([bbox, score, cls], dim=-1)[idx]  # filter\norig_img = orig_imgs[i] if is_list else orig_imgs\noh, ow = orig_img.shape[:2]\nif is_list:\npred[..., [0, 2]] *= ow\npred[..., [1, 3]] *= oh\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\ndef pre_transform(self, im):\n\"\"\"Pre-transform input image before inference.\n        Args:\n            im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n        Notes: The size must be square(640) and scaleFilled.\n        Returns:\n            (list): A list of transformed imgs.\n        \"\"\"\nreturn [LetterBox(self.imgsz, auto=False, scaleFill=True)(image=x) for x in im]\n</code></pre>"},{"location":"reference/models/rtdetr/predict/#ultralytics.models.rtdetr.predict.RTDETRPredictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Postprocess predictions and returns a list of Results objects.</p> Source code in <code>ultralytics/models/rtdetr/predict.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Postprocess predictions and returns a list of Results objects.\"\"\"\nnd = preds[0].shape[-1]\nbboxes, scores = preds[0].split((4, nd - 4), dim=-1)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, bbox in enumerate(bboxes):  # (300, 4)\nbbox = ops.xywh2xyxy(bbox)\nscore, cls = scores[i].max(-1, keepdim=True)  # (300, 1)\nidx = score.squeeze(-1) &gt; self.args.conf  # (300, )\nif self.args.classes is not None:\nidx = (cls == torch.tensor(self.args.classes, device=cls.device)).any(1) &amp; idx\npred = torch.cat([bbox, score, cls], dim=-1)[idx]  # filter\norig_img = orig_imgs[i] if is_list else orig_imgs\noh, ow = orig_img.shape[:2]\nif is_list:\npred[..., [0, 2]] *= ow\npred[..., [1, 3]] *= oh\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/rtdetr/predict/#ultralytics.models.rtdetr.predict.RTDETRPredictor.pre_transform","title":"<code>pre_transform(im)</code>","text":"<p>Pre-transform input image before inference.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>List(np.ndarray</code> <p>(N, 3, h, w) for tensor, [(h, w, 3) x N] for list.</p> required <p>Notes: The size must be square(640) and scaleFilled.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of transformed imgs.</p> Source code in <code>ultralytics/models/rtdetr/predict.py</code> <pre><code>def pre_transform(self, im):\n\"\"\"Pre-transform input image before inference.\n    Args:\n        im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n    Notes: The size must be square(640) and scaleFilled.\n    Returns:\n        (list): A list of transformed imgs.\n    \"\"\"\nreturn [LetterBox(self.imgsz, auto=False, scaleFill=True)(image=x) for x in im]\n</code></pre>"},{"location":"reference/models/rtdetr/train/","title":"Reference for <code>ultralytics/models/rtdetr/train.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/rtdetr/train.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/rtdetr/train/#ultralytics.models.rtdetr.train.RTDETRTrainer","title":"<code>ultralytics.models.rtdetr.train.RTDETRTrainer</code>","text":"<p>             Bases: <code>DetectionTrainer</code></p> <p>A class extending the DetectionTrainer class for training based on an RT-DETR detection model.</p> Notes <ul> <li>F.grid_sample used in rt-detr does not support the <code>deterministic=True</code> argument.</li> <li>AMP training can lead to NaN outputs and may produce errors during bipartite graph matching.</li> </ul> Example <pre><code>from ultralytics.models.rtdetr.train import RTDETRTrainer\nargs = dict(model='rtdetr-l.yaml', data='coco8.yaml', imgsz=640, epochs=3)\ntrainer = RTDETRTrainer(overrides=args)\ntrainer.train()\n</code></pre> Source code in <code>ultralytics/models/rtdetr/train.py</code> <pre><code>class RTDETRTrainer(DetectionTrainer):\n\"\"\"\n    A class extending the DetectionTrainer class for training based on an RT-DETR detection model.\n    Notes:\n        - F.grid_sample used in rt-detr does not support the `deterministic=True` argument.\n        - AMP training can lead to NaN outputs and may produce errors during bipartite graph matching.\n    Example:\n        ```python\n        from ultralytics.models.rtdetr.train import RTDETRTrainer\n        args = dict(model='rtdetr-l.yaml', data='coco8.yaml', imgsz=640, epochs=3)\n        trainer = RTDETRTrainer(overrides=args)\n        trainer.train()\n        ```\n    \"\"\"\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return a YOLO detection model.\"\"\"\nmodel = RTDETRDetectionModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\ndef build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"Build RTDETR Dataset\n        Args:\n            img_path (str): Path to the folder containing images.\n            mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n        \"\"\"\nreturn RTDETRDataset(\nimg_path=img_path,\nimgsz=self.args.imgsz,\nbatch_size=batch,\naugment=mode == 'train',  # no augmentation\nhyp=self.args,\nrect=False,  # no rect\ncache=self.args.cache or None,\nprefix=colorstr(f'{mode}: '),\ndata=self.data)\ndef get_validator(self):\n\"\"\"Returns a DetectionValidator for RTDETR model validation.\"\"\"\nself.loss_names = 'giou_loss', 'cls_loss', 'l1_loss'\nreturn RTDETRValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\ndef preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images by scaling and converting to float.\"\"\"\nbatch = super().preprocess_batch(batch)\nbs = len(batch['img'])\nbatch_idx = batch['batch_idx']\ngt_bbox, gt_class = [], []\nfor i in range(bs):\ngt_bbox.append(batch['bboxes'][batch_idx == i].to(batch_idx.device))\ngt_class.append(batch['cls'][batch_idx == i].to(device=batch_idx.device, dtype=torch.long))\nreturn batch\n</code></pre>"},{"location":"reference/models/rtdetr/train/#ultralytics.models.rtdetr.train.RTDETRTrainer.build_dataset","title":"<code>build_dataset(img_path, mode='val', batch=None)</code>","text":"<p>Build RTDETR Dataset</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the folder containing images.</p> required <code>mode</code> <code>str</code> <p><code>train</code> mode or <code>val</code> mode, users are able to customize different augmentations for each mode.</p> <code>'val'</code> <code>batch</code> <code>int</code> <p>Size of batches, this is for <code>rect</code>. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/models/rtdetr/train.py</code> <pre><code>def build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"Build RTDETR Dataset\n    Args:\n        img_path (str): Path to the folder containing images.\n        mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n        batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n    \"\"\"\nreturn RTDETRDataset(\nimg_path=img_path,\nimgsz=self.args.imgsz,\nbatch_size=batch,\naugment=mode == 'train',  # no augmentation\nhyp=self.args,\nrect=False,  # no rect\ncache=self.args.cache or None,\nprefix=colorstr(f'{mode}: '),\ndata=self.data)\n</code></pre>"},{"location":"reference/models/rtdetr/train/#ultralytics.models.rtdetr.train.RTDETRTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Return a YOLO detection model.</p> Source code in <code>ultralytics/models/rtdetr/train.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return a YOLO detection model.\"\"\"\nmodel = RTDETRDetectionModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\n</code></pre>"},{"location":"reference/models/rtdetr/train/#ultralytics.models.rtdetr.train.RTDETRTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Returns a DetectionValidator for RTDETR model validation.</p> Source code in <code>ultralytics/models/rtdetr/train.py</code> <pre><code>def get_validator(self):\n\"\"\"Returns a DetectionValidator for RTDETR model validation.\"\"\"\nself.loss_names = 'giou_loss', 'cls_loss', 'l1_loss'\nreturn RTDETRValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\n</code></pre>"},{"location":"reference/models/rtdetr/train/#ultralytics.models.rtdetr.train.RTDETRTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Preprocesses a batch of images by scaling and converting to float.</p> Source code in <code>ultralytics/models/rtdetr/train.py</code> <pre><code>def preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images by scaling and converting to float.\"\"\"\nbatch = super().preprocess_batch(batch)\nbs = len(batch['img'])\nbatch_idx = batch['batch_idx']\ngt_bbox, gt_class = [], []\nfor i in range(bs):\ngt_bbox.append(batch['bboxes'][batch_idx == i].to(batch_idx.device))\ngt_class.append(batch['cls'][batch_idx == i].to(device=batch_idx.device, dtype=torch.long))\nreturn batch\n</code></pre>"},{"location":"reference/models/rtdetr/val/","title":"Reference for <code>ultralytics/models/rtdetr/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/rtdetr/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRDataset","title":"<code>ultralytics.models.rtdetr.val.RTDETRDataset</code>","text":"<p>             Bases: <code>YOLODataset</code></p> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>class RTDETRDataset(YOLODataset):\ndef __init__(self, *args, data=None, **kwargs):\nsuper().__init__(*args, data=data, use_segments=False, use_keypoints=False, **kwargs)\n# NOTE: add stretch version load_image for rtdetr mosaic\ndef load_image(self, i):\n\"\"\"Loads 1 image from dataset index 'i', returns (im, resized hw).\"\"\"\nim, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\nif im is None:  # not cached in RAM\nif fn.exists():  # load npy\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif im is None:\nraise FileNotFoundError(f'Image Not Found {f}')\nh0, w0 = im.shape[:2]  # orig hw\nim = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)\n# Add to buffer if training with augmentations\nif self.augment:\nself.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\nself.buffer.append(i)\nif len(self.buffer) &gt;= self.max_buffer_length:\nj = self.buffer.pop(0)\nself.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\nreturn im, (h0, w0), im.shape[:2]\nreturn self.ims[i], self.im_hw0[i], self.im_hw[i]\ndef build_transforms(self, hyp=None):\n\"\"\"Temporary, only for evaluation.\"\"\"\nif self.augment:\nhyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\nhyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\ntransforms = v8_transforms(self, self.imgsz, hyp, stretch=True)\nelse:\n# transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), auto=False, scaleFill=True)])\ntransforms = Compose([])\ntransforms.append(\nFormat(bbox_format='xywh',\nnormalize=True,\nreturn_mask=self.use_segments,\nreturn_keypoint=self.use_keypoints,\nbatch_idx=True,\nmask_ratio=hyp.mask_ratio,\nmask_overlap=hyp.overlap_mask))\nreturn transforms\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRDataset.build_transforms","title":"<code>build_transforms(hyp=None)</code>","text":"<p>Temporary, only for evaluation.</p> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>def build_transforms(self, hyp=None):\n\"\"\"Temporary, only for evaluation.\"\"\"\nif self.augment:\nhyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\nhyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\ntransforms = v8_transforms(self, self.imgsz, hyp, stretch=True)\nelse:\n# transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), auto=False, scaleFill=True)])\ntransforms = Compose([])\ntransforms.append(\nFormat(bbox_format='xywh',\nnormalize=True,\nreturn_mask=self.use_segments,\nreturn_keypoint=self.use_keypoints,\nbatch_idx=True,\nmask_ratio=hyp.mask_ratio,\nmask_overlap=hyp.overlap_mask))\nreturn transforms\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRDataset.load_image","title":"<code>load_image(i)</code>","text":"<p>Loads 1 image from dataset index 'i', returns (im, resized hw).</p> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>def load_image(self, i):\n\"\"\"Loads 1 image from dataset index 'i', returns (im, resized hw).\"\"\"\nim, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\nif im is None:  # not cached in RAM\nif fn.exists():  # load npy\nim = np.load(fn)\nelse:  # read image\nim = cv2.imread(f)  # BGR\nif im is None:\nraise FileNotFoundError(f'Image Not Found {f}')\nh0, w0 = im.shape[:2]  # orig hw\nim = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)\n# Add to buffer if training with augmentations\nif self.augment:\nself.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\nself.buffer.append(i)\nif len(self.buffer) &gt;= self.max_buffer_length:\nj = self.buffer.pop(0)\nself.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\nreturn im, (h0, w0), im.shape[:2]\nreturn self.ims[i], self.im_hw0[i], self.im_hw[i]\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRValidator","title":"<code>ultralytics.models.rtdetr.val.RTDETRValidator</code>","text":"<p>             Bases: <code>DetectionValidator</code></p> <p>A class extending the DetectionValidator class for validation based on an RT-DETR detection model.</p> Example <pre><code>from ultralytics.models.rtdetr import RTDETRValidator\nargs = dict(model='rtdetr-l.pt', data='coco8.yaml')\nvalidator = RTDETRValidator(args=args)\nvalidator()\n</code></pre> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>class RTDETRValidator(DetectionValidator):\n\"\"\"\n    A class extending the DetectionValidator class for validation based on an RT-DETR detection model.\n    Example:\n        ```python\n        from ultralytics.models.rtdetr import RTDETRValidator\n        args = dict(model='rtdetr-l.pt', data='coco8.yaml')\n        validator = RTDETRValidator(args=args)\n        validator()\n        ```\n    \"\"\"\ndef build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"\n        Build an RTDETR Dataset.\n        Args:\n            img_path (str): Path to the folder containing images.\n            mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n        \"\"\"\nreturn RTDETRDataset(\nimg_path=img_path,\nimgsz=self.args.imgsz,\nbatch_size=batch,\naugment=False,  # no augmentation\nhyp=self.args,\nrect=False,  # no rect\ncache=self.args.cache or None,\nprefix=colorstr(f'{mode}: '),\ndata=self.data)\ndef postprocess(self, preds):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nbs, _, nd = preds[0].shape\nbboxes, scores = preds[0].split((4, nd - 4), dim=-1)\nbboxes *= self.args.imgsz\noutputs = [torch.zeros((0, 6), device=bboxes.device)] * bs\nfor i, bbox in enumerate(bboxes):  # (300, 4)\nbbox = ops.xywh2xyxy(bbox)\nscore, cls = scores[i].max(-1)  # (300, )\n# Do not need threshold for evaluation as only got 300 boxes here.\n# idx = score &gt; self.args.conf\npred = torch.cat([bbox, score[..., None], cls[..., None]], dim=-1)  # filter\n# sort by confidence to correctly get internal metrics.\npred = pred[score.argsort(descending=True)]\noutputs[i] = pred  # [idx]\nreturn outputs\ndef update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\npredn[..., [0, 2]] *= shape[1] / self.args.imgsz  # native-space pred\npredn[..., [1, 3]] *= shape[0] / self.args.imgsz  # native-space pred\n# Evaluate\nif nl:\ntbox = ops.xywh2xyxy(bbox)  # target boxes\ntbox[..., [0, 2]] *= shape[1]  # native-space pred\ntbox[..., [1, 3]] *= shape[0]  # native-space pred\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\n# NOTE: To get correct metrics, the inputs of `_process_batch` should always be float32 type.\ncorrect_bboxes = self._process_batch(predn.float(), labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\nself.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\nif self.args.save_txt:\nfile = self.save_dir / 'labels' / f'{Path(batch[\"im_file\"][si]).stem}.txt'\nself.save_one_txt(predn, self.args.save_conf, shape, file)\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRValidator.build_dataset","title":"<code>build_dataset(img_path, mode='val', batch=None)</code>","text":"<p>Build an RTDETR Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the folder containing images.</p> required <code>mode</code> <code>str</code> <p><code>train</code> mode or <code>val</code> mode, users are able to customize different augmentations for each mode.</p> <code>'val'</code> <code>batch</code> <code>int</code> <p>Size of batches, this is for <code>rect</code>. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>def build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"\n    Build an RTDETR Dataset.\n    Args:\n        img_path (str): Path to the folder containing images.\n        mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n        batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n    \"\"\"\nreturn RTDETRDataset(\nimg_path=img_path,\nimgsz=self.args.imgsz,\nbatch_size=batch,\naugment=False,  # no augmentation\nhyp=self.args,\nrect=False,  # no rect\ncache=self.args.cache or None,\nprefix=colorstr(f'{mode}: '),\ndata=self.data)\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRValidator.postprocess","title":"<code>postprocess(preds)</code>","text":"<p>Apply Non-maximum suppression to prediction outputs.</p> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>def postprocess(self, preds):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nbs, _, nd = preds[0].shape\nbboxes, scores = preds[0].split((4, nd - 4), dim=-1)\nbboxes *= self.args.imgsz\noutputs = [torch.zeros((0, 6), device=bboxes.device)] * bs\nfor i, bbox in enumerate(bboxes):  # (300, 4)\nbbox = ops.xywh2xyxy(bbox)\nscore, cls = scores[i].max(-1)  # (300, )\n# Do not need threshold for evaluation as only got 300 boxes here.\n# idx = score &gt; self.args.conf\npred = torch.cat([bbox, score[..., None], cls[..., None]], dim=-1)  # filter\n# sort by confidence to correctly get internal metrics.\npred = pred[score.argsort(descending=True)]\noutputs[i] = pred  # [idx]\nreturn outputs\n</code></pre>"},{"location":"reference/models/rtdetr/val/#ultralytics.models.rtdetr.val.RTDETRValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Metrics.</p> Source code in <code>ultralytics/models/rtdetr/val.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\npredn[..., [0, 2]] *= shape[1] / self.args.imgsz  # native-space pred\npredn[..., [1, 3]] *= shape[0] / self.args.imgsz  # native-space pred\n# Evaluate\nif nl:\ntbox = ops.xywh2xyxy(bbox)  # target boxes\ntbox[..., [0, 2]] *= shape[1]  # native-space pred\ntbox[..., [1, 3]] *= shape[0]  # native-space pred\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\n# NOTE: To get correct metrics, the inputs of `_process_batch` should always be float32 type.\ncorrect_bboxes = self._process_batch(predn.float(), labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\nself.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\nif self.args.save_txt:\nfile = self.save_dir / 'labels' / f'{Path(batch[\"im_file\"][si]).stem}.txt'\nself.save_one_txt(predn, self.args.save_conf, shape, file)\n</code></pre>"},{"location":"reference/models/sam/amg/","title":"Reference for <code>ultralytics/models/sam/amg.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/amg.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.is_box_near_crop_edge","title":"<code>ultralytics.models.sam.amg.is_box_near_crop_edge(boxes, crop_box, orig_box, atol=20.0)</code>","text":"<p>Return a boolean tensor indicating if boxes are near the crop edge.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def is_box_near_crop_edge(boxes: torch.Tensor,\ncrop_box: List[int],\norig_box: List[int],\natol: float = 20.0) -&gt; torch.Tensor:\n\"\"\"Return a boolean tensor indicating if boxes are near the crop edge.\"\"\"\ncrop_box_torch = torch.as_tensor(crop_box, dtype=torch.float, device=boxes.device)\norig_box_torch = torch.as_tensor(orig_box, dtype=torch.float, device=boxes.device)\nboxes = uncrop_boxes_xyxy(boxes, crop_box).float()\nnear_crop_edge = torch.isclose(boxes, crop_box_torch[None, :], atol=atol, rtol=0)\nnear_image_edge = torch.isclose(boxes, orig_box_torch[None, :], atol=atol, rtol=0)\nnear_crop_edge = torch.logical_and(near_crop_edge, ~near_image_edge)\nreturn torch.any(near_crop_edge, dim=1)\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.batch_iterator","title":"<code>ultralytics.models.sam.amg.batch_iterator(batch_size, *args)</code>","text":"<p>Yield batches of data from the input arguments.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def batch_iterator(batch_size: int, *args) -&gt; Generator[List[Any], None, None]:\n\"\"\"Yield batches of data from the input arguments.\"\"\"\nassert args and all(len(a) == len(args[0]) for a in args), 'Batched iteration must have same-size inputs.'\nn_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)\nfor b in range(n_batches):\nyield [arg[b * batch_size:(b + 1) * batch_size] for arg in args]\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.calculate_stability_score","title":"<code>ultralytics.models.sam.amg.calculate_stability_score(masks, mask_threshold, threshold_offset)</code>","text":"<p>Computes the stability score for a batch of masks. The stability score is the IoU between the binary masks obtained by thresholding the predicted mask logits at high and low values.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def calculate_stability_score(masks: torch.Tensor, mask_threshold: float, threshold_offset: float) -&gt; torch.Tensor:\n\"\"\"\n    Computes the stability score for a batch of masks. The stability\n    score is the IoU between the binary masks obtained by thresholding\n    the predicted mask logits at high and low values.\n    \"\"\"\n# One mask is always contained inside the other.\n# Save memory by preventing unnecessary cast to torch.int64\nintersections = ((masks &gt; (mask_threshold + threshold_offset)).sum(-1, dtype=torch.int16).sum(-1,\ndtype=torch.int32))\nunions = ((masks &gt; (mask_threshold - threshold_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32))\nreturn intersections / unions\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.build_point_grid","title":"<code>ultralytics.models.sam.amg.build_point_grid(n_per_side)</code>","text":"<p>Generate a 2D grid of evenly spaced points in the range [0,1]x[0,1].</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def build_point_grid(n_per_side: int) -&gt; np.ndarray:\n\"\"\"Generate a 2D grid of evenly spaced points in the range [0,1]x[0,1].\"\"\"\noffset = 1 / (2 * n_per_side)\npoints_one_side = np.linspace(offset, 1 - offset, n_per_side)\npoints_x = np.tile(points_one_side[None, :], (n_per_side, 1))\npoints_y = np.tile(points_one_side[:, None], (1, n_per_side))\nreturn np.stack([points_x, points_y], axis=-1).reshape(-1, 2)\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.build_all_layer_point_grids","title":"<code>ultralytics.models.sam.amg.build_all_layer_point_grids(n_per_side, n_layers, scale_per_layer)</code>","text":"<p>Generate point grids for all crop layers.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def build_all_layer_point_grids(n_per_side: int, n_layers: int, scale_per_layer: int) -&gt; List[np.ndarray]:\n\"\"\"Generate point grids for all crop layers.\"\"\"\nreturn [build_point_grid(int(n_per_side / (scale_per_layer ** i))) for i in range(n_layers + 1)]\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.generate_crop_boxes","title":"<code>ultralytics.models.sam.amg.generate_crop_boxes(im_size, n_layers, overlap_ratio)</code>","text":"<p>Generates a list of crop boxes of different sizes. Each layer has (2i)2 boxes for the ith layer.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def generate_crop_boxes(im_size: Tuple[int, ...], n_layers: int,\noverlap_ratio: float) -&gt; Tuple[List[List[int]], List[int]]:\n\"\"\"Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\"\"\"\ncrop_boxes, layer_idxs = [], []\nim_h, im_w = im_size\nshort_side = min(im_h, im_w)\n# Original image\ncrop_boxes.append([0, 0, im_w, im_h])\nlayer_idxs.append(0)\ndef crop_len(orig_len, n_crops, overlap):\n\"\"\"Crops bounding boxes to the size of the input image.\"\"\"\nreturn int(math.ceil((overlap * (n_crops - 1) + orig_len) / n_crops))\nfor i_layer in range(n_layers):\nn_crops_per_side = 2 ** (i_layer + 1)\noverlap = int(overlap_ratio * short_side * (2 / n_crops_per_side))\ncrop_w = crop_len(im_w, n_crops_per_side, overlap)\ncrop_h = crop_len(im_h, n_crops_per_side, overlap)\ncrop_box_x0 = [int((crop_w - overlap) * i) for i in range(n_crops_per_side)]\ncrop_box_y0 = [int((crop_h - overlap) * i) for i in range(n_crops_per_side)]\n# Crops in XYWH format\nfor x0, y0 in product(crop_box_x0, crop_box_y0):\nbox = [x0, y0, min(x0 + crop_w, im_w), min(y0 + crop_h, im_h)]\ncrop_boxes.append(box)\nlayer_idxs.append(i_layer + 1)\nreturn crop_boxes, layer_idxs\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.uncrop_boxes_xyxy","title":"<code>ultralytics.models.sam.amg.uncrop_boxes_xyxy(boxes, crop_box)</code>","text":"<p>Uncrop bounding boxes by adding the crop box offset.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def uncrop_boxes_xyxy(boxes: torch.Tensor, crop_box: List[int]) -&gt; torch.Tensor:\n\"\"\"Uncrop bounding boxes by adding the crop box offset.\"\"\"\nx0, y0, _, _ = crop_box\noffset = torch.tensor([[x0, y0, x0, y0]], device=boxes.device)\n# Check if boxes has a channel dimension\nif len(boxes.shape) == 3:\noffset = offset.unsqueeze(1)\nreturn boxes + offset\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.uncrop_points","title":"<code>ultralytics.models.sam.amg.uncrop_points(points, crop_box)</code>","text":"<p>Uncrop points by adding the crop box offset.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def uncrop_points(points: torch.Tensor, crop_box: List[int]) -&gt; torch.Tensor:\n\"\"\"Uncrop points by adding the crop box offset.\"\"\"\nx0, y0, _, _ = crop_box\noffset = torch.tensor([[x0, y0]], device=points.device)\n# Check if points has a channel dimension\nif len(points.shape) == 3:\noffset = offset.unsqueeze(1)\nreturn points + offset\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.uncrop_masks","title":"<code>ultralytics.models.sam.amg.uncrop_masks(masks, crop_box, orig_h, orig_w)</code>","text":"<p>Uncrop masks by padding them to the original image size.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def uncrop_masks(masks: torch.Tensor, crop_box: List[int], orig_h: int, orig_w: int) -&gt; torch.Tensor:\n\"\"\"Uncrop masks by padding them to the original image size.\"\"\"\nx0, y0, x1, y1 = crop_box\nif x0 == 0 and y0 == 0 and x1 == orig_w and y1 == orig_h:\nreturn masks\n# Coordinate transform masks\npad_x, pad_y = orig_w - (x1 - x0), orig_h - (y1 - y0)\npad = (x0, pad_x - x0, y0, pad_y - y0)\nreturn torch.nn.functional.pad(masks, pad, value=0)\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.remove_small_regions","title":"<code>ultralytics.models.sam.amg.remove_small_regions(mask, area_thresh, mode)</code>","text":"<p>Remove small disconnected regions or holes in a mask, returning the mask and a modification indicator.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def remove_small_regions(mask: np.ndarray, area_thresh: float, mode: str) -&gt; Tuple[np.ndarray, bool]:\n\"\"\"Remove small disconnected regions or holes in a mask, returning the mask and a modification indicator.\"\"\"\nimport cv2  # type: ignore\nassert mode in {'holes', 'islands'}\ncorrect_holes = mode == 'holes'\nworking_mask = (correct_holes ^ mask).astype(np.uint8)\nn_labels, regions, stats, _ = cv2.connectedComponentsWithStats(working_mask, 8)\nsizes = stats[:, -1][1:]  # Row 0 is background label\nsmall_regions = [i + 1 for i, s in enumerate(sizes) if s &lt; area_thresh]\nif not small_regions:\nreturn mask, False\nfill_labels = [0] + small_regions\nif not correct_holes:\n# If every region is below threshold, keep largest\nfill_labels = [i for i in range(n_labels) if i not in fill_labels] or [int(np.argmax(sizes)) + 1]\nmask = np.isin(regions, fill_labels)\nreturn mask, True\n</code></pre>"},{"location":"reference/models/sam/amg/#ultralytics.models.sam.amg.batched_mask_to_box","title":"<code>ultralytics.models.sam.amg.batched_mask_to_box(masks)</code>","text":"<p>Calculates boxes in XYXY format around masks. Return [0,0,0,0] for an empty mask. For input shape C1xC2x...xHxW, the output shape is C1xC2x...x4.</p> Source code in <code>ultralytics/models/sam/amg.py</code> <pre><code>def batched_mask_to_box(masks: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Calculates boxes in XYXY format around masks. Return [0,0,0,0] for\n    an empty mask. For input shape C1xC2x...xHxW, the output shape is C1xC2x...x4.\n    \"\"\"\n# torch.max below raises an error on empty inputs, just skip in this case\nif torch.numel(masks) == 0:\nreturn torch.zeros(*masks.shape[:-2], 4, device=masks.device)\n# Normalize shape to CxHxW\nshape = masks.shape\nh, w = shape[-2:]\nmasks = masks.flatten(0, -3) if len(shape) &gt; 2 else masks.unsqueeze(0)\n# Get top and bottom edges\nin_height, _ = torch.max(masks, dim=-1)\nin_height_coords = in_height * torch.arange(h, device=in_height.device)[None, :]\nbottom_edges, _ = torch.max(in_height_coords, dim=-1)\nin_height_coords = in_height_coords + h * (~in_height)\ntop_edges, _ = torch.min(in_height_coords, dim=-1)\n# Get left and right edges\nin_width, _ = torch.max(masks, dim=-2)\nin_width_coords = in_width * torch.arange(w, device=in_width.device)[None, :]\nright_edges, _ = torch.max(in_width_coords, dim=-1)\nin_width_coords = in_width_coords + w * (~in_width)\nleft_edges, _ = torch.min(in_width_coords, dim=-1)\n# If the mask is empty the right edge will be to the left of the left edge.\n# Replace these boxes with [0, 0, 0, 0]\nempty_filter = (right_edges &lt; left_edges) | (bottom_edges &lt; top_edges)\nout = torch.stack([left_edges, top_edges, right_edges, bottom_edges], dim=-1)\nout = out * (~empty_filter).unsqueeze(-1)\n# Return to original shape\nreturn out.reshape(*shape[:-2], 4) if len(shape) &gt; 2 else out[0]\n</code></pre>"},{"location":"reference/models/sam/build/","title":"Reference for <code>ultralytics/models/sam/build.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/build.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build.build_sam_vit_h","title":"<code>ultralytics.models.sam.build.build_sam_vit_h(checkpoint=None)</code>","text":"<p>Build and return a Segment Anything Model (SAM) h-size model.</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def build_sam_vit_h(checkpoint=None):\n\"\"\"Build and return a Segment Anything Model (SAM) h-size model.\"\"\"\nreturn _build_sam(\nencoder_embed_dim=1280,\nencoder_depth=32,\nencoder_num_heads=16,\nencoder_global_attn_indexes=[7, 15, 23, 31],\ncheckpoint=checkpoint,\n)\n</code></pre>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build.build_sam_vit_l","title":"<code>ultralytics.models.sam.build.build_sam_vit_l(checkpoint=None)</code>","text":"<p>Build and return a Segment Anything Model (SAM) l-size model.</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def build_sam_vit_l(checkpoint=None):\n\"\"\"Build and return a Segment Anything Model (SAM) l-size model.\"\"\"\nreturn _build_sam(\nencoder_embed_dim=1024,\nencoder_depth=24,\nencoder_num_heads=16,\nencoder_global_attn_indexes=[5, 11, 17, 23],\ncheckpoint=checkpoint,\n)\n</code></pre>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build.build_sam_vit_b","title":"<code>ultralytics.models.sam.build.build_sam_vit_b(checkpoint=None)</code>","text":"<p>Build and return a Segment Anything Model (SAM) b-size model.</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def build_sam_vit_b(checkpoint=None):\n\"\"\"Build and return a Segment Anything Model (SAM) b-size model.\"\"\"\nreturn _build_sam(\nencoder_embed_dim=768,\nencoder_depth=12,\nencoder_num_heads=12,\nencoder_global_attn_indexes=[2, 5, 8, 11],\ncheckpoint=checkpoint,\n)\n</code></pre>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build.build_mobile_sam","title":"<code>ultralytics.models.sam.build.build_mobile_sam(checkpoint=None)</code>","text":"<p>Build and return Mobile Segment Anything Model (Mobile-SAM).</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def build_mobile_sam(checkpoint=None):\n\"\"\"Build and return Mobile Segment Anything Model (Mobile-SAM).\"\"\"\nreturn _build_sam(\nencoder_embed_dim=[64, 128, 160, 320],\nencoder_depth=[2, 2, 6, 2],\nencoder_num_heads=[2, 4, 5, 10],\nencoder_global_attn_indexes=None,\nmobile_sam=True,\ncheckpoint=checkpoint,\n)\n</code></pre>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build._build_sam","title":"<code>ultralytics.models.sam.build._build_sam(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint=None, mobile_sam=False)</code>","text":"<p>Builds the selected SAM model architecture.</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def _build_sam(encoder_embed_dim,\nencoder_depth,\nencoder_num_heads,\nencoder_global_attn_indexes,\ncheckpoint=None,\nmobile_sam=False):\n\"\"\"Builds the selected SAM model architecture.\"\"\"\nprompt_embed_dim = 256\nimage_size = 1024\nvit_patch_size = 16\nimage_embedding_size = image_size // vit_patch_size\nimage_encoder = (TinyViT(\nimg_size=1024,\nin_chans=3,\nnum_classes=1000,\nembed_dims=encoder_embed_dim,\ndepths=encoder_depth,\nnum_heads=encoder_num_heads,\nwindow_sizes=[7, 7, 14, 7],\nmlp_ratio=4.0,\ndrop_rate=0.0,\ndrop_path_rate=0.0,\nuse_checkpoint=False,\nmbconv_expand_ratio=4.0,\nlocal_conv_size=3,\nlayer_lr_decay=0.8,\n) if mobile_sam else ImageEncoderViT(\ndepth=encoder_depth,\nembed_dim=encoder_embed_dim,\nimg_size=image_size,\nmlp_ratio=4,\nnorm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\nnum_heads=encoder_num_heads,\npatch_size=vit_patch_size,\nqkv_bias=True,\nuse_rel_pos=True,\nglobal_attn_indexes=encoder_global_attn_indexes,\nwindow_size=14,\nout_chans=prompt_embed_dim,\n))\nsam = Sam(\nimage_encoder=image_encoder,\nprompt_encoder=PromptEncoder(\nembed_dim=prompt_embed_dim,\nimage_embedding_size=(image_embedding_size, image_embedding_size),\ninput_image_size=(image_size, image_size),\nmask_in_chans=16,\n),\nmask_decoder=MaskDecoder(\nnum_multimask_outputs=3,\ntransformer=TwoWayTransformer(\ndepth=2,\nembedding_dim=prompt_embed_dim,\nmlp_dim=2048,\nnum_heads=8,\n),\ntransformer_dim=prompt_embed_dim,\niou_head_depth=3,\niou_head_hidden_dim=256,\n),\npixel_mean=[123.675, 116.28, 103.53],\npixel_std=[58.395, 57.12, 57.375],\n)\nif checkpoint is not None:\ncheckpoint = attempt_download_asset(checkpoint)\nwith open(checkpoint, 'rb') as f:\nstate_dict = torch.load(f)\nsam.load_state_dict(state_dict)\nsam.eval()\n# sam.load_state_dict(torch.load(checkpoint), strict=True)\n# sam.eval()\nreturn sam\n</code></pre>"},{"location":"reference/models/sam/build/#ultralytics.models.sam.build.build_sam","title":"<code>ultralytics.models.sam.build.build_sam(ckpt='sam_b.pt')</code>","text":"<p>Build a SAM model specified by ckpt.</p> Source code in <code>ultralytics/models/sam/build.py</code> <pre><code>def build_sam(ckpt='sam_b.pt'):\n\"\"\"Build a SAM model specified by ckpt.\"\"\"\nmodel_builder = None\nfor k in sam_model_map.keys():\nif ckpt.endswith(k):\nmodel_builder = sam_model_map.get(k)\nif not model_builder:\nraise FileNotFoundError(f'{ckpt} is not a supported sam model. Available models are: \\n {sam_model_map.keys()}')\nreturn model_builder(ckpt)\n</code></pre>"},{"location":"reference/models/sam/model/","title":"Reference for <code>ultralytics/models/sam/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/sam/model/#ultralytics.models.sam.model.SAM","title":"<code>ultralytics.models.sam.model.SAM</code>","text":"<p>             Bases: <code>Model</code></p> <p>SAM model interface.</p> Source code in <code>ultralytics/models/sam/model.py</code> <pre><code>class SAM(Model):\n\"\"\"\n    SAM model interface.\n    \"\"\"\ndef __init__(self, model='sam_b.pt') -&gt; None:\nif model and Path(model).suffix not in ('.pt', '.pth'):\nraise NotImplementedError('SAM prediction requires pre-trained *.pt or *.pth model.')\nsuper().__init__(model=model, task='segment')\ndef _load(self, weights: str, task=None):\nself.model = build_sam(weights)\ndef predict(self, source, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n\"\"\"Predicts and returns segmentation masks for given image or video source.\"\"\"\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024)\nkwargs.update(overrides)\nprompts = dict(bboxes=bboxes, points=points, labels=labels)\nreturn super().predict(source, stream, prompts=prompts, **kwargs)\ndef __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\nreturn self.predict(source, stream, bboxes, points, labels, **kwargs)\ndef info(self, detailed=False, verbose=True):\n\"\"\"\n        Logs model info.\n        Args:\n            detailed (bool): Show detailed information about model.\n            verbose (bool): Controls verbosity.\n        \"\"\"\nreturn model_info(self.model, detailed=detailed, verbose=verbose)\n@property\ndef task_map(self):\nreturn {'segment': {'predictor': Predictor}}\n</code></pre>"},{"location":"reference/models/sam/model/#ultralytics.models.sam.model.SAM.__call__","title":"<code>__call__(source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs)</code>","text":"<p>Calls the 'predict' function with given arguments to perform object detection.</p> Source code in <code>ultralytics/models/sam/model.py</code> <pre><code>def __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\nreturn self.predict(source, stream, bboxes, points, labels, **kwargs)\n</code></pre>"},{"location":"reference/models/sam/model/#ultralytics.models.sam.model.SAM.info","title":"<code>info(detailed=False, verbose=True)</code>","text":"<p>Logs model info.</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>Show detailed information about model.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Controls verbosity.</p> <code>True</code> Source code in <code>ultralytics/models/sam/model.py</code> <pre><code>def info(self, detailed=False, verbose=True):\n\"\"\"\n    Logs model info.\n    Args:\n        detailed (bool): Show detailed information about model.\n        verbose (bool): Controls verbosity.\n    \"\"\"\nreturn model_info(self.model, detailed=detailed, verbose=verbose)\n</code></pre>"},{"location":"reference/models/sam/model/#ultralytics.models.sam.model.SAM.predict","title":"<code>predict(source, stream=False, bboxes=None, points=None, labels=None, **kwargs)</code>","text":"<p>Predicts and returns segmentation masks for given image or video source.</p> Source code in <code>ultralytics/models/sam/model.py</code> <pre><code>def predict(self, source, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n\"\"\"Predicts and returns segmentation masks for given image or video source.\"\"\"\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024)\nkwargs.update(overrides)\nprompts = dict(bboxes=bboxes, points=points, labels=labels)\nreturn super().predict(source, stream, prompts=prompts, **kwargs)\n</code></pre>"},{"location":"reference/models/sam/predict/","title":"Reference for <code>ultralytics/models/sam/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor","title":"<code>ultralytics.models.sam.predict.Predictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>class Predictor(BasePredictor):\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\nif overrides is None:\noverrides = {}\noverrides.update(dict(task='segment', mode='predict', imgsz=1024))\nsuper().__init__(cfg, overrides, _callbacks)\n# SAM needs retina_masks=True, or the results would be a mess.\nself.args.retina_masks = True\n# Args for set_image\nself.im = None\nself.features = None\n# Args for set_prompts\nself.prompts = {}\n# Args for segment everything\nself.segment_all = False\ndef preprocess(self, im):\n\"\"\"Prepares input image before inference.\n        Args:\n            im (torch.Tensor | List(np.ndarray)): BCHW for tensor, [(HWC) x B] for list.\n        \"\"\"\nif self.im is not None:\nreturn self.im\nnot_tensor = not isinstance(im, torch.Tensor)\nif not_tensor:\nim = np.stack(self.pre_transform(im))\nim = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\nim = np.ascontiguousarray(im)  # contiguous\nim = torch.from_numpy(im)\nimg = im.to(self.device)\nimg = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\nif not_tensor:\nimg = (img - self.mean) / self.std\nreturn img\ndef pre_transform(self, im):\n\"\"\"\n        Pre-transform input image before inference.\n        Args:\n            im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n        Returns:\n            (list): A list of transformed images.\n        \"\"\"\nassert len(im) == 1, 'SAM model has not supported batch inference yet!'\nreturn [LetterBox(self.args.imgsz, auto=False, center=False)(image=x) for x in im]\ndef inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False, *args, **kwargs):\n\"\"\"\n        Predict masks for the given input prompts, using the currently set image.\n        Args:\n            im (torch.Tensor): The preprocessed image, (N, C, H, W).\n            bboxes (np.ndarray | List, None): (N, 4), in XYXY format.\n            points (np.ndarray | List, None): (N, 2), Each point is in (X,Y) in pixels.\n            labels (np.ndarray | List, None): (N, ), labels for the point prompts.\n                1 indicates a foreground point and 0 indicates a background point.\n            masks (np.ndarray, None): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form (N, H, W), where\n                for SAM, H=W=256.\n            multimask_output (bool): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n        Returns:\n            (np.ndarray): The output masks in CxHxW format, where C is the\n                number of masks, and (H, W) is the original image size.\n            (np.ndarray): An array of length C containing the model's\n                predictions for the quality of each mask.\n            (np.ndarray): An array of shape CxHxW, where C is the number\n                of masks and H=W=256. These low resolution logits can be passed to\n                a subsequent iteration as mask input.\n        \"\"\"\n# Get prompts from self.prompts first\nbboxes = self.prompts.pop('bboxes', bboxes)\npoints = self.prompts.pop('points', points)\nmasks = self.prompts.pop('masks', masks)\nif all(i is None for i in [bboxes, points, masks]):\nreturn self.generate(im, *args, **kwargs)\nreturn self.prompt_inference(im, bboxes, points, labels, masks, multimask_output)\ndef prompt_inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False):\n\"\"\"\n        Predict masks for the given input prompts, using the currently set image.\n        Args:\n            im (torch.Tensor): The preprocessed image, (N, C, H, W).\n            bboxes (np.ndarray | List, None): (N, 4), in XYXY format.\n            points (np.ndarray | List, None): (N, 2), Each point is in (X,Y) in pixels.\n            labels (np.ndarray | List, None): (N, ), labels for the point prompts.\n                1 indicates a foreground point and 0 indicates a background point.\n            masks (np.ndarray, None): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form (N, H, W), where\n                for SAM, H=W=256.\n            multimask_output (bool): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n        Returns:\n            (np.ndarray): The output masks in CxHxW format, where C is the\n                number of masks, and (H, W) is the original image size.\n            (np.ndarray): An array of length C containing the model's\n                predictions for the quality of each mask.\n            (np.ndarray): An array of shape CxHxW, where C is the number\n                of masks and H=W=256. These low resolution logits can be passed to\n                a subsequent iteration as mask input.\n        \"\"\"\nfeatures = self.model.image_encoder(im) if self.features is None else self.features\nsrc_shape, dst_shape = self.batch[1][0].shape[:2], im.shape[2:]\nr = 1.0 if self.segment_all else min(dst_shape[0] / src_shape[0], dst_shape[1] / src_shape[1])\n# Transform input prompts\nif points is not None:\npoints = torch.as_tensor(points, dtype=torch.float32, device=self.device)\npoints = points[None] if points.ndim == 1 else points\n# Assuming labels are all positive if users don't pass labels.\nif labels is None:\nlabels = np.ones(points.shape[0])\nlabels = torch.as_tensor(labels, dtype=torch.int32, device=self.device)\npoints *= r\n# (N, 2) --&gt; (N, 1, 2), (N, ) --&gt; (N, 1)\npoints, labels = points[:, None, :], labels[:, None]\nif bboxes is not None:\nbboxes = torch.as_tensor(bboxes, dtype=torch.float32, device=self.device)\nbboxes = bboxes[None] if bboxes.ndim == 1 else bboxes\nbboxes *= r\nif masks is not None:\nmasks = torch.as_tensor(masks, dtype=torch.float32, device=self.device)\nmasks = masks[:, None, :, :]\npoints = (points, labels) if points is not None else None\n# Embed prompts\nsparse_embeddings, dense_embeddings = self.model.prompt_encoder(\npoints=points,\nboxes=bboxes,\nmasks=masks,\n)\n# Predict masks\npred_masks, pred_scores = self.model.mask_decoder(\nimage_embeddings=features,\nimage_pe=self.model.prompt_encoder.get_dense_pe(),\nsparse_prompt_embeddings=sparse_embeddings,\ndense_prompt_embeddings=dense_embeddings,\nmultimask_output=multimask_output,\n)\n# (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )\n# `d` could be 1 or 3 depends on `multimask_output`.\nreturn pred_masks.flatten(0, 1), pred_scores.flatten(0, 1)\ndef generate(self,\nim,\ncrop_n_layers=0,\ncrop_overlap_ratio=512 / 1500,\ncrop_downscale_factor=1,\npoint_grids=None,\npoints_stride=32,\npoints_batch_size=64,\nconf_thres=0.88,\nstability_score_thresh=0.95,\nstability_score_offset=0.95,\ncrop_nms_thresh=0.7):\n\"\"\"Segment the whole image.\n        Args:\n            im (torch.Tensor): The preprocessed image, (N, C, H, W).\n            crop_n_layers (int): If &gt;0, mask prediction will be run again on\n                crops of the image. Sets the number of layers to run, where each\n                layer has 2**i_layer number of image crops.\n            crop_overlap_ratio (float): Sets the degree to which crops overlap.\n                In the first crop layer, crops will overlap by this fraction of\n                the image length. Later layers with more crops scale down this overlap.\n            crop_downscale_factor (int): The number of points-per-side\n                sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n            point_grids (list(np.ndarray), None): A list over explicit grids\n                of points used for sampling, normalized to [0,1]. The nth grid in the\n                list is used in the nth crop layer. Exclusive with points_per_side.\n            points_stride (int, None): The number of points to be sampled\n                along one side of the image. The total number of points is\n                points_per_side**2. If None, 'point_grids' must provide explicit\n                point sampling.\n            points_batch_size (int): Sets the number of points run simultaneously\n                by the model. Higher numbers may be faster but use more GPU memory.\n            conf_thres (float): A filtering threshold in [0,1], using the\n                model's predicted mask quality.\n            stability_score_thresh (float): A filtering threshold in [0,1], using\n                the stability of the mask under changes to the cutoff used to binarize\n                the model's mask predictions.\n            stability_score_offset (float): The amount to shift the cutoff when\n                calculated the stability score.\n            crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n                suppression to filter duplicate masks between different crops.\n        \"\"\"\nself.segment_all = True\nih, iw = im.shape[2:]\ncrop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)\nif point_grids is None:\npoint_grids = build_all_layer_point_grids(\npoints_stride,\ncrop_n_layers,\ncrop_downscale_factor,\n)\npred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []\nfor crop_region, layer_idx in zip(crop_regions, layer_idxs):\nx1, y1, x2, y2 = crop_region\nw, h = x2 - x1, y2 - y1\narea = torch.tensor(w * h, device=im.device)\npoints_scale = np.array([[w, h]])  # w, h\n# Crop image and interpolate to input size\ncrop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode='bilinear', align_corners=False)\n# (num_points, 2)\npoints_for_image = point_grids[layer_idx] * points_scale\ncrop_masks, crop_scores, crop_bboxes = [], [], []\nfor (points, ) in batch_iterator(points_batch_size, points_for_image):\npred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)\n# Interpolate predicted masks to input size\npred_mask = F.interpolate(pred_mask[None], (h, w), mode='bilinear', align_corners=False)[0]\nidx = pred_score &gt; conf_thres\npred_mask, pred_score = pred_mask[idx], pred_score[idx]\nstability_score = calculate_stability_score(pred_mask, self.model.mask_threshold,\nstability_score_offset)\nidx = stability_score &gt; stability_score_thresh\npred_mask, pred_score = pred_mask[idx], pred_score[idx]\n# Bool type is much more memory-efficient.\npred_mask = pred_mask &gt; self.model.mask_threshold\n# (N, 4)\npred_bbox = batched_mask_to_box(pred_mask).float()\nkeep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])\nif not torch.all(keep_mask):\npred_bbox = pred_bbox[keep_mask]\npred_mask = pred_mask[keep_mask]\npred_score = pred_score[keep_mask]\ncrop_masks.append(pred_mask)\ncrop_bboxes.append(pred_bbox)\ncrop_scores.append(pred_score)\n# Do nms within this crop\ncrop_masks = torch.cat(crop_masks)\ncrop_bboxes = torch.cat(crop_bboxes)\ncrop_scores = torch.cat(crop_scores)\nkeep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS\ncrop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)\ncrop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)\ncrop_scores = crop_scores[keep]\npred_masks.append(crop_masks)\npred_bboxes.append(crop_bboxes)\npred_scores.append(crop_scores)\nregion_areas.append(area.expand(len(crop_masks)))\npred_masks = torch.cat(pred_masks)\npred_bboxes = torch.cat(pred_bboxes)\npred_scores = torch.cat(pred_scores)\nregion_areas = torch.cat(region_areas)\n# Remove duplicate masks between crops\nif len(crop_regions) &gt; 1:\nscores = 1 / region_areas\nkeep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)\npred_masks = pred_masks[keep]\npred_bboxes = pred_bboxes[keep]\npred_scores = pred_scores[keep]\nreturn pred_masks, pred_scores, pred_bboxes\ndef setup_model(self, model, verbose=True):\n\"\"\"Set up YOLO model with specified thresholds and device.\"\"\"\ndevice = select_device(self.args.device, verbose=verbose)\nif model is None:\nmodel = build_sam(self.args.model)\nmodel.eval()\nself.model = model.to(device)\nself.device = device\nself.mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1).to(device)\nself.std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1).to(device)\n# TODO: Temporary settings for compatibility\nself.model.pt = False\nself.model.triton = False\nself.model.stride = 32\nself.model.fp16 = False\nself.done_warmup = True\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes inference output predictions to create detection masks for objects.\"\"\"\n# (N, 1, H, W), (N, 1)\npred_masks, pred_scores = preds[:2]\npred_bboxes = preds[2] if self.segment_all else None\nnames = dict(enumerate(str(i) for i in range(len(pred_masks))))\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, masks in enumerate([pred_masks]):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif pred_bboxes is not None:\npred_bboxes = ops.scale_boxes(img.shape[2:], pred_bboxes.float(), orig_img.shape, padding=False)\ncls = torch.arange(len(pred_masks), dtype=torch.int32, device=pred_masks.device)\npred_bboxes = torch.cat([pred_bboxes, pred_scores[:, None], cls[:, None]], dim=-1)\nmasks = ops.scale_masks(masks[None].float(), orig_img.shape[:2], padding=False)[0]\nmasks = masks &gt; self.model.mask_threshold  # to bool\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=names, masks=masks, boxes=pred_bboxes))\n# Reset segment-all mode.\nself.segment_all = False\nreturn results\ndef setup_source(self, source):\n\"\"\"Sets up source and inference mode.\"\"\"\nif source is not None:\nsuper().setup_source(source)\ndef set_image(self, image):\n\"\"\"Set image in advance.\n        Args:\n            image (str | np.ndarray): image file path or np.ndarray image by cv2.\n        \"\"\"\nif self.model is None:\nmodel = build_sam(self.args.model)\nself.setup_model(model)\nself.setup_source(image)\nassert len(self.dataset) == 1, '`set_image` only supports setting one image!'\nfor batch in self.dataset:\nim = self.preprocess(batch[1])\nself.features = self.model.image_encoder(im)\nself.im = im\nbreak\ndef set_prompts(self, prompts):\n\"\"\"Set prompts in advance.\"\"\"\nself.prompts = prompts\ndef reset_image(self):\nself.im = None\nself.features = None\n@staticmethod\ndef remove_small_regions(masks, min_area=0, nms_thresh=0.7):\n\"\"\"\n        Removes small disconnected regions and holes in masks, then reruns\n        box NMS to remove any new duplicates. Requires open-cv as a dependency.\n        Args:\n            masks (torch.Tensor): Masks, (N, H, W).\n            min_area (int): Minimum area threshold.\n            nms_thresh (float): NMS threshold.\n        \"\"\"\nif len(masks) == 0:\nreturn masks\n# Filter small disconnected regions and holes\nnew_masks = []\nscores = []\nfor mask in masks:\nmask = mask.cpu().numpy()\nmask, changed = remove_small_regions(mask, min_area, mode='holes')\nunchanged = not changed\nmask, changed = remove_small_regions(mask, min_area, mode='islands')\nunchanged = unchanged and not changed\nnew_masks.append(torch.as_tensor(mask).unsqueeze(0))\n# Give score=0 to changed masks and score=1 to unchanged masks\n# so NMS will prefer ones that didn't need postprocessing\nscores.append(float(unchanged))\n# Recalculate boxes and remove any new duplicates\nnew_masks = torch.cat(new_masks, dim=0)\nboxes = batched_mask_to_box(new_masks)\nkeep = torchvision.ops.nms(\nboxes.float(),\ntorch.as_tensor(scores),\nnms_thresh,\n)\n# Only recalculate masks for masks that have changed\nfor i in keep:\nif scores[i] == 0.0:\nmasks[i] = new_masks[i]\nreturn masks[keep]\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.generate","title":"<code>generate(im, crop_n_layers=0, crop_overlap_ratio=512 / 1500, crop_downscale_factor=1, point_grids=None, points_stride=32, points_batch_size=64, conf_thres=0.88, stability_score_thresh=0.95, stability_score_offset=0.95, crop_nms_thresh=0.7)</code>","text":"<p>Segment the whole image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Tensor</code> <p>The preprocessed image, (N, C, H, W).</p> required <code>crop_n_layers</code> <code>int</code> <p>If &gt;0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.</p> <code>0</code> <code>crop_overlap_ratio</code> <code>float</code> <p>Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.</p> <code>512 / 1500</code> <code>crop_downscale_factor</code> <code>int</code> <p>The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.</p> <code>1</code> <code>point_grids</code> <code>(list(ndarray), None)</code> <p>A list over explicit grids of points used for sampling, normalized to [0,1]. The nth grid in the list is used in the nth crop layer. Exclusive with points_per_side.</p> <code>None</code> <code>points_stride</code> <code>(int, None)</code> <p>The number of points to be sampled along one side of the image. The total number of points is points_per_side**2. If None, 'point_grids' must provide explicit point sampling.</p> <code>32</code> <code>points_batch_size</code> <code>int</code> <p>Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.</p> <code>64</code> <code>conf_thres</code> <code>float</code> <p>A filtering threshold in [0,1], using the model's predicted mask quality.</p> <code>0.88</code> <code>stability_score_thresh</code> <code>float</code> <p>A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.</p> <code>0.95</code> <code>stability_score_offset</code> <code>float</code> <p>The amount to shift the cutoff when calculated the stability score.</p> <code>0.95</code> <code>crop_nms_thresh</code> <code>float</code> <p>The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.</p> <code>0.7</code> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def generate(self,\nim,\ncrop_n_layers=0,\ncrop_overlap_ratio=512 / 1500,\ncrop_downscale_factor=1,\npoint_grids=None,\npoints_stride=32,\npoints_batch_size=64,\nconf_thres=0.88,\nstability_score_thresh=0.95,\nstability_score_offset=0.95,\ncrop_nms_thresh=0.7):\n\"\"\"Segment the whole image.\n    Args:\n        im (torch.Tensor): The preprocessed image, (N, C, H, W).\n        crop_n_layers (int): If &gt;0, mask prediction will be run again on\n            crops of the image. Sets the number of layers to run, where each\n            layer has 2**i_layer number of image crops.\n        crop_overlap_ratio (float): Sets the degree to which crops overlap.\n            In the first crop layer, crops will overlap by this fraction of\n            the image length. Later layers with more crops scale down this overlap.\n        crop_downscale_factor (int): The number of points-per-side\n            sampled in layer n is scaled down by crop_n_points_downscale_factor**n.\n        point_grids (list(np.ndarray), None): A list over explicit grids\n            of points used for sampling, normalized to [0,1]. The nth grid in the\n            list is used in the nth crop layer. Exclusive with points_per_side.\n        points_stride (int, None): The number of points to be sampled\n            along one side of the image. The total number of points is\n            points_per_side**2. If None, 'point_grids' must provide explicit\n            point sampling.\n        points_batch_size (int): Sets the number of points run simultaneously\n            by the model. Higher numbers may be faster but use more GPU memory.\n        conf_thres (float): A filtering threshold in [0,1], using the\n            model's predicted mask quality.\n        stability_score_thresh (float): A filtering threshold in [0,1], using\n            the stability of the mask under changes to the cutoff used to binarize\n            the model's mask predictions.\n        stability_score_offset (float): The amount to shift the cutoff when\n            calculated the stability score.\n        crop_nms_thresh (float): The box IoU cutoff used by non-maximal\n            suppression to filter duplicate masks between different crops.\n    \"\"\"\nself.segment_all = True\nih, iw = im.shape[2:]\ncrop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)\nif point_grids is None:\npoint_grids = build_all_layer_point_grids(\npoints_stride,\ncrop_n_layers,\ncrop_downscale_factor,\n)\npred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []\nfor crop_region, layer_idx in zip(crop_regions, layer_idxs):\nx1, y1, x2, y2 = crop_region\nw, h = x2 - x1, y2 - y1\narea = torch.tensor(w * h, device=im.device)\npoints_scale = np.array([[w, h]])  # w, h\n# Crop image and interpolate to input size\ncrop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode='bilinear', align_corners=False)\n# (num_points, 2)\npoints_for_image = point_grids[layer_idx] * points_scale\ncrop_masks, crop_scores, crop_bboxes = [], [], []\nfor (points, ) in batch_iterator(points_batch_size, points_for_image):\npred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)\n# Interpolate predicted masks to input size\npred_mask = F.interpolate(pred_mask[None], (h, w), mode='bilinear', align_corners=False)[0]\nidx = pred_score &gt; conf_thres\npred_mask, pred_score = pred_mask[idx], pred_score[idx]\nstability_score = calculate_stability_score(pred_mask, self.model.mask_threshold,\nstability_score_offset)\nidx = stability_score &gt; stability_score_thresh\npred_mask, pred_score = pred_mask[idx], pred_score[idx]\n# Bool type is much more memory-efficient.\npred_mask = pred_mask &gt; self.model.mask_threshold\n# (N, 4)\npred_bbox = batched_mask_to_box(pred_mask).float()\nkeep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])\nif not torch.all(keep_mask):\npred_bbox = pred_bbox[keep_mask]\npred_mask = pred_mask[keep_mask]\npred_score = pred_score[keep_mask]\ncrop_masks.append(pred_mask)\ncrop_bboxes.append(pred_bbox)\ncrop_scores.append(pred_score)\n# Do nms within this crop\ncrop_masks = torch.cat(crop_masks)\ncrop_bboxes = torch.cat(crop_bboxes)\ncrop_scores = torch.cat(crop_scores)\nkeep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS\ncrop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)\ncrop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)\ncrop_scores = crop_scores[keep]\npred_masks.append(crop_masks)\npred_bboxes.append(crop_bboxes)\npred_scores.append(crop_scores)\nregion_areas.append(area.expand(len(crop_masks)))\npred_masks = torch.cat(pred_masks)\npred_bboxes = torch.cat(pred_bboxes)\npred_scores = torch.cat(pred_scores)\nregion_areas = torch.cat(region_areas)\n# Remove duplicate masks between crops\nif len(crop_regions) &gt; 1:\nscores = 1 / region_areas\nkeep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)\npred_masks = pred_masks[keep]\npred_bboxes = pred_bboxes[keep]\npred_scores = pred_scores[keep]\nreturn pred_masks, pred_scores, pred_bboxes\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.inference","title":"<code>inference(im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False, *args, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Tensor</code> <p>The preprocessed image, (N, C, H, W).</p> required <code>bboxes</code> <code>(ndarray | List, None)</code> <p>(N, 4), in XYXY format.</p> <code>None</code> <code>points</code> <code>(ndarray | List, None)</code> <p>(N, 2), Each point is in (X,Y) in pixels.</p> <code>None</code> <code>labels</code> <code>(ndarray | List, None)</code> <p>(N, ), labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>masks</code> <code>(ndarray, None)</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form (N, H, W), where for SAM, H=W=256.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The output masks in CxHxW format, where C is the number of masks, and (H, W) is the original image size.</p> <code>ndarray</code> <p>An array of length C containing the model's predictions for the quality of each mask.</p> <code>ndarray</code> <p>An array of shape CxHxW, where C is the number of masks and H=W=256. These low resolution logits can be passed to a subsequent iteration as mask input.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False, *args, **kwargs):\n\"\"\"\n    Predict masks for the given input prompts, using the currently set image.\n    Args:\n        im (torch.Tensor): The preprocessed image, (N, C, H, W).\n        bboxes (np.ndarray | List, None): (N, 4), in XYXY format.\n        points (np.ndarray | List, None): (N, 2), Each point is in (X,Y) in pixels.\n        labels (np.ndarray | List, None): (N, ), labels for the point prompts.\n            1 indicates a foreground point and 0 indicates a background point.\n        masks (np.ndarray, None): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form (N, H, W), where\n            for SAM, H=W=256.\n        multimask_output (bool): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n    Returns:\n        (np.ndarray): The output masks in CxHxW format, where C is the\n            number of masks, and (H, W) is the original image size.\n        (np.ndarray): An array of length C containing the model's\n            predictions for the quality of each mask.\n        (np.ndarray): An array of shape CxHxW, where C is the number\n            of masks and H=W=256. These low resolution logits can be passed to\n            a subsequent iteration as mask input.\n    \"\"\"\n# Get prompts from self.prompts first\nbboxes = self.prompts.pop('bboxes', bboxes)\npoints = self.prompts.pop('points', points)\nmasks = self.prompts.pop('masks', masks)\nif all(i is None for i in [bboxes, points, masks]):\nreturn self.generate(im, *args, **kwargs)\nreturn self.prompt_inference(im, bboxes, points, labels, masks, multimask_output)\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Post-processes inference output predictions to create detection masks for objects.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes inference output predictions to create detection masks for objects.\"\"\"\n# (N, 1, H, W), (N, 1)\npred_masks, pred_scores = preds[:2]\npred_bboxes = preds[2] if self.segment_all else None\nnames = dict(enumerate(str(i) for i in range(len(pred_masks))))\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, masks in enumerate([pred_masks]):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif pred_bboxes is not None:\npred_bboxes = ops.scale_boxes(img.shape[2:], pred_bboxes.float(), orig_img.shape, padding=False)\ncls = torch.arange(len(pred_masks), dtype=torch.int32, device=pred_masks.device)\npred_bboxes = torch.cat([pred_bboxes, pred_scores[:, None], cls[:, None]], dim=-1)\nmasks = ops.scale_masks(masks[None].float(), orig_img.shape[:2], padding=False)[0]\nmasks = masks &gt; self.model.mask_threshold  # to bool\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=names, masks=masks, boxes=pred_bboxes))\n# Reset segment-all mode.\nself.segment_all = False\nreturn results\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.pre_transform","title":"<code>pre_transform(im)</code>","text":"<p>Pre-transform input image before inference.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>List(np.ndarray</code> <p>(N, 3, h, w) for tensor, [(h, w, 3) x N] for list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of transformed images.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def pre_transform(self, im):\n\"\"\"\n    Pre-transform input image before inference.\n    Args:\n        im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n    Returns:\n        (list): A list of transformed images.\n    \"\"\"\nassert len(im) == 1, 'SAM model has not supported batch inference yet!'\nreturn [LetterBox(self.args.imgsz, auto=False, center=False)(image=x) for x in im]\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.preprocess","title":"<code>preprocess(im)</code>","text":"<p>Prepares input image before inference.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>torch.Tensor | List(np.ndarray</code> <p>BCHW for tensor, [(HWC) x B] for list.</p> required Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def preprocess(self, im):\n\"\"\"Prepares input image before inference.\n    Args:\n        im (torch.Tensor | List(np.ndarray)): BCHW for tensor, [(HWC) x B] for list.\n    \"\"\"\nif self.im is not None:\nreturn self.im\nnot_tensor = not isinstance(im, torch.Tensor)\nif not_tensor:\nim = np.stack(self.pre_transform(im))\nim = im[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW, (n, 3, h, w)\nim = np.ascontiguousarray(im)  # contiguous\nim = torch.from_numpy(im)\nimg = im.to(self.device)\nimg = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\nif not_tensor:\nimg = (img - self.mean) / self.std\nreturn img\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.prompt_inference","title":"<code>prompt_inference(im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Tensor</code> <p>The preprocessed image, (N, C, H, W).</p> required <code>bboxes</code> <code>(ndarray | List, None)</code> <p>(N, 4), in XYXY format.</p> <code>None</code> <code>points</code> <code>(ndarray | List, None)</code> <p>(N, 2), Each point is in (X,Y) in pixels.</p> <code>None</code> <code>labels</code> <code>(ndarray | List, None)</code> <p>(N, ), labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>masks</code> <code>(ndarray, None)</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form (N, H, W), where for SAM, H=W=256.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The output masks in CxHxW format, where C is the number of masks, and (H, W) is the original image size.</p> <code>ndarray</code> <p>An array of length C containing the model's predictions for the quality of each mask.</p> <code>ndarray</code> <p>An array of shape CxHxW, where C is the number of masks and H=W=256. These low resolution logits can be passed to a subsequent iteration as mask input.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def prompt_inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False):\n\"\"\"\n    Predict masks for the given input prompts, using the currently set image.\n    Args:\n        im (torch.Tensor): The preprocessed image, (N, C, H, W).\n        bboxes (np.ndarray | List, None): (N, 4), in XYXY format.\n        points (np.ndarray | List, None): (N, 2), Each point is in (X,Y) in pixels.\n        labels (np.ndarray | List, None): (N, ), labels for the point prompts.\n            1 indicates a foreground point and 0 indicates a background point.\n        masks (np.ndarray, None): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form (N, H, W), where\n            for SAM, H=W=256.\n        multimask_output (bool): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n    Returns:\n        (np.ndarray): The output masks in CxHxW format, where C is the\n            number of masks, and (H, W) is the original image size.\n        (np.ndarray): An array of length C containing the model's\n            predictions for the quality of each mask.\n        (np.ndarray): An array of shape CxHxW, where C is the number\n            of masks and H=W=256. These low resolution logits can be passed to\n            a subsequent iteration as mask input.\n    \"\"\"\nfeatures = self.model.image_encoder(im) if self.features is None else self.features\nsrc_shape, dst_shape = self.batch[1][0].shape[:2], im.shape[2:]\nr = 1.0 if self.segment_all else min(dst_shape[0] / src_shape[0], dst_shape[1] / src_shape[1])\n# Transform input prompts\nif points is not None:\npoints = torch.as_tensor(points, dtype=torch.float32, device=self.device)\npoints = points[None] if points.ndim == 1 else points\n# Assuming labels are all positive if users don't pass labels.\nif labels is None:\nlabels = np.ones(points.shape[0])\nlabels = torch.as_tensor(labels, dtype=torch.int32, device=self.device)\npoints *= r\n# (N, 2) --&gt; (N, 1, 2), (N, ) --&gt; (N, 1)\npoints, labels = points[:, None, :], labels[:, None]\nif bboxes is not None:\nbboxes = torch.as_tensor(bboxes, dtype=torch.float32, device=self.device)\nbboxes = bboxes[None] if bboxes.ndim == 1 else bboxes\nbboxes *= r\nif masks is not None:\nmasks = torch.as_tensor(masks, dtype=torch.float32, device=self.device)\nmasks = masks[:, None, :, :]\npoints = (points, labels) if points is not None else None\n# Embed prompts\nsparse_embeddings, dense_embeddings = self.model.prompt_encoder(\npoints=points,\nboxes=bboxes,\nmasks=masks,\n)\n# Predict masks\npred_masks, pred_scores = self.model.mask_decoder(\nimage_embeddings=features,\nimage_pe=self.model.prompt_encoder.get_dense_pe(),\nsparse_prompt_embeddings=sparse_embeddings,\ndense_prompt_embeddings=dense_embeddings,\nmultimask_output=multimask_output,\n)\n# (N, d, H, W) --&gt; (N*d, H, W), (N, d) --&gt; (N*d, )\n# `d` could be 1 or 3 depends on `multimask_output`.\nreturn pred_masks.flatten(0, 1), pred_scores.flatten(0, 1)\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.remove_small_regions","title":"<code>remove_small_regions(masks, min_area=0, nms_thresh=0.7)</code>  <code>staticmethod</code>","text":"<p>Removes small disconnected regions and holes in masks, then reruns box NMS to remove any new duplicates. Requires open-cv as a dependency.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Tensor</code> <p>Masks, (N, H, W).</p> required <code>min_area</code> <code>int</code> <p>Minimum area threshold.</p> <code>0</code> <code>nms_thresh</code> <code>float</code> <p>NMS threshold.</p> <code>0.7</code> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>@staticmethod\ndef remove_small_regions(masks, min_area=0, nms_thresh=0.7):\n\"\"\"\n    Removes small disconnected regions and holes in masks, then reruns\n    box NMS to remove any new duplicates. Requires open-cv as a dependency.\n    Args:\n        masks (torch.Tensor): Masks, (N, H, W).\n        min_area (int): Minimum area threshold.\n        nms_thresh (float): NMS threshold.\n    \"\"\"\nif len(masks) == 0:\nreturn masks\n# Filter small disconnected regions and holes\nnew_masks = []\nscores = []\nfor mask in masks:\nmask = mask.cpu().numpy()\nmask, changed = remove_small_regions(mask, min_area, mode='holes')\nunchanged = not changed\nmask, changed = remove_small_regions(mask, min_area, mode='islands')\nunchanged = unchanged and not changed\nnew_masks.append(torch.as_tensor(mask).unsqueeze(0))\n# Give score=0 to changed masks and score=1 to unchanged masks\n# so NMS will prefer ones that didn't need postprocessing\nscores.append(float(unchanged))\n# Recalculate boxes and remove any new duplicates\nnew_masks = torch.cat(new_masks, dim=0)\nboxes = batched_mask_to_box(new_masks)\nkeep = torchvision.ops.nms(\nboxes.float(),\ntorch.as_tensor(scores),\nnms_thresh,\n)\n# Only recalculate masks for masks that have changed\nfor i in keep:\nif scores[i] == 0.0:\nmasks[i] = new_masks[i]\nreturn masks[keep]\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.set_image","title":"<code>set_image(image)</code>","text":"<p>Set image in advance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ndarray</code> <p>image file path or np.ndarray image by cv2.</p> required Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def set_image(self, image):\n\"\"\"Set image in advance.\n    Args:\n        image (str | np.ndarray): image file path or np.ndarray image by cv2.\n    \"\"\"\nif self.model is None:\nmodel = build_sam(self.args.model)\nself.setup_model(model)\nself.setup_source(image)\nassert len(self.dataset) == 1, '`set_image` only supports setting one image!'\nfor batch in self.dataset:\nim = self.preprocess(batch[1])\nself.features = self.model.image_encoder(im)\nself.im = im\nbreak\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.set_prompts","title":"<code>set_prompts(prompts)</code>","text":"<p>Set prompts in advance.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def set_prompts(self, prompts):\n\"\"\"Set prompts in advance.\"\"\"\nself.prompts = prompts\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.setup_model","title":"<code>setup_model(model, verbose=True)</code>","text":"<p>Set up YOLO model with specified thresholds and device.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def setup_model(self, model, verbose=True):\n\"\"\"Set up YOLO model with specified thresholds and device.\"\"\"\ndevice = select_device(self.args.device, verbose=verbose)\nif model is None:\nmodel = build_sam(self.args.model)\nmodel.eval()\nself.model = model.to(device)\nself.device = device\nself.mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1).to(device)\nself.std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1).to(device)\n# TODO: Temporary settings for compatibility\nself.model.pt = False\nself.model.triton = False\nself.model.stride = 32\nself.model.fp16 = False\nself.done_warmup = True\n</code></pre>"},{"location":"reference/models/sam/predict/#ultralytics.models.sam.predict.Predictor.setup_source","title":"<code>setup_source(source)</code>","text":"<p>Sets up source and inference mode.</p> Source code in <code>ultralytics/models/sam/predict.py</code> <pre><code>def setup_source(self, source):\n\"\"\"Sets up source and inference mode.\"\"\"\nif source is not None:\nsuper().setup_source(source)\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/","title":"Reference for <code>ultralytics/models/sam/modules/decoders.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/decoders.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MaskDecoder","title":"<code>ultralytics.models.sam.modules.decoders.MaskDecoder</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>class MaskDecoder(nn.Module):\ndef __init__(\nself,\n*,\ntransformer_dim: int,\ntransformer: nn.Module,\nnum_multimask_outputs: int = 3,\nactivation: Type[nn.Module] = nn.GELU,\niou_head_depth: int = 3,\niou_head_hidden_dim: int = 256,\n) -&gt; None:\n\"\"\"\n        Predicts masks given an image and prompt embeddings, using a transformer architecture.\n        Args:\n            transformer_dim (int): the channel dimension of the transformer module\n            transformer (nn.Module): the transformer used to predict masks\n            num_multimask_outputs (int): the number of masks to predict when disambiguating masks\n            activation (nn.Module): the type of activation to use when upscaling masks\n            iou_head_depth (int): the depth of the MLP used to predict mask quality\n            iou_head_hidden_dim (int): the hidden dimension of the MLP used to predict mask quality\n        \"\"\"\nsuper().__init__()\nself.transformer_dim = transformer_dim\nself.transformer = transformer\nself.num_multimask_outputs = num_multimask_outputs\nself.iou_token = nn.Embedding(1, transformer_dim)\nself.num_mask_tokens = num_multimask_outputs + 1\nself.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\nself.output_upscaling = nn.Sequential(\nnn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\nLayerNorm2d(transformer_dim // 4),\nactivation(),\nnn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\nactivation(),\n)\nself.output_hypernetworks_mlps = nn.ModuleList([\nMLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for _ in range(self.num_mask_tokens)])\nself.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)\ndef forward(\nself,\nimage_embeddings: torch.Tensor,\nimage_pe: torch.Tensor,\nsparse_prompt_embeddings: torch.Tensor,\ndense_prompt_embeddings: torch.Tensor,\nmultimask_output: bool,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"\n        Predict masks given image and prompt embeddings.\n        Args:\n            image_embeddings (torch.Tensor): the embeddings from the image encoder\n            image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n            sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n            dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n            multimask_output (bool): Whether to return multiple masks or a single mask.\n        Returns:\n            torch.Tensor: batched predicted masks\n            torch.Tensor: batched predictions of mask quality\n        \"\"\"\nmasks, iou_pred = self.predict_masks(\nimage_embeddings=image_embeddings,\nimage_pe=image_pe,\nsparse_prompt_embeddings=sparse_prompt_embeddings,\ndense_prompt_embeddings=dense_prompt_embeddings,\n)\n# Select the correct mask or masks for output\nmask_slice = slice(1, None) if multimask_output else slice(0, 1)\nmasks = masks[:, mask_slice, :, :]\niou_pred = iou_pred[:, mask_slice]\n# Prepare output\nreturn masks, iou_pred\ndef predict_masks(\nself,\nimage_embeddings: torch.Tensor,\nimage_pe: torch.Tensor,\nsparse_prompt_embeddings: torch.Tensor,\ndense_prompt_embeddings: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n# Concatenate output tokens\noutput_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\noutput_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\ntokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n# Expand per-image data in batch direction to be per-mask\nsrc = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\nsrc = src + dense_prompt_embeddings\npos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\nb, c, h, w = src.shape\n# Run the transformer\nhs, src = self.transformer(src, pos_src, tokens)\niou_token_out = hs[:, 0, :]\nmask_tokens_out = hs[:, 1:(1 + self.num_mask_tokens), :]\n# Upscale mask embeddings and predict masks using the mask tokens\nsrc = src.transpose(1, 2).view(b, c, h, w)\nupscaled_embedding = self.output_upscaling(src)\nhyper_in_list: List[torch.Tensor] = [\nself.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]) for i in range(self.num_mask_tokens)]\nhyper_in = torch.stack(hyper_in_list, dim=1)\nb, c, h, w = upscaled_embedding.shape\nmasks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n# Generate mask quality predictions\niou_pred = self.iou_prediction_head(iou_token_out)\nreturn masks, iou_pred\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MaskDecoder.__init__","title":"<code>__init__(*, transformer_dim, transformer, num_multimask_outputs=3, activation=nn.GELU, iou_head_depth=3, iou_head_hidden_dim=256)</code>","text":"<p>Predicts masks given an image and prompt embeddings, using a transformer architecture.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_dim</code> <code>int</code> <p>the channel dimension of the transformer module</p> required <code>transformer</code> <code>Module</code> <p>the transformer used to predict masks</p> required <code>num_multimask_outputs</code> <code>int</code> <p>the number of masks to predict when disambiguating masks</p> <code>3</code> <code>activation</code> <code>Module</code> <p>the type of activation to use when upscaling masks</p> <code>GELU</code> <code>iou_head_depth</code> <code>int</code> <p>the depth of the MLP used to predict mask quality</p> <code>3</code> <code>iou_head_hidden_dim</code> <code>int</code> <p>the hidden dimension of the MLP used to predict mask quality</p> <code>256</code> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>def __init__(\nself,\n*,\ntransformer_dim: int,\ntransformer: nn.Module,\nnum_multimask_outputs: int = 3,\nactivation: Type[nn.Module] = nn.GELU,\niou_head_depth: int = 3,\niou_head_hidden_dim: int = 256,\n) -&gt; None:\n\"\"\"\n    Predicts masks given an image and prompt embeddings, using a transformer architecture.\n    Args:\n        transformer_dim (int): the channel dimension of the transformer module\n        transformer (nn.Module): the transformer used to predict masks\n        num_multimask_outputs (int): the number of masks to predict when disambiguating masks\n        activation (nn.Module): the type of activation to use when upscaling masks\n        iou_head_depth (int): the depth of the MLP used to predict mask quality\n        iou_head_hidden_dim (int): the hidden dimension of the MLP used to predict mask quality\n    \"\"\"\nsuper().__init__()\nself.transformer_dim = transformer_dim\nself.transformer = transformer\nself.num_multimask_outputs = num_multimask_outputs\nself.iou_token = nn.Embedding(1, transformer_dim)\nself.num_mask_tokens = num_multimask_outputs + 1\nself.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\nself.output_upscaling = nn.Sequential(\nnn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\nLayerNorm2d(transformer_dim // 4),\nactivation(),\nnn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\nactivation(),\n)\nself.output_hypernetworks_mlps = nn.ModuleList([\nMLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for _ in range(self.num_mask_tokens)])\nself.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MaskDecoder.forward","title":"<code>forward(image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)</code>","text":"<p>Predict masks given image and prompt embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image_embeddings</code> <code>Tensor</code> <p>the embeddings from the image encoder</p> required <code>image_pe</code> <code>Tensor</code> <p>positional encoding with the shape of image_embeddings</p> required <code>sparse_prompt_embeddings</code> <code>Tensor</code> <p>the embeddings of the points and boxes</p> required <code>dense_prompt_embeddings</code> <code>Tensor</code> <p>the embeddings of the mask inputs</p> required <code>multimask_output</code> <code>bool</code> <p>Whether to return multiple masks or a single mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: batched predicted masks</p> <code>Tensor</code> <p>torch.Tensor: batched predictions of mask quality</p> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>def forward(\nself,\nimage_embeddings: torch.Tensor,\nimage_pe: torch.Tensor,\nsparse_prompt_embeddings: torch.Tensor,\ndense_prompt_embeddings: torch.Tensor,\nmultimask_output: bool,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"\n    Predict masks given image and prompt embeddings.\n    Args:\n        image_embeddings (torch.Tensor): the embeddings from the image encoder\n        image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n        sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n        dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n        multimask_output (bool): Whether to return multiple masks or a single mask.\n    Returns:\n        torch.Tensor: batched predicted masks\n        torch.Tensor: batched predictions of mask quality\n    \"\"\"\nmasks, iou_pred = self.predict_masks(\nimage_embeddings=image_embeddings,\nimage_pe=image_pe,\nsparse_prompt_embeddings=sparse_prompt_embeddings,\ndense_prompt_embeddings=dense_prompt_embeddings,\n)\n# Select the correct mask or masks for output\nmask_slice = slice(1, None) if multimask_output else slice(0, 1)\nmasks = masks[:, mask_slice, :, :]\niou_pred = iou_pred[:, mask_slice]\n# Prepare output\nreturn masks, iou_pred\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MaskDecoder.predict_masks","title":"<code>predict_masks(image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)</code>","text":"<p>Predicts masks. See 'forward' for more details.</p> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>def predict_masks(\nself,\nimage_embeddings: torch.Tensor,\nimage_pe: torch.Tensor,\nsparse_prompt_embeddings: torch.Tensor,\ndense_prompt_embeddings: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n# Concatenate output tokens\noutput_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\noutput_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\ntokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n# Expand per-image data in batch direction to be per-mask\nsrc = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\nsrc = src + dense_prompt_embeddings\npos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\nb, c, h, w = src.shape\n# Run the transformer\nhs, src = self.transformer(src, pos_src, tokens)\niou_token_out = hs[:, 0, :]\nmask_tokens_out = hs[:, 1:(1 + self.num_mask_tokens), :]\n# Upscale mask embeddings and predict masks using the mask tokens\nsrc = src.transpose(1, 2).view(b, c, h, w)\nupscaled_embedding = self.output_upscaling(src)\nhyper_in_list: List[torch.Tensor] = [\nself.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]) for i in range(self.num_mask_tokens)]\nhyper_in = torch.stack(hyper_in_list, dim=1)\nb, c, h, w = upscaled_embedding.shape\nmasks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n# Generate mask quality predictions\niou_pred = self.iou_prediction_head(iou_token_out)\nreturn masks, iou_pred\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MLP","title":"<code>ultralytics.models.sam.modules.decoders.MLP</code>","text":"<p>             Bases: <code>Module</code></p> <p>Lightly adapted from https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py</p> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>class MLP(nn.Module):\n\"\"\"\n    Lightly adapted from\n    https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py\n    \"\"\"\ndef __init__(\nself,\ninput_dim: int,\nhidden_dim: int,\noutput_dim: int,\nnum_layers: int,\nsigmoid_output: bool = False,\n) -&gt; None:\nsuper().__init__()\nself.num_layers = num_layers\nh = [hidden_dim] * (num_layers - 1)\nself.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\nself.sigmoid_output = sigmoid_output\ndef forward(self, x):\n\"\"\"Executes feedforward within the neural network module and applies activation.\"\"\"\nfor i, layer in enumerate(self.layers):\nx = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\nif self.sigmoid_output:\nx = torch.sigmoid(x)\nreturn x\n</code></pre>"},{"location":"reference/models/sam/modules/decoders/#ultralytics.models.sam.modules.decoders.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Executes feedforward within the neural network module and applies activation.</p> Source code in <code>ultralytics/models/sam/modules/decoders.py</code> <pre><code>def forward(self, x):\n\"\"\"Executes feedforward within the neural network module and applies activation.\"\"\"\nfor i, layer in enumerate(self.layers):\nx = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\nif self.sigmoid_output:\nx = torch.sigmoid(x)\nreturn x\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/","title":"Reference for <code>ultralytics/models/sam/modules/encoders.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/encoders.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.ImageEncoderViT","title":"<code>ultralytics.models.sam.modules.encoders.ImageEncoderViT</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class ImageEncoderViT(nn.Module):\ndef __init__(\nself,\nimg_size: int = 1024,\npatch_size: int = 16,\nin_chans: int = 3,\nembed_dim: int = 768,\ndepth: int = 12,\nnum_heads: int = 12,\nmlp_ratio: float = 4.0,\nout_chans: int = 256,\nqkv_bias: bool = True,\nnorm_layer: Type[nn.Module] = nn.LayerNorm,\nact_layer: Type[nn.Module] = nn.GELU,\nuse_abs_pos: bool = True,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\nwindow_size: int = 0,\nglobal_attn_indexes: Tuple[int, ...] = (),\n) -&gt; None:\n\"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\nsuper().__init__()\nself.img_size = img_size\nself.patch_embed = PatchEmbed(\nkernel_size=(patch_size, patch_size),\nstride=(patch_size, patch_size),\nin_chans=in_chans,\nembed_dim=embed_dim,\n)\nself.pos_embed: Optional[nn.Parameter] = None\nif use_abs_pos:\n# Initialize absolute positional embedding with pretrain image size.\nself.pos_embed = nn.Parameter(torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim))\nself.blocks = nn.ModuleList()\nfor i in range(depth):\nblock = Block(\ndim=embed_dim,\nnum_heads=num_heads,\nmlp_ratio=mlp_ratio,\nqkv_bias=qkv_bias,\nnorm_layer=norm_layer,\nact_layer=act_layer,\nuse_rel_pos=use_rel_pos,\nrel_pos_zero_init=rel_pos_zero_init,\nwindow_size=window_size if i not in global_attn_indexes else 0,\ninput_size=(img_size // patch_size, img_size // patch_size),\n)\nself.blocks.append(block)\nself.neck = nn.Sequential(\nnn.Conv2d(\nembed_dim,\nout_chans,\nkernel_size=1,\nbias=False,\n),\nLayerNorm2d(out_chans),\nnn.Conv2d(\nout_chans,\nout_chans,\nkernel_size=3,\npadding=1,\nbias=False,\n),\nLayerNorm2d(out_chans),\n)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = self.patch_embed(x)\nif self.pos_embed is not None:\nx = x + self.pos_embed\nfor blk in self.blocks:\nx = blk(x)\nreturn self.neck(x.permute(0, 3, 1, 2))\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.ImageEncoderViT.__init__","title":"<code>__init__(img_size=1024, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, out_chans=256, qkv_bias=True, norm_layer=nn.LayerNorm, act_layer=nn.GELU, use_abs_pos=True, use_rel_pos=False, rel_pos_zero_init=True, window_size=0, global_attn_indexes=())</code>","text":"<p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Input image size.</p> <code>1024</code> <code>patch_size</code> <code>int</code> <p>Patch size.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input image channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Patch embedding dimension.</p> <code>768</code> <code>depth</code> <code>int</code> <p>Depth of ViT.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads in each ViT block.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value.</p> <code>True</code> <code>norm_layer</code> <code>Module</code> <p>Normalization layer.</p> <code>LayerNorm</code> <code>act_layer</code> <code>Module</code> <p>Activation layer.</p> <code>GELU</code> <code>use_abs_pos</code> <code>bool</code> <p>If True, use absolute positional embeddings.</p> <code>True</code> <code>use_rel_pos</code> <code>bool</code> <p>If True, add relative positional embeddings to the attention map.</p> <code>False</code> <code>rel_pos_zero_init</code> <code>bool</code> <p>If True, zero initialize relative positional parameters.</p> <code>True</code> <code>window_size</code> <code>int</code> <p>Window size for window attention blocks.</p> <code>0</code> <code>global_attn_indexes</code> <code>list</code> <p>Indexes for blocks using global attention.</p> <code>()</code> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def __init__(\nself,\nimg_size: int = 1024,\npatch_size: int = 16,\nin_chans: int = 3,\nembed_dim: int = 768,\ndepth: int = 12,\nnum_heads: int = 12,\nmlp_ratio: float = 4.0,\nout_chans: int = 256,\nqkv_bias: bool = True,\nnorm_layer: Type[nn.Module] = nn.LayerNorm,\nact_layer: Type[nn.Module] = nn.GELU,\nuse_abs_pos: bool = True,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\nwindow_size: int = 0,\nglobal_attn_indexes: Tuple[int, ...] = (),\n) -&gt; None:\n\"\"\"\n    Args:\n        img_size (int): Input image size.\n        patch_size (int): Patch size.\n        in_chans (int): Number of input image channels.\n        embed_dim (int): Patch embedding dimension.\n        depth (int): Depth of ViT.\n        num_heads (int): Number of attention heads in each ViT block.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value.\n        norm_layer (nn.Module): Normalization layer.\n        act_layer (nn.Module): Activation layer.\n        use_abs_pos (bool): If True, use absolute positional embeddings.\n        use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n        rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n        window_size (int): Window size for window attention blocks.\n        global_attn_indexes (list): Indexes for blocks using global attention.\n    \"\"\"\nsuper().__init__()\nself.img_size = img_size\nself.patch_embed = PatchEmbed(\nkernel_size=(patch_size, patch_size),\nstride=(patch_size, patch_size),\nin_chans=in_chans,\nembed_dim=embed_dim,\n)\nself.pos_embed: Optional[nn.Parameter] = None\nif use_abs_pos:\n# Initialize absolute positional embedding with pretrain image size.\nself.pos_embed = nn.Parameter(torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim))\nself.blocks = nn.ModuleList()\nfor i in range(depth):\nblock = Block(\ndim=embed_dim,\nnum_heads=num_heads,\nmlp_ratio=mlp_ratio,\nqkv_bias=qkv_bias,\nnorm_layer=norm_layer,\nact_layer=act_layer,\nuse_rel_pos=use_rel_pos,\nrel_pos_zero_init=rel_pos_zero_init,\nwindow_size=window_size if i not in global_attn_indexes else 0,\ninput_size=(img_size // patch_size, img_size // patch_size),\n)\nself.blocks.append(block)\nself.neck = nn.Sequential(\nnn.Conv2d(\nembed_dim,\nout_chans,\nkernel_size=1,\nbias=False,\n),\nLayerNorm2d(out_chans),\nnn.Conv2d(\nout_chans,\nout_chans,\nkernel_size=3,\npadding=1,\nbias=False,\n),\nLayerNorm2d(out_chans),\n)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PromptEncoder","title":"<code>ultralytics.models.sam.modules.encoders.PromptEncoder</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class PromptEncoder(nn.Module):\ndef __init__(\nself,\nembed_dim: int,\nimage_embedding_size: Tuple[int, int],\ninput_image_size: Tuple[int, int],\nmask_in_chans: int,\nactivation: Type[nn.Module] = nn.GELU,\n) -&gt; None:\n\"\"\"\n        Encodes prompts for input to SAM's mask decoder.\n        Args:\n          embed_dim (int): The prompts' embedding dimension\n          image_embedding_size (tuple(int, int)): The spatial size of the\n            image embedding, as (H, W).\n          input_image_size (int): The padded size of the image as input\n            to the image encoder, as (H, W).\n          mask_in_chans (int): The number of hidden channels used for\n            encoding input masks.\n          activation (nn.Module): The activation to use when encoding\n            input masks.\n        \"\"\"\nsuper().__init__()\nself.embed_dim = embed_dim\nself.input_image_size = input_image_size\nself.image_embedding_size = image_embedding_size\nself.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\nself.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\npoint_embeddings = [nn.Embedding(1, embed_dim) for _ in range(self.num_point_embeddings)]\nself.point_embeddings = nn.ModuleList(point_embeddings)\nself.not_a_point_embed = nn.Embedding(1, embed_dim)\nself.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])\nself.mask_downscaling = nn.Sequential(\nnn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\nLayerNorm2d(mask_in_chans // 4),\nactivation(),\nnn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\nLayerNorm2d(mask_in_chans),\nactivation(),\nnn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n)\nself.no_mask_embed = nn.Embedding(1, embed_dim)\ndef get_dense_pe(self) -&gt; torch.Tensor:\n\"\"\"\n        Returns the positional encoding used to encode point prompts,\n        applied to a dense set of points the shape of the image encoding.\n        Returns:\n          torch.Tensor: Positional encoding with shape 1x(embed_dim)x(embedding_h)x(embedding_w)\n        \"\"\"\nreturn self.pe_layer(self.image_embedding_size).unsqueeze(0)\ndef _embed_points(\nself,\npoints: torch.Tensor,\nlabels: torch.Tensor,\npad: bool,\n) -&gt; torch.Tensor:\n\"\"\"Embeds point prompts.\"\"\"\npoints = points + 0.5  # Shift to center of pixel\nif pad:\npadding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\npadding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\npoints = torch.cat([points, padding_point], dim=1)\nlabels = torch.cat([labels, padding_label], dim=1)\npoint_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)\npoint_embedding[labels == -1] = 0.0\npoint_embedding[labels == -1] += self.not_a_point_embed.weight\npoint_embedding[labels == 0] += self.point_embeddings[0].weight\npoint_embedding[labels == 1] += self.point_embeddings[1].weight\nreturn point_embedding\ndef _embed_boxes(self, boxes: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Embeds box prompts.\"\"\"\nboxes = boxes + 0.5  # Shift to center of pixel\ncoords = boxes.reshape(-1, 2, 2)\ncorner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\ncorner_embedding[:, 0, :] += self.point_embeddings[2].weight\ncorner_embedding[:, 1, :] += self.point_embeddings[3].weight\nreturn corner_embedding\ndef _embed_masks(self, masks: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Embeds mask inputs.\"\"\"\nreturn self.mask_downscaling(masks)\ndef _get_batch_size(\nself,\npoints: Optional[Tuple[torch.Tensor, torch.Tensor]],\nboxes: Optional[torch.Tensor],\nmasks: Optional[torch.Tensor],\n) -&gt; int:\n\"\"\"\n        Gets the batch size of the output given the batch size of the input prompts.\n        \"\"\"\nif points is not None:\nreturn points[0].shape[0]\nelif boxes is not None:\nreturn boxes.shape[0]\nelif masks is not None:\nreturn masks.shape[0]\nelse:\nreturn 1\ndef _get_device(self) -&gt; torch.device:\nreturn self.point_embeddings[0].weight.device\ndef forward(\nself,\npoints: Optional[Tuple[torch.Tensor, torch.Tensor]],\nboxes: Optional[torch.Tensor],\nmasks: Optional[torch.Tensor],\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"\n        Embeds different types of prompts, returning both sparse and dense embeddings.\n        Args:\n          points (tuple(torch.Tensor, torch.Tensor), None): point coordinates and labels to embed.\n          boxes (torch.Tensor, None): boxes to embed\n          masks (torch.Tensor, None): masks to embed\n        Returns:\n          torch.Tensor: sparse embeddings for the points and boxes, with shape BxNx(embed_dim), where N is determined\n            by the number of input points and boxes.\n          torch.Tensor: dense embeddings for the masks, in the shape Bx(embed_dim)x(embed_H)x(embed_W)\n        \"\"\"\nbs = self._get_batch_size(points, boxes, masks)\nsparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\nif points is not None:\ncoords, labels = points\npoint_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\nsparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\nif boxes is not None:\nbox_embeddings = self._embed_boxes(boxes)\nsparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\nif masks is not None:\ndense_embeddings = self._embed_masks(masks)\nelse:\ndense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1,\n1).expand(bs, -1, self.image_embedding_size[0],\nself.image_embedding_size[1])\nreturn sparse_embeddings, dense_embeddings\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PromptEncoder.__init__","title":"<code>__init__(embed_dim, image_embedding_size, input_image_size, mask_in_chans, activation=nn.GELU)</code>","text":"<p>Encodes prompts for input to SAM's mask decoder.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The prompts' embedding dimension</p> required <code>image_embedding_size</code> <code>tuple(int, int</code> <p>The spatial size of the image embedding, as (H, W).</p> required <code>input_image_size</code> <code>int</code> <p>The padded size of the image as input to the image encoder, as (H, W).</p> required <code>mask_in_chans</code> <code>int</code> <p>The number of hidden channels used for encoding input masks.</p> required <code>activation</code> <code>Module</code> <p>The activation to use when encoding input masks.</p> <code>GELU</code> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def __init__(\nself,\nembed_dim: int,\nimage_embedding_size: Tuple[int, int],\ninput_image_size: Tuple[int, int],\nmask_in_chans: int,\nactivation: Type[nn.Module] = nn.GELU,\n) -&gt; None:\n\"\"\"\n    Encodes prompts for input to SAM's mask decoder.\n    Args:\n      embed_dim (int): The prompts' embedding dimension\n      image_embedding_size (tuple(int, int)): The spatial size of the\n        image embedding, as (H, W).\n      input_image_size (int): The padded size of the image as input\n        to the image encoder, as (H, W).\n      mask_in_chans (int): The number of hidden channels used for\n        encoding input masks.\n      activation (nn.Module): The activation to use when encoding\n        input masks.\n    \"\"\"\nsuper().__init__()\nself.embed_dim = embed_dim\nself.input_image_size = input_image_size\nself.image_embedding_size = image_embedding_size\nself.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\nself.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\npoint_embeddings = [nn.Embedding(1, embed_dim) for _ in range(self.num_point_embeddings)]\nself.point_embeddings = nn.ModuleList(point_embeddings)\nself.not_a_point_embed = nn.Embedding(1, embed_dim)\nself.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])\nself.mask_downscaling = nn.Sequential(\nnn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\nLayerNorm2d(mask_in_chans // 4),\nactivation(),\nnn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\nLayerNorm2d(mask_in_chans),\nactivation(),\nnn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n)\nself.no_mask_embed = nn.Embedding(1, embed_dim)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PromptEncoder.forward","title":"<code>forward(points, boxes, masks)</code>","text":"<p>Embeds different types of prompts, returning both sparse and dense embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>(tuple(Tensor, Tensor), None)</code> <p>point coordinates and labels to embed.</p> required <code>boxes</code> <code>(Tensor, None)</code> <p>boxes to embed</p> required <code>masks</code> <code>(Tensor, None)</code> <p>masks to embed</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: sparse embeddings for the points and boxes, with shape BxNx(embed_dim), where N is determined by the number of input points and boxes.</p> <code>Tensor</code> <p>torch.Tensor: dense embeddings for the masks, in the shape Bx(embed_dim)x(embed_H)x(embed_W)</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def forward(\nself,\npoints: Optional[Tuple[torch.Tensor, torch.Tensor]],\nboxes: Optional[torch.Tensor],\nmasks: Optional[torch.Tensor],\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"\n    Embeds different types of prompts, returning both sparse and dense embeddings.\n    Args:\n      points (tuple(torch.Tensor, torch.Tensor), None): point coordinates and labels to embed.\n      boxes (torch.Tensor, None): boxes to embed\n      masks (torch.Tensor, None): masks to embed\n    Returns:\n      torch.Tensor: sparse embeddings for the points and boxes, with shape BxNx(embed_dim), where N is determined\n        by the number of input points and boxes.\n      torch.Tensor: dense embeddings for the masks, in the shape Bx(embed_dim)x(embed_H)x(embed_W)\n    \"\"\"\nbs = self._get_batch_size(points, boxes, masks)\nsparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\nif points is not None:\ncoords, labels = points\npoint_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\nsparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\nif boxes is not None:\nbox_embeddings = self._embed_boxes(boxes)\nsparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\nif masks is not None:\ndense_embeddings = self._embed_masks(masks)\nelse:\ndense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1,\n1).expand(bs, -1, self.image_embedding_size[0],\nself.image_embedding_size[1])\nreturn sparse_embeddings, dense_embeddings\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PromptEncoder.get_dense_pe","title":"<code>get_dense_pe()</code>","text":"<p>Returns the positional encoding used to encode point prompts, applied to a dense set of points the shape of the image encoding.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Positional encoding with shape 1x(embed_dim)x(embedding_h)x(embedding_w)</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def get_dense_pe(self) -&gt; torch.Tensor:\n\"\"\"\n    Returns the positional encoding used to encode point prompts,\n    applied to a dense set of points the shape of the image encoding.\n    Returns:\n      torch.Tensor: Positional encoding with shape 1x(embed_dim)x(embedding_h)x(embedding_w)\n    \"\"\"\nreturn self.pe_layer(self.image_embedding_size).unsqueeze(0)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PositionEmbeddingRandom","title":"<code>ultralytics.models.sam.modules.encoders.PositionEmbeddingRandom</code>","text":"<p>             Bases: <code>Module</code></p> <p>Positional encoding using random spatial frequencies.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class PositionEmbeddingRandom(nn.Module):\n\"\"\"\n    Positional encoding using random spatial frequencies.\n    \"\"\"\ndef __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -&gt; None:\nsuper().__init__()\nif scale is None or scale &lt;= 0.0:\nscale = 1.0\nself.register_buffer(\n'positional_encoding_gaussian_matrix',\nscale * torch.randn((2, num_pos_feats)),\n)\ndef _pe_encoding(self, coords: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n# assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\ncoords = 2 * coords - 1\ncoords = coords @ self.positional_encoding_gaussian_matrix\ncoords = 2 * np.pi * coords\n# outputs d_1 x ... x d_n x C shape\nreturn torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\ndef forward(self, size: Tuple[int, int]) -&gt; torch.Tensor:\n\"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\nh, w = size\ndevice: Any = self.positional_encoding_gaussian_matrix.device\ngrid = torch.ones((h, w), device=device, dtype=torch.float32)\ny_embed = grid.cumsum(dim=0) - 0.5\nx_embed = grid.cumsum(dim=1) - 0.5\ny_embed = y_embed / h\nx_embed = x_embed / w\npe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\nreturn pe.permute(2, 0, 1)  # C x H x W\ndef forward_with_coords(self, coords_input: torch.Tensor, image_size: Tuple[int, int]) -&gt; torch.Tensor:\n\"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\ncoords = coords_input.clone()\ncoords[:, :, 0] = coords[:, :, 0] / image_size[1]\ncoords[:, :, 1] = coords[:, :, 1] / image_size[0]\nreturn self._pe_encoding(coords.to(torch.float))  # B x N x C\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PositionEmbeddingRandom.forward","title":"<code>forward(size)</code>","text":"<p>Generate positional encoding for a grid of the specified size.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def forward(self, size: Tuple[int, int]) -&gt; torch.Tensor:\n\"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\nh, w = size\ndevice: Any = self.positional_encoding_gaussian_matrix.device\ngrid = torch.ones((h, w), device=device, dtype=torch.float32)\ny_embed = grid.cumsum(dim=0) - 0.5\nx_embed = grid.cumsum(dim=1) - 0.5\ny_embed = y_embed / h\nx_embed = x_embed / w\npe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\nreturn pe.permute(2, 0, 1)  # C x H x W\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PositionEmbeddingRandom.forward_with_coords","title":"<code>forward_with_coords(coords_input, image_size)</code>","text":"<p>Positionally encode points that are not normalized to [0,1].</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def forward_with_coords(self, coords_input: torch.Tensor, image_size: Tuple[int, int]) -&gt; torch.Tensor:\n\"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\ncoords = coords_input.clone()\ncoords[:, :, 0] = coords[:, :, 0] / image_size[1]\ncoords[:, :, 1] = coords[:, :, 1] / image_size[0]\nreturn self._pe_encoding(coords.to(torch.float))  # B x N x C\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.Block","title":"<code>ultralytics.models.sam.modules.encoders.Block</code>","text":"<p>             Bases: <code>Module</code></p> <p>Transformer blocks with support of window attention and residual propagation blocks</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class Block(nn.Module):\n\"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\ndef __init__(\nself,\ndim: int,\nnum_heads: int,\nmlp_ratio: float = 4.0,\nqkv_bias: bool = True,\nnorm_layer: Type[nn.Module] = nn.LayerNorm,\nact_layer: Type[nn.Module] = nn.GELU,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\nwindow_size: int = 0,\ninput_size: Optional[Tuple[int, int]] = None,\n) -&gt; None:\n\"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks. If it equals 0, then\n                use global attention.\n            input_size (tuple(int, int), None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\nsuper().__init__()\nself.norm1 = norm_layer(dim)\nself.attn = Attention(\ndim,\nnum_heads=num_heads,\nqkv_bias=qkv_bias,\nuse_rel_pos=use_rel_pos,\nrel_pos_zero_init=rel_pos_zero_init,\ninput_size=input_size if window_size == 0 else (window_size, window_size),\n)\nself.norm2 = norm_layer(dim)\nself.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\nself.window_size = window_size\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nshortcut = x\nx = self.norm1(x)\n# Window partition\nif self.window_size &gt; 0:\nH, W = x.shape[1], x.shape[2]\nx, pad_hw = window_partition(x, self.window_size)\nx = self.attn(x)\n# Reverse window partition\nif self.window_size &gt; 0:\nx = window_unpartition(x, self.window_size, pad_hw, (H, W))\nx = shortcut + x\nreturn x + self.mlp(self.norm2(x))\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.Block.__init__","title":"<code>__init__(dim, num_heads, mlp_ratio=4.0, qkv_bias=True, norm_layer=nn.LayerNorm, act_layer=nn.GELU, use_rel_pos=False, rel_pos_zero_init=True, window_size=0, input_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads in each ViT block.</p> required <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value.</p> <code>True</code> <code>norm_layer</code> <code>Module</code> <p>Normalization layer.</p> <code>LayerNorm</code> <code>act_layer</code> <code>Module</code> <p>Activation layer.</p> <code>GELU</code> <code>use_rel_pos</code> <code>bool</code> <p>If True, add relative positional embeddings to the attention map.</p> <code>False</code> <code>rel_pos_zero_init</code> <code>bool</code> <p>If True, zero initialize relative positional parameters.</p> <code>True</code> <code>window_size</code> <code>int</code> <p>Window size for window attention blocks. If it equals 0, then use global attention.</p> <code>0</code> <code>input_size</code> <code>(tuple(int, int), None)</code> <p>Input resolution for calculating the relative positional parameter size.</p> <code>None</code> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def __init__(\nself,\ndim: int,\nnum_heads: int,\nmlp_ratio: float = 4.0,\nqkv_bias: bool = True,\nnorm_layer: Type[nn.Module] = nn.LayerNorm,\nact_layer: Type[nn.Module] = nn.GELU,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\nwindow_size: int = 0,\ninput_size: Optional[Tuple[int, int]] = None,\n) -&gt; None:\n\"\"\"\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads in each ViT block.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value.\n        norm_layer (nn.Module): Normalization layer.\n        act_layer (nn.Module): Activation layer.\n        use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n        rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n        window_size (int): Window size for window attention blocks. If it equals 0, then\n            use global attention.\n        input_size (tuple(int, int), None): Input resolution for calculating the relative\n            positional parameter size.\n    \"\"\"\nsuper().__init__()\nself.norm1 = norm_layer(dim)\nself.attn = Attention(\ndim,\nnum_heads=num_heads,\nqkv_bias=qkv_bias,\nuse_rel_pos=use_rel_pos,\nrel_pos_zero_init=rel_pos_zero_init,\ninput_size=input_size if window_size == 0 else (window_size, window_size),\n)\nself.norm2 = norm_layer(dim)\nself.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\nself.window_size = window_size\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.Attention","title":"<code>ultralytics.models.sam.modules.encoders.Attention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Multi-head Attention block with relative position embeddings.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class Attention(nn.Module):\n\"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\ndef __init__(\nself,\ndim: int,\nnum_heads: int = 8,\nqkv_bias: bool = True,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\ninput_size: Optional[Tuple[int, int]] = None,\n) -&gt; None:\n\"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads.\n            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            input_size (tuple(int, int), None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\nsuper().__init__()\nself.num_heads = num_heads\nhead_dim = dim // num_heads\nself.scale = head_dim ** -0.5\nself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\nself.proj = nn.Linear(dim, dim)\nself.use_rel_pos = use_rel_pos\nif self.use_rel_pos:\nassert (input_size is not None), 'Input size must be provided if using relative positional encoding.'\n# initialize relative positional embeddings\nself.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\nself.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nB, H, W, _ = x.shape\n# qkv with shape (3, B, nHead, H * W, C)\nqkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n# q, k, v with shape (B * nHead, H * W, C)\nq, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\nattn = (q * self.scale) @ k.transpose(-2, -1)\nif self.use_rel_pos:\nattn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\nattn = attn.softmax(dim=-1)\nx = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\nreturn self.proj(x)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.Attention.__init__","title":"<code>__init__(dim, num_heads=8, qkv_bias=True, use_rel_pos=False, rel_pos_zero_init=True, input_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>8</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value.</p> <code>True</code> <code>rel_pos_zero_init</code> <code>bool</code> <p>If True, zero initialize relative positional parameters.</p> <code>True</code> <code>input_size</code> <code>(tuple(int, int), None)</code> <p>Input resolution for calculating the relative positional parameter size.</p> <code>None</code> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def __init__(\nself,\ndim: int,\nnum_heads: int = 8,\nqkv_bias: bool = True,\nuse_rel_pos: bool = False,\nrel_pos_zero_init: bool = True,\ninput_size: Optional[Tuple[int, int]] = None,\n) -&gt; None:\n\"\"\"\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n        rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n        input_size (tuple(int, int), None): Input resolution for calculating the relative\n            positional parameter size.\n    \"\"\"\nsuper().__init__()\nself.num_heads = num_heads\nhead_dim = dim // num_heads\nself.scale = head_dim ** -0.5\nself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\nself.proj = nn.Linear(dim, dim)\nself.use_rel_pos = use_rel_pos\nif self.use_rel_pos:\nassert (input_size is not None), 'Input size must be provided if using relative positional encoding.'\n# initialize relative positional embeddings\nself.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\nself.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PatchEmbed","title":"<code>ultralytics.models.sam.modules.encoders.PatchEmbed</code>","text":"<p>             Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>class PatchEmbed(nn.Module):\n\"\"\"\n    Image to Patch Embedding.\n    \"\"\"\ndef __init__(\nself,\nkernel_size: Tuple[int, int] = (16, 16),\nstride: Tuple[int, int] = (16, 16),\npadding: Tuple[int, int] = (0, 0),\nin_chans: int = 3,\nembed_dim: int = 768,\n) -&gt; None:\n\"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n        \"\"\"\nsuper().__init__()\nself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.proj(x).permute(0, 2, 3, 1)  # B C H W -&gt; B H W C\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.PatchEmbed.__init__","title":"<code>__init__(kernel_size=(16, 16), stride=(16, 16), padding=(0, 0), in_chans=3, embed_dim=768)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>Tuple</code> <p>kernel size of the projection layer.</p> <code>(16, 16)</code> <code>stride</code> <code>Tuple</code> <p>stride of the projection layer.</p> <code>(16, 16)</code> <code>padding</code> <code>Tuple</code> <p>padding size of the projection layer.</p> <code>(0, 0)</code> <code>in_chans</code> <code>int</code> <p>Number of input image channels.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>Patch embedding dimension.</p> <code>768</code> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def __init__(\nself,\nkernel_size: Tuple[int, int] = (16, 16),\nstride: Tuple[int, int] = (16, 16),\npadding: Tuple[int, int] = (0, 0),\nin_chans: int = 3,\nembed_dim: int = 768,\n) -&gt; None:\n\"\"\"\n    Args:\n        kernel_size (Tuple): kernel size of the projection layer.\n        stride (Tuple): stride of the projection layer.\n        padding (Tuple): padding size of the projection layer.\n        in_chans (int): Number of input image channels.\n        embed_dim (int): Patch embedding dimension.\n    \"\"\"\nsuper().__init__()\nself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.window_partition","title":"<code>ultralytics.models.sam.modules.encoders.window_partition(x, window_size)</code>","text":"<p>Partition into non-overlapping windows with padding if needed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>input tokens with [B, H, W, C].</p> required <code>window_size</code> <code>int</code> <p>window size.</p> required <p>Returns:</p> Name Type Description <code>windows</code> <code>Tensor</code> <p>windows after partition with [B * num_windows, window_size, window_size, C].</p> <code>(Hp, Wp)</code> <p>padded height and width before partition</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def window_partition(x: torch.Tensor, window_size: int) -&gt; Tuple[torch.Tensor, Tuple[int, int]]:\n\"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\"\nB, H, W, C = x.shape\npad_h = (window_size - H % window_size) % window_size\npad_w = (window_size - W % window_size) % window_size\nif pad_h &gt; 0 or pad_w &gt; 0:\nx = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\nHp, Wp = H + pad_h, W + pad_w\nx = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\nwindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\nreturn windows, (Hp, Wp)\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.window_unpartition","title":"<code>ultralytics.models.sam.modules.encoders.window_unpartition(windows, window_size, pad_hw, hw)</code>","text":"<p>Window unpartition into original sequences and removing padding.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>tensor</code> <p>input tokens with [B * num_windows, window_size, window_size, C].</p> required <code>window_size</code> <code>int</code> <p>window size.</p> required <code>pad_hw</code> <code>Tuple</code> <p>padded height and width (Hp, Wp).</p> required <code>hw</code> <code>Tuple</code> <p>original height and width (H, W) before padding.</p> required <p>Returns:</p> Name Type Description <code>x</code> <code>Tensor</code> <p>unpartitioned sequences with [B, H, W, C].</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def window_unpartition(windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int],\nhw: Tuple[int, int]) -&gt; torch.Tensor:\n\"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\nHp, Wp = pad_hw\nH, W = hw\nB = windows.shape[0] // (Hp * Wp // window_size // window_size)\nx = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\nx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\nif Hp &gt; H or Wp &gt; W:\nx = x[:, :H, :W, :].contiguous()\nreturn x\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.get_rel_pos","title":"<code>ultralytics.models.sam.modules.encoders.get_rel_pos(q_size, k_size, rel_pos)</code>","text":"<p>Get relative positional embeddings according to the relative positions of     query and key sizes.</p> <p>Parameters:</p> Name Type Description Default <code>q_size</code> <code>int</code> <p>size of query q.</p> required <code>k_size</code> <code>int</code> <p>size of key k.</p> required <code>rel_pos</code> <code>Tensor</code> <p>relative position embeddings (L, C).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Extracted positional embeddings according to relative positions.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\nmax_rel_dist = int(2 * max(q_size, k_size) - 1)\n# Interpolate rel pos if needed.\nif rel_pos.shape[0] != max_rel_dist:\n# Interpolate rel pos.\nrel_pos_resized = F.interpolate(\nrel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\nsize=max_rel_dist,\nmode='linear',\n)\nrel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\nelse:\nrel_pos_resized = rel_pos\n# Scale the coords with short length if shapes for q and k are different.\nq_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\nk_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\nrelative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\nreturn rel_pos_resized[relative_coords.long()]\n</code></pre>"},{"location":"reference/models/sam/modules/encoders/#ultralytics.models.sam.modules.encoders.add_decomposed_rel_pos","title":"<code>ultralytics.models.sam.modules.encoders.add_decomposed_rel_pos(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)</code>","text":"<p>Calculate decomposed Relative Positional Embeddings from :paper:<code>mvitv2</code>. https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950</p> <p>Parameters:</p> Name Type Description Default <code>attn</code> <code>Tensor</code> <p>attention map.</p> required <code>q</code> <code>Tensor</code> <p>query q in the attention layer with shape (B, q_h * q_w, C).</p> required <code>rel_pos_h</code> <code>Tensor</code> <p>relative position embeddings (Lh, C) for height axis.</p> required <code>rel_pos_w</code> <code>Tensor</code> <p>relative position embeddings (Lw, C) for width axis.</p> required <code>q_size</code> <code>Tuple</code> <p>spatial sequence size of query q with (q_h, q_w).</p> required <code>k_size</code> <code>Tuple</code> <p>spatial sequence size of key k with (k_h, k_w).</p> required <p>Returns:</p> Name Type Description <code>attn</code> <code>Tensor</code> <p>attention map with added relative positional embeddings.</p> Source code in <code>ultralytics/models/sam/modules/encoders.py</code> <pre><code>def add_decomposed_rel_pos(\nattn: torch.Tensor,\nq: torch.Tensor,\nrel_pos_h: torch.Tensor,\nrel_pos_w: torch.Tensor,\nq_size: Tuple[int, int],\nk_size: Tuple[int, int],\n) -&gt; torch.Tensor:\n\"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n    Args:\n        attn (Tensor): attention map.\n        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\nq_h, q_w = q_size\nk_h, k_w = k_size\nRh = get_rel_pos(q_h, k_h, rel_pos_h)\nRw = get_rel_pos(q_w, k_w, rel_pos_w)\nB, _, dim = q.shape\nr_q = q.reshape(B, q_h, q_w, dim)\nrel_h = torch.einsum('bhwc,hkc-&gt;bhwk', r_q, Rh)\nrel_w = torch.einsum('bhwc,wkc-&gt;bhwk', r_q, Rw)\nattn = (attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]).view(\nB, q_h * q_w, k_h * k_w)\nreturn attn\n</code></pre>"},{"location":"reference/models/sam/modules/sam/","title":"Reference for <code>ultralytics/models/sam/modules/sam.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/sam.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/sam/modules/sam/#ultralytics.models.sam.modules.sam.Sam","title":"<code>ultralytics.models.sam.modules.sam.Sam</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/sam.py</code> <pre><code>class Sam(nn.Module):\nmask_threshold: float = 0.0\nimage_format: str = 'RGB'\ndef __init__(self,\nimage_encoder: ImageEncoderViT,\nprompt_encoder: PromptEncoder,\nmask_decoder: MaskDecoder,\npixel_mean: List[float] = None,\npixel_std: List[float] = None) -&gt; None:\n\"\"\"\n        SAM predicts object masks from an image and input prompts.\n        Args:\n          image_encoder (ImageEncoderViT): The backbone used to encode the image into image embeddings that allow for\n            efficient mask prediction.\n          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n          mask_decoder (MaskDecoder): Predicts masks from the image embeddings and encoded prompts.\n          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n          pixel_std (list(float)): Std values for normalizing pixels in the input image.\n        \"\"\"\nif pixel_mean is None:\npixel_mean = [123.675, 116.28, 103.53]\nif pixel_std is None:\npixel_std = [58.395, 57.12, 57.375]\nsuper().__init__()\nself.image_encoder = image_encoder\nself.prompt_encoder = prompt_encoder\nself.mask_decoder = mask_decoder\nself.register_buffer('pixel_mean', torch.Tensor(pixel_mean).view(-1, 1, 1), False)\nself.register_buffer('pixel_std', torch.Tensor(pixel_std).view(-1, 1, 1), False)\n@property\ndef device(self) -&gt; Any:\nreturn self.pixel_mean.device\n@torch.no_grad()\ndef forward(\nself,\nbatched_input: List[Dict[str, Any]],\nmultimask_output: bool,\n) -&gt; List[Dict[str, torch.Tensor]]:\n\"\"\"\n        Predicts masks end-to-end from provided images and prompts. If prompts are not known in advance, using\n        SamPredictor is recommended over calling the model directly.\n        Args:\n          batched_input (list(dict)): A list over input images, each a dictionary with the following keys. A prompt\n              key can be excluded if it is not present.\n              'image': The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n              'original_size': (tuple(int, int)) The original size of the image before transformation, as (H, W).\n              'point_coords': (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already\n                transformed to the input frame of the model.\n              'point_labels': (torch.Tensor) Batched labels for point prompts, with shape BxN.\n              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4. Already transformed to the input frame of\n                the model.\n              'mask_inputs': (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.\n          multimask_output (bool): Whether the model should predict multiple disambiguating masks, or return a single\n            mask.\n        Returns:\n          (list(dict)): A list over input images, where each element is as dictionary with the following keys.\n              'masks': (torch.Tensor) Batched binary mask predictions, with shape BxCxHxW, where B is the number of\n                input prompts, C is determined by multimask_output, and (H, W) is the original size of the image.\n              'iou_predictions': (torch.Tensor) The model's predictions of mask quality, in shape BxC.\n              'low_res_logits': (torch.Tensor) Low resolution logits with shape BxCxHxW, where H=W=256. Can be passed\n                as mask input to subsequent iterations of prediction.\n        \"\"\"\ninput_images = torch.stack([self.preprocess(x['image']) for x in batched_input], dim=0)\nimage_embeddings = self.image_encoder(input_images)\noutputs = []\nfor image_record, curr_embedding in zip(batched_input, image_embeddings):\nif 'point_coords' in image_record:\npoints = (image_record['point_coords'], image_record['point_labels'])\nelse:\npoints = None\nsparse_embeddings, dense_embeddings = self.prompt_encoder(\npoints=points,\nboxes=image_record.get('boxes', None),\nmasks=image_record.get('mask_inputs', None),\n)\nlow_res_masks, iou_predictions = self.mask_decoder(\nimage_embeddings=curr_embedding.unsqueeze(0),\nimage_pe=self.prompt_encoder.get_dense_pe(),\nsparse_prompt_embeddings=sparse_embeddings,\ndense_prompt_embeddings=dense_embeddings,\nmultimask_output=multimask_output,\n)\nmasks = self.postprocess_masks(\nlow_res_masks,\ninput_size=image_record['image'].shape[-2:],\noriginal_size=image_record['original_size'],\n)\nmasks = masks &gt; self.mask_threshold\noutputs.append({\n'masks': masks,\n'iou_predictions': iou_predictions,\n'low_res_logits': low_res_masks, })\nreturn outputs\ndef postprocess_masks(\nself,\nmasks: torch.Tensor,\ninput_size: Tuple[int, ...],\noriginal_size: Tuple[int, ...],\n) -&gt; torch.Tensor:\n\"\"\"\n        Remove padding and upscale masks to the original image size.\n        Args:\n          masks (torch.Tensor): Batched masks from the mask_decoder, in BxCxHxW format.\n          input_size (tuple(int, int)): The size of the model input image, in (H, W) format. Used to remove padding.\n          original_size (tuple(int, int)): The original image size before resizing for input to the model, in (H, W).\n        Returns:\n          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W) is given by original_size.\n        \"\"\"\nmasks = F.interpolate(\nmasks,\n(self.image_encoder.img_size, self.image_encoder.img_size),\nmode='bilinear',\nalign_corners=False,\n)\nmasks = masks[..., :input_size[0], :input_size[1]]\nreturn F.interpolate(masks, original_size, mode='bilinear', align_corners=False)\ndef preprocess(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Normalize pixel values and pad to a square input.\"\"\"\n# Normalize colors\nx = (x - self.pixel_mean) / self.pixel_std\n# Pad\nh, w = x.shape[-2:]\npadh = self.image_encoder.img_size - h\npadw = self.image_encoder.img_size - w\nreturn F.pad(x, (0, padw, 0, padh))\n</code></pre>"},{"location":"reference/models/sam/modules/sam/#ultralytics.models.sam.modules.sam.Sam.__init__","title":"<code>__init__(image_encoder, prompt_encoder, mask_decoder, pixel_mean=None, pixel_std=None)</code>","text":"<p>SAM predicts object masks from an image and input prompts.</p> <p>Parameters:</p> Name Type Description Default <code>image_encoder</code> <code>ImageEncoderViT</code> <p>The backbone used to encode the image into image embeddings that allow for efficient mask prediction.</p> required <code>prompt_encoder</code> <code>PromptEncoder</code> <p>Encodes various types of input prompts.</p> required <code>mask_decoder</code> <code>MaskDecoder</code> <p>Predicts masks from the image embeddings and encoded prompts.</p> required <code>pixel_mean</code> <code>list(float</code> <p>Mean values for normalizing pixels in the input image.</p> <code>None</code> <code>pixel_std</code> <code>list(float</code> <p>Std values for normalizing pixels in the input image.</p> <code>None</code> Source code in <code>ultralytics/models/sam/modules/sam.py</code> <pre><code>def __init__(self,\nimage_encoder: ImageEncoderViT,\nprompt_encoder: PromptEncoder,\nmask_decoder: MaskDecoder,\npixel_mean: List[float] = None,\npixel_std: List[float] = None) -&gt; None:\n\"\"\"\n    SAM predicts object masks from an image and input prompts.\n    Args:\n      image_encoder (ImageEncoderViT): The backbone used to encode the image into image embeddings that allow for\n        efficient mask prediction.\n      prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n      mask_decoder (MaskDecoder): Predicts masks from the image embeddings and encoded prompts.\n      pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n      pixel_std (list(float)): Std values for normalizing pixels in the input image.\n    \"\"\"\nif pixel_mean is None:\npixel_mean = [123.675, 116.28, 103.53]\nif pixel_std is None:\npixel_std = [58.395, 57.12, 57.375]\nsuper().__init__()\nself.image_encoder = image_encoder\nself.prompt_encoder = prompt_encoder\nself.mask_decoder = mask_decoder\nself.register_buffer('pixel_mean', torch.Tensor(pixel_mean).view(-1, 1, 1), False)\nself.register_buffer('pixel_std', torch.Tensor(pixel_std).view(-1, 1, 1), False)\n</code></pre>"},{"location":"reference/models/sam/modules/sam/#ultralytics.models.sam.modules.sam.Sam.forward","title":"<code>forward(batched_input, multimask_output)</code>","text":"<p>Predicts masks end-to-end from provided images and prompts. If prompts are not known in advance, using SamPredictor is recommended over calling the model directly.</p> <p>Parameters:</p> Name Type Description Default <code>batched_input</code> <code>list(dict</code> <p>A list over input images, each a dictionary with the following keys. A prompt   key can be excluded if it is not present.   'image': The image as a torch tensor in 3xHxW format, already transformed for input to the model.   'original_size': (tuple(int, int)) The original size of the image before transformation, as (H, W).   'point_coords': (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already     transformed to the input frame of the model.   'point_labels': (torch.Tensor) Batched labels for point prompts, with shape BxN.   'boxes': (torch.Tensor) Batched box inputs, with shape Bx4. Already transformed to the input frame of     the model.   'mask_inputs': (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.</p> required <code>multimask_output</code> <code>bool</code> <p>Whether the model should predict multiple disambiguating masks, or return a single mask.</p> required <p>Returns:</p> Type Description <code>list(dict)</code> <p>A list over input images, where each element is as dictionary with the following keys.   'masks': (torch.Tensor) Batched binary mask predictions, with shape BxCxHxW, where B is the number of     input prompts, C is determined by multimask_output, and (H, W) is the original size of the image.   'iou_predictions': (torch.Tensor) The model's predictions of mask quality, in shape BxC.   'low_res_logits': (torch.Tensor) Low resolution logits with shape BxCxHxW, where H=W=256. Can be passed     as mask input to subsequent iterations of prediction.</p> Source code in <code>ultralytics/models/sam/modules/sam.py</code> <pre><code>@torch.no_grad()\ndef forward(\nself,\nbatched_input: List[Dict[str, Any]],\nmultimask_output: bool,\n) -&gt; List[Dict[str, torch.Tensor]]:\n\"\"\"\n    Predicts masks end-to-end from provided images and prompts. If prompts are not known in advance, using\n    SamPredictor is recommended over calling the model directly.\n    Args:\n      batched_input (list(dict)): A list over input images, each a dictionary with the following keys. A prompt\n          key can be excluded if it is not present.\n          'image': The image as a torch tensor in 3xHxW format, already transformed for input to the model.\n          'original_size': (tuple(int, int)) The original size of the image before transformation, as (H, W).\n          'point_coords': (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already\n            transformed to the input frame of the model.\n          'point_labels': (torch.Tensor) Batched labels for point prompts, with shape BxN.\n          'boxes': (torch.Tensor) Batched box inputs, with shape Bx4. Already transformed to the input frame of\n            the model.\n          'mask_inputs': (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.\n      multimask_output (bool): Whether the model should predict multiple disambiguating masks, or return a single\n        mask.\n    Returns:\n      (list(dict)): A list over input images, where each element is as dictionary with the following keys.\n          'masks': (torch.Tensor) Batched binary mask predictions, with shape BxCxHxW, where B is the number of\n            input prompts, C is determined by multimask_output, and (H, W) is the original size of the image.\n          'iou_predictions': (torch.Tensor) The model's predictions of mask quality, in shape BxC.\n          'low_res_logits': (torch.Tensor) Low resolution logits with shape BxCxHxW, where H=W=256. Can be passed\n            as mask input to subsequent iterations of prediction.\n    \"\"\"\ninput_images = torch.stack([self.preprocess(x['image']) for x in batched_input], dim=0)\nimage_embeddings = self.image_encoder(input_images)\noutputs = []\nfor image_record, curr_embedding in zip(batched_input, image_embeddings):\nif 'point_coords' in image_record:\npoints = (image_record['point_coords'], image_record['point_labels'])\nelse:\npoints = None\nsparse_embeddings, dense_embeddings = self.prompt_encoder(\npoints=points,\nboxes=image_record.get('boxes', None),\nmasks=image_record.get('mask_inputs', None),\n)\nlow_res_masks, iou_predictions = self.mask_decoder(\nimage_embeddings=curr_embedding.unsqueeze(0),\nimage_pe=self.prompt_encoder.get_dense_pe(),\nsparse_prompt_embeddings=sparse_embeddings,\ndense_prompt_embeddings=dense_embeddings,\nmultimask_output=multimask_output,\n)\nmasks = self.postprocess_masks(\nlow_res_masks,\ninput_size=image_record['image'].shape[-2:],\noriginal_size=image_record['original_size'],\n)\nmasks = masks &gt; self.mask_threshold\noutputs.append({\n'masks': masks,\n'iou_predictions': iou_predictions,\n'low_res_logits': low_res_masks, })\nreturn outputs\n</code></pre>"},{"location":"reference/models/sam/modules/sam/#ultralytics.models.sam.modules.sam.Sam.postprocess_masks","title":"<code>postprocess_masks(masks, input_size, original_size)</code>","text":"<p>Remove padding and upscale masks to the original image size.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Tensor</code> <p>Batched masks from the mask_decoder, in BxCxHxW format.</p> required <code>input_size</code> <code>tuple(int, int</code> <p>The size of the model input image, in (H, W) format. Used to remove padding.</p> required <code>original_size</code> <code>tuple(int, int</code> <p>The original image size before resizing for input to the model, in (H, W).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Batched masks in BxCxHxW format, where (H, W) is given by original_size.</p> Source code in <code>ultralytics/models/sam/modules/sam.py</code> <pre><code>def postprocess_masks(\nself,\nmasks: torch.Tensor,\ninput_size: Tuple[int, ...],\noriginal_size: Tuple[int, ...],\n) -&gt; torch.Tensor:\n\"\"\"\n    Remove padding and upscale masks to the original image size.\n    Args:\n      masks (torch.Tensor): Batched masks from the mask_decoder, in BxCxHxW format.\n      input_size (tuple(int, int)): The size of the model input image, in (H, W) format. Used to remove padding.\n      original_size (tuple(int, int)): The original image size before resizing for input to the model, in (H, W).\n    Returns:\n      (torch.Tensor): Batched masks in BxCxHxW format, where (H, W) is given by original_size.\n    \"\"\"\nmasks = F.interpolate(\nmasks,\n(self.image_encoder.img_size, self.image_encoder.img_size),\nmode='bilinear',\nalign_corners=False,\n)\nmasks = masks[..., :input_size[0], :input_size[1]]\nreturn F.interpolate(masks, original_size, mode='bilinear', align_corners=False)\n</code></pre>"},{"location":"reference/models/sam/modules/sam/#ultralytics.models.sam.modules.sam.Sam.preprocess","title":"<code>preprocess(x)</code>","text":"<p>Normalize pixel values and pad to a square input.</p> Source code in <code>ultralytics/models/sam/modules/sam.py</code> <pre><code>def preprocess(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Normalize pixel values and pad to a square input.\"\"\"\n# Normalize colors\nx = (x - self.pixel_mean) / self.pixel_std\n# Pad\nh, w = x.shape[-2:]\npadh = self.image_encoder.img_size - h\npadw = self.image_encoder.img_size - w\nreturn F.pad(x, (0, padw, 0, padh))\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/","title":"Reference for <code>ultralytics/models/sam/modules/tiny_encoder.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/tiny_encoder.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.Conv2d_BN","title":"<code>ultralytics.models.sam.modules.tiny_encoder.Conv2d_BN</code>","text":"<p>             Bases: <code>Sequential</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class Conv2d_BN(torch.nn.Sequential):\ndef __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1, groups=1, bn_weight_init=1):\nsuper().__init__()\nself.add_module('c', torch.nn.Conv2d(a, b, ks, stride, pad, dilation, groups, bias=False))\nbn = torch.nn.BatchNorm2d(b)\ntorch.nn.init.constant_(bn.weight, bn_weight_init)\ntorch.nn.init.constant_(bn.bias, 0)\nself.add_module('bn', bn)\n@torch.no_grad()\ndef fuse(self):\nc, bn = self._modules.values()\nw = bn.weight / (bn.running_var + bn.eps) ** 0.5\nw = c.weight * w[:, None, None, None]\nb = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5\nm = torch.nn.Conv2d(w.size(1) * self.c.groups,\nw.size(0),\nw.shape[2:],\nstride=self.c.stride,\npadding=self.c.padding,\ndilation=self.c.dilation,\ngroups=self.c.groups)\nm.weight.data.copy_(w)\nm.bias.data.copy_(b)\nreturn m\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.PatchEmbed","title":"<code>ultralytics.models.sam.modules.tiny_encoder.PatchEmbed</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class PatchEmbed(nn.Module):\ndef __init__(self, in_chans, embed_dim, resolution, activation):\nsuper().__init__()\nimg_size: Tuple[int, int] = to_2tuple(resolution)\nself.patches_resolution = (img_size[0] // 4, img_size[1] // 4)\nself.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\nself.in_chans = in_chans\nself.embed_dim = embed_dim\nn = embed_dim\nself.seq = nn.Sequential(\nConv2d_BN(in_chans, n // 2, 3, 2, 1),\nactivation(),\nConv2d_BN(n // 2, n, 3, 2, 1),\n)\ndef forward(self, x):\nreturn self.seq(x)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.MBConv","title":"<code>ultralytics.models.sam.modules.tiny_encoder.MBConv</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class MBConv(nn.Module):\ndef __init__(self, in_chans, out_chans, expand_ratio, activation, drop_path):\nsuper().__init__()\nself.in_chans = in_chans\nself.hidden_chans = int(in_chans * expand_ratio)\nself.out_chans = out_chans\nself.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\nself.act1 = activation()\nself.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans, ks=3, stride=1, pad=1, groups=self.hidden_chans)\nself.act2 = activation()\nself.conv3 = Conv2d_BN(self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)\nself.act3 = activation()\n# NOTE: `DropPath` is needed only for training.\n# self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()\nself.drop_path = nn.Identity()\ndef forward(self, x):\nshortcut = x\nx = self.conv1(x)\nx = self.act1(x)\nx = self.conv2(x)\nx = self.act2(x)\nx = self.conv3(x)\nx = self.drop_path(x)\nx += shortcut\nreturn self.act3(x)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.PatchMerging","title":"<code>ultralytics.models.sam.modules.tiny_encoder.PatchMerging</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class PatchMerging(nn.Module):\ndef __init__(self, input_resolution, dim, out_dim, activation):\nsuper().__init__()\nself.input_resolution = input_resolution\nself.dim = dim\nself.out_dim = out_dim\nself.act = activation()\nself.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)\nstride_c = 1 if out_dim in [320, 448, 576] else 2\nself.conv2 = Conv2d_BN(out_dim, out_dim, 3, stride_c, 1, groups=out_dim)\nself.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)\ndef forward(self, x):\nif x.ndim == 3:\nH, W = self.input_resolution\nB = len(x)\n# (B, C, H, W)\nx = x.view(B, H, W, -1).permute(0, 3, 1, 2)\nx = self.conv1(x)\nx = self.act(x)\nx = self.conv2(x)\nx = self.act(x)\nx = self.conv3(x)\nreturn x.flatten(2).transpose(1, 2)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.ConvLayer","title":"<code>ultralytics.models.sam.modules.tiny_encoder.ConvLayer</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class ConvLayer(nn.Module):\ndef __init__(\nself,\ndim,\ninput_resolution,\ndepth,\nactivation,\ndrop_path=0.,\ndownsample=None,\nuse_checkpoint=False,\nout_dim=None,\nconv_expand_ratio=4.,\n):\nsuper().__init__()\nself.dim = dim\nself.input_resolution = input_resolution\nself.depth = depth\nself.use_checkpoint = use_checkpoint\n# build blocks\nself.blocks = nn.ModuleList([\nMBConv(\ndim,\ndim,\nconv_expand_ratio,\nactivation,\ndrop_path[i] if isinstance(drop_path, list) else drop_path,\n) for i in range(depth)])\n# patch merging layer\nself.downsample = None if downsample is None else downsample(\ninput_resolution, dim=dim, out_dim=out_dim, activation=activation)\ndef forward(self, x):\nfor blk in self.blocks:\nx = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)\nreturn x if self.downsample is None else self.downsample(x)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.Mlp","title":"<code>ultralytics.models.sam.modules.tiny_encoder.Mlp</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class Mlp(nn.Module):\ndef __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\nsuper().__init__()\nout_features = out_features or in_features\nhidden_features = hidden_features or in_features\nself.norm = nn.LayerNorm(in_features)\nself.fc1 = nn.Linear(in_features, hidden_features)\nself.fc2 = nn.Linear(hidden_features, out_features)\nself.act = act_layer()\nself.drop = nn.Dropout(drop)\ndef forward(self, x):\nx = self.norm(x)\nx = self.fc1(x)\nx = self.act(x)\nx = self.drop(x)\nx = self.fc2(x)\nreturn self.drop(x)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.Attention","title":"<code>ultralytics.models.sam.modules.tiny_encoder.Attention</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class Attention(torch.nn.Module):\ndef __init__(\nself,\ndim,\nkey_dim,\nnum_heads=8,\nattn_ratio=4,\nresolution=(14, 14),\n):\nsuper().__init__()\n# (h, w)\nassert isinstance(resolution, tuple) and len(resolution) == 2\nself.num_heads = num_heads\nself.scale = key_dim ** -0.5\nself.key_dim = key_dim\nself.nh_kd = nh_kd = key_dim * num_heads\nself.d = int(attn_ratio * key_dim)\nself.dh = int(attn_ratio * key_dim) * num_heads\nself.attn_ratio = attn_ratio\nh = self.dh + nh_kd * 2\nself.norm = nn.LayerNorm(dim)\nself.qkv = nn.Linear(dim, h)\nself.proj = nn.Linear(self.dh, dim)\npoints = list(itertools.product(range(resolution[0]), range(resolution[1])))\nN = len(points)\nattention_offsets = {}\nidxs = []\nfor p1 in points:\nfor p2 in points:\noffset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\nif offset not in attention_offsets:\nattention_offsets[offset] = len(attention_offsets)\nidxs.append(attention_offsets[offset])\nself.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets)))\nself.register_buffer('attention_bias_idxs', torch.LongTensor(idxs).view(N, N), persistent=False)\n@torch.no_grad()\ndef train(self, mode=True):\nsuper().train(mode)\nif mode and hasattr(self, 'ab'):\ndel self.ab\nelse:\nself.ab = self.attention_biases[:, self.attention_bias_idxs]\ndef forward(self, x):  # x (B,N,C)\nB, N, _ = x.shape\n# Normalization\nx = self.norm(x)\nqkv = self.qkv(x)\n# (B, N, num_heads, d)\nq, k, v = qkv.view(B, N, self.num_heads, -1).split([self.key_dim, self.key_dim, self.d], dim=3)\n# (B, num_heads, N, d)\nq = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\nself.ab = self.ab.to(self.attention_biases.device)\nattn = ((q @ k.transpose(-2, -1)) * self.scale +\n(self.attention_biases[:, self.attention_bias_idxs] if self.training else self.ab))\nattn = attn.softmax(dim=-1)\nx = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\nreturn self.proj(x)\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.TinyViTBlock","title":"<code>ultralytics.models.sam.modules.tiny_encoder.TinyViTBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>TinyViT Block.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input channels.</p> required <code>input_resolution</code> <code>tuple[int, int]</code> <p>Input resolution.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>int</code> <p>Window size.</p> <code>7</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>drop</code> <code>float</code> <p>Dropout rate. Default: 0.0</p> <code>0.0</code> <code>drop_path</code> <code>float</code> <p>Stochastic depth rate. Default: 0.0</p> <code>0.0</code> <code>local_conv_size</code> <code>int</code> <p>the kernel size of the convolution between Attention and MLP. Default: 3</p> <code>3</code> <code>activation</code> <code>nn</code> <p>the activation function. Default: nn.GELU</p> <code>GELU</code> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class TinyViTBlock(nn.Module):\n\"\"\"\n    TinyViT Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int, int]): Input resolution.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        local_conv_size (int): the kernel size of the convolution between Attention and MLP. Default: 3\n        activation (torch.nn): the activation function. Default: nn.GELU\n    \"\"\"\ndef __init__(\nself,\ndim,\ninput_resolution,\nnum_heads,\nwindow_size=7,\nmlp_ratio=4.,\ndrop=0.,\ndrop_path=0.,\nlocal_conv_size=3,\nactivation=nn.GELU,\n):\nsuper().__init__()\nself.dim = dim\nself.input_resolution = input_resolution\nself.num_heads = num_heads\nassert window_size &gt; 0, 'window_size must be greater than 0'\nself.window_size = window_size\nself.mlp_ratio = mlp_ratio\n# NOTE: `DropPath` is needed only for training.\n# self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()\nself.drop_path = nn.Identity()\nassert dim % num_heads == 0, 'dim must be divisible by num_heads'\nhead_dim = dim // num_heads\nwindow_resolution = (window_size, window_size)\nself.attn = Attention(dim, head_dim, num_heads, attn_ratio=1, resolution=window_resolution)\nmlp_hidden_dim = int(dim * mlp_ratio)\nmlp_activation = activation\nself.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=mlp_activation, drop=drop)\npad = local_conv_size // 2\nself.local_conv = Conv2d_BN(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)\ndef forward(self, x):\nH, W = self.input_resolution\nB, L, C = x.shape\nassert L == H * W, 'input feature has wrong size'\nres_x = x\nif H == self.window_size and W == self.window_size:\nx = self.attn(x)\nelse:\nx = x.view(B, H, W, C)\npad_b = (self.window_size - H % self.window_size) % self.window_size\npad_r = (self.window_size - W % self.window_size) % self.window_size\npadding = pad_b &gt; 0 or pad_r &gt; 0\nif padding:\nx = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\npH, pW = H + pad_b, W + pad_r\nnH = pH // self.window_size\nnW = pW // self.window_size\n# window partition\nx = x.view(B, nH, self.window_size, nW, self.window_size,\nC).transpose(2, 3).reshape(B * nH * nW, self.window_size * self.window_size, C)\nx = self.attn(x)\n# window reverse\nx = x.view(B, nH, nW, self.window_size, self.window_size, C).transpose(2, 3).reshape(B, pH, pW, C)\nif padding:\nx = x[:, :H, :W].contiguous()\nx = x.view(B, L, C)\nx = res_x + self.drop_path(x)\nx = x.transpose(1, 2).reshape(B, C, H, W)\nx = self.local_conv(x)\nx = x.view(B, C, L).transpose(1, 2)\nreturn x + self.drop_path(self.mlp(x))\ndef extra_repr(self) -&gt; str:\nreturn f'dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, ' \\\n               f'window_size={self.window_size}, mlp_ratio={self.mlp_ratio}'\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.BasicLayer","title":"<code>ultralytics.models.sam.modules.tiny_encoder.BasicLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>A basic TinyViT layer for one stage.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Number of input channels.</p> required <code>input_resolution</code> <code>tuple[int]</code> <p>Input resolution.</p> required <code>depth</code> <code>int</code> <p>Number of blocks.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>int</code> <p>Local window size.</p> required <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>drop</code> <code>float</code> <p>Dropout rate. Default: 0.0</p> <code>0.0</code> <code>drop_path</code> <code>float | tuple[float]</code> <p>Stochastic depth rate. Default: 0.0</p> <code>0.0</code> <code>downsample</code> <code>Module | None</code> <p>Downsample layer at the end of the layer. Default: None</p> <code>None</code> <code>use_checkpoint</code> <code>bool</code> <p>Whether to use checkpointing to save memory. Default: False.</p> <code>False</code> <code>local_conv_size</code> <code>int</code> <p>the kernel size of the depthwise convolution between attention and MLP. Default: 3</p> <code>3</code> <code>activation</code> <code>nn</code> <p>the activation function. Default: nn.GELU</p> <code>GELU</code> <code>out_dim</code> <code>int | optional</code> <p>the output dimension of the layer. Default: None</p> <code>None</code> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class BasicLayer(nn.Module):\n\"\"\"\n    A basic TinyViT layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        local_conv_size (int): the kernel size of the depthwise convolution between attention and MLP. Default: 3\n        activation (torch.nn): the activation function. Default: nn.GELU\n        out_dim (int | optional): the output dimension of the layer. Default: None\n    \"\"\"\ndef __init__(\nself,\ndim,\ninput_resolution,\ndepth,\nnum_heads,\nwindow_size,\nmlp_ratio=4.,\ndrop=0.,\ndrop_path=0.,\ndownsample=None,\nuse_checkpoint=False,\nlocal_conv_size=3,\nactivation=nn.GELU,\nout_dim=None,\n):\nsuper().__init__()\nself.dim = dim\nself.input_resolution = input_resolution\nself.depth = depth\nself.use_checkpoint = use_checkpoint\n# build blocks\nself.blocks = nn.ModuleList([\nTinyViTBlock(\ndim=dim,\ninput_resolution=input_resolution,\nnum_heads=num_heads,\nwindow_size=window_size,\nmlp_ratio=mlp_ratio,\ndrop=drop,\ndrop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\nlocal_conv_size=local_conv_size,\nactivation=activation,\n) for i in range(depth)])\n# patch merging layer\nself.downsample = None if downsample is None else downsample(\ninput_resolution, dim=dim, out_dim=out_dim, activation=activation)\ndef forward(self, x):\nfor blk in self.blocks:\nx = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)\nreturn x if self.downsample is None else self.downsample(x)\ndef extra_repr(self) -&gt; str:\nreturn f'dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}'\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.LayerNorm2d","title":"<code>ultralytics.models.sam.modules.tiny_encoder.LayerNorm2d</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class LayerNorm2d(nn.Module):\ndef __init__(self, num_channels: int, eps: float = 1e-6) -&gt; None:\nsuper().__init__()\nself.weight = nn.Parameter(torch.ones(num_channels))\nself.bias = nn.Parameter(torch.zeros(num_channels))\nself.eps = eps\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nu = x.mean(1, keepdim=True)\ns = (x - u).pow(2).mean(1, keepdim=True)\nx = (x - u) / torch.sqrt(s + self.eps)\nreturn self.weight[:, None, None] * x + self.bias[:, None, None]\n</code></pre>"},{"location":"reference/models/sam/modules/tiny_encoder/#ultralytics.models.sam.modules.tiny_encoder.TinyViT","title":"<code>ultralytics.models.sam.modules.tiny_encoder.TinyViT</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/tiny_encoder.py</code> <pre><code>class TinyViT(nn.Module):\ndef __init__(\nself,\nimg_size=224,\nin_chans=3,\nnum_classes=1000,\nembed_dims=[96, 192, 384, 768],\ndepths=[2, 2, 6, 2],\nnum_heads=[3, 6, 12, 24],\nwindow_sizes=[7, 7, 14, 7],\nmlp_ratio=4.,\ndrop_rate=0.,\ndrop_path_rate=0.1,\nuse_checkpoint=False,\nmbconv_expand_ratio=4.0,\nlocal_conv_size=3,\nlayer_lr_decay=1.0,\n):\nsuper().__init__()\nself.img_size = img_size\nself.num_classes = num_classes\nself.depths = depths\nself.num_layers = len(depths)\nself.mlp_ratio = mlp_ratio\nactivation = nn.GELU\nself.patch_embed = PatchEmbed(in_chans=in_chans,\nembed_dim=embed_dims[0],\nresolution=img_size,\nactivation=activation)\npatches_resolution = self.patch_embed.patches_resolution\nself.patches_resolution = patches_resolution\n# stochastic depth\ndpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n# build layers\nself.layers = nn.ModuleList()\nfor i_layer in range(self.num_layers):\nkwargs = dict(\ndim=embed_dims[i_layer],\ninput_resolution=(patches_resolution[0] // (2 ** (i_layer - 1 if i_layer == 3 else i_layer)),\npatches_resolution[1] // (2 ** (i_layer - 1 if i_layer == 3 else i_layer))),\n#   input_resolution=(patches_resolution[0] // (2 ** i_layer),\n#                     patches_resolution[1] // (2 ** i_layer)),\ndepth=depths[i_layer],\ndrop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\ndownsample=PatchMerging if (i_layer &lt; self.num_layers - 1) else None,\nuse_checkpoint=use_checkpoint,\nout_dim=embed_dims[min(i_layer + 1,\nlen(embed_dims) - 1)],\nactivation=activation,\n)\nif i_layer == 0:\nlayer = ConvLayer(conv_expand_ratio=mbconv_expand_ratio, **kwargs)\nelse:\nlayer = BasicLayer(num_heads=num_heads[i_layer],\nwindow_size=window_sizes[i_layer],\nmlp_ratio=self.mlp_ratio,\ndrop=drop_rate,\nlocal_conv_size=local_conv_size,\n**kwargs)\nself.layers.append(layer)\n# Classifier head\nself.norm_head = nn.LayerNorm(embed_dims[-1])\nself.head = nn.Linear(embed_dims[-1], num_classes) if num_classes &gt; 0 else torch.nn.Identity()\n# init weights\nself.apply(self._init_weights)\nself.set_layer_lr_decay(layer_lr_decay)\nself.neck = nn.Sequential(\nnn.Conv2d(\nembed_dims[-1],\n256,\nkernel_size=1,\nbias=False,\n),\nLayerNorm2d(256),\nnn.Conv2d(\n256,\n256,\nkernel_size=3,\npadding=1,\nbias=False,\n),\nLayerNorm2d(256),\n)\ndef set_layer_lr_decay(self, layer_lr_decay):\ndecay_rate = layer_lr_decay\n# layers -&gt; blocks (depth)\ndepth = sum(self.depths)\nlr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]\ndef _set_lr_scale(m, scale):\nfor p in m.parameters():\np.lr_scale = scale\nself.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))\ni = 0\nfor layer in self.layers:\nfor block in layer.blocks:\nblock.apply(lambda x: _set_lr_scale(x, lr_scales[i]))\ni += 1\nif layer.downsample is not None:\nlayer.downsample.apply(lambda x: _set_lr_scale(x, lr_scales[i - 1]))\nassert i == depth\nfor m in [self.norm_head, self.head]:\nm.apply(lambda x: _set_lr_scale(x, lr_scales[-1]))\nfor k, p in self.named_parameters():\np.param_name = k\ndef _check_lr_scale(m):\nfor p in m.parameters():\nassert hasattr(p, 'lr_scale'), p.param_name\nself.apply(_check_lr_scale)\ndef _init_weights(self, m):\nif isinstance(m, nn.Linear):\n# NOTE: This initialization is needed only for training.\n# trunc_normal_(m.weight, std=.02)\nif m.bias is not None:\nnn.init.constant_(m.bias, 0)\nelif isinstance(m, nn.LayerNorm):\nnn.init.constant_(m.bias, 0)\nnn.init.constant_(m.weight, 1.0)\n@torch.jit.ignore\ndef no_weight_decay_keywords(self):\nreturn {'attention_biases'}\ndef forward_features(self, x):\n# x: (N, C, H, W)\nx = self.patch_embed(x)\nx = self.layers[0](x)\nstart_i = 1\nfor i in range(start_i, len(self.layers)):\nlayer = self.layers[i]\nx = layer(x)\nB, _, C = x.size()\nx = x.view(B, 64, 64, C)\nx = x.permute(0, 3, 1, 2)\nreturn self.neck(x)\ndef forward(self, x):\nreturn self.forward_features(x)\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/","title":"Reference for <code>ultralytics/models/sam/modules/transformer.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/sam/modules/transformer.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayTransformer","title":"<code>ultralytics.models.sam.modules.transformer.TwoWayTransformer</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>class TwoWayTransformer(nn.Module):\ndef __init__(\nself,\ndepth: int,\nembedding_dim: int,\nnum_heads: int,\nmlp_dim: int,\nactivation: Type[nn.Module] = nn.ReLU,\nattention_downsample_rate: int = 2,\n) -&gt; None:\n\"\"\"\n        A transformer decoder that attends to an input image using\n        queries whose positional embedding is supplied.\n        Args:\n          depth (int): number of layers in the transformer\n          embedding_dim (int): the channel dimension for the input embeddings\n          num_heads (int): the number of heads for multihead attention. Must\n            divide embedding_dim\n          mlp_dim (int): the channel dimension internal to the MLP block\n          activation (nn.Module): the activation to use in the MLP block\n        \"\"\"\nsuper().__init__()\nself.depth = depth\nself.embedding_dim = embedding_dim\nself.num_heads = num_heads\nself.mlp_dim = mlp_dim\nself.layers = nn.ModuleList()\nfor i in range(depth):\nself.layers.append(\nTwoWayAttentionBlock(\nembedding_dim=embedding_dim,\nnum_heads=num_heads,\nmlp_dim=mlp_dim,\nactivation=activation,\nattention_downsample_rate=attention_downsample_rate,\nskip_first_layer_pe=(i == 0),\n))\nself.final_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.norm_final_attn = nn.LayerNorm(embedding_dim)\ndef forward(\nself,\nimage_embedding: Tensor,\nimage_pe: Tensor,\npoint_embedding: Tensor,\n) -&gt; Tuple[Tensor, Tensor]:\n\"\"\"\n        Args:\n          image_embedding (torch.Tensor): image to attend to. Should be shape B x embedding_dim x h x w for any h and w.\n          image_pe (torch.Tensor): the positional encoding to add to the image. Must have same shape as image_embedding.\n          point_embedding (torch.Tensor): the embedding to add to the query points.\n            Must have shape B x N_points x embedding_dim for any N_points.\n        Returns:\n          (torch.Tensor): the processed point_embedding\n          (torch.Tensor): the processed image_embedding\n        \"\"\"\n# BxCxHxW -&gt; BxHWxC == B x N_image_tokens x C\nbs, c, h, w = image_embedding.shape\nimage_embedding = image_embedding.flatten(2).permute(0, 2, 1)\nimage_pe = image_pe.flatten(2).permute(0, 2, 1)\n# Prepare queries\nqueries = point_embedding\nkeys = image_embedding\n# Apply transformer blocks and final layernorm\nfor layer in self.layers:\nqueries, keys = layer(\nqueries=queries,\nkeys=keys,\nquery_pe=point_embedding,\nkey_pe=image_pe,\n)\n# Apply the final attention layer from the points to the image\nq = queries + point_embedding\nk = keys + image_pe\nattn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\nqueries = queries + attn_out\nqueries = self.norm_final_attn(queries)\nreturn queries, keys\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayTransformer.__init__","title":"<code>__init__(depth, embedding_dim, num_heads, mlp_dim, activation=nn.ReLU, attention_downsample_rate=2)</code>","text":"<p>A transformer decoder that attends to an input image using queries whose positional embedding is supplied.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>number of layers in the transformer</p> required <code>embedding_dim</code> <code>int</code> <p>the channel dimension for the input embeddings</p> required <code>num_heads</code> <code>int</code> <p>the number of heads for multihead attention. Must divide embedding_dim</p> required <code>mlp_dim</code> <code>int</code> <p>the channel dimension internal to the MLP block</p> required <code>activation</code> <code>Module</code> <p>the activation to use in the MLP block</p> <code>ReLU</code> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>def __init__(\nself,\ndepth: int,\nembedding_dim: int,\nnum_heads: int,\nmlp_dim: int,\nactivation: Type[nn.Module] = nn.ReLU,\nattention_downsample_rate: int = 2,\n) -&gt; None:\n\"\"\"\n    A transformer decoder that attends to an input image using\n    queries whose positional embedding is supplied.\n    Args:\n      depth (int): number of layers in the transformer\n      embedding_dim (int): the channel dimension for the input embeddings\n      num_heads (int): the number of heads for multihead attention. Must\n        divide embedding_dim\n      mlp_dim (int): the channel dimension internal to the MLP block\n      activation (nn.Module): the activation to use in the MLP block\n    \"\"\"\nsuper().__init__()\nself.depth = depth\nself.embedding_dim = embedding_dim\nself.num_heads = num_heads\nself.mlp_dim = mlp_dim\nself.layers = nn.ModuleList()\nfor i in range(depth):\nself.layers.append(\nTwoWayAttentionBlock(\nembedding_dim=embedding_dim,\nnum_heads=num_heads,\nmlp_dim=mlp_dim,\nactivation=activation,\nattention_downsample_rate=attention_downsample_rate,\nskip_first_layer_pe=(i == 0),\n))\nself.final_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.norm_final_attn = nn.LayerNorm(embedding_dim)\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayTransformer.forward","title":"<code>forward(image_embedding, image_pe, point_embedding)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_embedding</code> <code>Tensor</code> <p>image to attend to. Should be shape B x embedding_dim x h x w for any h and w.</p> required <code>image_pe</code> <code>Tensor</code> <p>the positional encoding to add to the image. Must have same shape as image_embedding.</p> required <code>point_embedding</code> <code>Tensor</code> <p>the embedding to add to the query points. Must have shape B x N_points x embedding_dim for any N_points.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the processed point_embedding</p> <code>Tensor</code> <p>the processed image_embedding</p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>def forward(\nself,\nimage_embedding: Tensor,\nimage_pe: Tensor,\npoint_embedding: Tensor,\n) -&gt; Tuple[Tensor, Tensor]:\n\"\"\"\n    Args:\n      image_embedding (torch.Tensor): image to attend to. Should be shape B x embedding_dim x h x w for any h and w.\n      image_pe (torch.Tensor): the positional encoding to add to the image. Must have same shape as image_embedding.\n      point_embedding (torch.Tensor): the embedding to add to the query points.\n        Must have shape B x N_points x embedding_dim for any N_points.\n    Returns:\n      (torch.Tensor): the processed point_embedding\n      (torch.Tensor): the processed image_embedding\n    \"\"\"\n# BxCxHxW -&gt; BxHWxC == B x N_image_tokens x C\nbs, c, h, w = image_embedding.shape\nimage_embedding = image_embedding.flatten(2).permute(0, 2, 1)\nimage_pe = image_pe.flatten(2).permute(0, 2, 1)\n# Prepare queries\nqueries = point_embedding\nkeys = image_embedding\n# Apply transformer blocks and final layernorm\nfor layer in self.layers:\nqueries, keys = layer(\nqueries=queries,\nkeys=keys,\nquery_pe=point_embedding,\nkey_pe=image_pe,\n)\n# Apply the final attention layer from the points to the image\nq = queries + point_embedding\nk = keys + image_pe\nattn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\nqueries = queries + attn_out\nqueries = self.norm_final_attn(queries)\nreturn queries, keys\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayAttentionBlock","title":"<code>ultralytics.models.sam.modules.transformer.TwoWayAttentionBlock</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>class TwoWayAttentionBlock(nn.Module):\ndef __init__(\nself,\nembedding_dim: int,\nnum_heads: int,\nmlp_dim: int = 2048,\nactivation: Type[nn.Module] = nn.ReLU,\nattention_downsample_rate: int = 2,\nskip_first_layer_pe: bool = False,\n) -&gt; None:\n\"\"\"\n        A transformer block with four layers: (1) self-attention of sparse inputs, (2) cross attention of sparse\n        inputs to dense inputs, (3) mlp block on sparse inputs, and (4) cross attention of dense inputs to sparse\n        inputs.\n        Args:\n          embedding_dim (int): the channel dimension of the embeddings\n          num_heads (int): the number of heads in the attention layers\n          mlp_dim (int): the hidden dimension of the mlp block\n          activation (nn.Module): the activation of the mlp block\n          skip_first_layer_pe (bool): skip the PE on the first layer\n        \"\"\"\nsuper().__init__()\nself.self_attn = Attention(embedding_dim, num_heads)\nself.norm1 = nn.LayerNorm(embedding_dim)\nself.cross_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.norm2 = nn.LayerNorm(embedding_dim)\nself.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\nself.norm3 = nn.LayerNorm(embedding_dim)\nself.norm4 = nn.LayerNorm(embedding_dim)\nself.cross_attn_image_to_token = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.skip_first_layer_pe = skip_first_layer_pe\ndef forward(self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor) -&gt; Tuple[Tensor, Tensor]:\n\"\"\"Apply self-attention and cross-attention to queries and keys and return the processed embeddings.\"\"\"\n# Self attention block\nif self.skip_first_layer_pe:\nqueries = self.self_attn(q=queries, k=queries, v=queries)\nelse:\nq = queries + query_pe\nattn_out = self.self_attn(q=q, k=q, v=queries)\nqueries = queries + attn_out\nqueries = self.norm1(queries)\n# Cross attention block, tokens attending to image embedding\nq = queries + query_pe\nk = keys + key_pe\nattn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\nqueries = queries + attn_out\nqueries = self.norm2(queries)\n# MLP block\nmlp_out = self.mlp(queries)\nqueries = queries + mlp_out\nqueries = self.norm3(queries)\n# Cross attention block, image embedding attending to tokens\nq = queries + query_pe\nk = keys + key_pe\nattn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\nkeys = keys + attn_out\nkeys = self.norm4(keys)\nreturn queries, keys\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayAttentionBlock.__init__","title":"<code>__init__(embedding_dim, num_heads, mlp_dim=2048, activation=nn.ReLU, attention_downsample_rate=2, skip_first_layer_pe=False)</code>","text":"<p>A transformer block with four layers: (1) self-attention of sparse inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp block on sparse inputs, and (4) cross attention of dense inputs to sparse inputs.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>the channel dimension of the embeddings</p> required <code>num_heads</code> <code>int</code> <p>the number of heads in the attention layers</p> required <code>mlp_dim</code> <code>int</code> <p>the hidden dimension of the mlp block</p> <code>2048</code> <code>activation</code> <code>Module</code> <p>the activation of the mlp block</p> <code>ReLU</code> <code>skip_first_layer_pe</code> <code>bool</code> <p>skip the PE on the first layer</p> <code>False</code> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>def __init__(\nself,\nembedding_dim: int,\nnum_heads: int,\nmlp_dim: int = 2048,\nactivation: Type[nn.Module] = nn.ReLU,\nattention_downsample_rate: int = 2,\nskip_first_layer_pe: bool = False,\n) -&gt; None:\n\"\"\"\n    A transformer block with four layers: (1) self-attention of sparse inputs, (2) cross attention of sparse\n    inputs to dense inputs, (3) mlp block on sparse inputs, and (4) cross attention of dense inputs to sparse\n    inputs.\n    Args:\n      embedding_dim (int): the channel dimension of the embeddings\n      num_heads (int): the number of heads in the attention layers\n      mlp_dim (int): the hidden dimension of the mlp block\n      activation (nn.Module): the activation of the mlp block\n      skip_first_layer_pe (bool): skip the PE on the first layer\n    \"\"\"\nsuper().__init__()\nself.self_attn = Attention(embedding_dim, num_heads)\nself.norm1 = nn.LayerNorm(embedding_dim)\nself.cross_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.norm2 = nn.LayerNorm(embedding_dim)\nself.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\nself.norm3 = nn.LayerNorm(embedding_dim)\nself.norm4 = nn.LayerNorm(embedding_dim)\nself.cross_attn_image_to_token = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)\nself.skip_first_layer_pe = skip_first_layer_pe\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.TwoWayAttentionBlock.forward","title":"<code>forward(queries, keys, query_pe, key_pe)</code>","text":"<p>Apply self-attention and cross-attention to queries and keys and return the processed embeddings.</p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>def forward(self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor) -&gt; Tuple[Tensor, Tensor]:\n\"\"\"Apply self-attention and cross-attention to queries and keys and return the processed embeddings.\"\"\"\n# Self attention block\nif self.skip_first_layer_pe:\nqueries = self.self_attn(q=queries, k=queries, v=queries)\nelse:\nq = queries + query_pe\nattn_out = self.self_attn(q=q, k=q, v=queries)\nqueries = queries + attn_out\nqueries = self.norm1(queries)\n# Cross attention block, tokens attending to image embedding\nq = queries + query_pe\nk = keys + key_pe\nattn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\nqueries = queries + attn_out\nqueries = self.norm2(queries)\n# MLP block\nmlp_out = self.mlp(queries)\nqueries = queries + mlp_out\nqueries = self.norm3(queries)\n# Cross attention block, image embedding attending to tokens\nq = queries + query_pe\nk = keys + key_pe\nattn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\nkeys = keys + attn_out\nkeys = self.norm4(keys)\nreturn queries, keys\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.Attention","title":"<code>ultralytics.models.sam.modules.transformer.Attention</code>","text":"<p>             Bases: <code>Module</code></p> <p>An attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and values.</p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>class Attention(nn.Module):\n\"\"\"\n    An attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n    values.\n    \"\"\"\ndef __init__(\nself,\nembedding_dim: int,\nnum_heads: int,\ndownsample_rate: int = 1,\n) -&gt; None:\nsuper().__init__()\nself.embedding_dim = embedding_dim\nself.internal_dim = embedding_dim // downsample_rate\nself.num_heads = num_heads\nassert self.internal_dim % num_heads == 0, 'num_heads must divide embedding_dim.'\nself.q_proj = nn.Linear(embedding_dim, self.internal_dim)\nself.k_proj = nn.Linear(embedding_dim, self.internal_dim)\nself.v_proj = nn.Linear(embedding_dim, self.internal_dim)\nself.out_proj = nn.Linear(self.internal_dim, embedding_dim)\ndef _separate_heads(self, x: Tensor, num_heads: int) -&gt; Tensor:\n\"\"\"Separate the input tensor into the specified number of attention heads.\"\"\"\nb, n, c = x.shape\nx = x.reshape(b, n, num_heads, c // num_heads)\nreturn x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\ndef _recombine_heads(self, x: Tensor) -&gt; Tensor:\n\"\"\"Recombine the separated attention heads into a single tensor.\"\"\"\nb, n_heads, n_tokens, c_per_head = x.shape\nx = x.transpose(1, 2)\nreturn x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\ndef forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Compute the attention output given the input query, key, and value tensors.\"\"\"\n# Input projections\nq = self.q_proj(q)\nk = self.k_proj(k)\nv = self.v_proj(v)\n# Separate into heads\nq = self._separate_heads(q, self.num_heads)\nk = self._separate_heads(k, self.num_heads)\nv = self._separate_heads(v, self.num_heads)\n# Attention\n_, _, _, c_per_head = q.shape\nattn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\nattn = attn / math.sqrt(c_per_head)\nattn = torch.softmax(attn, dim=-1)\n# Get output\nout = attn @ v\nout = self._recombine_heads(out)\nreturn self.out_proj(out)\n</code></pre>"},{"location":"reference/models/sam/modules/transformer/#ultralytics.models.sam.modules.transformer.Attention.forward","title":"<code>forward(q, k, v)</code>","text":"<p>Compute the attention output given the input query, key, and value tensors.</p> Source code in <code>ultralytics/models/sam/modules/transformer.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Compute the attention output given the input query, key, and value tensors.\"\"\"\n# Input projections\nq = self.q_proj(q)\nk = self.k_proj(k)\nv = self.v_proj(v)\n# Separate into heads\nq = self._separate_heads(q, self.num_heads)\nk = self._separate_heads(k, self.num_heads)\nv = self._separate_heads(v, self.num_heads)\n# Attention\n_, _, _, c_per_head = q.shape\nattn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\nattn = attn / math.sqrt(c_per_head)\nattn = torch.softmax(attn, dim=-1)\n# Get output\nout = attn @ v\nout = self._recombine_heads(out)\nreturn self.out_proj(out)\n</code></pre>"},{"location":"reference/models/utils/loss/","title":"Reference for <code>ultralytics/models/utils/loss.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/utils/loss.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/models/utils/loss/#ultralytics.models.utils.loss.DETRLoss","title":"<code>ultralytics.models.utils.loss.DETRLoss</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/models/utils/loss.py</code> <pre><code>class DETRLoss(nn.Module):\ndef __init__(self,\nnc=80,\nloss_gain=None,\naux_loss=True,\nuse_fl=True,\nuse_vfl=False,\nuse_uni_match=False,\nuni_match_ind=0):\n\"\"\"\n        DETR loss function.\n        Args:\n            nc (int): The number of classes.\n            loss_gain (dict): The coefficient of loss.\n            aux_loss (bool): If 'aux_loss = True', loss at each decoder layer are to be used.\n            use_vfl (bool): Use VarifocalLoss or not.\n            use_uni_match (bool): Whether to use a fixed layer to assign labels for auxiliary branch.\n            uni_match_ind (int): The fixed indices of a layer.\n        \"\"\"\nsuper().__init__()\nif loss_gain is None:\nloss_gain = {'class': 1, 'bbox': 5, 'giou': 2, 'no_object': 0.1, 'mask': 1, 'dice': 1}\nself.nc = nc\nself.matcher = HungarianMatcher(cost_gain={'class': 2, 'bbox': 5, 'giou': 2})\nself.loss_gain = loss_gain\nself.aux_loss = aux_loss\nself.fl = FocalLoss() if use_fl else None\nself.vfl = VarifocalLoss() if use_vfl else None\nself.use_uni_match = use_uni_match\nself.uni_match_ind = uni_match_ind\nself.device = None\ndef _get_loss_class(self, pred_scores, targets, gt_scores, num_gts, postfix=''):\n# logits: [b, query, num_classes], gt_class: list[[n, 1]]\nname_class = f'loss_class{postfix}'\nbs, nq = pred_scores.shape[:2]\n# one_hot = F.one_hot(targets, self.nc + 1)[..., :-1]  # (bs, num_queries, num_classes)\none_hot = torch.zeros((bs, nq, self.nc + 1), dtype=torch.int64, device=targets.device)\none_hot.scatter_(2, targets.unsqueeze(-1), 1)\none_hot = one_hot[..., :-1]\ngt_scores = gt_scores.view(bs, nq, 1) * one_hot\nif self.fl:\nif num_gts and self.vfl:\nloss_cls = self.vfl(pred_scores, gt_scores, one_hot)\nelse:\nloss_cls = self.fl(pred_scores, one_hot.float())\nloss_cls /= max(num_gts, 1) / nq\nelse:\nloss_cls = nn.BCEWithLogitsLoss(reduction='none')(pred_scores, gt_scores).mean(1).sum()  # YOLO CLS loss\nreturn {name_class: loss_cls.squeeze() * self.loss_gain['class']}\ndef _get_loss_bbox(self, pred_bboxes, gt_bboxes, postfix=''):\n# boxes: [b, query, 4], gt_bbox: list[[n, 4]]\nname_bbox = f'loss_bbox{postfix}'\nname_giou = f'loss_giou{postfix}'\nloss = {}\nif len(gt_bboxes) == 0:\nloss[name_bbox] = torch.tensor(0., device=self.device)\nloss[name_giou] = torch.tensor(0., device=self.device)\nreturn loss\nloss[name_bbox] = self.loss_gain['bbox'] * F.l1_loss(pred_bboxes, gt_bboxes, reduction='sum') / len(gt_bboxes)\nloss[name_giou] = 1.0 - bbox_iou(pred_bboxes, gt_bboxes, xywh=True, GIoU=True)\nloss[name_giou] = loss[name_giou].sum() / len(gt_bboxes)\nloss[name_giou] = self.loss_gain['giou'] * loss[name_giou]\nloss = {k: v.squeeze() for k, v in loss.items()}\nreturn loss\ndef _get_loss_mask(self, masks, gt_mask, match_indices, postfix=''):\n# masks: [b, query, h, w], gt_mask: list[[n, H, W]]\nname_mask = f'loss_mask{postfix}'\nname_dice = f'loss_dice{postfix}'\nloss = {}\nif sum(len(a) for a in gt_mask) == 0:\nloss[name_mask] = torch.tensor(0., device=self.device)\nloss[name_dice] = torch.tensor(0., device=self.device)\nreturn loss\nnum_gts = len(gt_mask)\nsrc_masks, target_masks = self._get_assigned_bboxes(masks, gt_mask, match_indices)\nsrc_masks = F.interpolate(src_masks.unsqueeze(0), size=target_masks.shape[-2:], mode='bilinear')[0]\n# TODO: torch does not have `sigmoid_focal_loss`, but it's not urgent since we don't use mask branch for now.\nloss[name_mask] = self.loss_gain['mask'] * F.sigmoid_focal_loss(src_masks, target_masks,\ntorch.tensor([num_gts], dtype=torch.float32))\nloss[name_dice] = self.loss_gain['dice'] * self._dice_loss(src_masks, target_masks, num_gts)\nreturn loss\ndef _dice_loss(self, inputs, targets, num_gts):\ninputs = F.sigmoid(inputs)\ninputs = inputs.flatten(1)\ntargets = targets.flatten(1)\nnumerator = 2 * (inputs * targets).sum(1)\ndenominator = inputs.sum(-1) + targets.sum(-1)\nloss = 1 - (numerator + 1) / (denominator + 1)\nreturn loss.sum() / num_gts\ndef _get_loss_aux(self,\npred_bboxes,\npred_scores,\ngt_bboxes,\ngt_cls,\ngt_groups,\nmatch_indices=None,\npostfix='',\nmasks=None,\ngt_mask=None):\n\"\"\"Get auxiliary losses\"\"\"\n# NOTE: loss class, bbox, giou, mask, dice\nloss = torch.zeros(5 if masks is not None else 3, device=pred_bboxes.device)\nif match_indices is None and self.use_uni_match:\nmatch_indices = self.matcher(pred_bboxes[self.uni_match_ind],\npred_scores[self.uni_match_ind],\ngt_bboxes,\ngt_cls,\ngt_groups,\nmasks=masks[self.uni_match_ind] if masks is not None else None,\ngt_mask=gt_mask)\nfor i, (aux_bboxes, aux_scores) in enumerate(zip(pred_bboxes, pred_scores)):\naux_masks = masks[i] if masks is not None else None\nloss_ = self._get_loss(aux_bboxes,\naux_scores,\ngt_bboxes,\ngt_cls,\ngt_groups,\nmasks=aux_masks,\ngt_mask=gt_mask,\npostfix=postfix,\nmatch_indices=match_indices)\nloss[0] += loss_[f'loss_class{postfix}']\nloss[1] += loss_[f'loss_bbox{postfix}']\nloss[2] += loss_[f'loss_giou{postfix}']\n# if masks is not None and gt_mask is not None:\n#     loss_ = self._get_loss_mask(aux_masks, gt_mask, match_indices, postfix)\n#     loss[3] += loss_[f'loss_mask{postfix}']\n#     loss[4] += loss_[f'loss_dice{postfix}']\nloss = {\nf'loss_class_aux{postfix}': loss[0],\nf'loss_bbox_aux{postfix}': loss[1],\nf'loss_giou_aux{postfix}': loss[2]}\n# if masks is not None and gt_mask is not None:\n#     loss[f'loss_mask_aux{postfix}'] = loss[3]\n#     loss[f'loss_dice_aux{postfix}'] = loss[4]\nreturn loss\ndef _get_index(self, match_indices):\nbatch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(match_indices)])\nsrc_idx = torch.cat([src for (src, _) in match_indices])\ndst_idx = torch.cat([dst for (_, dst) in match_indices])\nreturn (batch_idx, src_idx), dst_idx\ndef _get_assigned_bboxes(self, pred_bboxes, gt_bboxes, match_indices):\npred_assigned = torch.cat([\nt[I] if len(I) &gt; 0 else torch.zeros(0, t.shape[-1], device=self.device)\nfor t, (I, _) in zip(pred_bboxes, match_indices)])\ngt_assigned = torch.cat([\nt[J] if len(J) &gt; 0 else torch.zeros(0, t.shape[-1], device=self.device)\nfor t, (_, J) in zip(gt_bboxes, match_indices)])\nreturn pred_assigned, gt_assigned\ndef _get_loss(self,\npred_bboxes,\npred_scores,\ngt_bboxes,\ngt_cls,\ngt_groups,\nmasks=None,\ngt_mask=None,\npostfix='',\nmatch_indices=None):\n\"\"\"Get losses\"\"\"\nif match_indices is None:\nmatch_indices = self.matcher(pred_bboxes,\npred_scores,\ngt_bboxes,\ngt_cls,\ngt_groups,\nmasks=masks,\ngt_mask=gt_mask)\nidx, gt_idx = self._get_index(match_indices)\npred_bboxes, gt_bboxes = pred_bboxes[idx], gt_bboxes[gt_idx]\nbs, nq = pred_scores.shape[:2]\ntargets = torch.full((bs, nq), self.nc, device=pred_scores.device, dtype=gt_cls.dtype)\ntargets[idx] = gt_cls[gt_idx]\ngt_scores = torch.zeros([bs, nq], device=pred_scores.device)\nif len(gt_bboxes):\ngt_scores[idx] = bbox_iou(pred_bboxes.detach(), gt_bboxes, xywh=True).squeeze(-1)\nloss = {}\nloss.update(self._get_loss_class(pred_scores, targets, gt_scores, len(gt_bboxes), postfix))\nloss.update(self._get_loss_bbox(pred_bboxes, gt_bboxes, postfix))\n# if masks is not None and gt_mask is not None:\n#     loss.update(self._get_loss_mask(masks, gt_mask, match_indices, postfix))\nreturn loss\ndef forward(self, pred_bboxes, pred_scores, batch, postfix='', **kwargs):\n\"\"\"\n        Args:\n            pred_bboxes (torch.Tensor): [l, b, query, 4]\n            pred_scores (torch.Tensor): [l, b, query, num_classes]\n            batch (dict): A dict includes:\n                gt_cls (torch.Tensor) with shape [num_gts, ],\n                gt_bboxes (torch.Tensor): [num_gts, 4],\n                gt_groups (List(int)): a list of batch size length includes the number of gts of each image.\n            postfix (str): postfix of loss name.\n        \"\"\"\nself.device = pred_bboxes.device\nmatch_indices = kwargs.get('match_indices', None)\ngt_cls, gt_bboxes, gt_groups = batch['cls'], batch['bboxes'], batch['gt_groups']\ntotal_loss = self._get_loss(pred_bboxes[-1],\npred_scores[-1],\ngt_bboxes,\ngt_cls,\ngt_groups,\npostfix=postfix,\nmatch_indices=match_indices)\nif self.aux_loss:\ntotal_loss.update(\nself._get_loss_aux(pred_bboxes[:-1], pred_scores[:-1], gt_bboxes, gt_cls, gt_groups, match_indices,\npostfix))\nreturn total_loss\n</code></pre>"},{"location":"reference/models/utils/loss/#ultralytics.models.utils.loss.DETRLoss.__init__","title":"<code>__init__(nc=80, loss_gain=None, aux_loss=True, use_fl=True, use_vfl=False, use_uni_match=False, uni_match_ind=0)</code>","text":"<p>DETR loss function.</p> <p>Parameters:</p> Name Type Description Default <code>nc</code> <code>int</code> <p>The number of classes.</p> <code>80</code> <code>loss_gain</code> <code>dict</code> <p>The coefficient of loss.</p> <code>None</code> <code>aux_loss</code> <code>bool</code> <p>If 'aux_loss = True', loss at each decoder layer are to be used.</p> <code>True</code> <code>use_vfl</code> <code>bool</code> <p>Use VarifocalLoss or not.</p> <code>False</code> <code>use_uni_match</code> <code>bool</code> <p>Whether to use a fixed layer to assign labels for auxiliary branch.</p> <code>False</code> <code>uni_match_ind</code> <code>int</code> <p>The fixed indices of a layer.</p> <code>0</code> Source code in <code>ultralytics/models/utils/loss.py</code> <pre><code>def __init__(self,\nnc=80,\nloss_gain=None,\naux_loss=True,\nuse_fl=True,\nuse_vfl=False,\nuse_uni_match=False,\nuni_match_ind=0):\n\"\"\"\n    DETR loss function.\n    Args:\n        nc (int): The number of classes.\n        loss_gain (dict): The coefficient of loss.\n        aux_loss (bool): If 'aux_loss = True', loss at each decoder layer are to be used.\n        use_vfl (bool): Use VarifocalLoss or not.\n        use_uni_match (bool): Whether to use a fixed layer to assign labels for auxiliary branch.\n        uni_match_ind (int): The fixed indices of a layer.\n    \"\"\"\nsuper().__init__()\nif loss_gain is None:\nloss_gain = {'class': 1, 'bbox': 5, 'giou': 2, 'no_object': 0.1, 'mask': 1, 'dice': 1}\nself.nc = nc\nself.matcher = HungarianMatcher(cost_gain={'class': 2, 'bbox': 5, 'giou': 2})\nself.loss_gain = loss_gain\nself.aux_loss = aux_loss\nself.fl = FocalLoss() if use_fl else None\nself.vfl = VarifocalLoss() if use_vfl else None\nself.use_uni_match = use_uni_match\nself.uni_match_ind = uni_match_ind\nself.device = None\n</code></pre>"},{"location":"reference/models/utils/loss/#ultralytics.models.utils.loss.DETRLoss.forward","title":"<code>forward(pred_bboxes, pred_scores, batch, postfix='', **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred_bboxes</code> <code>Tensor</code> <p>[l, b, query, 4]</p> required <code>pred_scores</code> <code>Tensor</code> <p>[l, b, query, num_classes]</p> required <code>batch</code> <code>dict</code> <p>A dict includes: gt_cls (torch.Tensor) with shape [num_gts, ], gt_bboxes (torch.Tensor): [num_gts, 4], gt_groups (List(int)): a list of batch size length includes the number of gts of each image.</p> required <code>postfix</code> <code>str</code> <p>postfix of loss name.</p> <code>''</code> Source code in <code>ultralytics/models/utils/loss.py</code> <pre><code>def forward(self, pred_bboxes, pred_scores, batch, postfix='', **kwargs):\n\"\"\"\n    Args:\n        pred_bboxes (torch.Tensor): [l, b, query, 4]\n        pred_scores (torch.Tensor): [l, b, query, num_classes]\n        batch (dict): A dict includes:\n            gt_cls (torch.Tensor) with shape [num_gts, ],\n            gt_bboxes (torch.Tensor): [num_gts, 4],\n            gt_groups (List(int)): a list of batch size length includes the number of gts of each image.\n        postfix (str): postfix of loss name.\n    \"\"\"\nself.device = pred_bboxes.device\nmatch_indices = kwargs.get('match_indices', None)\ngt_cls, gt_bboxes, gt_groups = batch['cls'], batch['bboxes'], batch['gt_groups']\ntotal_loss = self._get_loss(pred_bboxes[-1],\npred_scores[-1],\ngt_bboxes,\ngt_cls,\ngt_groups,\npostfix=postfix,\nmatch_indices=match_indices)\nif self.aux_loss:\ntotal_loss.update(\nself._get_loss_aux(pred_bboxes[:-1], pred_scores[:-1], gt_bboxes, gt_cls, gt_groups, match_indices,\npostfix))\nreturn total_loss\n</code></pre>"},{"location":"reference/models/utils/loss/#ultralytics.models.utils.loss.RTDETRDetectionLoss","title":"<code>ultralytics.models.utils.loss.RTDETRDetectionLoss</code>","text":"<p>             Bases: <code>DETRLoss</code></p> Source code in <code>ultralytics/models/utils/loss.py</code> <pre><code>class RTDETRDetectionLoss(DETRLoss):\ndef forward(self, preds, batch, dn_bboxes=None, dn_scores=None, dn_meta=None):\npred_bboxes, pred_scores = preds\ntotal_loss = super().forward(pred_bboxes, pred_scores, batch)\nif dn_meta is not None:\ndn_pos_idx, dn_num_group = dn_meta['dn_pos_idx'], dn_meta['dn_num_group']\nassert len(batch['gt_groups']) == len(dn_pos_idx)\n# denoising match indices\nmatch_indices = self.get_dn_match_indices(dn_pos_idx, dn_num_group, batch['gt_groups'])\n# compute denoising training loss\ndn_loss = super().forward(dn_bboxes, dn_scores, batch, postfix='_dn', match_indices=match_indices)\ntotal_loss.update(dn_loss)\nelse:\ntotal_loss.update({f'{k}_dn': torch.tensor(0., device=self.device) for k in total_loss.keys()})\nreturn total_loss\n@staticmethod\ndef get_dn_match_indices(dn_pos_idx, dn_num_group, gt_groups):\n\"\"\"Get the match indices for denoising.\n        Args:\n            dn_pos_idx (List[torch.Tensor]): A list includes positive indices of denoising.\n            dn_num_group (int): The number of groups of denoising.\n            gt_groups (List(int)): a list of batch size length includes the number of gts of each image.\n        Returns:\n            dn_match_indices (List(tuple)): Matched indices.\n        \"\"\"\ndn_match_indices = []\nidx_groups = torch.as_tensor([0, *gt_groups[:-1]]).cumsum_(0)\nfor i, num_gt in enumerate(gt_groups):\nif num_gt &gt; 0:\ngt_idx = torch.arange(end=num_gt, dtype=torch.long) + idx_groups[i]\ngt_idx = gt_idx.repeat(dn_num_group)\nassert len(dn_pos_idx[i]) == len(gt_idx), 'Expected the same length, '\nf'but got {len(dn_pos_idx[i])} and {len(gt_idx)} respectively.'\ndn_match_indices.append((dn_pos_idx[i], gt_idx))\nelse:\ndn_match_indices.append((torch.zeros([0], dtype=torch.long), torch.zeros([0], dtype=torch.long)))\nreturn dn_match_indices\n</code></pre>"},{"location":"reference/models/utils/loss/#ultralytics.models.utils.loss.RTDETRDetectionLoss.get_dn_match_indices","title":"<code>get_dn_match_indices(dn_pos_idx, dn_num_group, gt_groups)</code>  <code>staticmethod</code>","text":"<p>Get the match indices for denoising.</p> <p>Parameters:</p> Name Type Description Default <code>dn_pos_idx</code> <code>List[torch.Tensor]</code> <p>A list includes positive indices of denoising.</p> required <code>dn_num_group</code> <code>int</code> <p>The number of groups of denoising.</p> required <code>gt_groups</code> <code>List(int</code> <p>a list of batch size length includes the number of gts of each image.</p> required <p>Returns:</p> Name Type Description <code>dn_match_indices</code> <code>List(tuple)</code> <p>Matched indices.</p> Source code in <code>ultralytics/models/utils/loss.py</code> <pre><code>@staticmethod\ndef get_dn_match_indices(dn_pos_idx, dn_num_group, gt_groups):\n\"\"\"Get the match indices for denoising.\n    Args:\n        dn_pos_idx (List[torch.Tensor]): A list includes positive indices of denoising.\n        dn_num_group (int): The number of groups of denoising.\n        gt_groups (List(int)): a list of batch size length includes the number of gts of each image.\n    Returns:\n        dn_match_indices (List(tuple)): Matched indices.\n    \"\"\"\ndn_match_indices = []\nidx_groups = torch.as_tensor([0, *gt_groups[:-1]]).cumsum_(0)\nfor i, num_gt in enumerate(gt_groups):\nif num_gt &gt; 0:\ngt_idx = torch.arange(end=num_gt, dtype=torch.long) + idx_groups[i]\ngt_idx = gt_idx.repeat(dn_num_group)\nassert len(dn_pos_idx[i]) == len(gt_idx), 'Expected the same length, '\nf'but got {len(dn_pos_idx[i])} and {len(gt_idx)} respectively.'\ndn_match_indices.append((dn_pos_idx[i], gt_idx))\nelse:\ndn_match_indices.append((torch.zeros([0], dtype=torch.long), torch.zeros([0], dtype=torch.long)))\nreturn dn_match_indices\n</code></pre>"},{"location":"reference/models/utils/ops/","title":"Reference for <code>ultralytics/models/utils/ops.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/utils/ops.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/models/utils/ops/#ultralytics.models.utils.ops.HungarianMatcher","title":"<code>ultralytics.models.utils.ops.HungarianMatcher</code>","text":"<p>             Bases: <code>Module</code></p> <p>A module implementing the HungarianMatcher, which is a differentiable module to solve the assignment problem in an end-to-end fashion.</p> <p>HungarianMatcher performs optimal assignment over predicted and ground truth bounding boxes using a cost function that considers classification scores, bounding box coordinates, and optionally, mask predictions.</p> <p>Attributes:</p> Name Type Description <code>cost_gain</code> <code>dict</code> <p>Dictionary of cost coefficients for different components: 'class', 'bbox', 'giou', 'mask', and 'dice'.</p> <code>use_fl</code> <code>bool</code> <p>Indicates whether to use Focal Loss for the classification cost calculation.</p> <code>with_mask</code> <code>bool</code> <p>Indicates whether the model makes mask predictions.</p> <code>num_sample_points</code> <code>int</code> <p>The number of sample points used in mask cost calculation.</p> <code>alpha</code> <code>float</code> <p>The alpha factor in Focal Loss calculation.</p> <code>gamma</code> <code>float</code> <p>The gamma factor in Focal Loss calculation.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Computes the assignment</p> <code>_cost_mask</code> <p>Computes the mask cost and dice cost if masks are predicted.</p> Source code in <code>ultralytics/models/utils/ops.py</code> <pre><code>class HungarianMatcher(nn.Module):\n\"\"\"\n    A module implementing the HungarianMatcher, which is a differentiable module to solve the assignment problem in\n    an end-to-end fashion.\n    HungarianMatcher performs optimal assignment over predicted and ground truth bounding boxes using a cost function\n    that considers classification scores, bounding box coordinates, and optionally, mask predictions.\n    Attributes:\n        cost_gain (dict): Dictionary of cost coefficients for different components: 'class', 'bbox', 'giou', 'mask', and 'dice'.\n        use_fl (bool): Indicates whether to use Focal Loss for the classification cost calculation.\n        with_mask (bool): Indicates whether the model makes mask predictions.\n        num_sample_points (int): The number of sample points used in mask cost calculation.\n        alpha (float): The alpha factor in Focal Loss calculation.\n        gamma (float): The gamma factor in Focal Loss calculation.\n    Methods:\n        forward(pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None): Computes the assignment\n        between predictions and ground truths for a batch.\n        _cost_mask(bs, num_gts, masks=None, gt_mask=None): Computes the mask cost and dice cost if masks are predicted.\n    \"\"\"\ndef __init__(self, cost_gain=None, use_fl=True, with_mask=False, num_sample_points=12544, alpha=0.25, gamma=2.0):\nsuper().__init__()\nif cost_gain is None:\ncost_gain = {'class': 1, 'bbox': 5, 'giou': 2, 'mask': 1, 'dice': 1}\nself.cost_gain = cost_gain\nself.use_fl = use_fl\nself.with_mask = with_mask\nself.num_sample_points = num_sample_points\nself.alpha = alpha\nself.gamma = gamma\ndef forward(self, pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None):\n\"\"\"\n        Forward pass for HungarianMatcher. This function computes costs based on prediction and ground truth\n        (classification cost, L1 cost between boxes and GIoU cost between boxes) and finds the optimal matching\n        between predictions and ground truth based on these costs.\n        Args:\n            pred_bboxes (Tensor): Predicted bounding boxes with shape [batch_size, num_queries, 4].\n            pred_scores (Tensor): Predicted scores with shape [batch_size, num_queries, num_classes].\n            gt_cls (torch.Tensor): Ground truth classes with shape [num_gts, ].\n            gt_bboxes (torch.Tensor): Ground truth bounding boxes with shape [num_gts, 4].\n            gt_groups (List[int]): List of length equal to batch size, containing the number of ground truths for\n                each image.\n            masks (Tensor, optional): Predicted masks with shape [batch_size, num_queries, height, width].\n                Defaults to None.\n            gt_mask (List[Tensor], optional): List of ground truth masks, each with shape [num_masks, Height, Width].\n                Defaults to None.\n        Returns:\n            (List[Tuple[Tensor, Tensor]]): A list of size batch_size, each element is a tuple (index_i, index_j), where:\n                - index_i is the tensor of indices of the selected predictions (in order)\n                - index_j is the tensor of indices of the corresponding selected ground truth targets (in order)\n                For each batch element, it holds:\n                    len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\nbs, nq, nc = pred_scores.shape\nif sum(gt_groups) == 0:\nreturn [(torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)) for _ in range(bs)]\n# We flatten to compute the cost matrices in a batch\n# [batch_size * num_queries, num_classes]\npred_scores = pred_scores.detach().view(-1, nc)\npred_scores = F.sigmoid(pred_scores) if self.use_fl else F.softmax(pred_scores, dim=-1)\n# [batch_size * num_queries, 4]\npred_bboxes = pred_bboxes.detach().view(-1, 4)\n# Compute the classification cost\npred_scores = pred_scores[:, gt_cls]\nif self.use_fl:\nneg_cost_class = (1 - self.alpha) * (pred_scores ** self.gamma) * (-(1 - pred_scores + 1e-8).log())\npos_cost_class = self.alpha * ((1 - pred_scores) ** self.gamma) * (-(pred_scores + 1e-8).log())\ncost_class = pos_cost_class - neg_cost_class\nelse:\ncost_class = -pred_scores\n# Compute the L1 cost between boxes\ncost_bbox = (pred_bboxes.unsqueeze(1) - gt_bboxes.unsqueeze(0)).abs().sum(-1)  # (bs*num_queries, num_gt)\n# Compute the GIoU cost between boxes, (bs*num_queries, num_gt)\ncost_giou = 1.0 - bbox_iou(pred_bboxes.unsqueeze(1), gt_bboxes.unsqueeze(0), xywh=True, GIoU=True).squeeze(-1)\n# Final cost matrix\nC = self.cost_gain['class'] * cost_class + \\\n            self.cost_gain['bbox'] * cost_bbox + \\\n            self.cost_gain['giou'] * cost_giou\n# Compute the mask cost and dice cost\nif self.with_mask:\nC += self._cost_mask(bs, gt_groups, masks, gt_mask)\nC = C.view(bs, nq, -1).cpu()\nindices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(gt_groups, -1))]\ngt_groups = torch.as_tensor([0, *gt_groups[:-1]]).cumsum_(0)\n# (idx for queries, idx for gt)\nreturn [(torch.tensor(i, dtype=torch.long), torch.tensor(j, dtype=torch.long) + gt_groups[k])\nfor k, (i, j) in enumerate(indices)]\ndef _cost_mask(self, bs, num_gts, masks=None, gt_mask=None):\nassert masks is not None and gt_mask is not None, 'Make sure the input has `mask` and `gt_mask`'\n# all masks share the same set of points for efficient matching\nsample_points = torch.rand([bs, 1, self.num_sample_points, 2])\nsample_points = 2.0 * sample_points - 1.0\nout_mask = F.grid_sample(masks.detach(), sample_points, align_corners=False).squeeze(-2)\nout_mask = out_mask.flatten(0, 1)\ntgt_mask = torch.cat(gt_mask).unsqueeze(1)\nsample_points = torch.cat([a.repeat(b, 1, 1, 1) for a, b in zip(sample_points, num_gts) if b &gt; 0])\ntgt_mask = F.grid_sample(tgt_mask, sample_points, align_corners=False).squeeze([1, 2])\nwith torch.cuda.amp.autocast(False):\n# binary cross entropy cost\npos_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.ones_like(out_mask), reduction='none')\nneg_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.zeros_like(out_mask), reduction='none')\ncost_mask = torch.matmul(pos_cost_mask, tgt_mask.T) + torch.matmul(neg_cost_mask, 1 - tgt_mask.T)\ncost_mask /= self.num_sample_points\n# dice cost\nout_mask = F.sigmoid(out_mask)\nnumerator = 2 * torch.matmul(out_mask, tgt_mask.T)\ndenominator = out_mask.sum(-1, keepdim=True) + tgt_mask.sum(-1).unsqueeze(0)\ncost_dice = 1 - (numerator + 1) / (denominator + 1)\nC = self.cost_gain['mask'] * cost_mask + self.cost_gain['dice'] * cost_dice\nreturn C\n</code></pre>"},{"location":"reference/models/utils/ops/#ultralytics.models.utils.ops.HungarianMatcher.forward","title":"<code>forward(pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None)</code>","text":"<p>Forward pass for HungarianMatcher. This function computes costs based on prediction and ground truth (classification cost, L1 cost between boxes and GIoU cost between boxes) and finds the optimal matching between predictions and ground truth based on these costs.</p> <p>Parameters:</p> Name Type Description Default <code>pred_bboxes</code> <code>Tensor</code> <p>Predicted bounding boxes with shape [batch_size, num_queries, 4].</p> required <code>pred_scores</code> <code>Tensor</code> <p>Predicted scores with shape [batch_size, num_queries, num_classes].</p> required <code>gt_cls</code> <code>Tensor</code> <p>Ground truth classes with shape [num_gts, ].</p> required <code>gt_bboxes</code> <code>Tensor</code> <p>Ground truth bounding boxes with shape [num_gts, 4].</p> required <code>gt_groups</code> <code>List[int]</code> <p>List of length equal to batch size, containing the number of ground truths for each image.</p> required <code>masks</code> <code>Tensor</code> <p>Predicted masks with shape [batch_size, num_queries, height, width]. Defaults to None.</p> <code>None</code> <code>gt_mask</code> <code>List[Tensor]</code> <p>List of ground truth masks, each with shape [num_masks, Height, Width]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Tensor, Tensor]]</code> <p>A list of size batch_size, each element is a tuple (index_i, index_j), where: - index_i is the tensor of indices of the selected predictions (in order) - index_j is the tensor of indices of the corresponding selected ground truth targets (in order) For each batch element, it holds:     len(index_i) = len(index_j) = min(num_queries, num_target_boxes)</p> Source code in <code>ultralytics/models/utils/ops.py</code> <pre><code>def forward(self, pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None):\n\"\"\"\n    Forward pass for HungarianMatcher. This function computes costs based on prediction and ground truth\n    (classification cost, L1 cost between boxes and GIoU cost between boxes) and finds the optimal matching\n    between predictions and ground truth based on these costs.\n    Args:\n        pred_bboxes (Tensor): Predicted bounding boxes with shape [batch_size, num_queries, 4].\n        pred_scores (Tensor): Predicted scores with shape [batch_size, num_queries, num_classes].\n        gt_cls (torch.Tensor): Ground truth classes with shape [num_gts, ].\n        gt_bboxes (torch.Tensor): Ground truth bounding boxes with shape [num_gts, 4].\n        gt_groups (List[int]): List of length equal to batch size, containing the number of ground truths for\n            each image.\n        masks (Tensor, optional): Predicted masks with shape [batch_size, num_queries, height, width].\n            Defaults to None.\n        gt_mask (List[Tensor], optional): List of ground truth masks, each with shape [num_masks, Height, Width].\n            Defaults to None.\n    Returns:\n        (List[Tuple[Tensor, Tensor]]): A list of size batch_size, each element is a tuple (index_i, index_j), where:\n            - index_i is the tensor of indices of the selected predictions (in order)\n            - index_j is the tensor of indices of the corresponding selected ground truth targets (in order)\n            For each batch element, it holds:\n                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n    \"\"\"\nbs, nq, nc = pred_scores.shape\nif sum(gt_groups) == 0:\nreturn [(torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)) for _ in range(bs)]\n# We flatten to compute the cost matrices in a batch\n# [batch_size * num_queries, num_classes]\npred_scores = pred_scores.detach().view(-1, nc)\npred_scores = F.sigmoid(pred_scores) if self.use_fl else F.softmax(pred_scores, dim=-1)\n# [batch_size * num_queries, 4]\npred_bboxes = pred_bboxes.detach().view(-1, 4)\n# Compute the classification cost\npred_scores = pred_scores[:, gt_cls]\nif self.use_fl:\nneg_cost_class = (1 - self.alpha) * (pred_scores ** self.gamma) * (-(1 - pred_scores + 1e-8).log())\npos_cost_class = self.alpha * ((1 - pred_scores) ** self.gamma) * (-(pred_scores + 1e-8).log())\ncost_class = pos_cost_class - neg_cost_class\nelse:\ncost_class = -pred_scores\n# Compute the L1 cost between boxes\ncost_bbox = (pred_bboxes.unsqueeze(1) - gt_bboxes.unsqueeze(0)).abs().sum(-1)  # (bs*num_queries, num_gt)\n# Compute the GIoU cost between boxes, (bs*num_queries, num_gt)\ncost_giou = 1.0 - bbox_iou(pred_bboxes.unsqueeze(1), gt_bboxes.unsqueeze(0), xywh=True, GIoU=True).squeeze(-1)\n# Final cost matrix\nC = self.cost_gain['class'] * cost_class + \\\n        self.cost_gain['bbox'] * cost_bbox + \\\n        self.cost_gain['giou'] * cost_giou\n# Compute the mask cost and dice cost\nif self.with_mask:\nC += self._cost_mask(bs, gt_groups, masks, gt_mask)\nC = C.view(bs, nq, -1).cpu()\nindices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(gt_groups, -1))]\ngt_groups = torch.as_tensor([0, *gt_groups[:-1]]).cumsum_(0)\n# (idx for queries, idx for gt)\nreturn [(torch.tensor(i, dtype=torch.long), torch.tensor(j, dtype=torch.long) + gt_groups[k])\nfor k, (i, j) in enumerate(indices)]\n</code></pre>"},{"location":"reference/models/utils/ops/#ultralytics.models.utils.ops.get_cdn_group","title":"<code>ultralytics.models.utils.ops.get_cdn_group(batch, num_classes, num_queries, class_embed, num_dn=100, cls_noise_ratio=0.5, box_noise_scale=1.0, training=False)</code>","text":"<p>Get contrastive denoising training group. This function creates a contrastive denoising training group with positive and negative samples from the ground truths (gt). It applies noise to the class labels and bounding box coordinates, and returns the modified labels, bounding boxes, attention mask and meta information.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>A dict that includes 'gt_cls' (torch.Tensor with shape [num_gts, ]), 'gt_bboxes' (torch.Tensor with shape [num_gts, 4]), 'gt_groups' (List(int)) which is a list of batch size length indicating the number of gts of each image.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>num_queries</code> <code>int</code> <p>Number of queries.</p> required <code>class_embed</code> <code>Tensor</code> <p>Embedding weights to map class labels to embedding space.</p> required <code>num_dn</code> <code>int</code> <p>Number of denoising. Defaults to 100.</p> <code>100</code> <code>cls_noise_ratio</code> <code>float</code> <p>Noise ratio for class labels. Defaults to 0.5.</p> <code>0.5</code> <code>box_noise_scale</code> <code>float</code> <p>Noise scale for bounding box coordinates. Defaults to 1.0.</p> <code>1.0</code> <code>training</code> <code>bool</code> <p>If it's in training mode. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Dict]]</code> <p>The modified class embeddings, bounding boxes, attention mask and meta information for denoising. If not in training mode or 'num_dn' is less than or equal to 0, the function returns None for all elements in the tuple.</p> Source code in <code>ultralytics/models/utils/ops.py</code> <pre><code>def get_cdn_group(batch,\nnum_classes,\nnum_queries,\nclass_embed,\nnum_dn=100,\ncls_noise_ratio=0.5,\nbox_noise_scale=1.0,\ntraining=False):\n\"\"\"\n    Get contrastive denoising training group. This function creates a contrastive denoising training group with\n    positive and negative samples from the ground truths (gt). It applies noise to the class labels and bounding\n    box coordinates, and returns the modified labels, bounding boxes, attention mask and meta information.\n    Args:\n        batch (dict): A dict that includes 'gt_cls' (torch.Tensor with shape [num_gts, ]), 'gt_bboxes'\n            (torch.Tensor with shape [num_gts, 4]), 'gt_groups' (List(int)) which is a list of batch size length\n            indicating the number of gts of each image.\n        num_classes (int): Number of classes.\n        num_queries (int): Number of queries.\n        class_embed (torch.Tensor): Embedding weights to map class labels to embedding space.\n        num_dn (int, optional): Number of denoising. Defaults to 100.\n        cls_noise_ratio (float, optional): Noise ratio for class labels. Defaults to 0.5.\n        box_noise_scale (float, optional): Noise scale for bounding box coordinates. Defaults to 1.0.\n        training (bool, optional): If it's in training mode. Defaults to False.\n    Returns:\n        (Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Dict]]): The modified class embeddings,\n            bounding boxes, attention mask and meta information for denoising. If not in training mode or 'num_dn'\n            is less than or equal to 0, the function returns None for all elements in the tuple.\n    \"\"\"\nif (not training) or num_dn &lt;= 0:\nreturn None, None, None, None\ngt_groups = batch['gt_groups']\ntotal_num = sum(gt_groups)\nmax_nums = max(gt_groups)\nif max_nums == 0:\nreturn None, None, None, None\nnum_group = num_dn // max_nums\nnum_group = 1 if num_group == 0 else num_group\n# pad gt to max_num of a batch\nbs = len(gt_groups)\ngt_cls = batch['cls']  # (bs*num, )\ngt_bbox = batch['bboxes']  # bs*num, 4\nb_idx = batch['batch_idx']\n# each group has positive and negative queries.\ndn_cls = gt_cls.repeat(2 * num_group)  # (2*num_group*bs*num, )\ndn_bbox = gt_bbox.repeat(2 * num_group, 1)  # 2*num_group*bs*num, 4\ndn_b_idx = b_idx.repeat(2 * num_group).view(-1)  # (2*num_group*bs*num, )\n# positive and negative mask\n# (bs*num*num_group, ), the second total_num*num_group part as negative samples\nneg_idx = torch.arange(total_num * num_group, dtype=torch.long, device=gt_bbox.device) + num_group * total_num\nif cls_noise_ratio &gt; 0:\n# half of bbox prob\nmask = torch.rand(dn_cls.shape) &lt; (cls_noise_ratio * 0.5)\nidx = torch.nonzero(mask).squeeze(-1)\n# randomly put a new one here\nnew_label = torch.randint_like(idx, 0, num_classes, dtype=dn_cls.dtype, device=dn_cls.device)\ndn_cls[idx] = new_label\nif box_noise_scale &gt; 0:\nknown_bbox = xywh2xyxy(dn_bbox)\ndiff = (dn_bbox[..., 2:] * 0.5).repeat(1, 2) * box_noise_scale  # 2*num_group*bs*num, 4\nrand_sign = torch.randint_like(dn_bbox, 0, 2) * 2.0 - 1.0\nrand_part = torch.rand_like(dn_bbox)\nrand_part[neg_idx] += 1.0\nrand_part *= rand_sign\nknown_bbox += rand_part * diff\nknown_bbox.clip_(min=0.0, max=1.0)\ndn_bbox = xyxy2xywh(known_bbox)\ndn_bbox = inverse_sigmoid(dn_bbox)\n# total denoising queries\nnum_dn = int(max_nums * 2 * num_group)\n# class_embed = torch.cat([class_embed, torch.zeros([1, class_embed.shape[-1]], device=class_embed.device)])\ndn_cls_embed = class_embed[dn_cls]  # bs*num * 2 * num_group, 256\npadding_cls = torch.zeros(bs, num_dn, dn_cls_embed.shape[-1], device=gt_cls.device)\npadding_bbox = torch.zeros(bs, num_dn, 4, device=gt_bbox.device)\nmap_indices = torch.cat([torch.tensor(range(num), dtype=torch.long) for num in gt_groups])\npos_idx = torch.stack([map_indices + max_nums * i for i in range(num_group)], dim=0)\nmap_indices = torch.cat([map_indices + max_nums * i for i in range(2 * num_group)])\npadding_cls[(dn_b_idx, map_indices)] = dn_cls_embed\npadding_bbox[(dn_b_idx, map_indices)] = dn_bbox\ntgt_size = num_dn + num_queries\nattn_mask = torch.zeros([tgt_size, tgt_size], dtype=torch.bool)\n# match query cannot see the reconstruct\nattn_mask[num_dn:, :num_dn] = True\n# reconstruct cannot see each other\nfor i in range(num_group):\nif i == 0:\nattn_mask[max_nums * 2 * i:max_nums * 2 * (i + 1), max_nums * 2 * (i + 1):num_dn] = True\nif i == num_group - 1:\nattn_mask[max_nums * 2 * i:max_nums * 2 * (i + 1), :max_nums * i * 2] = True\nelse:\nattn_mask[max_nums * 2 * i:max_nums * 2 * (i + 1), max_nums * 2 * (i + 1):num_dn] = True\nattn_mask[max_nums * 2 * i:max_nums * 2 * (i + 1), :max_nums * 2 * i] = True\ndn_meta = {\n'dn_pos_idx': [p.reshape(-1) for p in pos_idx.cpu().split(list(gt_groups), dim=1)],\n'dn_num_group': num_group,\n'dn_num_split': [num_dn, num_queries]}\nreturn padding_cls.to(class_embed.device), padding_bbox.to(class_embed.device), attn_mask.to(\nclass_embed.device), dn_meta\n</code></pre>"},{"location":"reference/models/utils/ops/#ultralytics.models.utils.ops.inverse_sigmoid","title":"<code>ultralytics.models.utils.ops.inverse_sigmoid(x, eps=1e-06)</code>","text":"<p>Inverse sigmoid function.</p> Source code in <code>ultralytics/models/utils/ops.py</code> <pre><code>def inverse_sigmoid(x, eps=1e-6):\n\"\"\"Inverse sigmoid function.\"\"\"\nx = x.clip(min=0., max=1.)\nreturn torch.log(x / (1 - x + eps) + eps)\n</code></pre>"},{"location":"reference/models/yolo/model/","title":"Reference for <code>ultralytics/models/yolo/model.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/model.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/model/#ultralytics.models.yolo.model.YOLO","title":"<code>ultralytics.models.yolo.model.YOLO</code>","text":"<p>             Bases: <code>Model</code></p> <p>YOLO (You Only Look Once) object detection model.</p> Source code in <code>ultralytics/models/yolo/model.py</code> <pre><code>class YOLO(Model):\n\"\"\"\n    YOLO (You Only Look Once) object detection model.\n    \"\"\"\n@property\ndef task_map(self):\n\"\"\"Map head to model, trainer, validator, and predictor classes\"\"\"\nreturn {\n'classify': {\n'model': ClassificationModel,\n'trainer': yolo.classify.ClassificationTrainer,\n'validator': yolo.classify.ClassificationValidator,\n'predictor': yolo.classify.ClassificationPredictor, },\n'detect': {\n'model': DetectionModel,\n'trainer': yolo.detect.DetectionTrainer,\n'validator': yolo.detect.DetectionValidator,\n'predictor': yolo.detect.DetectionPredictor, },\n'segment': {\n'model': SegmentationModel,\n'trainer': yolo.segment.SegmentationTrainer,\n'validator': yolo.segment.SegmentationValidator,\n'predictor': yolo.segment.SegmentationPredictor, },\n'pose': {\n'model': PoseModel,\n'trainer': yolo.pose.PoseTrainer,\n'validator': yolo.pose.PoseValidator,\n'predictor': yolo.pose.PosePredictor, }, }\n</code></pre>"},{"location":"reference/models/yolo/model/#ultralytics.models.yolo.model.YOLO.task_map","title":"<code>task_map</code>  <code>property</code>","text":"<p>Map head to model, trainer, validator, and predictor classes</p>"},{"location":"reference/models/yolo/classify/predict/","title":"Reference for <code>ultralytics/models/yolo/classify/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/classify/predict/#ultralytics.models.yolo.classify.predict.ClassificationPredictor","title":"<code>ultralytics.models.yolo.classify.predict.ClassificationPredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> <p>A class extending the BasePredictor class for prediction based on a classification model.</p> Notes <ul> <li>Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.</li> </ul> Example <pre><code>from ultralytics.utils import ASSETS\nfrom ultralytics.models.yolo.classify import ClassificationPredictor\nargs = dict(model='yolov8n-cls.pt', source=ASSETS)\npredictor = ClassificationPredictor(overrides=args)\npredictor.predict_cli()\n</code></pre> Source code in <code>ultralytics/models/yolo/classify/predict.py</code> <pre><code>class ClassificationPredictor(BasePredictor):\n\"\"\"\n    A class extending the BasePredictor class for prediction based on a classification model.\n    Notes:\n        - Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.\n    Example:\n        ```python\n        from ultralytics.utils import ASSETS\n        from ultralytics.models.yolo.classify import ClassificationPredictor\n        args = dict(model='yolov8n-cls.pt', source=ASSETS)\n        predictor = ClassificationPredictor(overrides=args)\n        predictor.predict_cli()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\nsuper().__init__(cfg, overrides, _callbacks)\nself.args.task = 'classify'\ndef preprocess(self, img):\n\"\"\"Converts input image to model-compatible data type.\"\"\"\nif not isinstance(img, torch.Tensor):\nimg = torch.stack([self.transforms(im) for im in img], dim=0)\nimg = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to(self.model.device)\nreturn img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions to return Results objects.\"\"\"\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, probs=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/classify/predict/#ultralytics.models.yolo.classify.predict.ClassificationPredictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Post-processes predictions to return Results objects.</p> Source code in <code>ultralytics/models/yolo/classify/predict.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions to return Results objects.\"\"\"\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, probs=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/classify/predict/#ultralytics.models.yolo.classify.predict.ClassificationPredictor.preprocess","title":"<code>preprocess(img)</code>","text":"<p>Converts input image to model-compatible data type.</p> Source code in <code>ultralytics/models/yolo/classify/predict.py</code> <pre><code>def preprocess(self, img):\n\"\"\"Converts input image to model-compatible data type.\"\"\"\nif not isinstance(img, torch.Tensor):\nimg = torch.stack([self.transforms(im) for im in img], dim=0)\nimg = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to(self.model.device)\nreturn img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\n</code></pre>"},{"location":"reference/models/yolo/classify/train/","title":"Reference for <code>ultralytics/models/yolo/classify/train.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/train.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer","title":"<code>ultralytics.models.yolo.classify.train.ClassificationTrainer</code>","text":"<p>             Bases: <code>BaseTrainer</code></p> <p>A class extending the BaseTrainer class for training based on a classification model.</p> Notes <ul> <li>Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.</li> </ul> Example <pre><code>from ultralytics.models.yolo.classify import ClassificationTrainer\nargs = dict(model='yolov8n-cls.pt', data='imagenet10', epochs=3)\ntrainer = ClassificationTrainer(overrides=args)\ntrainer.train()\n</code></pre> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>class ClassificationTrainer(BaseTrainer):\n\"\"\"\n    A class extending the BaseTrainer class for training based on a classification model.\n    Notes:\n        - Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.\n    Example:\n        ```python\n        from ultralytics.models.yolo.classify import ClassificationTrainer\n        args = dict(model='yolov8n-cls.pt', data='imagenet10', epochs=3)\n        trainer = ClassificationTrainer(overrides=args)\n        trainer.train()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a ClassificationTrainer object with optional configuration overrides and callbacks.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'classify'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = 224\nsuper().__init__(cfg, overrides, _callbacks)\ndef set_model_attributes(self):\n\"\"\"Set the YOLO model's class names from the loaded dataset.\"\"\"\nself.model.names = self.data['names']\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Returns a modified PyTorch model configured for training YOLO.\"\"\"\nmodel = ClassificationModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nfor m in model.modules():\nif not self.args.pretrained and hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nif isinstance(m, torch.nn.Dropout) and self.args.dropout:\nm.p = self.args.dropout  # set dropout\nfor p in model.parameters():\np.requires_grad = True  # for training\nreturn model\ndef setup_model(self):\n\"\"\"load/create/download model for any task\"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, ckpt = str(self.model), None\n# Load a YOLO model locally, from torchvision, or from Ultralytics assets\nif model.endswith('.pt'):\nself.model, ckpt = attempt_load_one_weight(model, device='cpu')\nfor p in self.model.parameters():\np.requires_grad = True  # for training\nelif model.split('.')[-1] in ('yaml', 'yml'):\nself.model = self.get_model(cfg=model)\nelif model in torchvision.models.__dict__:\nself.model = torchvision.models.__dict__[model](weights='IMAGENET1K_V1' if self.args.pretrained else None)\nelse:\nFileNotFoundError(f'ERROR: model={model} not found locally or online. Please check model name.')\nClassificationModel.reshape_outputs(self.model, self.data['nc'])\nreturn ckpt\ndef build_dataset(self, img_path, mode='train', batch=None):\nreturn ClassificationDataset(root=img_path, args=self.args, augment=mode == 'train', prefix=mode)\ndef get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"Returns PyTorch DataLoader with transforms to preprocess images for inference.\"\"\"\nwith torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\ndataset = self.build_dataset(dataset_path, mode)\nloader = build_dataloader(dataset, batch_size, self.args.workers, rank=rank)\n# Attach inference transforms\nif mode != 'train':\nif is_parallel(self.model):\nself.model.module.transforms = loader.dataset.torch_transforms\nelse:\nself.model.transforms = loader.dataset.torch_transforms\nreturn loader\ndef preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images and classes.\"\"\"\nbatch['img'] = batch['img'].to(self.device)\nbatch['cls'] = batch['cls'].to(self.device)\nreturn batch\ndef progress_string(self):\n\"\"\"Returns a formatted string showing training progress.\"\"\"\nreturn ('\\n' + '%11s' * (4 + len(self.loss_names))) % \\\n            ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')\ndef get_validator(self):\n\"\"\"Returns an instance of ClassificationValidator for validation.\"\"\"\nself.loss_names = ['loss']\nreturn yolo.classify.ClassificationValidator(self.test_loader, self.save_dir)\ndef label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n        Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for\n        segmentation &amp; detection\n        \"\"\"\nkeys = [f'{prefix}/{x}' for x in self.loss_names]\nif loss_items is None:\nreturn keys\nloss_items = [round(float(loss_items), 5)]\nreturn dict(zip(keys, loss_items))\ndef plot_metrics(self):\n\"\"\"Plots metrics from a CSV file.\"\"\"\nplot_results(file=self.csv, classify=True, on_plot=self.on_plot)  # save results.png\ndef final_eval(self):\n\"\"\"Evaluate trained model and save validation results.\"\"\"\nfor f in self.last, self.best:\nif f.exists():\nstrip_optimizer(f)  # strip optimizers\n# TODO: validate best.pt after training completes\n# if f is self.best:\n#     LOGGER.info(f'\\nValidating {f}...')\n#     self.validator.args.save_json = True\n#     self.metrics = self.validator(model=f)\n#     self.metrics.pop('fitness', None)\n#     self.run_callbacks('on_fit_epoch_end')\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\ndef plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples with their annotations.\"\"\"\nplot_images(\nimages=batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=batch['cls'].view(-1),  # warning: use .view(), not .squeeze() for Classify models\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initialize a ClassificationTrainer object with optional configuration overrides and callbacks.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a ClassificationTrainer object with optional configuration overrides and callbacks.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'classify'\nif overrides.get('imgsz') is None:\noverrides['imgsz'] = 224\nsuper().__init__(cfg, overrides, _callbacks)\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.final_eval","title":"<code>final_eval()</code>","text":"<p>Evaluate trained model and save validation results.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def final_eval(self):\n\"\"\"Evaluate trained model and save validation results.\"\"\"\nfor f in self.last, self.best:\nif f.exists():\nstrip_optimizer(f)  # strip optimizers\n# TODO: validate best.pt after training completes\n# if f is self.best:\n#     LOGGER.info(f'\\nValidating {f}...')\n#     self.validator.args.save_json = True\n#     self.metrics = self.validator(model=f)\n#     self.metrics.pop('fitness', None)\n#     self.run_callbacks('on_fit_epoch_end')\nLOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}\")\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size=16, rank=0, mode='train')</code>","text":"<p>Returns PyTorch DataLoader with transforms to preprocess images for inference.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"Returns PyTorch DataLoader with transforms to preprocess images for inference.\"\"\"\nwith torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\ndataset = self.build_dataset(dataset_path, mode)\nloader = build_dataloader(dataset, batch_size, self.args.workers, rank=rank)\n# Attach inference transforms\nif mode != 'train':\nif is_parallel(self.model):\nself.model.module.transforms = loader.dataset.torch_transforms\nelse:\nself.model.transforms = loader.dataset.torch_transforms\nreturn loader\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Returns a modified PyTorch model configured for training YOLO.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Returns a modified PyTorch model configured for training YOLO.\"\"\"\nmodel = ClassificationModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nfor m in model.modules():\nif not self.args.pretrained and hasattr(m, 'reset_parameters'):\nm.reset_parameters()\nif isinstance(m, torch.nn.Dropout) and self.args.dropout:\nm.p = self.args.dropout  # set dropout\nfor p in model.parameters():\np.requires_grad = True  # for training\nreturn model\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Returns an instance of ClassificationValidator for validation.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def get_validator(self):\n\"\"\"Returns an instance of ClassificationValidator for validation.\"\"\"\nself.loss_names = ['loss']\nreturn yolo.classify.ClassificationValidator(self.test_loader, self.save_dir)\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.label_loss_items","title":"<code>label_loss_items(loss_items=None, prefix='train')</code>","text":"<p>Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for segmentation &amp; detection</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n    Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for\n    segmentation &amp; detection\n    \"\"\"\nkeys = [f'{prefix}/{x}' for x in self.loss_names]\nif loss_items is None:\nreturn keys\nloss_items = [round(float(loss_items), 5)]\nreturn dict(zip(keys, loss_items))\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.plot_metrics","title":"<code>plot_metrics()</code>","text":"<p>Plots metrics from a CSV file.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def plot_metrics(self):\n\"\"\"Plots metrics from a CSV file.\"\"\"\nplot_results(file=self.csv, classify=True, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.plot_training_samples","title":"<code>plot_training_samples(batch, ni)</code>","text":"<p>Plots training samples with their annotations.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples with their annotations.\"\"\"\nplot_images(\nimages=batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=batch['cls'].view(-1),  # warning: use .view(), not .squeeze() for Classify models\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Preprocesses a batch of images and classes.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images and classes.\"\"\"\nbatch['img'] = batch['img'].to(self.device)\nbatch['cls'] = batch['cls'].to(self.device)\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.progress_string","title":"<code>progress_string()</code>","text":"<p>Returns a formatted string showing training progress.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def progress_string(self):\n\"\"\"Returns a formatted string showing training progress.\"\"\"\nreturn ('\\n' + '%11s' * (4 + len(self.loss_names))) % \\\n        ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>Set the YOLO model's class names from the loaded dataset.</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def set_model_attributes(self):\n\"\"\"Set the YOLO model's class names from the loaded dataset.\"\"\"\nself.model.names = self.data['names']\n</code></pre>"},{"location":"reference/models/yolo/classify/train/#ultralytics.models.yolo.classify.train.ClassificationTrainer.setup_model","title":"<code>setup_model()</code>","text":"<p>load/create/download model for any task</p> Source code in <code>ultralytics/models/yolo/classify/train.py</code> <pre><code>def setup_model(self):\n\"\"\"load/create/download model for any task\"\"\"\nif isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\nreturn\nmodel, ckpt = str(self.model), None\n# Load a YOLO model locally, from torchvision, or from Ultralytics assets\nif model.endswith('.pt'):\nself.model, ckpt = attempt_load_one_weight(model, device='cpu')\nfor p in self.model.parameters():\np.requires_grad = True  # for training\nelif model.split('.')[-1] in ('yaml', 'yml'):\nself.model = self.get_model(cfg=model)\nelif model in torchvision.models.__dict__:\nself.model = torchvision.models.__dict__[model](weights='IMAGENET1K_V1' if self.args.pretrained else None)\nelse:\nFileNotFoundError(f'ERROR: model={model} not found locally or online. Please check model name.')\nClassificationModel.reshape_outputs(self.model, self.data['nc'])\nreturn ckpt\n</code></pre>"},{"location":"reference/models/yolo/classify/val/","title":"Reference for <code>ultralytics/models/yolo/classify/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator","title":"<code>ultralytics.models.yolo.classify.val.ClassificationValidator</code>","text":"<p>             Bases: <code>BaseValidator</code></p> <p>A class extending the BaseValidator class for validation based on a classification model.</p> Notes <ul> <li>Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.</li> </ul> Example <pre><code>from ultralytics.models.yolo.classify import ClassificationValidator\nargs = dict(model='yolov8n-cls.pt', data='imagenet10')\nvalidator = ClassificationValidator(args=args)\nvalidator()\n</code></pre> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>class ClassificationValidator(BaseValidator):\n\"\"\"\n    A class extending the BaseValidator class for validation based on a classification model.\n    Notes:\n        - Torchvision classification models can also be passed to the 'model' argument, i.e. model='resnet18'.\n    Example:\n        ```python\n        from ultralytics.models.yolo.classify import ClassificationValidator\n        args = dict(model='yolov8n-cls.pt', data='imagenet10')\n        validator = ClassificationValidator(args=args)\n        validator()\n        ```\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initializes ClassificationValidator instance with args, dataloader, save_dir, and progress bar.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.targets = None\nself.pred = None\nself.args.task = 'classify'\nself.metrics = ClassifyMetrics()\ndef get_desc(self):\n\"\"\"Returns a formatted string summarizing classification metrics.\"\"\"\nreturn ('%22s' + '%11s' * 2) % ('classes', 'top1_acc', 'top5_acc')\ndef init_metrics(self, model):\n\"\"\"Initialize confusion matrix, class names, and top-1 and top-5 accuracy.\"\"\"\nself.names = model.names\nself.nc = len(model.names)\nself.confusion_matrix = ConfusionMatrix(nc=self.nc, task='classify')\nself.pred = []\nself.targets = []\ndef preprocess(self, batch):\n\"\"\"Preprocesses input batch and returns it.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True)\nbatch['img'] = batch['img'].half() if self.args.half else batch['img'].float()\nbatch['cls'] = batch['cls'].to(self.device)\nreturn batch\ndef update_metrics(self, preds, batch):\n\"\"\"Updates running metrics with model predictions and batch targets.\"\"\"\nn5 = min(len(self.model.names), 5)\nself.pred.append(preds.argsort(1, descending=True)[:, :n5])\nself.targets.append(batch['cls'])\ndef finalize_metrics(self, *args, **kwargs):\n\"\"\"Finalizes metrics of the model such as confusion_matrix and speed.\"\"\"\nself.confusion_matrix.process_cls_preds(self.pred, self.targets)\nif self.args.plots:\nfor normalize in True, False:\nself.confusion_matrix.plot(save_dir=self.save_dir,\nnames=self.names.values(),\nnormalize=normalize,\non_plot=self.on_plot)\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\ndef get_stats(self):\n\"\"\"Returns a dictionary of metrics obtained by processing targets and predictions.\"\"\"\nself.metrics.process(self.targets, self.pred)\nreturn self.metrics.results_dict\ndef build_dataset(self, img_path):\nreturn ClassificationDataset(root=img_path, args=self.args, augment=False, prefix=self.args.split)\ndef get_dataloader(self, dataset_path, batch_size):\n\"\"\"Builds and returns a data loader for classification tasks with given parameters.\"\"\"\ndataset = self.build_dataset(dataset_path)\nreturn build_dataloader(dataset, batch_size, self.args.workers, rank=-1)\ndef print_results(self):\n\"\"\"Prints evaluation metrics for YOLO object detection model.\"\"\"\npf = '%22s' + '%11.3g' * len(self.metrics.keys)  # print format\nLOGGER.info(pf % ('all', self.metrics.top1, self.metrics.top5))\ndef plot_val_samples(self, batch, ni):\n\"\"\"Plot validation image samples.\"\"\"\nplot_images(\nimages=batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=batch['cls'].view(-1),  # warning: use .view(), not .squeeze() for Classify models\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\ndef plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predicted bounding boxes on input images and saves the result.\"\"\"\nplot_images(batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=torch.argmax(preds, dim=1),\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initializes ClassificationValidator instance with args, dataloader, save_dir, and progress bar.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initializes ClassificationValidator instance with args, dataloader, save_dir, and progress bar.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.targets = None\nself.pred = None\nself.args.task = 'classify'\nself.metrics = ClassifyMetrics()\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.finalize_metrics","title":"<code>finalize_metrics(*args, **kwargs)</code>","text":"<p>Finalizes metrics of the model such as confusion_matrix and speed.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def finalize_metrics(self, *args, **kwargs):\n\"\"\"Finalizes metrics of the model such as confusion_matrix and speed.\"\"\"\nself.confusion_matrix.process_cls_preds(self.pred, self.targets)\nif self.args.plots:\nfor normalize in True, False:\nself.confusion_matrix.plot(save_dir=self.save_dir,\nnames=self.names.values(),\nnormalize=normalize,\non_plot=self.on_plot)\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size)</code>","text":"<p>Builds and returns a data loader for classification tasks with given parameters.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size):\n\"\"\"Builds and returns a data loader for classification tasks with given parameters.\"\"\"\ndataset = self.build_dataset(dataset_path)\nreturn build_dataloader(dataset, batch_size, self.args.workers, rank=-1)\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.get_desc","title":"<code>get_desc()</code>","text":"<p>Returns a formatted string summarizing classification metrics.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def get_desc(self):\n\"\"\"Returns a formatted string summarizing classification metrics.\"\"\"\nreturn ('%22s' + '%11s' * 2) % ('classes', 'top1_acc', 'top5_acc')\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.get_stats","title":"<code>get_stats()</code>","text":"<p>Returns a dictionary of metrics obtained by processing targets and predictions.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def get_stats(self):\n\"\"\"Returns a dictionary of metrics obtained by processing targets and predictions.\"\"\"\nself.metrics.process(self.targets, self.pred)\nreturn self.metrics.results_dict\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.init_metrics","title":"<code>init_metrics(model)</code>","text":"<p>Initialize confusion matrix, class names, and top-1 and top-5 accuracy.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def init_metrics(self, model):\n\"\"\"Initialize confusion matrix, class names, and top-1 and top-5 accuracy.\"\"\"\nself.names = model.names\nself.nc = len(model.names)\nself.confusion_matrix = ConfusionMatrix(nc=self.nc, task='classify')\nself.pred = []\nself.targets = []\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.plot_predictions","title":"<code>plot_predictions(batch, preds, ni)</code>","text":"<p>Plots predicted bounding boxes on input images and saves the result.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predicted bounding boxes on input images and saves the result.\"\"\"\nplot_images(batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=torch.argmax(preds, dim=1),\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.plot_val_samples","title":"<code>plot_val_samples(batch, ni)</code>","text":"<p>Plot validation image samples.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def plot_val_samples(self, batch, ni):\n\"\"\"Plot validation image samples.\"\"\"\nplot_images(\nimages=batch['img'],\nbatch_idx=torch.arange(len(batch['img'])),\ncls=batch['cls'].view(-1),  # warning: use .view(), not .squeeze() for Classify models\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.preprocess","title":"<code>preprocess(batch)</code>","text":"<p>Preprocesses input batch and returns it.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def preprocess(self, batch):\n\"\"\"Preprocesses input batch and returns it.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True)\nbatch['img'] = batch['img'].half() if self.args.half else batch['img'].float()\nbatch['cls'] = batch['cls'].to(self.device)\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.print_results","title":"<code>print_results()</code>","text":"<p>Prints evaluation metrics for YOLO object detection model.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def print_results(self):\n\"\"\"Prints evaluation metrics for YOLO object detection model.\"\"\"\npf = '%22s' + '%11.3g' * len(self.metrics.keys)  # print format\nLOGGER.info(pf % ('all', self.metrics.top1, self.metrics.top5))\n</code></pre>"},{"location":"reference/models/yolo/classify/val/#ultralytics.models.yolo.classify.val.ClassificationValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Updates running metrics with model predictions and batch targets.</p> Source code in <code>ultralytics/models/yolo/classify/val.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Updates running metrics with model predictions and batch targets.\"\"\"\nn5 = min(len(self.model.names), 5)\nself.pred.append(preds.argsort(1, descending=True)[:, :n5])\nself.targets.append(batch['cls'])\n</code></pre>"},{"location":"reference/models/yolo/detect/predict/","title":"Reference for <code>ultralytics/models/yolo/detect/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/detect/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/detect/predict/#ultralytics.models.yolo.detect.predict.DetectionPredictor","title":"<code>ultralytics.models.yolo.detect.predict.DetectionPredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> <p>A class extending the BasePredictor class for prediction based on a detection model.</p> Example <pre><code>from ultralytics.utils import ASSETS\nfrom ultralytics.models.yolo.detect import DetectionPredictor\nargs = dict(model='yolov8n.pt', source=ASSETS)\npredictor = DetectionPredictor(overrides=args)\npredictor.predict_cli()\n</code></pre> Source code in <code>ultralytics/models/yolo/detect/predict.py</code> <pre><code>class DetectionPredictor(BasePredictor):\n\"\"\"\n    A class extending the BasePredictor class for prediction based on a detection model.\n    Example:\n        ```python\n        from ultralytics.utils import ASSETS\n        from ultralytics.models.yolo.detect import DetectionPredictor\n        args = dict(model='yolov8n.pt', source=ASSETS)\n        predictor = DetectionPredictor(overrides=args)\n        predictor.predict_cli()\n        ```\n    \"\"\"\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/detect/predict/#ultralytics.models.yolo.detect.predict.DetectionPredictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Post-processes predictions and returns a list of Results objects.</p> Source code in <code>ultralytics/models/yolo/detect/predict.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/detect/train/","title":"Reference for <code>ultralytics/models/yolo/detect/train.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/detect/train.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer","title":"<code>ultralytics.models.yolo.detect.train.DetectionTrainer</code>","text":"<p>             Bases: <code>BaseTrainer</code></p> <p>A class extending the BaseTrainer class for training based on a detection model.</p> Example <pre><code>from ultralytics.models.yolo.detect import DetectionTrainer\nargs = dict(model='yolov8n.pt', data='coco8.yaml', epochs=3)\ntrainer = DetectionTrainer(overrides=args)\ntrainer.train()\n</code></pre> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>class DetectionTrainer(BaseTrainer):\n\"\"\"\n    A class extending the BaseTrainer class for training based on a detection model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.detect import DetectionTrainer\n        args = dict(model='yolov8n.pt', data='coco8.yaml', epochs=3)\n        trainer = DetectionTrainer(overrides=args)\n        trainer.train()\n        ```\n    \"\"\"\ndef build_dataset(self, img_path, mode='train', batch=None):\n\"\"\"\n        Build YOLO Dataset.\n        Args:\n            img_path (str): Path to the folder containing images.\n            mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n        \"\"\"\ngs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)\nreturn build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == 'val', stride=gs)\ndef get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"Construct and return dataloader.\"\"\"\nassert mode in ['train', 'val']\nwith torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\ndataset = self.build_dataset(dataset_path, mode, batch_size)\nshuffle = mode == 'train'\nif getattr(dataset, 'rect', False) and shuffle:\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\")\nshuffle = False\nworkers = self.args.workers if mode == 'train' else self.args.workers * 2\nreturn build_dataloader(dataset, batch_size, workers, shuffle, rank)  # return dataloader\ndef preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images by scaling and converting to float.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True).float() / 255\nreturn batch\ndef set_model_attributes(self):\n\"\"\"nl = de_parallel(self.model).model[-1].nl  # number of detection layers (to scale hyps).\"\"\"\n# self.args.box *= 3 / nl  # scale to layers\n# self.args.cls *= self.data[\"nc\"] / 80 * 3 / nl  # scale to classes and layers\n# self.args.cls *= (self.args.imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\nself.model.nc = self.data['nc']  # attach number of classes to model\nself.model.names = self.data['names']  # attach class names to model\nself.model.args = self.args  # attach hyperparameters to model\n# TODO: self.model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return a YOLO detection model.\"\"\"\nmodel = DetectionModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\ndef get_validator(self):\n\"\"\"Returns a DetectionValidator for YOLO model validation.\"\"\"\nself.loss_names = 'box_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.detect.DetectionValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\ndef label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n        Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for\n        segmentation &amp; detection\n        \"\"\"\nkeys = [f'{prefix}/{x}' for x in self.loss_names]\nif loss_items is not None:\nloss_items = [round(float(x), 5) for x in loss_items]  # convert tensors to 5 decimal place floats\nreturn dict(zip(keys, loss_items))\nelse:\nreturn keys\ndef progress_string(self):\n\"\"\"Returns a formatted string of training progress with epoch, GPU memory, loss, instances and size.\"\"\"\nreturn ('\\n' + '%11s' *\n(4 + len(self.loss_names))) % ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')\ndef plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples with their annotations.\"\"\"\nplot_images(images=batch['img'],\nbatch_idx=batch['batch_idx'],\ncls=batch['cls'].squeeze(-1),\nbboxes=batch['bboxes'],\npaths=batch['im_file'],\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\ndef plot_metrics(self):\n\"\"\"Plots metrics from a CSV file.\"\"\"\nplot_results(file=self.csv, on_plot=self.on_plot)  # save results.png\ndef plot_training_labels(self):\n\"\"\"Create a labeled training plot of the YOLO model.\"\"\"\nboxes = np.concatenate([lb['bboxes'] for lb in self.train_loader.dataset.labels], 0)\ncls = np.concatenate([lb['cls'] for lb in self.train_loader.dataset.labels], 0)\nplot_labels(boxes, cls.squeeze(), names=self.data['names'], save_dir=self.save_dir, on_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.build_dataset","title":"<code>build_dataset(img_path, mode='train', batch=None)</code>","text":"<p>Build YOLO Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the folder containing images.</p> required <code>mode</code> <code>str</code> <p><code>train</code> mode or <code>val</code> mode, users are able to customize different augmentations for each mode.</p> <code>'train'</code> <code>batch</code> <code>int</code> <p>Size of batches, this is for <code>rect</code>. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def build_dataset(self, img_path, mode='train', batch=None):\n\"\"\"\n    Build YOLO Dataset.\n    Args:\n        img_path (str): Path to the folder containing images.\n        mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n        batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n    \"\"\"\ngs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)\nreturn build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == 'val', stride=gs)\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size=16, rank=0, mode='train')</code>","text":"<p>Construct and return dataloader.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode='train'):\n\"\"\"Construct and return dataloader.\"\"\"\nassert mode in ['train', 'val']\nwith torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\ndataset = self.build_dataset(dataset_path, mode, batch_size)\nshuffle = mode == 'train'\nif getattr(dataset, 'rect', False) and shuffle:\nLOGGER.warning(\"WARNING \u26a0\ufe0f 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\")\nshuffle = False\nworkers = self.args.workers if mode == 'train' else self.args.workers * 2\nreturn build_dataloader(dataset, batch_size, workers, shuffle, rank)  # return dataloader\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Return a YOLO detection model.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return a YOLO detection model.\"\"\"\nmodel = DetectionModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Returns a DetectionValidator for YOLO model validation.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def get_validator(self):\n\"\"\"Returns a DetectionValidator for YOLO model validation.\"\"\"\nself.loss_names = 'box_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.detect.DetectionValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.label_loss_items","title":"<code>label_loss_items(loss_items=None, prefix='train')</code>","text":"<p>Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for segmentation &amp; detection</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def label_loss_items(self, loss_items=None, prefix='train'):\n\"\"\"\n    Returns a loss dict with labelled training loss items tensor. Not needed for classification but necessary for\n    segmentation &amp; detection\n    \"\"\"\nkeys = [f'{prefix}/{x}' for x in self.loss_names]\nif loss_items is not None:\nloss_items = [round(float(x), 5) for x in loss_items]  # convert tensors to 5 decimal place floats\nreturn dict(zip(keys, loss_items))\nelse:\nreturn keys\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.plot_metrics","title":"<code>plot_metrics()</code>","text":"<p>Plots metrics from a CSV file.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def plot_metrics(self):\n\"\"\"Plots metrics from a CSV file.\"\"\"\nplot_results(file=self.csv, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.plot_training_labels","title":"<code>plot_training_labels()</code>","text":"<p>Create a labeled training plot of the YOLO model.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def plot_training_labels(self):\n\"\"\"Create a labeled training plot of the YOLO model.\"\"\"\nboxes = np.concatenate([lb['bboxes'] for lb in self.train_loader.dataset.labels], 0)\ncls = np.concatenate([lb['cls'] for lb in self.train_loader.dataset.labels], 0)\nplot_labels(boxes, cls.squeeze(), names=self.data['names'], save_dir=self.save_dir, on_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.plot_training_samples","title":"<code>plot_training_samples(batch, ni)</code>","text":"<p>Plots training samples with their annotations.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def plot_training_samples(self, batch, ni):\n\"\"\"Plots training samples with their annotations.\"\"\"\nplot_images(images=batch['img'],\nbatch_idx=batch['batch_idx'],\ncls=batch['cls'].squeeze(-1),\nbboxes=batch['bboxes'],\npaths=batch['im_file'],\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Preprocesses a batch of images by scaling and converting to float.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def preprocess_batch(self, batch):\n\"\"\"Preprocesses a batch of images by scaling and converting to float.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True).float() / 255\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.progress_string","title":"<code>progress_string()</code>","text":"<p>Returns a formatted string of training progress with epoch, GPU memory, loss, instances and size.</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def progress_string(self):\n\"\"\"Returns a formatted string of training progress with epoch, GPU memory, loss, instances and size.\"\"\"\nreturn ('\\n' + '%11s' *\n(4 + len(self.loss_names))) % ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')\n</code></pre>"},{"location":"reference/models/yolo/detect/train/#ultralytics.models.yolo.detect.train.DetectionTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>nl = de_parallel(self.model).model[-1].nl  # number of detection layers (to scale hyps).</p> Source code in <code>ultralytics/models/yolo/detect/train.py</code> <pre><code>def set_model_attributes(self):\n\"\"\"nl = de_parallel(self.model).model[-1].nl  # number of detection layers (to scale hyps).\"\"\"\n# self.args.box *= 3 / nl  # scale to layers\n# self.args.cls *= self.data[\"nc\"] / 80 * 3 / nl  # scale to classes and layers\n# self.args.cls *= (self.args.imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\nself.model.nc = self.data['nc']  # attach number of classes to model\nself.model.names = self.data['names']  # attach class names to model\nself.model.args = self.args  # attach hyperparameters to model\n</code></pre>"},{"location":"reference/models/yolo/detect/val/","title":"Reference for <code>ultralytics/models/yolo/detect/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/detect/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator","title":"<code>ultralytics.models.yolo.detect.val.DetectionValidator</code>","text":"<p>             Bases: <code>BaseValidator</code></p> <p>A class extending the BaseValidator class for validation based on a detection model.</p> Example <pre><code>from ultralytics.models.yolo.detect import DetectionValidator\nargs = dict(model='yolov8n.pt', data='coco8.yaml')\nvalidator = DetectionValidator(args=args)\nvalidator()\n</code></pre> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>class DetectionValidator(BaseValidator):\n\"\"\"\n    A class extending the BaseValidator class for validation based on a detection model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.detect import DetectionValidator\n        args = dict(model='yolov8n.pt', data='coco8.yaml')\n        validator = DetectionValidator(args=args)\n        validator()\n        ```\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize detection model with necessary variables and settings.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.nt_per_class = None\nself.is_coco = False\nself.class_map = None\nself.args.task = 'detect'\nself.metrics = DetMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\nself.iouv = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\nself.niou = self.iouv.numel()\nself.lb = []  # for autolabelling\ndef preprocess(self, batch):\n\"\"\"Preprocesses batch of images for YOLO training.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True)\nbatch['img'] = (batch['img'].half() if self.args.half else batch['img'].float()) / 255\nfor k in ['batch_idx', 'cls', 'bboxes']:\nbatch[k] = batch[k].to(self.device)\nif self.args.save_hybrid:\nheight, width = batch['img'].shape[2:]\nnb = len(batch['img'])\nbboxes = batch['bboxes'] * torch.tensor((width, height, width, height), device=self.device)\nself.lb = [\ntorch.cat([batch['cls'][batch['batch_idx'] == i], bboxes[batch['batch_idx'] == i]], dim=-1)\nfor i in range(nb)] if self.args.save_hybrid else []  # for autolabelling\nreturn batch\ndef init_metrics(self, model):\n\"\"\"Initialize evaluation metrics for YOLO.\"\"\"\nval = self.data.get(self.args.split, '')  # validation path\nself.is_coco = isinstance(val, str) and 'coco' in val and val.endswith(f'{os.sep}val2017.txt')  # is COCO\nself.class_map = converter.coco80_to_coco91_class() if self.is_coco else list(range(1000))\nself.args.save_json |= self.is_coco and not self.training  # run on final val if training COCO\nself.names = model.names\nself.nc = len(model.names)\nself.metrics.names = self.names\nself.metrics.plot = self.args.plots\nself.confusion_matrix = ConfusionMatrix(nc=self.nc)\nself.seen = 0\nself.jdict = []\nself.stats = []\ndef get_desc(self):\n\"\"\"Return a formatted string summarizing class metrics of YOLO model.\"\"\"\nreturn ('%22s' + '%11s' * 6) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)')\ndef postprocess(self, preds):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det)\ndef update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn, labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\nself.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\nif self.args.save_txt:\nfile = self.save_dir / 'labels' / f'{Path(batch[\"im_file\"][si]).stem}.txt'\nself.save_one_txt(predn, self.args.save_conf, shape, file)\ndef finalize_metrics(self, *args, **kwargs):\n\"\"\"Set final values for metrics speed and confusion matrix.\"\"\"\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\ndef get_stats(self):\n\"\"\"Returns metrics statistics and results dictionary.\"\"\"\nstats = [torch.cat(x, 0).cpu().numpy() for x in zip(*self.stats)]  # to numpy\nif len(stats) and stats[0].any():\nself.metrics.process(*stats)\nself.nt_per_class = np.bincount(stats[-1].astype(int), minlength=self.nc)  # number of targets per class\nreturn self.metrics.results_dict\ndef print_results(self):\n\"\"\"Prints training/validation set metrics per class.\"\"\"\npf = '%22s' + '%11i' * 2 + '%11.3g' * len(self.metrics.keys)  # print format\nLOGGER.info(pf % ('all', self.seen, self.nt_per_class.sum(), *self.metrics.mean_results()))\nif self.nt_per_class.sum() == 0:\nLOGGER.warning(\nf'WARNING \u26a0\ufe0f no labels found in {self.args.task} set, can not compute metrics without labels')\n# Print results per class\nif self.args.verbose and not self.training and self.nc &gt; 1 and len(self.stats):\nfor i, c in enumerate(self.metrics.ap_class_index):\nLOGGER.info(pf % (self.names[c], self.seen, self.nt_per_class[c], *self.metrics.class_result(i)))\nif self.args.plots:\nfor normalize in True, False:\nself.confusion_matrix.plot(save_dir=self.save_dir,\nnames=self.names.values(),\nnormalize=normalize,\non_plot=self.on_plot)\ndef _process_batch(self, detections, labels):\n\"\"\"\n        Return correct prediction matrix.\n        Args:\n            detections (torch.Tensor): Tensor of shape [N, 6] representing detections.\n                Each detection is of the format: x1, y1, x2, y2, conf, class.\n            labels (torch.Tensor): Tensor of shape [M, 5] representing labels.\n                Each label is of the format: class, x1, y1, x2, y2.\n        Returns:\n            (torch.Tensor): Correct prediction matrix of shape [N, 10] for 10 IoU levels.\n        \"\"\"\niou = box_iou(labels[:, 1:], detections[:, :4])\nreturn self.match_predictions(detections[:, 5], labels[:, 0], iou)\ndef build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"\n        Build YOLO Dataset.\n        Args:\n            img_path (str): Path to the folder containing images.\n            mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n        \"\"\"\ngs = max(int(de_parallel(self.model).stride if self.model else 0), 32)\nreturn build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, stride=gs)\ndef get_dataloader(self, dataset_path, batch_size):\n\"\"\"Construct and return dataloader.\"\"\"\ndataset = self.build_dataset(dataset_path, batch=batch_size, mode='val')\nreturn build_dataloader(dataset, batch_size, self.args.workers, shuffle=False, rank=-1)  # return dataloader\ndef plot_val_samples(self, batch, ni):\n\"\"\"Plot validation image samples.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\ndef plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predicted bounding boxes on input images and saves the result.\"\"\"\nplot_images(batch['img'],\n*output_to_target(preds, max_det=self.args.max_det),\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\ndef save_one_txt(self, predn, save_conf, shape, file):\n\"\"\"Save YOLO detections to a txt file in normalized coordinates in a specific format.\"\"\"\ngn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\nfor *xyxy, conf, cls in predn.tolist():\nxywh = (ops.xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\nline = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\nwith open(file, 'a') as f:\nf.write(('%g ' * len(line)).rstrip() % line + '\\n')\ndef pred_to_json(self, predn, filename):\n\"\"\"Serialize YOLO predictions to COCO json format.\"\"\"\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\nfor p, b in zip(predn.tolist(), box.tolist()):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'score': round(p[4], 5)})\ndef eval_json(self, stats):\n\"\"\"Evaluates YOLO output in JSON format and returns performance statistics.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/instances_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\neval = COCOeval(anno, pred, 'bbox')\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # images to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nstats[self.metrics.keys[-1]], stats[self.metrics.keys[-2]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initialize detection model with necessary variables and settings.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize detection model with necessary variables and settings.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.nt_per_class = None\nself.is_coco = False\nself.class_map = None\nself.args.task = 'detect'\nself.metrics = DetMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\nself.iouv = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\nself.niou = self.iouv.numel()\nself.lb = []  # for autolabelling\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.build_dataset","title":"<code>build_dataset(img_path, mode='val', batch=None)</code>","text":"<p>Build YOLO Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the folder containing images.</p> required <code>mode</code> <code>str</code> <p><code>train</code> mode or <code>val</code> mode, users are able to customize different augmentations for each mode.</p> <code>'val'</code> <code>batch</code> <code>int</code> <p>Size of batches, this is for <code>rect</code>. Defaults to None.</p> <code>None</code> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def build_dataset(self, img_path, mode='val', batch=None):\n\"\"\"\n    Build YOLO Dataset.\n    Args:\n        img_path (str): Path to the folder containing images.\n        mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n        batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n    \"\"\"\ngs = max(int(de_parallel(self.model).stride if self.model else 0), 32)\nreturn build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, stride=gs)\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.eval_json","title":"<code>eval_json(stats)</code>","text":"<p>Evaluates YOLO output in JSON format and returns performance statistics.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def eval_json(self, stats):\n\"\"\"Evaluates YOLO output in JSON format and returns performance statistics.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/instances_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\neval = COCOeval(anno, pred, 'bbox')\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # images to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nstats[self.metrics.keys[-1]], stats[self.metrics.keys[-2]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.finalize_metrics","title":"<code>finalize_metrics(*args, **kwargs)</code>","text":"<p>Set final values for metrics speed and confusion matrix.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def finalize_metrics(self, *args, **kwargs):\n\"\"\"Set final values for metrics speed and confusion matrix.\"\"\"\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size)</code>","text":"<p>Construct and return dataloader.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size):\n\"\"\"Construct and return dataloader.\"\"\"\ndataset = self.build_dataset(dataset_path, batch=batch_size, mode='val')\nreturn build_dataloader(dataset, batch_size, self.args.workers, shuffle=False, rank=-1)  # return dataloader\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.get_desc","title":"<code>get_desc()</code>","text":"<p>Return a formatted string summarizing class metrics of YOLO model.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def get_desc(self):\n\"\"\"Return a formatted string summarizing class metrics of YOLO model.\"\"\"\nreturn ('%22s' + '%11s' * 6) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)')\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.get_stats","title":"<code>get_stats()</code>","text":"<p>Returns metrics statistics and results dictionary.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def get_stats(self):\n\"\"\"Returns metrics statistics and results dictionary.\"\"\"\nstats = [torch.cat(x, 0).cpu().numpy() for x in zip(*self.stats)]  # to numpy\nif len(stats) and stats[0].any():\nself.metrics.process(*stats)\nself.nt_per_class = np.bincount(stats[-1].astype(int), minlength=self.nc)  # number of targets per class\nreturn self.metrics.results_dict\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.init_metrics","title":"<code>init_metrics(model)</code>","text":"<p>Initialize evaluation metrics for YOLO.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def init_metrics(self, model):\n\"\"\"Initialize evaluation metrics for YOLO.\"\"\"\nval = self.data.get(self.args.split, '')  # validation path\nself.is_coco = isinstance(val, str) and 'coco' in val and val.endswith(f'{os.sep}val2017.txt')  # is COCO\nself.class_map = converter.coco80_to_coco91_class() if self.is_coco else list(range(1000))\nself.args.save_json |= self.is_coco and not self.training  # run on final val if training COCO\nself.names = model.names\nself.nc = len(model.names)\nself.metrics.names = self.names\nself.metrics.plot = self.args.plots\nself.confusion_matrix = ConfusionMatrix(nc=self.nc)\nself.seen = 0\nself.jdict = []\nself.stats = []\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.plot_predictions","title":"<code>plot_predictions(batch, preds, ni)</code>","text":"<p>Plots predicted bounding boxes on input images and saves the result.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predicted bounding boxes on input images and saves the result.\"\"\"\nplot_images(batch['img'],\n*output_to_target(preds, max_det=self.args.max_det),\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.plot_val_samples","title":"<code>plot_val_samples(batch, ni)</code>","text":"<p>Plot validation image samples.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def plot_val_samples(self, batch, ni):\n\"\"\"Plot validation image samples.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.postprocess","title":"<code>postprocess(preds)</code>","text":"<p>Apply Non-maximum suppression to prediction outputs.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def postprocess(self, preds):\n\"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det)\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.pred_to_json","title":"<code>pred_to_json(predn, filename)</code>","text":"<p>Serialize YOLO predictions to COCO json format.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def pred_to_json(self, predn, filename):\n\"\"\"Serialize YOLO predictions to COCO json format.\"\"\"\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\nfor p, b in zip(predn.tolist(), box.tolist()):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'score': round(p[4], 5)})\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.preprocess","title":"<code>preprocess(batch)</code>","text":"<p>Preprocesses batch of images for YOLO training.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def preprocess(self, batch):\n\"\"\"Preprocesses batch of images for YOLO training.\"\"\"\nbatch['img'] = batch['img'].to(self.device, non_blocking=True)\nbatch['img'] = (batch['img'].half() if self.args.half else batch['img'].float()) / 255\nfor k in ['batch_idx', 'cls', 'bboxes']:\nbatch[k] = batch[k].to(self.device)\nif self.args.save_hybrid:\nheight, width = batch['img'].shape[2:]\nnb = len(batch['img'])\nbboxes = batch['bboxes'] * torch.tensor((width, height, width, height), device=self.device)\nself.lb = [\ntorch.cat([batch['cls'][batch['batch_idx'] == i], bboxes[batch['batch_idx'] == i]], dim=-1)\nfor i in range(nb)] if self.args.save_hybrid else []  # for autolabelling\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.print_results","title":"<code>print_results()</code>","text":"<p>Prints training/validation set metrics per class.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def print_results(self):\n\"\"\"Prints training/validation set metrics per class.\"\"\"\npf = '%22s' + '%11i' * 2 + '%11.3g' * len(self.metrics.keys)  # print format\nLOGGER.info(pf % ('all', self.seen, self.nt_per_class.sum(), *self.metrics.mean_results()))\nif self.nt_per_class.sum() == 0:\nLOGGER.warning(\nf'WARNING \u26a0\ufe0f no labels found in {self.args.task} set, can not compute metrics without labels')\n# Print results per class\nif self.args.verbose and not self.training and self.nc &gt; 1 and len(self.stats):\nfor i, c in enumerate(self.metrics.ap_class_index):\nLOGGER.info(pf % (self.names[c], self.seen, self.nt_per_class[c], *self.metrics.class_result(i)))\nif self.args.plots:\nfor normalize in True, False:\nself.confusion_matrix.plot(save_dir=self.save_dir,\nnames=self.names.values(),\nnormalize=normalize,\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.save_one_txt","title":"<code>save_one_txt(predn, save_conf, shape, file)</code>","text":"<p>Save YOLO detections to a txt file in normalized coordinates in a specific format.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def save_one_txt(self, predn, save_conf, shape, file):\n\"\"\"Save YOLO detections to a txt file in normalized coordinates in a specific format.\"\"\"\ngn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\nfor *xyxy, conf, cls in predn.tolist():\nxywh = (ops.xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\nline = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\nwith open(file, 'a') as f:\nf.write(('%g ' * len(line)).rstrip() % line + '\\n')\n</code></pre>"},{"location":"reference/models/yolo/detect/val/#ultralytics.models.yolo.detect.val.DetectionValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Metrics.</p> Source code in <code>ultralytics/models/yolo/detect/val.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn, labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\nself.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\nif self.args.save_txt:\nfile = self.save_dir / 'labels' / f'{Path(batch[\"im_file\"][si]).stem}.txt'\nself.save_one_txt(predn, self.args.save_conf, shape, file)\n</code></pre>"},{"location":"reference/models/yolo/pose/predict/","title":"Reference for <code>ultralytics/models/yolo/pose/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/pose/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/pose/predict/#ultralytics.models.yolo.pose.predict.PosePredictor","title":"<code>ultralytics.models.yolo.pose.predict.PosePredictor</code>","text":"<p>             Bases: <code>DetectionPredictor</code></p> <p>A class extending the DetectionPredictor class for prediction based on a pose model.</p> Example <pre><code>from ultralytics.utils import ASSETS\nfrom ultralytics.models.yolo.pose import PosePredictor\nargs = dict(model='yolov8n-pose.pt', source=ASSETS)\npredictor = PosePredictor(overrides=args)\npredictor.predict_cli()\n</code></pre> Source code in <code>ultralytics/models/yolo/pose/predict.py</code> <pre><code>class PosePredictor(DetectionPredictor):\n\"\"\"\n    A class extending the DetectionPredictor class for prediction based on a pose model.\n    Example:\n        ```python\n        from ultralytics.utils import ASSETS\n        from ultralytics.models.yolo.pose import PosePredictor\n        args = dict(model='yolov8n-pose.pt', source=ASSETS)\n        predictor = PosePredictor(overrides=args)\n        predictor.predict_cli()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\nsuper().__init__(cfg, overrides, _callbacks)\nself.args.task = 'pose'\nif isinstance(self.args.device, str) and self.args.device.lower() == 'mps':\nLOGGER.warning(\"WARNING \u26a0\ufe0f Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. \"\n'See https://github.com/ultralytics/ultralytics/issues/4031.')\ndef postprocess(self, preds, img, orig_imgs):\n\"\"\"Return detection results for a given input image or list of images.\"\"\"\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes,\nnc=len(self.model.names))\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape).round()\npred_kpts = pred[:, 6:].view(len(pred), *self.model.kpt_shape) if len(pred) else pred[:, 6:]\npred_kpts = ops.scale_coords(img.shape[2:], pred_kpts, orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(\nResults(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6], keypoints=pred_kpts))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/pose/predict/#ultralytics.models.yolo.pose.predict.PosePredictor.postprocess","title":"<code>postprocess(preds, img, orig_imgs)</code>","text":"<p>Return detection results for a given input image or list of images.</p> Source code in <code>ultralytics/models/yolo/pose/predict.py</code> <pre><code>def postprocess(self, preds, img, orig_imgs):\n\"\"\"Return detection results for a given input image or list of images.\"\"\"\npreds = ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nclasses=self.args.classes,\nnc=len(self.model.names))\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nfor i, pred in enumerate(preds):\norig_img = orig_imgs[i] if is_list else orig_imgs\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape).round()\npred_kpts = pred[:, 6:].view(len(pred), *self.model.kpt_shape) if len(pred) else pred[:, 6:]\npred_kpts = ops.scale_coords(img.shape[2:], pred_kpts, orig_img.shape)\nimg_path = self.batch[0][i]\nresults.append(\nResults(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6], keypoints=pred_kpts))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/pose/train/","title":"Reference for <code>ultralytics/models/yolo/pose/train.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/pose/train.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer","title":"<code>ultralytics.models.yolo.pose.train.PoseTrainer</code>","text":"<p>             Bases: <code>DetectionTrainer</code></p> <p>A class extending the DetectionTrainer class for training based on a pose model.</p> Example <pre><code>from ultralytics.models.yolo.pose import PoseTrainer\nargs = dict(model='yolov8n-pose.pt', data='coco8-pose.yaml', epochs=3)\ntrainer = PoseTrainer(overrides=args)\ntrainer.train()\n</code></pre> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>class PoseTrainer(yolo.detect.DetectionTrainer):\n\"\"\"\n    A class extending the DetectionTrainer class for training based on a pose model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.pose import PoseTrainer\n        args = dict(model='yolov8n-pose.pt', data='coco8-pose.yaml', epochs=3)\n        trainer = PoseTrainer(overrides=args)\n        trainer.train()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a PoseTrainer object with specified configurations and overrides.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'pose'\nsuper().__init__(cfg, overrides, _callbacks)\nif isinstance(self.args.device, str) and self.args.device.lower() == 'mps':\nLOGGER.warning(\"WARNING \u26a0\ufe0f Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. \"\n'See https://github.com/ultralytics/ultralytics/issues/4031.')\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Get pose estimation model with specified configuration and weights.\"\"\"\nmodel = PoseModel(cfg, ch=3, nc=self.data['nc'], data_kpt_shape=self.data['kpt_shape'], verbose=verbose)\nif weights:\nmodel.load(weights)\nreturn model\ndef set_model_attributes(self):\n\"\"\"Sets keypoints shape attribute of PoseModel.\"\"\"\nsuper().set_model_attributes()\nself.model.kpt_shape = self.data['kpt_shape']\ndef get_validator(self):\n\"\"\"Returns an instance of the PoseValidator class for validation.\"\"\"\nself.loss_names = 'box_loss', 'pose_loss', 'kobj_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.pose.PoseValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\ndef plot_training_samples(self, batch, ni):\n\"\"\"Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.\"\"\"\nimages = batch['img']\nkpts = batch['keypoints']\ncls = batch['cls'].squeeze(-1)\nbboxes = batch['bboxes']\npaths = batch['im_file']\nbatch_idx = batch['batch_idx']\nplot_images(images,\nbatch_idx,\ncls,\nbboxes,\nkpts=kpts,\npaths=paths,\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\ndef plot_metrics(self):\n\"\"\"Plots training/val metrics.\"\"\"\nplot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initialize a PoseTrainer object with specified configurations and overrides.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a PoseTrainer object with specified configurations and overrides.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'pose'\nsuper().__init__(cfg, overrides, _callbacks)\nif isinstance(self.args.device, str) and self.args.device.lower() == 'mps':\nLOGGER.warning(\"WARNING \u26a0\ufe0f Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. \"\n'See https://github.com/ultralytics/ultralytics/issues/4031.')\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Get pose estimation model with specified configuration and weights.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Get pose estimation model with specified configuration and weights.\"\"\"\nmodel = PoseModel(cfg, ch=3, nc=self.data['nc'], data_kpt_shape=self.data['kpt_shape'], verbose=verbose)\nif weights:\nmodel.load(weights)\nreturn model\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Returns an instance of the PoseValidator class for validation.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def get_validator(self):\n\"\"\"Returns an instance of the PoseValidator class for validation.\"\"\"\nself.loss_names = 'box_loss', 'pose_loss', 'kobj_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.pose.PoseValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.plot_metrics","title":"<code>plot_metrics()</code>","text":"<p>Plots training/val metrics.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def plot_metrics(self):\n\"\"\"Plots training/val metrics.\"\"\"\nplot_results(file=self.csv, pose=True, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.plot_training_samples","title":"<code>plot_training_samples(batch, ni)</code>","text":"<p>Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def plot_training_samples(self, batch, ni):\n\"\"\"Plot a batch of training samples with annotated class labels, bounding boxes, and keypoints.\"\"\"\nimages = batch['img']\nkpts = batch['keypoints']\ncls = batch['cls'].squeeze(-1)\nbboxes = batch['bboxes']\npaths = batch['im_file']\nbatch_idx = batch['batch_idx']\nplot_images(images,\nbatch_idx,\ncls,\nbboxes,\nkpts=kpts,\npaths=paths,\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/pose/train/#ultralytics.models.yolo.pose.train.PoseTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>Sets keypoints shape attribute of PoseModel.</p> Source code in <code>ultralytics/models/yolo/pose/train.py</code> <pre><code>def set_model_attributes(self):\n\"\"\"Sets keypoints shape attribute of PoseModel.\"\"\"\nsuper().set_model_attributes()\nself.model.kpt_shape = self.data['kpt_shape']\n</code></pre>"},{"location":"reference/models/yolo/pose/val/","title":"Reference for <code>ultralytics/models/yolo/pose/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/pose/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator","title":"<code>ultralytics.models.yolo.pose.val.PoseValidator</code>","text":"<p>             Bases: <code>DetectionValidator</code></p> <p>A class extending the DetectionValidator class for validation based on a pose model.</p> Example <pre><code>from ultralytics.models.yolo.pose import PoseValidator\nargs = dict(model='yolov8n-pose.pt', data='coco8-pose.yaml')\nvalidator = PoseValidator(args=args)\nvalidator()\n</code></pre> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>class PoseValidator(DetectionValidator):\n\"\"\"\n    A class extending the DetectionValidator class for validation based on a pose model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.pose import PoseValidator\n        args = dict(model='yolov8n-pose.pt', data='coco8-pose.yaml')\n        validator = PoseValidator(args=args)\n        validator()\n        ```\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize a 'PoseValidator' object with custom parameters and assigned attributes.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.sigma = None\nself.kpt_shape = None\nself.args.task = 'pose'\nself.metrics = PoseMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\nif isinstance(self.args.device, str) and self.args.device.lower() == 'mps':\nLOGGER.warning(\"WARNING \u26a0\ufe0f Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. \"\n'See https://github.com/ultralytics/ultralytics/issues/4031.')\ndef preprocess(self, batch):\n\"\"\"Preprocesses the batch by converting the 'keypoints' data into a float and moving it to the device.\"\"\"\nbatch = super().preprocess(batch)\nbatch['keypoints'] = batch['keypoints'].to(self.device).float()\nreturn batch\ndef get_desc(self):\n\"\"\"Returns description of evaluation metrics in string format.\"\"\"\nreturn ('%22s' + '%11s' * 10) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)', 'Pose(P',\n'R', 'mAP50', 'mAP50-95)')\ndef postprocess(self, preds):\n\"\"\"Apply non-maximum suppression and return detections with high confidence scores.\"\"\"\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nnc=self.nc)\ndef init_metrics(self, model):\n\"\"\"Initiate pose estimation metrics for YOLO model.\"\"\"\nsuper().init_metrics(model)\nself.kpt_shape = self.data['kpt_shape']\nis_pose = self.kpt_shape == [17, 3]\nnkpt = self.kpt_shape[0]\nself.sigma = OKS_SIGMA if is_pose else np.ones(nkpt) / nkpt\ndef update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nkpts = batch['keypoints'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nnk = kpts.shape[1]  # number of keypoints\nshape = batch['ori_shape'][si]\ncorrect_kpts = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, correct_kpts, *torch.zeros(\n(2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\npred_kpts = predn[:, 6:].view(npr, nk, -1)\nops.scale_coords(batch['img'][si].shape[1:], pred_kpts, shape, ratio_pad=batch['ratio_pad'][si])\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\ntkpts = kpts.clone()\ntkpts[..., 0] *= width\ntkpts[..., 1] *= height\ntkpts = ops.scale_coords(batch['img'][si].shape[1:], tkpts, shape, ratio_pad=batch['ratio_pad'][si])\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn[:, :6], labelsn)\ncorrect_kpts = self._process_batch(predn[:, :6], labelsn, pred_kpts, tkpts)\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\n# Append correct_masks, correct_boxes, pconf, pcls, tcls\nself.stats.append((correct_bboxes, correct_kpts, pred[:, 4], pred[:, 5], cls.squeeze(-1)))\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\n# if self.args.save_txt:\n#    save_one_txt(predn, save_conf, shape, file=save_dir / 'labels' / f'{path.stem}.txt')\ndef _process_batch(self, detections, labels, pred_kpts=None, gt_kpts=None):\n\"\"\"\n        Return correct prediction matrix.\n        Args:\n            detections (torch.Tensor): Tensor of shape [N, 6] representing detections.\n                Each detection is of the format: x1, y1, x2, y2, conf, class.\n            labels (torch.Tensor): Tensor of shape [M, 5] representing labels.\n                Each label is of the format: class, x1, y1, x2, y2.\n            pred_kpts (torch.Tensor, optional): Tensor of shape [N, 51] representing predicted keypoints.\n                51 corresponds to 17 keypoints each with 3 values.\n            gt_kpts (torch.Tensor, optional): Tensor of shape [N, 51] representing ground truth keypoints.\n        Returns:\n            torch.Tensor: Correct prediction matrix of shape [N, 10] for 10 IoU levels.\n        \"\"\"\nif pred_kpts is not None and gt_kpts is not None:\n# `0.53` is from https://github.com/jin-s13/xtcocoapi/blob/master/xtcocotools/cocoeval.py#L384\narea = ops.xyxy2xywh(labels[:, 1:])[:, 2:].prod(1) * 0.53\niou = kpt_iou(gt_kpts, pred_kpts, sigma=self.sigma, area=area)\nelse:  # boxes\niou = box_iou(labels[:, 1:], detections[:, :4])\nreturn self.match_predictions(detections[:, 5], labels[:, 0], iou)\ndef plot_val_samples(self, batch, ni):\n\"\"\"Plots and saves validation set samples with predicted bounding boxes and keypoints.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nkpts=batch['keypoints'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\ndef plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predictions for YOLO model.\"\"\"\npred_kpts = torch.cat([p[:, 6:].view(-1, *self.kpt_shape) for p in preds], 0)\nplot_images(batch['img'],\n*output_to_target(preds, max_det=self.args.max_det),\nkpts=pred_kpts,\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\ndef pred_to_json(self, predn, filename):\n\"\"\"Converts YOLO predictions to COCO JSON format.\"\"\"\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\nfor p, b in zip(predn.tolist(), box.tolist()):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'keypoints': p[6:],\n'score': round(p[4], 5)})\ndef eval_json(self, stats):\n\"\"\"Evaluates object detection model using COCO JSON format.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/person_keypoints_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\nfor i, eval in enumerate([COCOeval(anno, pred, 'bbox'), COCOeval(anno, pred, 'keypoints')]):\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nidx = i * 4 + 2\nstats[self.metrics.keys[idx + 1]], stats[\nself.metrics.keys[idx]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initialize a 'PoseValidator' object with custom parameters and assigned attributes.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize a 'PoseValidator' object with custom parameters and assigned attributes.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.sigma = None\nself.kpt_shape = None\nself.args.task = 'pose'\nself.metrics = PoseMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\nif isinstance(self.args.device, str) and self.args.device.lower() == 'mps':\nLOGGER.warning(\"WARNING \u26a0\ufe0f Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. \"\n'See https://github.com/ultralytics/ultralytics/issues/4031.')\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.eval_json","title":"<code>eval_json(stats)</code>","text":"<p>Evaluates object detection model using COCO JSON format.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def eval_json(self, stats):\n\"\"\"Evaluates object detection model using COCO JSON format.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/person_keypoints_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\nfor i, eval in enumerate([COCOeval(anno, pred, 'bbox'), COCOeval(anno, pred, 'keypoints')]):\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nidx = i * 4 + 2\nstats[self.metrics.keys[idx + 1]], stats[\nself.metrics.keys[idx]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.get_desc","title":"<code>get_desc()</code>","text":"<p>Returns description of evaluation metrics in string format.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def get_desc(self):\n\"\"\"Returns description of evaluation metrics in string format.\"\"\"\nreturn ('%22s' + '%11s' * 10) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)', 'Pose(P',\n'R', 'mAP50', 'mAP50-95)')\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.init_metrics","title":"<code>init_metrics(model)</code>","text":"<p>Initiate pose estimation metrics for YOLO model.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def init_metrics(self, model):\n\"\"\"Initiate pose estimation metrics for YOLO model.\"\"\"\nsuper().init_metrics(model)\nself.kpt_shape = self.data['kpt_shape']\nis_pose = self.kpt_shape == [17, 3]\nnkpt = self.kpt_shape[0]\nself.sigma = OKS_SIGMA if is_pose else np.ones(nkpt) / nkpt\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.plot_predictions","title":"<code>plot_predictions(batch, preds, ni)</code>","text":"<p>Plots predictions for YOLO model.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def plot_predictions(self, batch, preds, ni):\n\"\"\"Plots predictions for YOLO model.\"\"\"\npred_kpts = torch.cat([p[:, 6:].view(-1, *self.kpt_shape) for p in preds], 0)\nplot_images(batch['img'],\n*output_to_target(preds, max_det=self.args.max_det),\nkpts=pred_kpts,\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.plot_val_samples","title":"<code>plot_val_samples(batch, ni)</code>","text":"<p>Plots and saves validation set samples with predicted bounding boxes and keypoints.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def plot_val_samples(self, batch, ni):\n\"\"\"Plots and saves validation set samples with predicted bounding boxes and keypoints.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nkpts=batch['keypoints'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.postprocess","title":"<code>postprocess(preds)</code>","text":"<p>Apply non-maximum suppression and return detections with high confidence scores.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def postprocess(self, preds):\n\"\"\"Apply non-maximum suppression and return detections with high confidence scores.\"\"\"\nreturn ops.non_max_suppression(preds,\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nnc=self.nc)\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.pred_to_json","title":"<code>pred_to_json(predn, filename)</code>","text":"<p>Converts YOLO predictions to COCO JSON format.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def pred_to_json(self, predn, filename):\n\"\"\"Converts YOLO predictions to COCO JSON format.\"\"\"\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\nfor p, b in zip(predn.tolist(), box.tolist()):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'keypoints': p[6:],\n'score': round(p[4], 5)})\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.preprocess","title":"<code>preprocess(batch)</code>","text":"<p>Preprocesses the batch by converting the 'keypoints' data into a float and moving it to the device.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def preprocess(self, batch):\n\"\"\"Preprocesses the batch by converting the 'keypoints' data into a float and moving it to the device.\"\"\"\nbatch = super().preprocess(batch)\nbatch['keypoints'] = batch['keypoints'].to(self.device).float()\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/pose/val/#ultralytics.models.yolo.pose.val.PoseValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Metrics.</p> Source code in <code>ultralytics/models/yolo/pose/val.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, pred in enumerate(preds):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nkpts = batch['keypoints'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nnk = kpts.shape[1]  # number of keypoints\nshape = batch['ori_shape'][si]\ncorrect_kpts = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, correct_kpts, *torch.zeros(\n(2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\npred_kpts = predn[:, 6:].view(npr, nk, -1)\nops.scale_coords(batch['img'][si].shape[1:], pred_kpts, shape, ratio_pad=batch['ratio_pad'][si])\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\ntkpts = kpts.clone()\ntkpts[..., 0] *= width\ntkpts[..., 1] *= height\ntkpts = ops.scale_coords(batch['img'][si].shape[1:], tkpts, shape, ratio_pad=batch['ratio_pad'][si])\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn[:, :6], labelsn)\ncorrect_kpts = self._process_batch(predn[:, :6], labelsn, pred_kpts, tkpts)\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\n# Append correct_masks, correct_boxes, pconf, pcls, tcls\nself.stats.append((correct_bboxes, correct_kpts, pred[:, 4], pred[:, 5], cls.squeeze(-1)))\n# Save\nif self.args.save_json:\nself.pred_to_json(predn, batch['im_file'][si])\n</code></pre>"},{"location":"reference/models/yolo/segment/predict/","title":"Reference for <code>ultralytics/models/yolo/segment/predict.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/predict.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/segment/predict/#ultralytics.models.yolo.segment.predict.SegmentationPredictor","title":"<code>ultralytics.models.yolo.segment.predict.SegmentationPredictor</code>","text":"<p>             Bases: <code>DetectionPredictor</code></p> <p>A class extending the DetectionPredictor class for prediction based on a segmentation model.</p> Example <pre><code>from ultralytics.utils import ASSETS\nfrom ultralytics.models.yolo.segment import SegmentationPredictor\nargs = dict(model='yolov8n-seg.pt', source=ASSETS)\npredictor = SegmentationPredictor(overrides=args)\npredictor.predict_cli()\n</code></pre> Source code in <code>ultralytics/models/yolo/segment/predict.py</code> <pre><code>class SegmentationPredictor(DetectionPredictor):\n\"\"\"\n    A class extending the DetectionPredictor class for prediction based on a segmentation model.\n    Example:\n        ```python\n        from ultralytics.utils import ASSETS\n        from ultralytics.models.yolo.segment import SegmentationPredictor\n        args = dict(model='yolov8n-seg.pt', source=ASSETS)\n        predictor = SegmentationPredictor(overrides=args)\n        predictor.predict_cli()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\nsuper().__init__(cfg, overrides, _callbacks)\nself.args.task = 'segment'\ndef postprocess(self, preds, img, orig_imgs):\np = ops.non_max_suppression(preds[0],\nself.args.conf,\nself.args.iou,\nagnostic=self.args.agnostic_nms,\nmax_det=self.args.max_det,\nnc=len(self.model.names),\nclasses=self.args.classes)\nresults = []\nis_list = isinstance(orig_imgs, list)  # input images are a list, not a torch.Tensor\nproto = preds[1][-1] if len(preds[1]) == 3 else preds[1]  # second output is len 3 if pt, but only 1 if exported\nfor i, pred in enumerate(p):\norig_img = orig_imgs[i] if is_list else orig_imgs\nimg_path = self.batch[0][i]\nif not len(pred):  # save empty boxes\nmasks = None\nelif self.args.retina_masks:\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nmasks = ops.process_mask_native(proto[i], pred[:, 6:], pred[:, :4], orig_img.shape[:2])  # HWC\nelse:\nmasks = ops.process_mask(proto[i], pred[:, 6:], pred[:, :4], img.shape[2:], upsample=True)  # HWC\nif is_list:\npred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\nresults.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6], masks=masks))\nreturn results\n</code></pre>"},{"location":"reference/models/yolo/segment/train/","title":"Reference for <code>ultralytics/models/yolo/segment/train.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/train.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer","title":"<code>ultralytics.models.yolo.segment.train.SegmentationTrainer</code>","text":"<p>             Bases: <code>DetectionTrainer</code></p> <p>A class extending the DetectionTrainer class for training based on a segmentation model.</p> Example <pre><code>from ultralytics.models.yolo.segment import SegmentationTrainer\nargs = dict(model='yolov8n-seg.pt', data='coco8-seg.yaml', epochs=3)\ntrainer = SegmentationTrainer(overrides=args)\ntrainer.train()\n</code></pre> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>class SegmentationTrainer(yolo.detect.DetectionTrainer):\n\"\"\"\n    A class extending the DetectionTrainer class for training based on a segmentation model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.segment import SegmentationTrainer\n        args = dict(model='yolov8n-seg.pt', data='coco8-seg.yaml', epochs=3)\n        trainer = SegmentationTrainer(overrides=args)\n        trainer.train()\n        ```\n    \"\"\"\ndef __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a SegmentationTrainer object with given arguments.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'segment'\nsuper().__init__(cfg, overrides, _callbacks)\ndef get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return SegmentationModel initialized with specified config and weights.\"\"\"\nmodel = SegmentationModel(cfg, ch=3, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\ndef get_validator(self):\n\"\"\"Return an instance of SegmentationValidator for validation of YOLO model.\"\"\"\nself.loss_names = 'box_loss', 'seg_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.segment.SegmentationValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\ndef plot_training_samples(self, batch, ni):\n\"\"\"Creates a plot of training sample images with labels and box coordinates.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nbatch['masks'],\npaths=batch['im_file'],\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\ndef plot_metrics(self):\n\"\"\"Plots training/val metrics.\"\"\"\nplot_results(file=self.csv, segment=True, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer.__init__","title":"<code>__init__(cfg=DEFAULT_CFG, overrides=None, _callbacks=None)</code>","text":"<p>Initialize a SegmentationTrainer object with given arguments.</p> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n\"\"\"Initialize a SegmentationTrainer object with given arguments.\"\"\"\nif overrides is None:\noverrides = {}\noverrides['task'] = 'segment'\nsuper().__init__(cfg, overrides, _callbacks)\n</code></pre>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer.get_model","title":"<code>get_model(cfg=None, weights=None, verbose=True)</code>","text":"<p>Return SegmentationModel initialized with specified config and weights.</p> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>def get_model(self, cfg=None, weights=None, verbose=True):\n\"\"\"Return SegmentationModel initialized with specified config and weights.\"\"\"\nmodel = SegmentationModel(cfg, ch=3, nc=self.data['nc'], verbose=verbose and RANK == -1)\nif weights:\nmodel.load(weights)\nreturn model\n</code></pre>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer.get_validator","title":"<code>get_validator()</code>","text":"<p>Return an instance of SegmentationValidator for validation of YOLO model.</p> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>def get_validator(self):\n\"\"\"Return an instance of SegmentationValidator for validation of YOLO model.\"\"\"\nself.loss_names = 'box_loss', 'seg_loss', 'cls_loss', 'dfl_loss'\nreturn yolo.segment.SegmentationValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\n</code></pre>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer.plot_metrics","title":"<code>plot_metrics()</code>","text":"<p>Plots training/val metrics.</p> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>def plot_metrics(self):\n\"\"\"Plots training/val metrics.\"\"\"\nplot_results(file=self.csv, segment=True, on_plot=self.on_plot)  # save results.png\n</code></pre>"},{"location":"reference/models/yolo/segment/train/#ultralytics.models.yolo.segment.train.SegmentationTrainer.plot_training_samples","title":"<code>plot_training_samples(batch, ni)</code>","text":"<p>Creates a plot of training sample images with labels and box coordinates.</p> Source code in <code>ultralytics/models/yolo/segment/train.py</code> <pre><code>def plot_training_samples(self, batch, ni):\n\"\"\"Creates a plot of training sample images with labels and box coordinates.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nbatch['masks'],\npaths=batch['im_file'],\nfname=self.save_dir / f'train_batch{ni}.jpg',\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/segment/val/","title":"Reference for <code>ultralytics/models/yolo/segment/val.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/val.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator","title":"<code>ultralytics.models.yolo.segment.val.SegmentationValidator</code>","text":"<p>             Bases: <code>DetectionValidator</code></p> <p>A class extending the DetectionValidator class for validation based on a segmentation model.</p> Example <pre><code>from ultralytics.models.yolo.segment import SegmentationValidator\nargs = dict(model='yolov8n-seg.pt', data='coco8-seg.yaml')\nvalidator = SegmentationValidator(args=args)\nvalidator()\n</code></pre> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>class SegmentationValidator(DetectionValidator):\n\"\"\"\n    A class extending the DetectionValidator class for validation based on a segmentation model.\n    Example:\n        ```python\n        from ultralytics.models.yolo.segment import SegmentationValidator\n        args = dict(model='yolov8n-seg.pt', data='coco8-seg.yaml')\n        validator = SegmentationValidator(args=args)\n        validator()\n        ```\n    \"\"\"\ndef __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.plot_masks = None\nself.process = None\nself.args.task = 'segment'\nself.metrics = SegmentMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\ndef preprocess(self, batch):\n\"\"\"Preprocesses batch by converting masks to float and sending to device.\"\"\"\nbatch = super().preprocess(batch)\nbatch['masks'] = batch['masks'].to(self.device).float()\nreturn batch\ndef init_metrics(self, model):\n\"\"\"Initialize metrics and select mask processing function based on save_json flag.\"\"\"\nsuper().init_metrics(model)\nself.plot_masks = []\nif self.args.save_json:\ncheck_requirements('pycocotools&gt;=2.0.6')\nself.process = ops.process_mask_upsample  # more accurate\nelse:\nself.process = ops.process_mask  # faster\ndef get_desc(self):\n\"\"\"Return a formatted description of evaluation metrics.\"\"\"\nreturn ('%22s' + '%11s' * 10) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)', 'Mask(P',\n'R', 'mAP50', 'mAP50-95)')\ndef postprocess(self, preds):\n\"\"\"Post-processes YOLO predictions and returns output detections with proto.\"\"\"\np = ops.non_max_suppression(preds[0],\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nnc=self.nc)\nproto = preds[1][-1] if len(preds[1]) == 3 else preds[1]  # second output is len 3 if pt, but only 1 if exported\nreturn p, proto\ndef update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, (pred, proto) in enumerate(zip(preds[0], preds[1])):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_masks = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, correct_masks, *torch.zeros(\n(2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Masks\nmidx = [si] if self.args.overlap_mask else idx\ngt_masks = batch['masks'][midx]\npred_masks = self.process(proto, pred[:, 6:], pred[:, :4], shape=batch['img'][si].shape[1:])\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn, labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\ncorrect_masks = self._process_batch(predn,\nlabelsn,\npred_masks,\ngt_masks,\noverlap=self.args.overlap_mask,\nmasks=True)\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\n# Append correct_masks, correct_boxes, pconf, pcls, tcls\nself.stats.append((correct_bboxes, correct_masks, pred[:, 4], pred[:, 5], cls.squeeze(-1)))\npred_masks = torch.as_tensor(pred_masks, dtype=torch.uint8)\nif self.args.plots and self.batch_i &lt; 3:\nself.plot_masks.append(pred_masks[:15].cpu())  # filter top 15 to plot\n# Save\nif self.args.save_json:\npred_masks = ops.scale_image(pred_masks.permute(1, 2, 0).contiguous().cpu().numpy(),\nshape,\nratio_pad=batch['ratio_pad'][si])\nself.pred_to_json(predn, batch['im_file'][si], pred_masks)\n# if self.args.save_txt:\n#    save_one_txt(predn, save_conf, shape, file=save_dir / 'labels' / f'{path.stem}.txt')\ndef finalize_metrics(self, *args, **kwargs):\n\"\"\"Sets speed and confusion matrix for evaluation metrics.\"\"\"\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\ndef _process_batch(self, detections, labels, pred_masks=None, gt_masks=None, overlap=False, masks=False):\n\"\"\"\n        Return correct prediction matrix\n        Args:\n            detections (array[N, 6]), x1, y1, x2, y2, conf, class\n            labels (array[M, 5]), class, x1, y1, x2, y2\n        Returns:\n            correct (array[N, 10]), for 10 IoU levels\n        \"\"\"\nif masks:\nif overlap:\nnl = len(labels)\nindex = torch.arange(nl, device=gt_masks.device).view(nl, 1, 1) + 1\ngt_masks = gt_masks.repeat(nl, 1, 1)  # shape(1,640,640) -&gt; (n,640,640)\ngt_masks = torch.where(gt_masks == index, 1.0, 0.0)\nif gt_masks.shape[1:] != pred_masks.shape[1:]:\ngt_masks = F.interpolate(gt_masks[None], pred_masks.shape[1:], mode='bilinear', align_corners=False)[0]\ngt_masks = gt_masks.gt_(0.5)\niou = mask_iou(gt_masks.view(gt_masks.shape[0], -1), pred_masks.view(pred_masks.shape[0], -1))\nelse:  # boxes\niou = box_iou(labels[:, 1:], detections[:, :4])\nreturn self.match_predictions(detections[:, 5], labels[:, 0], iou)\ndef plot_val_samples(self, batch, ni):\n\"\"\"Plots validation samples with bounding box labels.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nbatch['masks'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\ndef plot_predictions(self, batch, preds, ni):\n\"\"\"Plots batch predictions with masks and bounding boxes.\"\"\"\nplot_images(\nbatch['img'],\n*output_to_target(preds[0], max_det=15),  # not set to self.args.max_det due to slow plotting speed\ntorch.cat(self.plot_masks, dim=0) if len(self.plot_masks) else self.plot_masks,\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\nself.plot_masks.clear()\ndef pred_to_json(self, predn, filename, pred_masks):\n\"\"\"Save one JSON result.\"\"\"\n# Example result = {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\nfrom pycocotools.mask import encode  # noqa\ndef single_encode(x):\n\"\"\"Encode predicted masks as RLE and append results to jdict.\"\"\"\nrle = encode(np.asarray(x[:, :, None], order='F', dtype='uint8'))[0]\nrle['counts'] = rle['counts'].decode('utf-8')\nreturn rle\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\npred_masks = np.transpose(pred_masks, (2, 0, 1))\nwith ThreadPool(NUM_THREADS) as pool:\nrles = pool.map(single_encode, pred_masks)\nfor i, (p, b) in enumerate(zip(predn.tolist(), box.tolist())):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'score': round(p[4], 5),\n'segmentation': rles[i]})\ndef eval_json(self, stats):\n\"\"\"Return COCO-style object detection evaluation metrics.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/instances_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\nfor i, eval in enumerate([COCOeval(anno, pred, 'bbox'), COCOeval(anno, pred, 'segm')]):\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nidx = i * 4 + 2\nstats[self.metrics.keys[idx + 1]], stats[\nself.metrics.keys[idx]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None)</code>","text":"<p>Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):\n\"\"\"Initialize SegmentationValidator and set task to 'segment', metrics to SegmentMetrics.\"\"\"\nsuper().__init__(dataloader, save_dir, pbar, args, _callbacks)\nself.plot_masks = None\nself.process = None\nself.args.task = 'segment'\nself.metrics = SegmentMetrics(save_dir=self.save_dir, on_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.eval_json","title":"<code>eval_json(stats)</code>","text":"<p>Return COCO-style object detection evaluation metrics.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def eval_json(self, stats):\n\"\"\"Return COCO-style object detection evaluation metrics.\"\"\"\nif self.args.save_json and self.is_coco and len(self.jdict):\nanno_json = self.data['path'] / 'annotations/instances_val2017.json'  # annotations\npred_json = self.save_dir / 'predictions.json'  # predictions\nLOGGER.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\ntry:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\ncheck_requirements('pycocotools&gt;=2.0.6')\nfrom pycocotools.coco import COCO  # noqa\nfrom pycocotools.cocoeval import COCOeval  # noqa\nfor x in anno_json, pred_json:\nassert x.is_file(), f'{x} file not found'\nanno = COCO(str(anno_json))  # init annotations api\npred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\nfor i, eval in enumerate([COCOeval(anno, pred, 'bbox'), COCOeval(anno, pred, 'segm')]):\nif self.is_coco:\neval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # im to eval\neval.evaluate()\neval.accumulate()\neval.summarize()\nidx = i * 4 + 2\nstats[self.metrics.keys[idx + 1]], stats[\nself.metrics.keys[idx]] = eval.stats[:2]  # update mAP50-95 and mAP50\nexcept Exception as e:\nLOGGER.warning(f'pycocotools unable to run: {e}')\nreturn stats\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.finalize_metrics","title":"<code>finalize_metrics(*args, **kwargs)</code>","text":"<p>Sets speed and confusion matrix for evaluation metrics.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def finalize_metrics(self, *args, **kwargs):\n\"\"\"Sets speed and confusion matrix for evaluation metrics.\"\"\"\nself.metrics.speed = self.speed\nself.metrics.confusion_matrix = self.confusion_matrix\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.get_desc","title":"<code>get_desc()</code>","text":"<p>Return a formatted description of evaluation metrics.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def get_desc(self):\n\"\"\"Return a formatted description of evaluation metrics.\"\"\"\nreturn ('%22s' + '%11s' * 10) % ('Class', 'Images', 'Instances', 'Box(P', 'R', 'mAP50', 'mAP50-95)', 'Mask(P',\n'R', 'mAP50', 'mAP50-95)')\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.init_metrics","title":"<code>init_metrics(model)</code>","text":"<p>Initialize metrics and select mask processing function based on save_json flag.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def init_metrics(self, model):\n\"\"\"Initialize metrics and select mask processing function based on save_json flag.\"\"\"\nsuper().init_metrics(model)\nself.plot_masks = []\nif self.args.save_json:\ncheck_requirements('pycocotools&gt;=2.0.6')\nself.process = ops.process_mask_upsample  # more accurate\nelse:\nself.process = ops.process_mask  # faster\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.plot_predictions","title":"<code>plot_predictions(batch, preds, ni)</code>","text":"<p>Plots batch predictions with masks and bounding boxes.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def plot_predictions(self, batch, preds, ni):\n\"\"\"Plots batch predictions with masks and bounding boxes.\"\"\"\nplot_images(\nbatch['img'],\n*output_to_target(preds[0], max_det=15),  # not set to self.args.max_det due to slow plotting speed\ntorch.cat(self.plot_masks, dim=0) if len(self.plot_masks) else self.plot_masks,\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_pred.jpg',\nnames=self.names,\non_plot=self.on_plot)  # pred\nself.plot_masks.clear()\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.plot_val_samples","title":"<code>plot_val_samples(batch, ni)</code>","text":"<p>Plots validation samples with bounding box labels.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def plot_val_samples(self, batch, ni):\n\"\"\"Plots validation samples with bounding box labels.\"\"\"\nplot_images(batch['img'],\nbatch['batch_idx'],\nbatch['cls'].squeeze(-1),\nbatch['bboxes'],\nbatch['masks'],\npaths=batch['im_file'],\nfname=self.save_dir / f'val_batch{ni}_labels.jpg',\nnames=self.names,\non_plot=self.on_plot)\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.postprocess","title":"<code>postprocess(preds)</code>","text":"<p>Post-processes YOLO predictions and returns output detections with proto.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def postprocess(self, preds):\n\"\"\"Post-processes YOLO predictions and returns output detections with proto.\"\"\"\np = ops.non_max_suppression(preds[0],\nself.args.conf,\nself.args.iou,\nlabels=self.lb,\nmulti_label=True,\nagnostic=self.args.single_cls,\nmax_det=self.args.max_det,\nnc=self.nc)\nproto = preds[1][-1] if len(preds[1]) == 3 else preds[1]  # second output is len 3 if pt, but only 1 if exported\nreturn p, proto\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.pred_to_json","title":"<code>pred_to_json(predn, filename, pred_masks)</code>","text":"<p>Save one JSON result.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def pred_to_json(self, predn, filename, pred_masks):\n\"\"\"Save one JSON result.\"\"\"\n# Example result = {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\nfrom pycocotools.mask import encode  # noqa\ndef single_encode(x):\n\"\"\"Encode predicted masks as RLE and append results to jdict.\"\"\"\nrle = encode(np.asarray(x[:, :, None], order='F', dtype='uint8'))[0]\nrle['counts'] = rle['counts'].decode('utf-8')\nreturn rle\nstem = Path(filename).stem\nimage_id = int(stem) if stem.isnumeric() else stem\nbox = ops.xyxy2xywh(predn[:, :4])  # xywh\nbox[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\npred_masks = np.transpose(pred_masks, (2, 0, 1))\nwith ThreadPool(NUM_THREADS) as pool:\nrles = pool.map(single_encode, pred_masks)\nfor i, (p, b) in enumerate(zip(predn.tolist(), box.tolist())):\nself.jdict.append({\n'image_id': image_id,\n'category_id': self.class_map[int(p[5])],\n'bbox': [round(x, 3) for x in b],\n'score': round(p[4], 5),\n'segmentation': rles[i]})\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.preprocess","title":"<code>preprocess(batch)</code>","text":"<p>Preprocesses batch by converting masks to float and sending to device.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def preprocess(self, batch):\n\"\"\"Preprocesses batch by converting masks to float and sending to device.\"\"\"\nbatch = super().preprocess(batch)\nbatch['masks'] = batch['masks'].to(self.device).float()\nreturn batch\n</code></pre>"},{"location":"reference/models/yolo/segment/val/#ultralytics.models.yolo.segment.val.SegmentationValidator.update_metrics","title":"<code>update_metrics(preds, batch)</code>","text":"<p>Metrics.</p> Source code in <code>ultralytics/models/yolo/segment/val.py</code> <pre><code>def update_metrics(self, preds, batch):\n\"\"\"Metrics.\"\"\"\nfor si, (pred, proto) in enumerate(zip(preds[0], preds[1])):\nidx = batch['batch_idx'] == si\ncls = batch['cls'][idx]\nbbox = batch['bboxes'][idx]\nnl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\nshape = batch['ori_shape'][si]\ncorrect_masks = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\ncorrect_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\nself.seen += 1\nif npr == 0:\nif nl:\nself.stats.append((correct_bboxes, correct_masks, *torch.zeros(\n(2, 0), device=self.device), cls.squeeze(-1)))\nif self.args.plots:\nself.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\ncontinue\n# Masks\nmidx = [si] if self.args.overlap_mask else idx\ngt_masks = batch['masks'][midx]\npred_masks = self.process(proto, pred[:, 6:], pred[:, :4], shape=batch['img'][si].shape[1:])\n# Predictions\nif self.args.single_cls:\npred[:, 5] = 0\npredn = pred.clone()\nops.scale_boxes(batch['img'][si].shape[1:], predn[:, :4], shape,\nratio_pad=batch['ratio_pad'][si])  # native-space pred\n# Evaluate\nif nl:\nheight, width = batch['img'].shape[2:]\ntbox = ops.xywh2xyxy(bbox) * torch.tensor(\n(width, height, width, height), device=self.device)  # target boxes\nops.scale_boxes(batch['img'][si].shape[1:], tbox, shape,\nratio_pad=batch['ratio_pad'][si])  # native-space labels\nlabelsn = torch.cat((cls, tbox), 1)  # native-space labels\ncorrect_bboxes = self._process_batch(predn, labelsn)\n# TODO: maybe remove these `self.` arguments as they already are member variable\ncorrect_masks = self._process_batch(predn,\nlabelsn,\npred_masks,\ngt_masks,\noverlap=self.args.overlap_mask,\nmasks=True)\nif self.args.plots:\nself.confusion_matrix.process_batch(predn, labelsn)\n# Append correct_masks, correct_boxes, pconf, pcls, tcls\nself.stats.append((correct_bboxes, correct_masks, pred[:, 4], pred[:, 5], cls.squeeze(-1)))\npred_masks = torch.as_tensor(pred_masks, dtype=torch.uint8)\nif self.args.plots and self.batch_i &lt; 3:\nself.plot_masks.append(pred_masks[:15].cpu())  # filter top 15 to plot\n# Save\nif self.args.save_json:\npred_masks = ops.scale_image(pred_masks.permute(1, 2, 0).contiguous().cpu().numpy(),\nshape,\nratio_pad=batch['ratio_pad'][si])\nself.pred_to_json(predn, batch['im_file'][si], pred_masks)\n</code></pre>"},{"location":"reference/nn/autobackend/","title":"Reference for <code>ultralytics/nn/autobackend.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/autobackend.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.AutoBackend","title":"<code>ultralytics.nn.autobackend.AutoBackend</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>class AutoBackend(nn.Module):\ndef __init__(self,\nweights='yolov8n.pt',\ndevice=torch.device('cpu'),\ndnn=False,\ndata=None,\nfp16=False,\nfuse=True,\nverbose=True):\n\"\"\"\n        MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n        Args:\n            weights (str): The path to the weights file. Default: 'yolov8n.pt'\n            device (torch.device): The device to run the model on.\n            dnn (bool): Use OpenCV DNN module for inference if True, defaults to False.\n            data (str | Path | optional): Additional data.yaml file for class names.\n            fp16 (bool): If True, use half precision. Default: False\n            fuse (bool): Whether to fuse the model or not. Default: True\n            verbose (bool): Whether to run in verbose mode or not. Default: True\n        Supported formats and their naming conventions:\n            | Format                | Suffix           |\n            |-----------------------|------------------|\n            | PyTorch               | *.pt             |\n            | TorchScript           | *.torchscript    |\n            | ONNX Runtime          | *.onnx           |\n            | ONNX OpenCV DNN       | *.onnx dnn=True  |\n            | OpenVINO              | *.xml            |\n            | CoreML                | *.mlpackage      |\n            | TensorRT              | *.engine         |\n            | TensorFlow SavedModel | *_saved_model    |\n            | TensorFlow GraphDef   | *.pb             |\n            | TensorFlow Lite       | *.tflite         |\n            | TensorFlow Edge TPU   | *_edgetpu.tflite |\n            | PaddlePaddle          | *_paddle_model   |\n            | ncnn                  | *_ncnn_model     |\n        \"\"\"\nsuper().__init__()\nw = str(weights[0] if isinstance(weights, list) else weights)\nnn_module = isinstance(weights, torch.nn.Module)\npt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, ncnn, triton = \\\n            self._model_type(w)\nfp16 &amp;= pt or jit or onnx or xml or engine or nn_module or triton  # FP16\nnhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\nstride = 32  # default stride\nmodel, metadata = None, None\n# Set device\ncuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\nif cuda and not any([nn_module, pt, jit, engine]):  # GPU dataloader formats\ndevice = torch.device('cpu')\ncuda = False\n# Download if not local\nif not (pt or triton or nn_module):\nw = attempt_download_asset(w)\n# Load model\nif nn_module:  # in-memory PyTorch model\nmodel = weights.to(device)\nmodel = model.fuse(verbose=verbose) if fuse else model\nif hasattr(model, 'kpt_shape'):\nkpt_shape = model.kpt_shape  # pose-only\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\npt = True\nelif pt:  # PyTorch\nfrom ultralytics.nn.tasks import attempt_load_weights\nmodel = attempt_load_weights(weights if isinstance(weights, list) else w,\ndevice=device,\ninplace=True,\nfuse=fuse)\nif hasattr(model, 'kpt_shape'):\nkpt_shape = model.kpt_shape  # pose-only\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\nelif jit:  # TorchScript\nLOGGER.info(f'Loading {w} for TorchScript inference...')\nextra_files = {'config.txt': ''}  # model metadata\nmodel = torch.jit.load(w, _extra_files=extra_files, map_location=device)\nmodel.half() if fp16 else model.float()\nif extra_files['config.txt']:  # load metadata dict\nmetadata = json.loads(extra_files['config.txt'], object_hook=lambda x: dict(x.items()))\nelif dnn:  # ONNX OpenCV DNN\nLOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\ncheck_requirements('opencv-python&gt;=4.5.4')\nnet = cv2.dnn.readNetFromONNX(w)\nelif onnx:  # ONNX Runtime\nLOGGER.info(f'Loading {w} for ONNX Runtime inference...')\ncheck_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\nimport onnxruntime\nproviders = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\nsession = onnxruntime.InferenceSession(w, providers=providers)\noutput_names = [x.name for x in session.get_outputs()]\nmetadata = session.get_modelmeta().custom_metadata_map  # metadata\nelif xml:  # OpenVINO\nLOGGER.info(f'Loading {w} for OpenVINO inference...')\ncheck_requirements('openvino&gt;=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nfrom openvino.runtime import Core, Layout, get_batch  # noqa\ncore = Core()\nw = Path(w)\nif not w.is_file():  # if not *.xml\nw = next(w.glob('*.xml'))  # get *.xml file from *_openvino_model dir\nov_model = core.read_model(model=str(w), weights=w.with_suffix('.bin'))\nif ov_model.get_parameters()[0].get_layout().empty:\nov_model.get_parameters()[0].set_layout(Layout('NCHW'))\nbatch_dim = get_batch(ov_model)\nif batch_dim.is_static:\nbatch_size = batch_dim.get_length()\nov_compiled_model = core.compile_model(ov_model, device_name='AUTO')  # AUTO selects best available device\nmetadata = w.parent / 'metadata.yaml'\nelif engine:  # TensorRT\nLOGGER.info(f'Loading {w} for TensorRT inference...')\ntry:\nimport tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download\nexcept ImportError:\nif LINUX:\ncheck_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\nimport tensorrt as trt  # noqa\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nif device.type == 'cpu':\ndevice = torch.device('cuda:0')\nBinding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\nlogger = trt.Logger(trt.Logger.INFO)\n# Read file\nwith open(w, 'rb') as f, trt.Runtime(logger) as runtime:\nmeta_len = int.from_bytes(f.read(4), byteorder='little')  # read metadata length\nmetadata = json.loads(f.read(meta_len).decode('utf-8'))  # read metadata\nmodel = runtime.deserialize_cuda_engine(f.read())  # read engine\ncontext = model.create_execution_context()\nbindings = OrderedDict()\noutput_names = []\nfp16 = False  # default updated below\ndynamic = False\nfor i in range(model.num_bindings):\nname = model.get_binding_name(i)\ndtype = trt.nptype(model.get_binding_dtype(i))\nif model.binding_is_input(i):\nif -1 in tuple(model.get_binding_shape(i)):  # dynamic\ndynamic = True\ncontext.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\nif dtype == np.float16:\nfp16 = True\nelse:  # output\noutput_names.append(name)\nshape = tuple(context.get_binding_shape(i))\nim = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\nbindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\nbinding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\nbatch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\nelif coreml:  # CoreML\nLOGGER.info(f'Loading {w} for CoreML inference...')\nimport coremltools as ct\nmodel = ct.models.MLModel(w)\nmetadata = dict(model.user_defined_metadata)\nelif saved_model:  # TF SavedModel\nLOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\nimport tensorflow as tf\nkeras = False  # assume TF1 saved_model\nmodel = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\nmetadata = Path(w) / 'metadata.yaml'\nelif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\nLOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\nimport tensorflow as tf\nfrom ultralytics.engine.exporter import gd_outputs\ndef wrap_frozen_graph(gd, inputs, outputs):\n\"\"\"Wrap frozen graphs for deployment.\"\"\"\nx = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=''), [])  # wrapped\nge = x.graph.as_graph_element\nreturn x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(w, 'rb') as f:\ngd.ParseFromString(f.read())\nfrozen_func = wrap_frozen_graph(gd, inputs='x:0', outputs=gd_outputs(gd))\nelif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\ntry:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\nimport tensorflow as tf\nInterpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate\nif edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\nLOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\ndelegate = {\n'Linux': 'libedgetpu.so.1',\n'Darwin': 'libedgetpu.1.dylib',\n'Windows': 'edgetpu.dll'}[platform.system()]\ninterpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\nelse:  # TFLite\nLOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\ninterpreter = Interpreter(model_path=w)  # load TFLite model\ninterpreter.allocate_tensors()  # allocate\ninput_details = interpreter.get_input_details()  # inputs\noutput_details = interpreter.get_output_details()  # outputs\n# Load metadata\nwith contextlib.suppress(zipfile.BadZipFile):\nwith zipfile.ZipFile(w, 'r') as model:\nmeta_file = model.namelist()[0]\nmetadata = ast.literal_eval(model.read(meta_file).decode('utf-8'))\nelif tfjs:  # TF.js\nraise NotImplementedError('YOLOv8 TF.js inference is not currently supported.')\nelif paddle:  # PaddlePaddle\nLOGGER.info(f'Loading {w} for PaddlePaddle inference...')\ncheck_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\nimport paddle.inference as pdi  # noqa\nw = Path(w)\nif not w.is_file():  # if not *.pdmodel\nw = next(w.rglob('*.pdmodel'))  # get *.pdmodel file from *_paddle_model dir\nconfig = pdi.Config(str(w), str(w.with_suffix('.pdiparams')))\nif cuda:\nconfig.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\npredictor = pdi.create_predictor(config)\ninput_handle = predictor.get_input_handle(predictor.get_input_names()[0])\noutput_names = predictor.get_output_names()\nmetadata = w.parents[1] / 'metadata.yaml'\nelif ncnn:  # ncnn\nLOGGER.info(f'Loading {w} for ncnn inference...')\ncheck_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn\nimport ncnn as pyncnn\nnet = pyncnn.Net()\nnet.opt.use_vulkan_compute = cuda\nw = Path(w)\nif not w.is_file():  # if not *.param\nw = next(w.glob('*.param'))  # get *.param file from *_ncnn_model dir\nnet.load_param(str(w))\nnet.load_model(str(w.with_suffix('.bin')))\nmetadata = w.parent / 'metadata.yaml'\nelif triton:  # NVIDIA Triton Inference Server\n\"\"\"TODO\n            check_requirements('tritonclient[all]')\n            from utils.triton import TritonRemoteModel\n            model = TritonRemoteModel(url=w)\n            nhwc = model.runtime.startswith(\"tensorflow\")\n            \"\"\"\nraise NotImplementedError('Triton Inference Server is not currently supported.')\nelse:\nfrom ultralytics.engine.exporter import export_formats\nraise TypeError(f\"model='{w}' is not a supported model format. \"\n'See https://docs.ultralytics.com/modes/predict for help.'\nf'\\n\\n{export_formats()}')\n# Load external metadata YAML\nif isinstance(metadata, (str, Path)) and Path(metadata).exists():\nmetadata = yaml_load(metadata)\nif metadata:\nfor k, v in metadata.items():\nif k in ('stride', 'batch'):\nmetadata[k] = int(v)\nelif k in ('imgsz', 'names', 'kpt_shape') and isinstance(v, str):\nmetadata[k] = eval(v)\nstride = metadata['stride']\ntask = metadata['task']\nbatch = metadata['batch']\nimgsz = metadata['imgsz']\nnames = metadata['names']\nkpt_shape = metadata.get('kpt_shape')\nelif not (pt or triton or nn_module):\nLOGGER.warning(f\"WARNING \u26a0\ufe0f Metadata not found for 'model={weights}'\")\n# Check names\nif 'names' not in locals():  # names missing\nnames = self._apply_default_class_names(data)\nnames = check_class_names(names)\nself.__dict__.update(locals())  # assign all variables to self\ndef forward(self, im, augment=False, visualize=False):\n\"\"\"\n        Runs inference on the YOLOv8 MultiBackend model.\n        Args:\n            im (torch.Tensor): The image tensor to perform inference on.\n            augment (bool): whether to perform data augmentation during inference, defaults to False\n            visualize (bool): whether to visualize the output predictions, defaults to False\n        Returns:\n            (tuple): Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)\n        \"\"\"\nb, ch, h, w = im.shape  # batch, channel, height, width\nif self.fp16 and im.dtype != torch.float16:\nim = im.half()  # to FP16\nif self.nhwc:\nim = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\nif self.pt or self.nn_module:  # PyTorch\ny = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\nelif self.jit:  # TorchScript\ny = self.model(im)\nelif self.dnn:  # ONNX OpenCV DNN\nim = im.cpu().numpy()  # torch to numpy\nself.net.setInput(im)\ny = self.net.forward()\nelif self.onnx:  # ONNX Runtime\nim = im.cpu().numpy()  # torch to numpy\ny = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\nelif self.xml:  # OpenVINO\nim = im.cpu().numpy()  # FP32\ny = list(self.ov_compiled_model(im).values())\nelif self.engine:  # TensorRT\nif self.dynamic and im.shape != self.bindings['images'].shape:\ni = self.model.get_binding_index('images')\nself.context.set_binding_shape(i, im.shape)  # reshape if dynamic\nself.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\nfor name in self.output_names:\ni = self.model.get_binding_index(name)\nself.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\ns = self.bindings['images'].shape\nassert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\nself.binding_addrs['images'] = int(im.data_ptr())\nself.context.execute_v2(list(self.binding_addrs.values()))\ny = [self.bindings[x].data for x in sorted(self.output_names)]\nelif self.coreml:  # CoreML\nim = im[0].cpu().numpy()\nim_pil = Image.fromarray((im * 255).astype('uint8'))\n# im = im.resize((192, 320), Image.BILINEAR)\ny = self.model.predict({'image': im_pil})  # coordinates are xywh normalized\nif 'confidence' in y:\nbox = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\nconf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\ny = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\nelif len(y) == 1:  # classification model\ny = list(y.values())\nelif len(y) == 2:  # segmentation model\ny = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\nelif self.paddle:  # PaddlePaddle\nim = im.cpu().numpy().astype(np.float32)\nself.input_handle.copy_from_cpu(im)\nself.predictor.run()\ny = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\nelif self.ncnn:  # ncnn\nmat_in = self.pyncnn.Mat(im[0].cpu().numpy())\nex = self.net.create_extractor()\ninput_names, output_names = self.net.input_names(), self.net.output_names()\nex.input(input_names[0], mat_in)\ny = []\nfor output_name in output_names:\nmat_out = self.pyncnn.Mat()\nex.extract(output_name, mat_out)\ny.append(np.array(mat_out)[None])\nelif self.triton:  # NVIDIA Triton Inference Server\ny = self.model(im)\nelse:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\nim = im.cpu().numpy()\nif self.saved_model:  # SavedModel\ny = self.model(im, training=False) if self.keras else self.model(im)\nif not isinstance(y, list):\ny = [y]\nelif self.pb:  # GraphDef\ny = self.frozen_func(x=self.tf.constant(im))\nif len(y) == 2 and len(self.names) == 999:  # segments and names not defined\nip, ib = (0, 1) if len(y[0].shape) == 4 else (1, 0)  # index of protos, boxes\nnc = y[ib].shape[1] - y[ip].shape[3] - 4  # y = (1, 160, 160, 32), (1, 116, 8400)\nself.names = {i: f'class{i}' for i in range(nc)}\nelse:  # Lite or Edge TPU\ndetails = self.input_details[0]\ninteger = details['dtype'] in (np.int8, np.int16)  # is TFLite quantized int8 or int16 model\nif integer:\nscale, zero_point = details['quantization']\nim = (im / scale + zero_point).astype(details['dtype'])  # de-scale\nself.interpreter.set_tensor(details['index'], im)\nself.interpreter.invoke()\ny = []\nfor output in self.output_details:\nx = self.interpreter.get_tensor(output['index'])\nif integer:\nscale, zero_point = output['quantization']\nx = (x.astype(np.float32) - zero_point) * scale  # re-scale\nif x.ndim &gt; 2:  # if task is not classification\n# Denormalize xywh by image size. See https://github.com/ultralytics/ultralytics/pull/1695\n# xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models\nx[:, [0, 2]] *= w\nx[:, [1, 3]] *= h\ny.append(x)\n# TF segment fixes: export is reversed vs ONNX export and protos are transposed\nif len(y) == 2:  # segment with (det, proto) output order reversed\nif len(y[1].shape) != 4:\ny = list(reversed(y))  # should be y = (1, 116, 8400), (1, 160, 160, 32)\ny[1] = np.transpose(y[1], (0, 3, 1, 2))  # should be y = (1, 116, 8400), (1, 32, 160, 160)\ny = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n# for x in y:\n#     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes\nif isinstance(y, (list, tuple)):\nreturn self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\nelse:\nreturn self.from_numpy(y)\ndef from_numpy(self, x):\n\"\"\"\n         Convert a numpy array to a tensor.\n         Args:\n             x (np.ndarray): The array to be converted.\n         Returns:\n             (torch.Tensor): The converted tensor\n         \"\"\"\nreturn torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x\ndef warmup(self, imgsz=(1, 3, 640, 640)):\n\"\"\"\n        Warm up the model by running one forward pass with a dummy input.\n        Args:\n            imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n        Returns:\n            (None): This method runs the forward pass and don't return any value\n        \"\"\"\nwarmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\nif any(warmup_types) and (self.device.type != 'cpu' or self.triton):\nim = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\nfor _ in range(2 if self.jit else 1):  #\nself.forward(im)  # warmup\n@staticmethod\ndef _apply_default_class_names(data):\n\"\"\"Applies default class names to an input YAML file or returns numerical class names.\"\"\"\nwith contextlib.suppress(Exception):\nreturn yaml_load(check_yaml(data))['names']\nreturn {i: f'class{i}' for i in range(999)}  # return default if above errors\n@staticmethod\ndef _model_type(p='path/to/model.pt'):\n\"\"\"\n        This function takes a path to a model file and returns the model type\n        Args:\n            p: path to the model file. Defaults to path/to/model.pt\n        \"\"\"\n# Return model type from model path, i.e. path='path/to/model.onnx' -&gt; type=onnx\n# types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]\nfrom ultralytics.engine.exporter import export_formats\nsf = list(export_formats().Suffix)  # export suffixes\nif not is_url(p, check=False) and not isinstance(p, str):\ncheck_suffix(p, sf)  # checks\nname = Path(p).name\ntypes = [s in name for s in sf]\ntypes[5] |= name.endswith('.mlmodel')  # retain support for older Apple CoreML *.mlmodel formats\ntypes[8] &amp;= not types[9]  # tflite &amp;= not edgetpu\nif any(types):\ntriton = False\nelse:\nurl = urlparse(p)  # if url may be Triton inference server\ntriton = all([any(s in url.scheme for s in ['http', 'grpc']), url.netloc])\nreturn types + [triton]\n</code></pre>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.AutoBackend.__init__","title":"<code>__init__(weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True, verbose=True)</code>","text":"<p>MultiBackend class for python inference on various platforms using Ultralytics YOLO.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>The path to the weights file. Default: 'yolov8n.pt'</p> <code>'yolov8n.pt'</code> <code>device</code> <code>device</code> <p>The device to run the model on.</p> <code>torch.device('cpu')</code> <code>dnn</code> <code>bool</code> <p>Use OpenCV DNN module for inference if True, defaults to False.</p> <code>False</code> <code>data</code> <code>str | Path | optional</code> <p>Additional data.yaml file for class names.</p> <code>None</code> <code>fp16</code> <code>bool</code> <p>If True, use half precision. Default: False</p> <code>False</code> <code>fuse</code> <code>bool</code> <p>Whether to fuse the model or not. Default: True</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to run in verbose mode or not. Default: True</p> <code>True</code> Supported formats and their naming conventions Format Suffix PyTorch *.pt TorchScript *.torchscript ONNX Runtime *.onnx ONNX OpenCV DNN *.onnx dnn=True OpenVINO *.xml CoreML *.mlpackage TensorRT *.engine TensorFlow SavedModel *_saved_model TensorFlow GraphDef *.pb TensorFlow Lite *.tflite TensorFlow Edge TPU *_edgetpu.tflite PaddlePaddle *_paddle_model ncnn *_ncnn_model Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def __init__(self,\nweights='yolov8n.pt',\ndevice=torch.device('cpu'),\ndnn=False,\ndata=None,\nfp16=False,\nfuse=True,\nverbose=True):\n\"\"\"\n    MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n    Args:\n        weights (str): The path to the weights file. Default: 'yolov8n.pt'\n        device (torch.device): The device to run the model on.\n        dnn (bool): Use OpenCV DNN module for inference if True, defaults to False.\n        data (str | Path | optional): Additional data.yaml file for class names.\n        fp16 (bool): If True, use half precision. Default: False\n        fuse (bool): Whether to fuse the model or not. Default: True\n        verbose (bool): Whether to run in verbose mode or not. Default: True\n    Supported formats and their naming conventions:\n        | Format                | Suffix           |\n        |-----------------------|------------------|\n        | PyTorch               | *.pt             |\n        | TorchScript           | *.torchscript    |\n        | ONNX Runtime          | *.onnx           |\n        | ONNX OpenCV DNN       | *.onnx dnn=True  |\n        | OpenVINO              | *.xml            |\n        | CoreML                | *.mlpackage      |\n        | TensorRT              | *.engine         |\n        | TensorFlow SavedModel | *_saved_model    |\n        | TensorFlow GraphDef   | *.pb             |\n        | TensorFlow Lite       | *.tflite         |\n        | TensorFlow Edge TPU   | *_edgetpu.tflite |\n        | PaddlePaddle          | *_paddle_model   |\n        | ncnn                  | *_ncnn_model     |\n    \"\"\"\nsuper().__init__()\nw = str(weights[0] if isinstance(weights, list) else weights)\nnn_module = isinstance(weights, torch.nn.Module)\npt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, ncnn, triton = \\\n        self._model_type(w)\nfp16 &amp;= pt or jit or onnx or xml or engine or nn_module or triton  # FP16\nnhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\nstride = 32  # default stride\nmodel, metadata = None, None\n# Set device\ncuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\nif cuda and not any([nn_module, pt, jit, engine]):  # GPU dataloader formats\ndevice = torch.device('cpu')\ncuda = False\n# Download if not local\nif not (pt or triton or nn_module):\nw = attempt_download_asset(w)\n# Load model\nif nn_module:  # in-memory PyTorch model\nmodel = weights.to(device)\nmodel = model.fuse(verbose=verbose) if fuse else model\nif hasattr(model, 'kpt_shape'):\nkpt_shape = model.kpt_shape  # pose-only\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\npt = True\nelif pt:  # PyTorch\nfrom ultralytics.nn.tasks import attempt_load_weights\nmodel = attempt_load_weights(weights if isinstance(weights, list) else w,\ndevice=device,\ninplace=True,\nfuse=fuse)\nif hasattr(model, 'kpt_shape'):\nkpt_shape = model.kpt_shape  # pose-only\nstride = max(int(model.stride.max()), 32)  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names\nmodel.half() if fp16 else model.float()\nself.model = model  # explicitly assign for to(), cpu(), cuda(), half()\nelif jit:  # TorchScript\nLOGGER.info(f'Loading {w} for TorchScript inference...')\nextra_files = {'config.txt': ''}  # model metadata\nmodel = torch.jit.load(w, _extra_files=extra_files, map_location=device)\nmodel.half() if fp16 else model.float()\nif extra_files['config.txt']:  # load metadata dict\nmetadata = json.loads(extra_files['config.txt'], object_hook=lambda x: dict(x.items()))\nelif dnn:  # ONNX OpenCV DNN\nLOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\ncheck_requirements('opencv-python&gt;=4.5.4')\nnet = cv2.dnn.readNetFromONNX(w)\nelif onnx:  # ONNX Runtime\nLOGGER.info(f'Loading {w} for ONNX Runtime inference...')\ncheck_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\nimport onnxruntime\nproviders = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\nsession = onnxruntime.InferenceSession(w, providers=providers)\noutput_names = [x.name for x in session.get_outputs()]\nmetadata = session.get_modelmeta().custom_metadata_map  # metadata\nelif xml:  # OpenVINO\nLOGGER.info(f'Loading {w} for OpenVINO inference...')\ncheck_requirements('openvino&gt;=2023.0')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\nfrom openvino.runtime import Core, Layout, get_batch  # noqa\ncore = Core()\nw = Path(w)\nif not w.is_file():  # if not *.xml\nw = next(w.glob('*.xml'))  # get *.xml file from *_openvino_model dir\nov_model = core.read_model(model=str(w), weights=w.with_suffix('.bin'))\nif ov_model.get_parameters()[0].get_layout().empty:\nov_model.get_parameters()[0].set_layout(Layout('NCHW'))\nbatch_dim = get_batch(ov_model)\nif batch_dim.is_static:\nbatch_size = batch_dim.get_length()\nov_compiled_model = core.compile_model(ov_model, device_name='AUTO')  # AUTO selects best available device\nmetadata = w.parent / 'metadata.yaml'\nelif engine:  # TensorRT\nLOGGER.info(f'Loading {w} for TensorRT inference...')\ntry:\nimport tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download\nexcept ImportError:\nif LINUX:\ncheck_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\nimport tensorrt as trt  # noqa\ncheck_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\nif device.type == 'cpu':\ndevice = torch.device('cuda:0')\nBinding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\nlogger = trt.Logger(trt.Logger.INFO)\n# Read file\nwith open(w, 'rb') as f, trt.Runtime(logger) as runtime:\nmeta_len = int.from_bytes(f.read(4), byteorder='little')  # read metadata length\nmetadata = json.loads(f.read(meta_len).decode('utf-8'))  # read metadata\nmodel = runtime.deserialize_cuda_engine(f.read())  # read engine\ncontext = model.create_execution_context()\nbindings = OrderedDict()\noutput_names = []\nfp16 = False  # default updated below\ndynamic = False\nfor i in range(model.num_bindings):\nname = model.get_binding_name(i)\ndtype = trt.nptype(model.get_binding_dtype(i))\nif model.binding_is_input(i):\nif -1 in tuple(model.get_binding_shape(i)):  # dynamic\ndynamic = True\ncontext.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\nif dtype == np.float16:\nfp16 = True\nelse:  # output\noutput_names.append(name)\nshape = tuple(context.get_binding_shape(i))\nim = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\nbindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\nbinding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\nbatch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\nelif coreml:  # CoreML\nLOGGER.info(f'Loading {w} for CoreML inference...')\nimport coremltools as ct\nmodel = ct.models.MLModel(w)\nmetadata = dict(model.user_defined_metadata)\nelif saved_model:  # TF SavedModel\nLOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\nimport tensorflow as tf\nkeras = False  # assume TF1 saved_model\nmodel = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\nmetadata = Path(w) / 'metadata.yaml'\nelif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\nLOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\nimport tensorflow as tf\nfrom ultralytics.engine.exporter import gd_outputs\ndef wrap_frozen_graph(gd, inputs, outputs):\n\"\"\"Wrap frozen graphs for deployment.\"\"\"\nx = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=''), [])  # wrapped\nge = x.graph.as_graph_element\nreturn x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\ngd = tf.Graph().as_graph_def()  # TF GraphDef\nwith open(w, 'rb') as f:\ngd.ParseFromString(f.read())\nfrozen_func = wrap_frozen_graph(gd, inputs='x:0', outputs=gd_outputs(gd))\nelif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\ntry:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\nimport tensorflow as tf\nInterpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate\nif edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\nLOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\ndelegate = {\n'Linux': 'libedgetpu.so.1',\n'Darwin': 'libedgetpu.1.dylib',\n'Windows': 'edgetpu.dll'}[platform.system()]\ninterpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\nelse:  # TFLite\nLOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\ninterpreter = Interpreter(model_path=w)  # load TFLite model\ninterpreter.allocate_tensors()  # allocate\ninput_details = interpreter.get_input_details()  # inputs\noutput_details = interpreter.get_output_details()  # outputs\n# Load metadata\nwith contextlib.suppress(zipfile.BadZipFile):\nwith zipfile.ZipFile(w, 'r') as model:\nmeta_file = model.namelist()[0]\nmetadata = ast.literal_eval(model.read(meta_file).decode('utf-8'))\nelif tfjs:  # TF.js\nraise NotImplementedError('YOLOv8 TF.js inference is not currently supported.')\nelif paddle:  # PaddlePaddle\nLOGGER.info(f'Loading {w} for PaddlePaddle inference...')\ncheck_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\nimport paddle.inference as pdi  # noqa\nw = Path(w)\nif not w.is_file():  # if not *.pdmodel\nw = next(w.rglob('*.pdmodel'))  # get *.pdmodel file from *_paddle_model dir\nconfig = pdi.Config(str(w), str(w.with_suffix('.pdiparams')))\nif cuda:\nconfig.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\npredictor = pdi.create_predictor(config)\ninput_handle = predictor.get_input_handle(predictor.get_input_names()[0])\noutput_names = predictor.get_output_names()\nmetadata = w.parents[1] / 'metadata.yaml'\nelif ncnn:  # ncnn\nLOGGER.info(f'Loading {w} for ncnn inference...')\ncheck_requirements('git+https://github.com/Tencent/ncnn.git' if ARM64 else 'ncnn')  # requires ncnn\nimport ncnn as pyncnn\nnet = pyncnn.Net()\nnet.opt.use_vulkan_compute = cuda\nw = Path(w)\nif not w.is_file():  # if not *.param\nw = next(w.glob('*.param'))  # get *.param file from *_ncnn_model dir\nnet.load_param(str(w))\nnet.load_model(str(w.with_suffix('.bin')))\nmetadata = w.parent / 'metadata.yaml'\nelif triton:  # NVIDIA Triton Inference Server\n\"\"\"TODO\n        check_requirements('tritonclient[all]')\n        from utils.triton import TritonRemoteModel\n        model = TritonRemoteModel(url=w)\n        nhwc = model.runtime.startswith(\"tensorflow\")\n        \"\"\"\nraise NotImplementedError('Triton Inference Server is not currently supported.')\nelse:\nfrom ultralytics.engine.exporter import export_formats\nraise TypeError(f\"model='{w}' is not a supported model format. \"\n'See https://docs.ultralytics.com/modes/predict for help.'\nf'\\n\\n{export_formats()}')\n# Load external metadata YAML\nif isinstance(metadata, (str, Path)) and Path(metadata).exists():\nmetadata = yaml_load(metadata)\nif metadata:\nfor k, v in metadata.items():\nif k in ('stride', 'batch'):\nmetadata[k] = int(v)\nelif k in ('imgsz', 'names', 'kpt_shape') and isinstance(v, str):\nmetadata[k] = eval(v)\nstride = metadata['stride']\ntask = metadata['task']\nbatch = metadata['batch']\nimgsz = metadata['imgsz']\nnames = metadata['names']\nkpt_shape = metadata.get('kpt_shape')\nelif not (pt or triton or nn_module):\nLOGGER.warning(f\"WARNING \u26a0\ufe0f Metadata not found for 'model={weights}'\")\n# Check names\nif 'names' not in locals():  # names missing\nnames = self._apply_default_class_names(data)\nnames = check_class_names(names)\nself.__dict__.update(locals())  # assign all variables to self\n</code></pre>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.AutoBackend.forward","title":"<code>forward(im, augment=False, visualize=False)</code>","text":"<p>Runs inference on the YOLOv8 MultiBackend model.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Tensor</code> <p>The image tensor to perform inference on.</p> required <code>augment</code> <code>bool</code> <p>whether to perform data augmentation during inference, defaults to False</p> <code>False</code> <code>visualize</code> <code>bool</code> <p>whether to visualize the output predictions, defaults to False</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def forward(self, im, augment=False, visualize=False):\n\"\"\"\n    Runs inference on the YOLOv8 MultiBackend model.\n    Args:\n        im (torch.Tensor): The image tensor to perform inference on.\n        augment (bool): whether to perform data augmentation during inference, defaults to False\n        visualize (bool): whether to visualize the output predictions, defaults to False\n    Returns:\n        (tuple): Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)\n    \"\"\"\nb, ch, h, w = im.shape  # batch, channel, height, width\nif self.fp16 and im.dtype != torch.float16:\nim = im.half()  # to FP16\nif self.nhwc:\nim = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\nif self.pt or self.nn_module:  # PyTorch\ny = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\nelif self.jit:  # TorchScript\ny = self.model(im)\nelif self.dnn:  # ONNX OpenCV DNN\nim = im.cpu().numpy()  # torch to numpy\nself.net.setInput(im)\ny = self.net.forward()\nelif self.onnx:  # ONNX Runtime\nim = im.cpu().numpy()  # torch to numpy\ny = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\nelif self.xml:  # OpenVINO\nim = im.cpu().numpy()  # FP32\ny = list(self.ov_compiled_model(im).values())\nelif self.engine:  # TensorRT\nif self.dynamic and im.shape != self.bindings['images'].shape:\ni = self.model.get_binding_index('images')\nself.context.set_binding_shape(i, im.shape)  # reshape if dynamic\nself.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\nfor name in self.output_names:\ni = self.model.get_binding_index(name)\nself.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\ns = self.bindings['images'].shape\nassert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\nself.binding_addrs['images'] = int(im.data_ptr())\nself.context.execute_v2(list(self.binding_addrs.values()))\ny = [self.bindings[x].data for x in sorted(self.output_names)]\nelif self.coreml:  # CoreML\nim = im[0].cpu().numpy()\nim_pil = Image.fromarray((im * 255).astype('uint8'))\n# im = im.resize((192, 320), Image.BILINEAR)\ny = self.model.predict({'image': im_pil})  # coordinates are xywh normalized\nif 'confidence' in y:\nbox = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\nconf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\ny = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\nelif len(y) == 1:  # classification model\ny = list(y.values())\nelif len(y) == 2:  # segmentation model\ny = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\nelif self.paddle:  # PaddlePaddle\nim = im.cpu().numpy().astype(np.float32)\nself.input_handle.copy_from_cpu(im)\nself.predictor.run()\ny = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\nelif self.ncnn:  # ncnn\nmat_in = self.pyncnn.Mat(im[0].cpu().numpy())\nex = self.net.create_extractor()\ninput_names, output_names = self.net.input_names(), self.net.output_names()\nex.input(input_names[0], mat_in)\ny = []\nfor output_name in output_names:\nmat_out = self.pyncnn.Mat()\nex.extract(output_name, mat_out)\ny.append(np.array(mat_out)[None])\nelif self.triton:  # NVIDIA Triton Inference Server\ny = self.model(im)\nelse:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\nim = im.cpu().numpy()\nif self.saved_model:  # SavedModel\ny = self.model(im, training=False) if self.keras else self.model(im)\nif not isinstance(y, list):\ny = [y]\nelif self.pb:  # GraphDef\ny = self.frozen_func(x=self.tf.constant(im))\nif len(y) == 2 and len(self.names) == 999:  # segments and names not defined\nip, ib = (0, 1) if len(y[0].shape) == 4 else (1, 0)  # index of protos, boxes\nnc = y[ib].shape[1] - y[ip].shape[3] - 4  # y = (1, 160, 160, 32), (1, 116, 8400)\nself.names = {i: f'class{i}' for i in range(nc)}\nelse:  # Lite or Edge TPU\ndetails = self.input_details[0]\ninteger = details['dtype'] in (np.int8, np.int16)  # is TFLite quantized int8 or int16 model\nif integer:\nscale, zero_point = details['quantization']\nim = (im / scale + zero_point).astype(details['dtype'])  # de-scale\nself.interpreter.set_tensor(details['index'], im)\nself.interpreter.invoke()\ny = []\nfor output in self.output_details:\nx = self.interpreter.get_tensor(output['index'])\nif integer:\nscale, zero_point = output['quantization']\nx = (x.astype(np.float32) - zero_point) * scale  # re-scale\nif x.ndim &gt; 2:  # if task is not classification\n# Denormalize xywh by image size. See https://github.com/ultralytics/ultralytics/pull/1695\n# xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models\nx[:, [0, 2]] *= w\nx[:, [1, 3]] *= h\ny.append(x)\n# TF segment fixes: export is reversed vs ONNX export and protos are transposed\nif len(y) == 2:  # segment with (det, proto) output order reversed\nif len(y[1].shape) != 4:\ny = list(reversed(y))  # should be y = (1, 116, 8400), (1, 160, 160, 32)\ny[1] = np.transpose(y[1], (0, 3, 1, 2))  # should be y = (1, 116, 8400), (1, 32, 160, 160)\ny = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n# for x in y:\n#     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes\nif isinstance(y, (list, tuple)):\nreturn self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\nelse:\nreturn self.from_numpy(y)\n</code></pre>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.AutoBackend.from_numpy","title":"<code>from_numpy(x)</code>","text":"<p>Convert a numpy array to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The array to be converted.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The converted tensor</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def from_numpy(self, x):\n\"\"\"\n     Convert a numpy array to a tensor.\n     Args:\n         x (np.ndarray): The array to be converted.\n     Returns:\n         (torch.Tensor): The converted tensor\n     \"\"\"\nreturn torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x\n</code></pre>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.AutoBackend.warmup","title":"<code>warmup(imgsz=(1, 3, 640, 640))</code>","text":"<p>Warm up the model by running one forward pass with a dummy input.</p> <p>Parameters:</p> Name Type Description Default <code>imgsz</code> <code>tuple</code> <p>The shape of the dummy input tensor in the format (batch_size, channels, height, width)</p> <code>(1, 3, 640, 640)</code> <p>Returns:</p> Type Description <code>None</code> <p>This method runs the forward pass and don't return any value</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def warmup(self, imgsz=(1, 3, 640, 640)):\n\"\"\"\n    Warm up the model by running one forward pass with a dummy input.\n    Args:\n        imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n    Returns:\n        (None): This method runs the forward pass and don't return any value\n    \"\"\"\nwarmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\nif any(warmup_types) and (self.device.type != 'cpu' or self.triton):\nim = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\nfor _ in range(2 if self.jit else 1):  #\nself.forward(im)  # warmup\n</code></pre>"},{"location":"reference/nn/autobackend/#ultralytics.nn.autobackend.check_class_names","title":"<code>ultralytics.nn.autobackend.check_class_names(names)</code>","text":"<p>Check class names. Map imagenet class codes to human-readable names if required. Convert lists to dicts.</p> Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def check_class_names(names):\n\"\"\"Check class names. Map imagenet class codes to human-readable names if required. Convert lists to dicts.\"\"\"\nif isinstance(names, list):  # names is a list\nnames = dict(enumerate(names))  # convert to dict\nif isinstance(names, dict):\n# Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'\nnames = {int(k): str(v) for k, v in names.items()}\nn = len(names)\nif max(names.keys()) &gt;= n:\nraise KeyError(f'{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices '\nf'{min(names.keys())}-{max(names.keys())} defined in your dataset YAML.')\nif isinstance(names[0], str) and names[0].startswith('n0'):  # imagenet class codes, i.e. 'n01440764'\nmap = yaml_load(ROOT / 'cfg/datasets/ImageNet.yaml')['map']  # human-readable names\nnames = {k: map[v] for k, v in names.items()}\nreturn names\n</code></pre>"},{"location":"reference/nn/tasks/","title":"Reference for <code>ultralytics/nn/tasks.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/tasks.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel","title":"<code>ultralytics.nn.tasks.BaseModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class BaseModel(nn.Module):\n\"\"\"\n    The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.\n    \"\"\"\ndef forward(self, x, *args, **kwargs):\n\"\"\"\n        Forward pass of the model on a single scale.\n        Wrapper for `_forward_once` method.\n        Args:\n            x (torch.Tensor | dict): The input image tensor or a dict including image tensor and gt labels.\n        Returns:\n            (torch.Tensor): The output of the network.\n        \"\"\"\nif isinstance(x, dict):  # for cases of training and validating while training.\nreturn self.loss(x, *args, **kwargs)\nreturn self.predict(x, *args, **kwargs)\ndef predict(self, x, profile=False, visualize=False, augment=False):\n\"\"\"\n        Perform a forward pass through the network.\n        Args:\n            x (torch.Tensor): The input tensor to the model.\n            profile (bool):  Print the computation time of each layer if True, defaults to False.\n            visualize (bool): Save the feature maps of the model if True, defaults to False.\n            augment (bool): Augment image during prediction, defaults to False.\n        Returns:\n            (torch.Tensor): The last output of the model.\n        \"\"\"\nif augment:\nreturn self._predict_augment(x)\nreturn self._predict_once(x, profile, visualize)\ndef _predict_once(self, x, profile=False, visualize=False):\n\"\"\"\n        Perform a forward pass through the network.\n        Args:\n            x (torch.Tensor): The input tensor to the model.\n            profile (bool):  Print the computation time of each layer if True, defaults to False.\n            visualize (bool): Save the feature maps of the model if True, defaults to False.\n        Returns:\n            (torch.Tensor): The last output of the model.\n        \"\"\"\ny, dt = [], []  # outputs\nfor m in self.model:\nif m.f != -1:  # if not from previous layer\nx = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\nif profile:\nself._profile_one_layer(m, x, dt)\nx = m(x)  # run\ny.append(x if m.i in self.save else None)  # save output\nif visualize:\nfeature_visualization(x, m.type, m.i, save_dir=visualize)\nreturn x\ndef _predict_augment(self, x):\n\"\"\"Perform augmentations on input image x and return augmented inference.\"\"\"\nLOGGER.warning(f'WARNING \u26a0\ufe0f {self.__class__.__name__} does not support augmented inference yet. '\nf'Reverting to single-scale inference instead.')\nreturn self._predict_once(x)\ndef _profile_one_layer(self, m, x, dt):\n\"\"\"\n        Profile the computation time and FLOPs of a single layer of the model on a given input.\n        Appends the results to the provided list.\n        Args:\n            m (nn.Module): The layer to be profiled.\n            x (torch.Tensor): The input data to the layer.\n            dt (list): A list to store the computation time of the layer.\n        Returns:\n            None\n        \"\"\"\nc = m == self.model[-1] and isinstance(x, list)  # is final layer list, copy input as inplace fix\nflops = thop.profile(m, inputs=[x.copy() if c else x], verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs\nt = time_sync()\nfor _ in range(10):\nm(x.copy() if c else x)\ndt.append((time_sync() - t) * 100)\nif m == self.model[0]:\nLOGGER.info(f\"{'time (ms)':&gt;10s} {'GFLOPs':&gt;10s} {'params':&gt;10s}  module\")\nLOGGER.info(f'{dt[-1]:10.2f} {flops:10.2f} {m.np:10.0f}  {m.type}')\nif c:\nLOGGER.info(f\"{sum(dt):10.2f} {'-':&gt;10s} {'-':&gt;10s}  Total\")\ndef fuse(self, verbose=True):\n\"\"\"\n        Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the\n        computation efficiency.\n        Returns:\n            (nn.Module): The fused model is returned.\n        \"\"\"\nif not self.is_fused():\nfor m in self.model.modules():\nif isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, 'bn'):\nif isinstance(m, Conv2):\nm.fuse_convs()\nm.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, ConvTranspose) and hasattr(m, 'bn'):\nm.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, RepConv):\nm.fuse_convs()\nm.forward = m.forward_fuse  # update forward\nself.info(verbose=verbose)\nreturn self\ndef is_fused(self, thresh=10):\n\"\"\"\n        Check if the model has less than a certain threshold of BatchNorm layers.\n        Args:\n            thresh (int, optional): The threshold number of BatchNorm layers. Default is 10.\n        Returns:\n            (bool): True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.\n        \"\"\"\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nreturn sum(isinstance(v, bn) for v in self.modules()) &lt; thresh  # True if &lt; 'thresh' BatchNorm layers in model\ndef info(self, detailed=False, verbose=True, imgsz=640):\n\"\"\"\n        Prints model information\n        Args:\n            detailed (bool): if True, prints out detailed information about the model. Defaults to False\n            verbose (bool): if True, prints out the model information. Defaults to False\n            imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n        \"\"\"\nreturn model_info(self, detailed=detailed, verbose=verbose, imgsz=imgsz)\ndef _apply(self, fn):\n\"\"\"\n        Applies a function to all the tensors in the model that are not parameters or registered buffers.\n        Args:\n            fn (function): the function to apply to the model\n        Returns:\n            A model that is a Detect() object.\n        \"\"\"\nself = super()._apply(fn)\nm = self.model[-1]  # Detect()\nif isinstance(m, (Detect, Segment)):\nm.stride = fn(m.stride)\nm.anchors = fn(m.anchors)\nm.strides = fn(m.strides)\nreturn self\ndef load(self, weights, verbose=True):\n\"\"\"\n        Load the weights into the model.\n        Args:\n            weights (dict | torch.nn.Module): The pre-trained weights to be loaded.\n            verbose (bool, optional): Whether to log the transfer progress. Defaults to True.\n        \"\"\"\nmodel = weights['model'] if isinstance(weights, dict) else weights  # torchvision models are not dicts\ncsd = model.float().state_dict()  # checkpoint state_dict as FP32\ncsd = intersect_dicts(csd, self.state_dict())  # intersect\nself.load_state_dict(csd, strict=False)  # load\nif verbose:\nLOGGER.info(f'Transferred {len(csd)}/{len(self.model.state_dict())} items from pretrained weights')\ndef loss(self, batch, preds=None):\n\"\"\"\n        Compute loss\n        Args:\n            batch (dict): Batch to compute loss on\n            preds (torch.Tensor | List[torch.Tensor]): Predictions.\n        \"\"\"\nif not hasattr(self, 'criterion'):\nself.criterion = self.init_criterion()\npreds = self.forward(batch['img']) if preds is None else preds\nreturn self.criterion(preds, batch)\ndef init_criterion(self):\nraise NotImplementedError('compute_loss() needs to be implemented by task heads')\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.forward","title":"<code>forward(x, *args, **kwargs)</code>","text":"<p>Forward pass of the model on a single scale. Wrapper for <code>_forward_once</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | dict</code> <p>The input image tensor or a dict including image tensor and gt labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the network.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def forward(self, x, *args, **kwargs):\n\"\"\"\n    Forward pass of the model on a single scale.\n    Wrapper for `_forward_once` method.\n    Args:\n        x (torch.Tensor | dict): The input image tensor or a dict including image tensor and gt labels.\n    Returns:\n        (torch.Tensor): The output of the network.\n    \"\"\"\nif isinstance(x, dict):  # for cases of training and validating while training.\nreturn self.loss(x, *args, **kwargs)\nreturn self.predict(x, *args, **kwargs)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.fuse","title":"<code>fuse(verbose=True)</code>","text":"<p>Fuse the <code>Conv2d()</code> and <code>BatchNorm2d()</code> layers of the model into a single layer, in order to improve the computation efficiency.</p> <p>Returns:</p> Type Description <code>Module</code> <p>The fused model is returned.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def fuse(self, verbose=True):\n\"\"\"\n    Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the\n    computation efficiency.\n    Returns:\n        (nn.Module): The fused model is returned.\n    \"\"\"\nif not self.is_fused():\nfor m in self.model.modules():\nif isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, 'bn'):\nif isinstance(m, Conv2):\nm.fuse_convs()\nm.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, ConvTranspose) and hasattr(m, 'bn'):\nm.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)\ndelattr(m, 'bn')  # remove batchnorm\nm.forward = m.forward_fuse  # update forward\nif isinstance(m, RepConv):\nm.fuse_convs()\nm.forward = m.forward_fuse  # update forward\nself.info(verbose=verbose)\nreturn self\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.info","title":"<code>info(detailed=False, verbose=True, imgsz=640)</code>","text":"<p>Prints model information</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>if True, prints out detailed information about the model. Defaults to False</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>if True, prints out the model information. Defaults to False</p> <code>True</code> <code>imgsz</code> <code>int</code> <p>the size of the image that the model will be trained on. Defaults to 640</p> <code>640</code> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def info(self, detailed=False, verbose=True, imgsz=640):\n\"\"\"\n    Prints model information\n    Args:\n        detailed (bool): if True, prints out detailed information about the model. Defaults to False\n        verbose (bool): if True, prints out the model information. Defaults to False\n        imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n    \"\"\"\nreturn model_info(self, detailed=detailed, verbose=verbose, imgsz=imgsz)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.is_fused","title":"<code>is_fused(thresh=10)</code>","text":"<p>Check if the model has less than a certain threshold of BatchNorm layers.</p> <p>Parameters:</p> Name Type Description Default <code>thresh</code> <code>int</code> <p>The threshold number of BatchNorm layers. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def is_fused(self, thresh=10):\n\"\"\"\n    Check if the model has less than a certain threshold of BatchNorm layers.\n    Args:\n        thresh (int, optional): The threshold number of BatchNorm layers. Default is 10.\n    Returns:\n        (bool): True if the number of BatchNorm layers in the model is less than the threshold, False otherwise.\n    \"\"\"\nbn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\nreturn sum(isinstance(v, bn) for v in self.modules()) &lt; thresh  # True if &lt; 'thresh' BatchNorm layers in model\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.load","title":"<code>load(weights, verbose=True)</code>","text":"<p>Load the weights into the model.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>dict | Module</code> <p>The pre-trained weights to be loaded.</p> required <code>verbose</code> <code>bool</code> <p>Whether to log the transfer progress. Defaults to True.</p> <code>True</code> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def load(self, weights, verbose=True):\n\"\"\"\n    Load the weights into the model.\n    Args:\n        weights (dict | torch.nn.Module): The pre-trained weights to be loaded.\n        verbose (bool, optional): Whether to log the transfer progress. Defaults to True.\n    \"\"\"\nmodel = weights['model'] if isinstance(weights, dict) else weights  # torchvision models are not dicts\ncsd = model.float().state_dict()  # checkpoint state_dict as FP32\ncsd = intersect_dicts(csd, self.state_dict())  # intersect\nself.load_state_dict(csd, strict=False)  # load\nif verbose:\nLOGGER.info(f'Transferred {len(csd)}/{len(self.model.state_dict())} items from pretrained weights')\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.loss","title":"<code>loss(batch, preds=None)</code>","text":"<p>Compute loss</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>Batch to compute loss on</p> required <code>preds</code> <code>Tensor | List[torch.Tensor]</code> <p>Predictions.</p> <code>None</code> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def loss(self, batch, preds=None):\n\"\"\"\n    Compute loss\n    Args:\n        batch (dict): Batch to compute loss on\n        preds (torch.Tensor | List[torch.Tensor]): Predictions.\n    \"\"\"\nif not hasattr(self, 'criterion'):\nself.criterion = self.init_criterion()\npreds = self.forward(batch['img']) if preds is None else preds\nreturn self.criterion(preds, batch)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.BaseModel.predict","title":"<code>predict(x, profile=False, visualize=False, augment=False)</code>","text":"<p>Perform a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the model.</p> required <code>profile</code> <code>bool</code> <p>Print the computation time of each layer if True, defaults to False.</p> <code>False</code> <code>visualize</code> <code>bool</code> <p>Save the feature maps of the model if True, defaults to False.</p> <code>False</code> <code>augment</code> <code>bool</code> <p>Augment image during prediction, defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The last output of the model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def predict(self, x, profile=False, visualize=False, augment=False):\n\"\"\"\n    Perform a forward pass through the network.\n    Args:\n        x (torch.Tensor): The input tensor to the model.\n        profile (bool):  Print the computation time of each layer if True, defaults to False.\n        visualize (bool): Save the feature maps of the model if True, defaults to False.\n        augment (bool): Augment image during prediction, defaults to False.\n    Returns:\n        (torch.Tensor): The last output of the model.\n    \"\"\"\nif augment:\nreturn self._predict_augment(x)\nreturn self._predict_once(x, profile, visualize)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.DetectionModel","title":"<code>ultralytics.nn.tasks.DetectionModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>YOLOv8 detection model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class DetectionModel(BaseModel):\n\"\"\"YOLOv8 detection model.\"\"\"\ndef __init__(self, cfg='yolov8n.yaml', ch=3, nc=None, verbose=True):  # model, input channels, number of classes\nsuper().__init__()\nself.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict\n# Define model\nch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\nif nc and nc != self.yaml['nc']:\nLOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\nself.yaml['nc'] = nc  # override YAML value\nself.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\nself.names = {i: f'{i}' for i in range(self.yaml['nc'])}  # default names dict\nself.inplace = self.yaml.get('inplace', True)\n# Build strides\nm = self.model[-1]  # Detect()\nif isinstance(m, (Detect, Segment, Pose)):\ns = 256  # 2x min stride\nm.inplace = self.inplace\nforward = lambda x: self.forward(x)[0] if isinstance(m, (Segment, Pose)) else self.forward(x)\nm.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))])  # forward\nself.stride = m.stride\nm.bias_init()  # only run once\nelse:\nself.stride = torch.Tensor([32])  # default stride for i.e. RTDETR\n# Init weights, biases\ninitialize_weights(self)\nif verbose:\nself.info()\nLOGGER.info('')\ndef _predict_augment(self, x):\n\"\"\"Perform augmentations on input image x and return augmented inference and train outputs.\"\"\"\nimg_size = x.shape[-2:]  # height, width\ns = [1, 0.83, 0.67]  # scales\nf = [None, 3, None]  # flips (2-ud, 3-lr)\ny = []  # outputs\nfor si, fi in zip(s, f):\nxi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\nyi = super().predict(xi)[0]  # forward\n# cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\nyi = self._descale_pred(yi, fi, si, img_size)\ny.append(yi)\ny = self._clip_augmented(y)  # clip augmented tails\nreturn torch.cat(y, -1), None  # augmented inference, train\n@staticmethod\ndef _descale_pred(p, flips, scale, img_size, dim=1):\n\"\"\"De-scale predictions following augmented inference (inverse operation).\"\"\"\np[:, :4] /= scale  # de-scale\nx, y, wh, cls = p.split((1, 1, 2, p.shape[dim] - 4), dim)\nif flips == 2:\ny = img_size[0] - y  # de-flip ud\nelif flips == 3:\nx = img_size[1] - x  # de-flip lr\nreturn torch.cat((x, y, wh, cls), dim)\ndef _clip_augmented(self, y):\n\"\"\"Clip YOLOv5 augmented inference tails.\"\"\"\nnl = self.model[-1].nl  # number of detection layers (P3-P5)\ng = sum(4 ** x for x in range(nl))  # grid points\ne = 1  # exclude layer count\ni = (y[0].shape[-1] // g) * sum(4 ** x for x in range(e))  # indices\ny[0] = y[0][..., :-i]  # large\ni = (y[-1].shape[-1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indices\ny[-1] = y[-1][..., i:]  # small\nreturn y\ndef init_criterion(self):\nreturn v8DetectionLoss(self)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.SegmentationModel","title":"<code>ultralytics.nn.tasks.SegmentationModel</code>","text":"<p>             Bases: <code>DetectionModel</code></p> <p>YOLOv8 segmentation model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class SegmentationModel(DetectionModel):\n\"\"\"YOLOv8 segmentation model.\"\"\"\ndef __init__(self, cfg='yolov8n-seg.yaml', ch=3, nc=None, verbose=True):\n\"\"\"Initialize YOLOv8 segmentation model with given config and parameters.\"\"\"\nsuper().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\ndef init_criterion(self):\nreturn v8SegmentationLoss(self)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.SegmentationModel.__init__","title":"<code>__init__(cfg='yolov8n-seg.yaml', ch=3, nc=None, verbose=True)</code>","text":"<p>Initialize YOLOv8 segmentation model with given config and parameters.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def __init__(self, cfg='yolov8n-seg.yaml', ch=3, nc=None, verbose=True):\n\"\"\"Initialize YOLOv8 segmentation model with given config and parameters.\"\"\"\nsuper().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.PoseModel","title":"<code>ultralytics.nn.tasks.PoseModel</code>","text":"<p>             Bases: <code>DetectionModel</code></p> <p>YOLOv8 pose model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class PoseModel(DetectionModel):\n\"\"\"YOLOv8 pose model.\"\"\"\ndef __init__(self, cfg='yolov8n-pose.yaml', ch=3, nc=None, data_kpt_shape=(None, None), verbose=True):\n\"\"\"Initialize YOLOv8 Pose model.\"\"\"\nif not isinstance(cfg, dict):\ncfg = yaml_model_load(cfg)  # load model YAML\nif any(data_kpt_shape) and list(data_kpt_shape) != list(cfg['kpt_shape']):\nLOGGER.info(f\"Overriding model.yaml kpt_shape={cfg['kpt_shape']} with kpt_shape={data_kpt_shape}\")\ncfg['kpt_shape'] = data_kpt_shape\nsuper().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\ndef init_criterion(self):\nreturn v8PoseLoss(self)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.PoseModel.__init__","title":"<code>__init__(cfg='yolov8n-pose.yaml', ch=3, nc=None, data_kpt_shape=(None, None), verbose=True)</code>","text":"<p>Initialize YOLOv8 Pose model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def __init__(self, cfg='yolov8n-pose.yaml', ch=3, nc=None, data_kpt_shape=(None, None), verbose=True):\n\"\"\"Initialize YOLOv8 Pose model.\"\"\"\nif not isinstance(cfg, dict):\ncfg = yaml_model_load(cfg)  # load model YAML\nif any(data_kpt_shape) and list(data_kpt_shape) != list(cfg['kpt_shape']):\nLOGGER.info(f\"Overriding model.yaml kpt_shape={cfg['kpt_shape']} with kpt_shape={data_kpt_shape}\")\ncfg['kpt_shape'] = data_kpt_shape\nsuper().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.ClassificationModel","title":"<code>ultralytics.nn.tasks.ClassificationModel</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>YOLOv8 classification model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class ClassificationModel(BaseModel):\n\"\"\"YOLOv8 classification model.\"\"\"\ndef __init__(self,\ncfg='yolov8n-cls.yaml',\nmodel=None,\nch=3,\nnc=None,\ncutoff=10,\nverbose=True):  # YAML, model, channels, number of classes, cutoff index, verbose flag\nsuper().__init__()\nself._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg, ch, nc, verbose)\ndef _from_detection_model(self, model, nc=1000, cutoff=10):\n\"\"\"Create a YOLOv5 classification model from a YOLOv5 detection model.\"\"\"\nfrom ultralytics.nn.autobackend import AutoBackend\nif isinstance(model, AutoBackend):\nmodel = model.model  # unwrap DetectMultiBackend\nmodel.model = model.model[:cutoff]  # backbone\nm = model.model[-1]  # last layer\nch = m.conv.in_channels if hasattr(m, 'conv') else m.cv1.conv.in_channels  # ch into module\nc = Classify(ch, nc)  # Classify()\nc.i, c.f, c.type = m.i, m.f, 'models.common.Classify'  # index, from, type\nmodel.model[-1] = c  # replace\nself.model = model.model\nself.stride = model.stride\nself.save = []\nself.nc = nc\ndef _from_yaml(self, cfg, ch, nc, verbose):\n\"\"\"Set YOLOv8 model configurations and define the model architecture.\"\"\"\nself.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict\n# Define model\nch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\nif nc and nc != self.yaml['nc']:\nLOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\nself.yaml['nc'] = nc  # override YAML value\nelif not nc and not self.yaml.get('nc', None):\nraise ValueError('nc not specified. Must specify nc in model.yaml or function arguments.')\nself.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\nself.stride = torch.Tensor([1])  # no stride constraints\nself.names = {i: f'{i}' for i in range(self.yaml['nc'])}  # default names dict\nself.info()\n@staticmethod\ndef reshape_outputs(model, nc):\n\"\"\"Update a TorchVision classification model to class count 'n' if required.\"\"\"\nname, m = list((model.model if hasattr(model, 'model') else model).named_children())[-1]  # last module\nif isinstance(m, Classify):  # YOLO Classify() head\nif m.linear.out_features != nc:\nm.linear = nn.Linear(m.linear.in_features, nc)\nelif isinstance(m, nn.Linear):  # ResNet, EfficientNet\nif m.out_features != nc:\nsetattr(model, name, nn.Linear(m.in_features, nc))\nelif isinstance(m, nn.Sequential):\ntypes = [type(x) for x in m]\nif nn.Linear in types:\ni = types.index(nn.Linear)  # nn.Linear index\nif m[i].out_features != nc:\nm[i] = nn.Linear(m[i].in_features, nc)\nelif nn.Conv2d in types:\ni = types.index(nn.Conv2d)  # nn.Conv2d index\nif m[i].out_channels != nc:\nm[i] = nn.Conv2d(m[i].in_channels, nc, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)\ndef init_criterion(self):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nreturn v8ClassificationLoss()\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.ClassificationModel.init_criterion","title":"<code>init_criterion()</code>","text":"<p>Compute the classification loss between predictions and true labels.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def init_criterion(self):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nreturn v8ClassificationLoss()\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.ClassificationModel.reshape_outputs","title":"<code>reshape_outputs(model, nc)</code>  <code>staticmethod</code>","text":"<p>Update a TorchVision classification model to class count 'n' if required.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>@staticmethod\ndef reshape_outputs(model, nc):\n\"\"\"Update a TorchVision classification model to class count 'n' if required.\"\"\"\nname, m = list((model.model if hasattr(model, 'model') else model).named_children())[-1]  # last module\nif isinstance(m, Classify):  # YOLO Classify() head\nif m.linear.out_features != nc:\nm.linear = nn.Linear(m.linear.in_features, nc)\nelif isinstance(m, nn.Linear):  # ResNet, EfficientNet\nif m.out_features != nc:\nsetattr(model, name, nn.Linear(m.in_features, nc))\nelif isinstance(m, nn.Sequential):\ntypes = [type(x) for x in m]\nif nn.Linear in types:\ni = types.index(nn.Linear)  # nn.Linear index\nif m[i].out_features != nc:\nm[i] = nn.Linear(m[i].in_features, nc)\nelif nn.Conv2d in types:\ni = types.index(nn.Conv2d)  # nn.Conv2d index\nif m[i].out_channels != nc:\nm[i] = nn.Conv2d(m[i].in_channels, nc, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.RTDETRDetectionModel","title":"<code>ultralytics.nn.tasks.RTDETRDetectionModel</code>","text":"<p>             Bases: <code>DetectionModel</code></p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class RTDETRDetectionModel(DetectionModel):\ndef __init__(self, cfg='rtdetr-l.yaml', ch=3, nc=None, verbose=True):\nsuper().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\ndef init_criterion(self):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nfrom ultralytics.models.utils.loss import RTDETRDetectionLoss\nreturn RTDETRDetectionLoss(nc=self.nc, use_vfl=True)\ndef loss(self, batch, preds=None):\nif not hasattr(self, 'criterion'):\nself.criterion = self.init_criterion()\nimg = batch['img']\n# NOTE: preprocess gt_bbox and gt_labels to list.\nbs = len(img)\nbatch_idx = batch['batch_idx']\ngt_groups = [(batch_idx == i).sum().item() for i in range(bs)]\ntargets = {\n'cls': batch['cls'].to(img.device, dtype=torch.long).view(-1),\n'bboxes': batch['bboxes'].to(device=img.device),\n'batch_idx': batch_idx.to(img.device, dtype=torch.long).view(-1),\n'gt_groups': gt_groups}\npreds = self.predict(img, batch=targets) if preds is None else preds\ndec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta = preds if self.training else preds[1]\nif dn_meta is None:\ndn_bboxes, dn_scores = None, None\nelse:\ndn_bboxes, dec_bboxes = torch.split(dec_bboxes, dn_meta['dn_num_split'], dim=2)\ndn_scores, dec_scores = torch.split(dec_scores, dn_meta['dn_num_split'], dim=2)\ndec_bboxes = torch.cat([enc_bboxes.unsqueeze(0), dec_bboxes])  # (7, bs, 300, 4)\ndec_scores = torch.cat([enc_scores.unsqueeze(0), dec_scores])\nloss = self.criterion((dec_bboxes, dec_scores),\ntargets,\ndn_bboxes=dn_bboxes,\ndn_scores=dn_scores,\ndn_meta=dn_meta)\n# NOTE: There are like 12 losses in RTDETR, backward with all losses but only show the main three losses.\nreturn sum(loss.values()), torch.as_tensor([loss[k].detach() for k in ['loss_giou', 'loss_class', 'loss_bbox']],\ndevice=img.device)\ndef predict(self, x, profile=False, visualize=False, batch=None, augment=False):\n\"\"\"\n        Perform a forward pass through the network.\n        Args:\n            x (torch.Tensor): The input tensor to the model\n            profile (bool):  Print the computation time of each layer if True, defaults to False.\n            visualize (bool): Save the feature maps of the model if True, defaults to False\n            batch (dict): A dict including gt boxes and labels from dataloader.\n        Returns:\n            (torch.Tensor): The last output of the model.\n        \"\"\"\ny, dt = [], []  # outputs\nfor m in self.model[:-1]:  # except the head part\nif m.f != -1:  # if not from previous layer\nx = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\nif profile:\nself._profile_one_layer(m, x, dt)\nx = m(x)  # run\ny.append(x if m.i in self.save else None)  # save output\nif visualize:\nfeature_visualization(x, m.type, m.i, save_dir=visualize)\nhead = self.model[-1]\nx = head([y[j] for j in head.f], batch)  # head inference\nreturn x\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.RTDETRDetectionModel.init_criterion","title":"<code>init_criterion()</code>","text":"<p>Compute the classification loss between predictions and true labels.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def init_criterion(self):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nfrom ultralytics.models.utils.loss import RTDETRDetectionLoss\nreturn RTDETRDetectionLoss(nc=self.nc, use_vfl=True)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.RTDETRDetectionModel.predict","title":"<code>predict(x, profile=False, visualize=False, batch=None, augment=False)</code>","text":"<p>Perform a forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the model</p> required <code>profile</code> <code>bool</code> <p>Print the computation time of each layer if True, defaults to False.</p> <code>False</code> <code>visualize</code> <code>bool</code> <p>Save the feature maps of the model if True, defaults to False</p> <code>False</code> <code>batch</code> <code>dict</code> <p>A dict including gt boxes and labels from dataloader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The last output of the model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def predict(self, x, profile=False, visualize=False, batch=None, augment=False):\n\"\"\"\n    Perform a forward pass through the network.\n    Args:\n        x (torch.Tensor): The input tensor to the model\n        profile (bool):  Print the computation time of each layer if True, defaults to False.\n        visualize (bool): Save the feature maps of the model if True, defaults to False\n        batch (dict): A dict including gt boxes and labels from dataloader.\n    Returns:\n        (torch.Tensor): The last output of the model.\n    \"\"\"\ny, dt = [], []  # outputs\nfor m in self.model[:-1]:  # except the head part\nif m.f != -1:  # if not from previous layer\nx = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\nif profile:\nself._profile_one_layer(m, x, dt)\nx = m(x)  # run\ny.append(x if m.i in self.save else None)  # save output\nif visualize:\nfeature_visualization(x, m.type, m.i, save_dir=visualize)\nhead = self.model[-1]\nx = head([y[j] for j in head.f], batch)  # head inference\nreturn x\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.Ensemble","title":"<code>ultralytics.nn.tasks.Ensemble</code>","text":"<p>             Bases: <code>ModuleList</code></p> <p>Ensemble of models.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class Ensemble(nn.ModuleList):\n\"\"\"Ensemble of models.\"\"\"\ndef __init__(self):\n\"\"\"Initialize an ensemble of models.\"\"\"\nsuper().__init__()\ndef forward(self, x, augment=False, profile=False, visualize=False):\n\"\"\"Function generates the YOLOv5 network's final layer.\"\"\"\ny = [module(x, augment, profile, visualize)[0] for module in self]\n# y = torch.stack(y).max(0)[0]  # max ensemble\n# y = torch.stack(y).mean(0)  # mean ensemble\ny = torch.cat(y, 2)  # nms ensemble, y shape(B, HW, C)\nreturn y, None  # inference, train output\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.Ensemble.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an ensemble of models.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def __init__(self):\n\"\"\"Initialize an ensemble of models.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.Ensemble.forward","title":"<code>forward(x, augment=False, profile=False, visualize=False)</code>","text":"<p>Function generates the YOLOv5 network's final layer.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def forward(self, x, augment=False, profile=False, visualize=False):\n\"\"\"Function generates the YOLOv5 network's final layer.\"\"\"\ny = [module(x, augment, profile, visualize)[0] for module in self]\n# y = torch.stack(y).max(0)[0]  # max ensemble\n# y = torch.stack(y).mean(0)  # mean ensemble\ny = torch.cat(y, 2)  # nms ensemble, y shape(B, HW, C)\nreturn y, None  # inference, train output\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.temporary_modules","title":"<code>ultralytics.nn.tasks.temporary_modules(modules=None)</code>","text":"<p>Context manager for temporarily adding or modifying modules in Python's module cache (<code>sys.modules</code>).</p> <p>This function can be used to change the module paths during runtime. It's useful when refactoring code, where you've moved a module from one location to another, but you still want to support the old import paths for backwards compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>dict</code> <p>A dictionary mapping old module paths to new module paths.</p> <code>None</code> Example <pre><code>with temporary_modules({'old.module.path': 'new.module.path'}):\nimport old.module.path  # this will now import new.module.path\n</code></pre> Note <p>The changes are only in effect inside the context manager and are undone once the context manager exits. Be aware that directly manipulating <code>sys.modules</code> can lead to unpredictable results, especially in larger applications or libraries. Use this function with caution.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>@contextlib.contextmanager\ndef temporary_modules(modules=None):\n\"\"\"\n    Context manager for temporarily adding or modifying modules in Python's module cache (`sys.modules`).\n    This function can be used to change the module paths during runtime. It's useful when refactoring code,\n    where you've moved a module from one location to another, but you still want to support the old import\n    paths for backwards compatibility.\n    Args:\n        modules (dict, optional): A dictionary mapping old module paths to new module paths.\n    Example:\n        ```python\n        with temporary_modules({'old.module.path': 'new.module.path'}):\n            import old.module.path  # this will now import new.module.path\n        ```\n    Note:\n        The changes are only in effect inside the context manager and are undone once the context manager exits.\n        Be aware that directly manipulating `sys.modules` can lead to unpredictable results, especially in larger\n        applications or libraries. Use this function with caution.\n    \"\"\"\nif not modules:\nmodules = {}\nimport importlib\nimport sys\ntry:\n# Set modules in sys.modules under their old name\nfor old, new in modules.items():\nsys.modules[old] = importlib.import_module(new)\nyield\nfinally:\n# Remove the temporary module paths\nfor old in modules:\nif old in sys.modules:\ndel sys.modules[old]\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.torch_safe_load","title":"<code>ultralytics.nn.tasks.torch_safe_load(weight)</code>","text":"<p>This function attempts to load a PyTorch model with the torch.load() function. If a ModuleNotFoundError is raised, it catches the error, logs a warning message, and attempts to install the missing module via the check_requirements() function. After installation, the function again attempts to load the model using torch.load().</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>str</code> <p>The file path of the PyTorch model.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The loaded PyTorch model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def torch_safe_load(weight):\n\"\"\"\n    This function attempts to load a PyTorch model with the torch.load() function. If a ModuleNotFoundError is raised,\n    it catches the error, logs a warning message, and attempts to install the missing module via the\n    check_requirements() function. After installation, the function again attempts to load the model using torch.load().\n    Args:\n        weight (str): The file path of the PyTorch model.\n    Returns:\n        (dict): The loaded PyTorch model.\n    \"\"\"\nfrom ultralytics.utils.downloads import attempt_download_asset\ncheck_suffix(file=weight, suffix='.pt')\nfile = attempt_download_asset(weight)  # search online if missing locally\ntry:\nwith temporary_modules({\n'ultralytics.yolo.utils': 'ultralytics.utils',\n'ultralytics.yolo.v8': 'ultralytics.models.yolo',\n'ultralytics.yolo.data': 'ultralytics.data'}):  # for legacy 8.0 Classify and Pose models\nreturn torch.load(file, map_location='cpu'), file  # load\nexcept ModuleNotFoundError as e:  # e.name is missing module name\nif e.name == 'models':\nraise TypeError(\nemojis(f'ERROR \u274c\ufe0f {weight} appears to be an Ultralytics YOLOv5 model originally trained '\nf'with https://github.com/ultralytics/yolov5.\\nThis model is NOT forwards compatible with '\nf'YOLOv8 at https://github.com/ultralytics/ultralytics.'\nf\"\\nRecommend fixes are to train a new model using the latest 'ultralytics' package or to \"\nf\"run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\")) from e\nLOGGER.warning(f\"WARNING \u26a0\ufe0f {weight} appears to require '{e.name}', which is not in ultralytics requirements.\"\nf\"\\nAutoInstall will run now for '{e.name}' but this feature will be removed in the future.\"\nf\"\\nRecommend fixes are to train a new model using the latest 'ultralytics' package or to \"\nf\"run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\")\ncheck_requirements(e.name)  # install missing module\nreturn torch.load(file, map_location='cpu'), file  # load\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.attempt_load_weights","title":"<code>ultralytics.nn.tasks.attempt_load_weights(weights, device=None, inplace=True, fuse=False)</code>","text":"<p>Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def attempt_load_weights(weights, device=None, inplace=True, fuse=False):\n\"\"\"Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a.\"\"\"\nensemble = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\nckpt, w = torch_safe_load(w)  # load ckpt\nargs = {**DEFAULT_CFG_DICT, **ckpt['train_args']} if 'train_args' in ckpt else None  # combined args\nmodel = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\n# Model compatibility updates\nmodel.args = args  # attach args to model\nmodel.pt_path = w  # attach *.pt file path to model\nmodel.task = guess_model_task(model)\nif not hasattr(model, 'stride'):\nmodel.stride = torch.tensor([32.])\n# Append\nensemble.append(model.fuse().eval() if fuse and hasattr(model, 'fuse') else model.eval())  # model in eval mode\n# Module updates\nfor m in ensemble.modules():\nt = type(m)\nif t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Segment):\nm.inplace = inplace\nelif t is nn.Upsample and not hasattr(m, 'recompute_scale_factor'):\nm.recompute_scale_factor = None  # torch 1.11.0 compatibility\n# Return model\nif len(ensemble) == 1:\nreturn ensemble[-1]\n# Return ensemble\nLOGGER.info(f'Ensemble created with {weights}\\n')\nfor k in 'names', 'nc', 'yaml':\nsetattr(ensemble, k, getattr(ensemble[0], k))\nensemble.stride = ensemble[torch.argmax(torch.tensor([m.stride.max() for m in ensemble])).int()].stride\nassert all(ensemble[0].nc == m.nc for m in ensemble), f'Models differ in class counts {[m.nc for m in ensemble]}'\nreturn ensemble\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.attempt_load_one_weight","title":"<code>ultralytics.nn.tasks.attempt_load_one_weight(weight, device=None, inplace=True, fuse=False)</code>","text":"<p>Loads a single model weights.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def attempt_load_one_weight(weight, device=None, inplace=True, fuse=False):\n\"\"\"Loads a single model weights.\"\"\"\nckpt, weight = torch_safe_load(weight)  # load ckpt\nargs = {**DEFAULT_CFG_DICT, **(ckpt.get('train_args', {}))}  # combine model and default args, preferring model args\nmodel = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\n# Model compatibility updates\nmodel.args = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # attach args to model\nmodel.pt_path = weight  # attach *.pt file path to model\nmodel.task = guess_model_task(model)\nif not hasattr(model, 'stride'):\nmodel.stride = torch.tensor([32.])\nmodel = model.fuse().eval() if fuse and hasattr(model, 'fuse') else model.eval()  # model in eval mode\n# Module updates\nfor m in model.modules():\nt = type(m)\nif t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Segment):\nm.inplace = inplace\nelif t is nn.Upsample and not hasattr(m, 'recompute_scale_factor'):\nm.recompute_scale_factor = None  # torch 1.11.0 compatibility\n# Return model and ckpt\nreturn model, ckpt\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.parse_model","title":"<code>ultralytics.nn.tasks.parse_model(d, ch, verbose=True)</code>","text":"<p>Parse a YOLO model.yaml dictionary into a PyTorch model.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)\n\"\"\"Parse a YOLO model.yaml dictionary into a PyTorch model.\"\"\"\nimport ast\n# Args\nmax_channels = float('inf')\nnc, act, scales = (d.get(x) for x in ('nc', 'activation', 'scales'))\ndepth, width, kpt_shape = (d.get(x, 1.0) for x in ('depth_multiple', 'width_multiple', 'kpt_shape'))\nif scales:\nscale = d.get('scale')\nif not scale:\nscale = tuple(scales.keys())[0]\nLOGGER.warning(f\"WARNING \u26a0\ufe0f no model scale passed. Assuming scale='{scale}'.\")\ndepth, width, max_channels = scales[scale]\nif act:\nConv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()\nif verbose:\nLOGGER.info(f\"{colorstr('activation:')} {act}\")  # print\nif verbose:\nLOGGER.info(f\"\\n{'':&gt;3}{'from':&gt;20}{'n':&gt;3}{'params':&gt;10}  {'module':&lt;45}{'arguments':&lt;30}\")\nch = [ch]\nlayers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\nfor i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\nm = getattr(torch.nn, m[3:]) if 'nn.' in m else globals()[m]  # get module\nfor j, a in enumerate(args):\nif isinstance(a, str):\nwith contextlib.suppress(ValueError):\nargs[j] = locals()[a] if a in locals() else ast.literal_eval(a)\nn = n_ = max(round(n * depth), 1) if n &gt; 1 else n  # depth gain\nif m in (Classify, Conv, ConvTranspose, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, Focus,\nBottleneckCSP, C1, C2, C2f, C3, C3TR, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, RepC3):\nc1, c2 = ch[f], args[0]\nif c2 != nc:  # if c2 not equal to number of classes (i.e. for Classify() output)\nc2 = make_divisible(min(c2, max_channels) * width, 8)\nargs = [c1, c2, *args[1:]]\nif m in (BottleneckCSP, C1, C2, C2f, C3, C3TR, C3Ghost, C3x, RepC3):\nargs.insert(2, n)  # number of repeats\nn = 1\nelif m is AIFI:\nargs = [ch[f], *args]\nelif m in (HGStem, HGBlock):\nc1, cm, c2 = ch[f], args[0], args[1]\nargs = [c1, cm, c2, *args[2:]]\nif m is HGBlock:\nargs.insert(4, n)  # number of repeats\nn = 1\nelif m is nn.BatchNorm2d:\nargs = [ch[f]]\nelif m is Concat:\nc2 = sum(ch[x] for x in f)\nelif m in (Detect, Segment, Pose):\nargs.append([ch[x] for x in f])\nif m is Segment:\nargs[2] = make_divisible(min(args[2], max_channels) * width, 8)\nelif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1\nargs.insert(1, [ch[x] for x in f])\nelse:\nc2 = ch[f]\nm_ = nn.Sequential(*(m(*args) for _ in range(n))) if n &gt; 1 else m(*args)  # module\nt = str(m)[8:-2].replace('__main__.', '')  # module type\nm.np = sum(x.numel() for x in m_.parameters())  # number params\nm_.i, m_.f, m_.type = i, f, t  # attach index, 'from' index, type\nif verbose:\nLOGGER.info(f'{i:&gt;3}{str(f):&gt;20}{n_:&gt;3}{m.np:10.0f}  {t:&lt;45}{str(args):&lt;30}')  # print\nsave.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\nlayers.append(m_)\nif i == 0:\nch = []\nch.append(c2)\nreturn nn.Sequential(*layers), sorted(save)\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.yaml_model_load","title":"<code>ultralytics.nn.tasks.yaml_model_load(path)</code>","text":"<p>Load a YOLOv8 model from a YAML file.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def yaml_model_load(path):\n\"\"\"Load a YOLOv8 model from a YAML file.\"\"\"\nimport re\npath = Path(path)\nif path.stem in (f'yolov{d}{x}6' for x in 'nsmlx' for d in (5, 8)):\nnew_stem = re.sub(r'(\\d+)([nslmx])6(.+)?$', r'\\1\\2-p6\\3', path.stem)\nLOGGER.warning(f'WARNING \u26a0\ufe0f Ultralytics YOLO P6 models now use -p6 suffix. Renaming {path.stem} to {new_stem}.')\npath = path.with_name(new_stem + path.suffix)\nunified_path = re.sub(r'(\\d+)([nslmx])(.+)?$', r'\\1\\3', str(path))  # i.e. yolov8x.yaml -&gt; yolov8.yaml\nyaml_file = check_yaml(unified_path, hard=False) or check_yaml(path)\nd = yaml_load(yaml_file)  # model dict\nd['scale'] = guess_model_scale(path)\nd['yaml_file'] = str(path)\nreturn d\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.guess_model_scale","title":"<code>ultralytics.nn.tasks.guess_model_scale(model_path)</code>","text":"<p>Takes a path to a YOLO model's YAML file as input and extracts the size character of the model's scale. The function uses regular expression matching to find the pattern of the model scale in the YAML file name, which is denoted by n, s, m, l, or x. The function returns the size character of the model scale as a string.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The path to the YOLO model's YAML file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The size character of the model's scale, which can be n, s, m, l, or x.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def guess_model_scale(model_path):\n\"\"\"\n    Takes a path to a YOLO model's YAML file as input and extracts the size character of the model's scale.\n    The function uses regular expression matching to find the pattern of the model scale in the YAML file name,\n    which is denoted by n, s, m, l, or x. The function returns the size character of the model scale as a string.\n    Args:\n        model_path (str | Path): The path to the YOLO model's YAML file.\n    Returns:\n        (str): The size character of the model's scale, which can be n, s, m, l, or x.\n    \"\"\"\nwith contextlib.suppress(AttributeError):\nimport re\nreturn re.search(r'yolov\\d+([nslmx])', Path(model_path).stem).group(1)  # n, s, m, l, or x\nreturn ''\n</code></pre>"},{"location":"reference/nn/tasks/#ultralytics.nn.tasks.guess_model_task","title":"<code>ultralytics.nn.tasks.guess_model_task(model)</code>","text":"<p>Guess the task of a PyTorch model from its architecture or configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | dict</code> <p>PyTorch model or model configuration in YAML format.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Task of the model ('detect', 'segment', 'classify', 'pose').</p> <p>Raises:</p> Type Description <code>SyntaxError</code> <p>If the task of the model could not be determined.</p> Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def guess_model_task(model):\n\"\"\"\n    Guess the task of a PyTorch model from its architecture or configuration.\n    Args:\n        model (nn.Module | dict): PyTorch model or model configuration in YAML format.\n    Returns:\n        (str): Task of the model ('detect', 'segment', 'classify', 'pose').\n    Raises:\n        SyntaxError: If the task of the model could not be determined.\n    \"\"\"\ndef cfg2task(cfg):\n\"\"\"Guess from YAML dictionary.\"\"\"\nm = cfg['head'][-1][-2].lower()  # output module name\nif m in ('classify', 'classifier', 'cls', 'fc'):\nreturn 'classify'\nif m == 'detect':\nreturn 'detect'\nif m == 'segment':\nreturn 'segment'\nif m == 'pose':\nreturn 'pose'\n# Guess from model cfg\nif isinstance(model, dict):\nwith contextlib.suppress(Exception):\nreturn cfg2task(model)\n# Guess from PyTorch model\nif isinstance(model, nn.Module):  # PyTorch model\nfor x in 'model.args', 'model.model.args', 'model.model.model.args':\nwith contextlib.suppress(Exception):\nreturn eval(x)['task']\nfor x in 'model.yaml', 'model.model.yaml', 'model.model.model.yaml':\nwith contextlib.suppress(Exception):\nreturn cfg2task(eval(x))\nfor m in model.modules():\nif isinstance(m, Detect):\nreturn 'detect'\nelif isinstance(m, Segment):\nreturn 'segment'\nelif isinstance(m, Classify):\nreturn 'classify'\nelif isinstance(m, Pose):\nreturn 'pose'\n# Guess from model filename\nif isinstance(model, (str, Path)):\nmodel = Path(model)\nif '-seg' in model.stem or 'segment' in model.parts:\nreturn 'segment'\nelif '-cls' in model.stem or 'classify' in model.parts:\nreturn 'classify'\nelif '-pose' in model.stem or 'pose' in model.parts:\nreturn 'pose'\nelif 'detect' in model.parts:\nreturn 'detect'\n# Unable to determine task from model\nLOGGER.warning(\"WARNING \u26a0\ufe0f Unable to automatically guess model task, assuming 'task=detect'. \"\n\"Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify', or 'pose'.\")\nreturn 'detect'  # assume detect\n</code></pre>"},{"location":"reference/nn/modules/block/","title":"Reference for <code>ultralytics/nn/modules/block.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/block.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.DFL","title":"<code>ultralytics.nn.modules.block.DFL</code>","text":"<p>             Bases: <code>Module</code></p> <p>Integral module of Distribution Focal Loss (DFL). Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class DFL(nn.Module):\n\"\"\"\n    Integral module of Distribution Focal Loss (DFL).\n    Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n    \"\"\"\ndef __init__(self, c1=16):\n\"\"\"Initialize a convolutional layer with a given number of input channels.\"\"\"\nsuper().__init__()\nself.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\nx = torch.arange(c1, dtype=torch.float)\nself.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\nself.c1 = c1\ndef forward(self, x):\n\"\"\"Applies a transformer layer on input tensor 'x' and returns a tensor.\"\"\"\nb, c, a = x.shape  # batch, channels, anchors\nreturn self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.DFL.__init__","title":"<code>__init__(c1=16)</code>","text":"<p>Initialize a convolutional layer with a given number of input channels.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def __init__(self, c1=16):\n\"\"\"Initialize a convolutional layer with a given number of input channels.\"\"\"\nsuper().__init__()\nself.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\nx = torch.arange(c1, dtype=torch.float)\nself.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\nself.c1 = c1\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.DFL.forward","title":"<code>forward(x)</code>","text":"<p>Applies a transformer layer on input tensor 'x' and returns a tensor.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies a transformer layer on input tensor 'x' and returns a tensor.\"\"\"\nb, c, a = x.shape  # batch, channels, anchors\nreturn self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.Proto","title":"<code>ultralytics.nn.modules.block.Proto</code>","text":"<p>             Bases: <code>Module</code></p> <p>YOLOv8 mask Proto module for segmentation models.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class Proto(nn.Module):\n\"\"\"YOLOv8 mask Proto module for segmentation models.\"\"\"\ndef __init__(self, c1, c_=256, c2=32):  # ch_in, number of protos, number of masks\nsuper().__init__()\nself.cv1 = Conv(c1, c_, k=3)\nself.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')\nself.cv2 = Conv(c_, c_, k=3)\nself.cv3 = Conv(c_, c2)\ndef forward(self, x):\n\"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\nreturn self.cv3(self.cv2(self.upsample(self.cv1(x))))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.Proto.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through layers using an upsampled input image.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\nreturn self.cv3(self.cv2(self.upsample(self.cv1(x))))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.HGStem","title":"<code>ultralytics.nn.modules.block.HGStem</code>","text":"<p>             Bases: <code>Module</code></p> <p>StemBlock of PPHGNetV2 with 5 convolutions and one maxpool2d. https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class HGStem(nn.Module):\n\"\"\"StemBlock of PPHGNetV2 with 5 convolutions and one maxpool2d.\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py\n    \"\"\"\ndef __init__(self, c1, cm, c2):\nsuper().__init__()\nself.stem1 = Conv(c1, cm, 3, 2, act=nn.ReLU())\nself.stem2a = Conv(cm, cm // 2, 2, 1, 0, act=nn.ReLU())\nself.stem2b = Conv(cm // 2, cm, 2, 1, 0, act=nn.ReLU())\nself.stem3 = Conv(cm * 2, cm, 3, 2, act=nn.ReLU())\nself.stem4 = Conv(cm, c2, 1, 1, act=nn.ReLU())\nself.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, ceil_mode=True)\ndef forward(self, x):\n\"\"\"Forward pass of a PPHGNetV2 backbone layer.\"\"\"\nx = self.stem1(x)\nx = F.pad(x, [0, 1, 0, 1])\nx2 = self.stem2a(x)\nx2 = F.pad(x2, [0, 1, 0, 1])\nx2 = self.stem2b(x2)\nx1 = self.pool(x)\nx = torch.cat([x1, x2], dim=1)\nx = self.stem3(x)\nx = self.stem4(x)\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.HGStem.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of a PPHGNetV2 backbone layer.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass of a PPHGNetV2 backbone layer.\"\"\"\nx = self.stem1(x)\nx = F.pad(x, [0, 1, 0, 1])\nx2 = self.stem2a(x)\nx2 = F.pad(x2, [0, 1, 0, 1])\nx2 = self.stem2b(x2)\nx1 = self.pool(x)\nx = torch.cat([x1, x2], dim=1)\nx = self.stem3(x)\nx = self.stem4(x)\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.HGBlock","title":"<code>ultralytics.nn.modules.block.HGBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>HG_Block of PPHGNetV2 with 2 convolutions and LightConv. https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class HGBlock(nn.Module):\n\"\"\"HG_Block of PPHGNetV2 with 2 convolutions and LightConv.\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py\n    \"\"\"\ndef __init__(self, c1, cm, c2, k=3, n=6, lightconv=False, shortcut=False, act=nn.ReLU()):\nsuper().__init__()\nblock = LightConv if lightconv else Conv\nself.m = nn.ModuleList(block(c1 if i == 0 else cm, cm, k=k, act=act) for i in range(n))\nself.sc = Conv(c1 + n * cm, c2 // 2, 1, 1, act=act)  # squeeze conv\nself.ec = Conv(c2 // 2, c2, 1, 1, act=act)  # excitation conv\nself.add = shortcut and c1 == c2\ndef forward(self, x):\n\"\"\"Forward pass of a PPHGNetV2 backbone layer.\"\"\"\ny = [x]\ny.extend(m(y[-1]) for m in self.m)\ny = self.ec(self.sc(torch.cat(y, 1)))\nreturn y + x if self.add else y\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.HGBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of a PPHGNetV2 backbone layer.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass of a PPHGNetV2 backbone layer.\"\"\"\ny = [x]\ny.extend(m(y[-1]) for m in self.m)\ny = self.ec(self.sc(torch.cat(y, 1)))\nreturn y + x if self.add else y\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.SPP","title":"<code>ultralytics.nn.modules.block.SPP</code>","text":"<p>             Bases: <code>Module</code></p> <p>Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class SPP(nn.Module):\n\"\"\"Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729.\"\"\"\ndef __init__(self, c1, c2, k=(5, 9, 13)):\n\"\"\"Initialize the SPP layer with input/output channels and pooling kernel sizes.\"\"\"\nsuper().__init__()\nc_ = c1 // 2  # hidden channels\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\nself.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\ndef forward(self, x):\n\"\"\"Forward pass of the SPP layer, performing spatial pyramid pooling.\"\"\"\nx = self.cv1(x)\nreturn self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.SPP.__init__","title":"<code>__init__(c1, c2, k=(5, 9, 13))</code>","text":"<p>Initialize the SPP layer with input/output channels and pooling kernel sizes.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def __init__(self, c1, c2, k=(5, 9, 13)):\n\"\"\"Initialize the SPP layer with input/output channels and pooling kernel sizes.\"\"\"\nsuper().__init__()\nc_ = c1 // 2  # hidden channels\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\nself.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.SPP.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SPP layer, performing spatial pyramid pooling.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass of the SPP layer, performing spatial pyramid pooling.\"\"\"\nx = self.cv1(x)\nreturn self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.SPPF","title":"<code>ultralytics.nn.modules.block.SPPF</code>","text":"<p>             Bases: <code>Module</code></p> <p>Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class SPPF(nn.Module):\n\"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher.\"\"\"\ndef __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))\nsuper().__init__()\nc_ = c1 // 2  # hidden channels\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = Conv(c_ * 4, c2, 1, 1)\nself.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\ndef forward(self, x):\n\"\"\"Forward pass through Ghost Convolution block.\"\"\"\nx = self.cv1(x)\ny1 = self.m(x)\ny2 = self.m(y1)\nreturn self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.SPPF.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through Ghost Convolution block.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass through Ghost Convolution block.\"\"\"\nx = self.cv1(x)\ny1 = self.m(x)\ny2 = self.m(y1)\nreturn self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C1","title":"<code>ultralytics.nn.modules.block.C1</code>","text":"<p>             Bases: <code>Module</code></p> <p>CSP Bottleneck with 1 convolution.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C1(nn.Module):\n\"\"\"CSP Bottleneck with 1 convolution.\"\"\"\ndef __init__(self, c1, c2, n=1):  # ch_in, ch_out, number\nsuper().__init__()\nself.cv1 = Conv(c1, c2, 1, 1)\nself.m = nn.Sequential(*(Conv(c2, c2, 3) for _ in range(n)))\ndef forward(self, x):\n\"\"\"Applies cross-convolutions to input in the C3 module.\"\"\"\ny = self.cv1(x)\nreturn self.m(y) + y\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C1.forward","title":"<code>forward(x)</code>","text":"<p>Applies cross-convolutions to input in the C3 module.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies cross-convolutions to input in the C3 module.\"\"\"\ny = self.cv1(x)\nreturn self.m(y) + y\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C2","title":"<code>ultralytics.nn.modules.block.C2</code>","text":"<p>             Bases: <code>Module</code></p> <p>CSP Bottleneck with 2 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C2(nn.Module):\n\"\"\"CSP Bottleneck with 2 convolutions.\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\nsuper().__init__()\nself.c = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, 2 * self.c, 1, 1)\nself.cv2 = Conv(2 * self.c, c2, 1)  # optional act=FReLU(c2)\n# self.attention = ChannelAttention(2 * self.c)  # or SpatialAttention()\nself.m = nn.Sequential(*(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n)))\ndef forward(self, x):\n\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\na, b = self.cv1(x).chunk(2, 1)\nreturn self.cv2(torch.cat((self.m(a), b), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C2.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the CSP bottleneck with 2 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\na, b = self.cv1(x).chunk(2, 1)\nreturn self.cv2(torch.cat((self.m(a), b), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C2f","title":"<code>ultralytics.nn.modules.block.C2f</code>","text":"<p>             Bases: <code>Module</code></p> <p>Faster Implementation of CSP Bottleneck with 2 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C2f(nn.Module):\n\"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\nsuper().__init__()\nself.c = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, 2 * self.c, 1, 1)\nself.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\nself.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\ndef forward(self, x):\n\"\"\"Forward pass through C2f layer.\"\"\"\ny = list(self.cv1(x).chunk(2, 1))\ny.extend(m(y[-1]) for m in self.m)\nreturn self.cv2(torch.cat(y, 1))\ndef forward_split(self, x):\n\"\"\"Forward pass using split() instead of chunk().\"\"\"\ny = list(self.cv1(x).split((self.c, self.c), 1))\ny.extend(m(y[-1]) for m in self.m)\nreturn self.cv2(torch.cat(y, 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C2f.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through C2f layer.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass through C2f layer.\"\"\"\ny = list(self.cv1(x).chunk(2, 1))\ny.extend(m(y[-1]) for m in self.m)\nreturn self.cv2(torch.cat(y, 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C2f.forward_split","title":"<code>forward_split(x)</code>","text":"<p>Forward pass using split() instead of chunk().</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward_split(self, x):\n\"\"\"Forward pass using split() instead of chunk().\"\"\"\ny = list(self.cv1(x).split((self.c, self.c), 1))\ny.extend(m(y[-1]) for m in self.m)\nreturn self.cv2(torch.cat(y, 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3","title":"<code>ultralytics.nn.modules.block.C3</code>","text":"<p>             Bases: <code>Module</code></p> <p>CSP Bottleneck with 3 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C3(nn.Module):\n\"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\nsuper().__init__()\nc_ = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = Conv(c1, c_, 1, 1)\nself.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\nself.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))\ndef forward(self, x):\n\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\nreturn self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the CSP bottleneck with 2 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\nreturn self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3x","title":"<code>ultralytics.nn.modules.block.C3x</code>","text":"<p>             Bases: <code>C3</code></p> <p>C3 module with cross-convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C3x(C3):\n\"\"\"C3 module with cross-convolutions.\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize C3TR instance and set default parameters.\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nself.c_ = int(c2 * e)\nself.m = nn.Sequential(*(Bottleneck(self.c_, self.c_, shortcut, g, k=((1, 3), (3, 1)), e=1) for _ in range(n)))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3x.__init__","title":"<code>__init__(c1, c2, n=1, shortcut=True, g=1, e=0.5)</code>","text":"<p>Initialize C3TR instance and set default parameters.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize C3TR instance and set default parameters.\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nself.c_ = int(c2 * e)\nself.m = nn.Sequential(*(Bottleneck(self.c_, self.c_, shortcut, g, k=((1, 3), (3, 1)), e=1) for _ in range(n)))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.RepC3","title":"<code>ultralytics.nn.modules.block.RepC3</code>","text":"<p>             Bases: <code>Module</code></p> <p>Rep C3.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class RepC3(nn.Module):\n\"\"\"Rep C3.\"\"\"\ndef __init__(self, c1, c2, n=3, e=1.0):\nsuper().__init__()\nc_ = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, c2, 1, 1)\nself.cv2 = Conv(c1, c2, 1, 1)\nself.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])\nself.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()\ndef forward(self, x):\n\"\"\"Forward pass of RT-DETR neck layer.\"\"\"\nreturn self.cv3(self.m(self.cv1(x)) + self.cv2(x))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.RepC3.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of RT-DETR neck layer.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass of RT-DETR neck layer.\"\"\"\nreturn self.cv3(self.m(self.cv1(x)) + self.cv2(x))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3TR","title":"<code>ultralytics.nn.modules.block.C3TR</code>","text":"<p>             Bases: <code>C3</code></p> <p>C3 module with TransformerBlock().</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C3TR(C3):\n\"\"\"C3 module with TransformerBlock().\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize C3Ghost module with GhostBottleneck().\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nc_ = int(c2 * e)\nself.m = TransformerBlock(c_, c_, 4, n)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3TR.__init__","title":"<code>__init__(c1, c2, n=1, shortcut=True, g=1, e=0.5)</code>","text":"<p>Initialize C3Ghost module with GhostBottleneck().</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize C3Ghost module with GhostBottleneck().\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nc_ = int(c2 * e)\nself.m = TransformerBlock(c_, c_, 4, n)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3Ghost","title":"<code>ultralytics.nn.modules.block.C3Ghost</code>","text":"<p>             Bases: <code>C3</code></p> <p>C3 module with GhostBottleneck().</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class C3Ghost(C3):\n\"\"\"C3 module with GhostBottleneck().\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling.\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nc_ = int(c2 * e)  # hidden channels\nself.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.C3Ghost.__init__","title":"<code>__init__(c1, c2, n=1, shortcut=True, g=1, e=0.5)</code>","text":"<p>Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n\"\"\"Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling.\"\"\"\nsuper().__init__(c1, c2, n, shortcut, g, e)\nc_ = int(c2 * e)  # hidden channels\nself.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.GhostBottleneck","title":"<code>ultralytics.nn.modules.block.GhostBottleneck</code>","text":"<p>             Bases: <code>Module</code></p> <p>Ghost Bottleneck https://github.com/huawei-noah/ghostnet.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class GhostBottleneck(nn.Module):\n\"\"\"Ghost Bottleneck https://github.com/huawei-noah/ghostnet.\"\"\"\ndef __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride\nsuper().__init__()\nc_ = c2 // 2\nself.conv = nn.Sequential(\nGhostConv(c1, c_, 1, 1),  # pw\nDWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\nGhostConv(c_, c2, 1, 1, act=False))  # pw-linear\nself.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1,\nact=False)) if s == 2 else nn.Identity()\ndef forward(self, x):\n\"\"\"Applies skip connection and concatenation to input tensor.\"\"\"\nreturn self.conv(x) + self.shortcut(x)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.GhostBottleneck.forward","title":"<code>forward(x)</code>","text":"<p>Applies skip connection and concatenation to input tensor.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies skip connection and concatenation to input tensor.\"\"\"\nreturn self.conv(x) + self.shortcut(x)\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.Bottleneck","title":"<code>ultralytics.nn.modules.block.Bottleneck</code>","text":"<p>             Bases: <code>Module</code></p> <p>Standard bottleneck.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class Bottleneck(nn.Module):\n\"\"\"Standard bottleneck.\"\"\"\ndef __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand\nsuper().__init__()\nc_ = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, c_, k[0], 1)\nself.cv2 = Conv(c_, c2, k[1], 1, g=g)\nself.add = shortcut and c1 == c2\ndef forward(self, x):\n\"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\nreturn x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.Bottleneck.forward","title":"<code>forward(x)</code>","text":"<p>'forward()' applies the YOLOv5 FPN to input data.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\nreturn x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.BottleneckCSP","title":"<code>ultralytics.nn.modules.block.BottleneckCSP</code>","text":"<p>             Bases: <code>Module</code></p> <p>CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>class BottleneckCSP(nn.Module):\n\"\"\"CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks.\"\"\"\ndef __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\nsuper().__init__()\nc_ = int(c2 * e)  # hidden channels\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\nself.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\nself.cv4 = Conv(2 * c_, c2, 1, 1)\nself.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\nself.act = nn.SiLU()\nself.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\ndef forward(self, x):\n\"\"\"Applies a CSP bottleneck with 3 convolutions.\"\"\"\ny1 = self.cv3(self.m(self.cv1(x)))\ny2 = self.cv2(x)\nreturn self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))\n</code></pre>"},{"location":"reference/nn/modules/block/#ultralytics.nn.modules.block.BottleneckCSP.forward","title":"<code>forward(x)</code>","text":"<p>Applies a CSP bottleneck with 3 convolutions.</p> Source code in <code>ultralytics/nn/modules/block.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies a CSP bottleneck with 3 convolutions.\"\"\"\ny1 = self.cv3(self.m(self.cv1(x)))\ny2 = self.cv2(x)\nreturn self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))\n</code></pre>"},{"location":"reference/nn/modules/conv/","title":"Reference for <code>ultralytics/nn/modules/conv.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/conv.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv","title":"<code>ultralytics.nn.modules.conv.Conv</code>","text":"<p>             Bases: <code>Module</code></p> <p>Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class Conv(nn.Module):\n\"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\ndefault_act = nn.SiLU()  # default activation\ndef __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__()\nself.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\nself.bn = nn.BatchNorm2d(c2)\nself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\ndef forward(self, x):\n\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x)))\ndef forward_fuse(self, x):\n\"\"\"Perform transposed convolution of 2D data.\"\"\"\nreturn self.act(self.conv(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv.__init__","title":"<code>__init__(c1, c2, k=1, s=1, p=None, g=1, d=1, act=True)</code>","text":"<p>Initialize Conv layer with given arguments including activation.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__()\nself.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\nself.bn = nn.BatchNorm2d(c2)\nself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv.forward","title":"<code>forward(x)</code>","text":"<p>Apply convolution, batch normalization and activation to input tensor.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv.forward_fuse","title":"<code>forward_fuse(x)</code>","text":"<p>Perform transposed convolution of 2D data.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward_fuse(self, x):\n\"\"\"Perform transposed convolution of 2D data.\"\"\"\nreturn self.act(self.conv(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv2","title":"<code>ultralytics.nn.modules.conv.Conv2</code>","text":"<p>             Bases: <code>Conv</code></p> <p>Simplified RepConv module with Conv fusing.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class Conv2(Conv):\n\"\"\"Simplified RepConv module with Conv fusing.\"\"\"\ndef __init__(self, c1, c2, k=3, s=1, p=None, g=1, d=1, act=True):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__(c1, c2, k, s, p, g=g, d=d, act=act)\nself.cv2 = nn.Conv2d(c1, c2, 1, s, autopad(1, p, d), groups=g, dilation=d, bias=False)  # add 1x1 conv\ndef forward(self, x):\n\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x) + self.cv2(x)))\ndef forward_fuse(self, x):\n\"\"\"Apply fused convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x)))\ndef fuse_convs(self):\n\"\"\"Fuse parallel convolutions.\"\"\"\nw = torch.zeros_like(self.conv.weight.data)\ni = [x // 2 for x in w.shape[2:]]\nw[:, :, i[0]:i[0] + 1, i[1]:i[1] + 1] = self.cv2.weight.data.clone()\nself.conv.weight.data += w\nself.__delattr__('cv2')\nself.forward = self.forward_fuse\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv2.__init__","title":"<code>__init__(c1, c2, k=3, s=1, p=None, g=1, d=1, act=True)</code>","text":"<p>Initialize Conv layer with given arguments including activation.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, c1, c2, k=3, s=1, p=None, g=1, d=1, act=True):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__(c1, c2, k, s, p, g=g, d=d, act=act)\nself.cv2 = nn.Conv2d(c1, c2, 1, s, autopad(1, p, d), groups=g, dilation=d, bias=False)  # add 1x1 conv\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv2.forward","title":"<code>forward(x)</code>","text":"<p>Apply convolution, batch normalization and activation to input tensor.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x) + self.cv2(x)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv2.forward_fuse","title":"<code>forward_fuse(x)</code>","text":"<p>Apply fused convolution, batch normalization and activation to input tensor.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward_fuse(self, x):\n\"\"\"Apply fused convolution, batch normalization and activation to input tensor.\"\"\"\nreturn self.act(self.bn(self.conv(x)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Conv2.fuse_convs","title":"<code>fuse_convs()</code>","text":"<p>Fuse parallel convolutions.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def fuse_convs(self):\n\"\"\"Fuse parallel convolutions.\"\"\"\nw = torch.zeros_like(self.conv.weight.data)\ni = [x // 2 for x in w.shape[2:]]\nw[:, :, i[0]:i[0] + 1, i[1]:i[1] + 1] = self.cv2.weight.data.clone()\nself.conv.weight.data += w\nself.__delattr__('cv2')\nself.forward = self.forward_fuse\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.LightConv","title":"<code>ultralytics.nn.modules.conv.LightConv</code>","text":"<p>             Bases: <code>Module</code></p> <p>Light convolution with args(ch_in, ch_out, kernel). https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class LightConv(nn.Module):\n\"\"\"Light convolution with args(ch_in, ch_out, kernel).\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py\n    \"\"\"\ndef __init__(self, c1, c2, k=1, act=nn.ReLU()):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__()\nself.conv1 = Conv(c1, c2, 1, act=False)\nself.conv2 = DWConv(c2, c2, k, act=act)\ndef forward(self, x):\n\"\"\"Apply 2 convolutions to input tensor.\"\"\"\nreturn self.conv2(self.conv1(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.LightConv.__init__","title":"<code>__init__(c1, c2, k=1, act=nn.ReLU())</code>","text":"<p>Initialize Conv layer with given arguments including activation.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, c1, c2, k=1, act=nn.ReLU()):\n\"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\nsuper().__init__()\nself.conv1 = Conv(c1, c2, 1, act=False)\nself.conv2 = DWConv(c2, c2, k, act=act)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.LightConv.forward","title":"<code>forward(x)</code>","text":"<p>Apply 2 convolutions to input tensor.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Apply 2 convolutions to input tensor.\"\"\"\nreturn self.conv2(self.conv1(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.DWConv","title":"<code>ultralytics.nn.modules.conv.DWConv</code>","text":"<p>             Bases: <code>Conv</code></p> <p>Depth-wise convolution.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class DWConv(Conv):\n\"\"\"Depth-wise convolution.\"\"\"\ndef __init__(self, c1, c2, k=1, s=1, d=1, act=True):  # ch_in, ch_out, kernel, stride, dilation, activation\nsuper().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.DWConvTranspose2d","title":"<code>ultralytics.nn.modules.conv.DWConvTranspose2d</code>","text":"<p>             Bases: <code>ConvTranspose2d</code></p> <p>Depth-wise transpose convolution.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class DWConvTranspose2d(nn.ConvTranspose2d):\n\"\"\"Depth-wise transpose convolution.\"\"\"\ndef __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out\nsuper().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.ConvTranspose","title":"<code>ultralytics.nn.modules.conv.ConvTranspose</code>","text":"<p>             Bases: <code>Module</code></p> <p>Convolution transpose 2d layer.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class ConvTranspose(nn.Module):\n\"\"\"Convolution transpose 2d layer.\"\"\"\ndefault_act = nn.SiLU()  # default activation\ndef __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n\"\"\"Initialize ConvTranspose2d layer with batch normalization and activation function.\"\"\"\nsuper().__init__()\nself.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\nself.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\nself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\ndef forward(self, x):\n\"\"\"Applies transposed convolutions, batch normalization and activation to input.\"\"\"\nreturn self.act(self.bn(self.conv_transpose(x)))\ndef forward_fuse(self, x):\n\"\"\"Applies activation and convolution transpose operation to input.\"\"\"\nreturn self.act(self.conv_transpose(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.ConvTranspose.__init__","title":"<code>__init__(c1, c2, k=2, s=2, p=0, bn=True, act=True)</code>","text":"<p>Initialize ConvTranspose2d layer with batch normalization and activation function.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):\n\"\"\"Initialize ConvTranspose2d layer with batch normalization and activation function.\"\"\"\nsuper().__init__()\nself.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)\nself.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()\nself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.ConvTranspose.forward","title":"<code>forward(x)</code>","text":"<p>Applies transposed convolutions, batch normalization and activation to input.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies transposed convolutions, batch normalization and activation to input.\"\"\"\nreturn self.act(self.bn(self.conv_transpose(x)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.ConvTranspose.forward_fuse","title":"<code>forward_fuse(x)</code>","text":"<p>Applies activation and convolution transpose operation to input.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward_fuse(self, x):\n\"\"\"Applies activation and convolution transpose operation to input.\"\"\"\nreturn self.act(self.conv_transpose(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Focus","title":"<code>ultralytics.nn.modules.conv.Focus</code>","text":"<p>             Bases: <code>Module</code></p> <p>Focus wh information into c-space.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class Focus(nn.Module):\n\"\"\"Focus wh information into c-space.\"\"\"\ndef __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\nsuper().__init__()\nself.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)\n# self.contract = Contract(gain=2)\ndef forward(self, x):  # x(b,c,w,h) -&gt; y(b,4c,w/2,h/2)\nreturn self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.GhostConv","title":"<code>ultralytics.nn.modules.conv.GhostConv</code>","text":"<p>             Bases: <code>Module</code></p> <p>Ghost Convolution https://github.com/huawei-noah/ghostnet.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class GhostConv(nn.Module):\n\"\"\"Ghost Convolution https://github.com/huawei-noah/ghostnet.\"\"\"\ndef __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\nsuper().__init__()\nc_ = c2 // 2  # hidden channels\nself.cv1 = Conv(c1, c_, k, s, None, g, act=act)\nself.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)\ndef forward(self, x):\n\"\"\"Forward propagation through a Ghost Bottleneck layer with skip connection.\"\"\"\ny = self.cv1(x)\nreturn torch.cat((y, self.cv2(y)), 1)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.GhostConv.forward","title":"<code>forward(x)</code>","text":"<p>Forward propagation through a Ghost Bottleneck layer with skip connection.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward propagation through a Ghost Bottleneck layer with skip connection.\"\"\"\ny = self.cv1(x)\nreturn torch.cat((y, self.cv2(y)), 1)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.RepConv","title":"<code>ultralytics.nn.modules.conv.RepConv</code>","text":"<p>             Bases: <code>Module</code></p> <p>RepConv is a basic rep-style block, including training and deploy status. This module is used in RT-DETR. Based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class RepConv(nn.Module):\n\"\"\"\n    RepConv is a basic rep-style block, including training and deploy status. This module is used in RT-DETR.\n    Based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py\n    \"\"\"\ndefault_act = nn.SiLU()  # default activation\ndef __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):\nsuper().__init__()\nassert k == 3 and p == 1\nself.g = g\nself.c1 = c1\nself.c2 = c2\nself.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\nself.bn = nn.BatchNorm2d(num_features=c1) if bn and c2 == c1 and s == 1 else None\nself.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)\nself.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False)\ndef forward_fuse(self, x):\n\"\"\"Forward process\"\"\"\nreturn self.act(self.conv(x))\ndef forward(self, x):\n\"\"\"Forward process\"\"\"\nid_out = 0 if self.bn is None else self.bn(x)\nreturn self.act(self.conv1(x) + self.conv2(x) + id_out)\ndef get_equivalent_kernel_bias(self):\nkernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)\nkernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)\nkernelid, biasid = self._fuse_bn_tensor(self.bn)\nreturn kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\ndef _pad_1x1_to_3x3_tensor(self, kernel1x1):\nif kernel1x1 is None:\nreturn 0\nelse:\nreturn torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])\ndef _fuse_bn_tensor(self, branch):\nif branch is None:\nreturn 0, 0\nif isinstance(branch, Conv):\nkernel = branch.conv.weight\nrunning_mean = branch.bn.running_mean\nrunning_var = branch.bn.running_var\ngamma = branch.bn.weight\nbeta = branch.bn.bias\neps = branch.bn.eps\nelif isinstance(branch, nn.BatchNorm2d):\nif not hasattr(self, 'id_tensor'):\ninput_dim = self.c1 // self.g\nkernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)\nfor i in range(self.c1):\nkernel_value[i, i % input_dim, 1, 1] = 1\nself.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\nkernel = self.id_tensor\nrunning_mean = branch.running_mean\nrunning_var = branch.running_var\ngamma = branch.weight\nbeta = branch.bias\neps = branch.eps\nstd = (running_var + eps).sqrt()\nt = (gamma / std).reshape(-1, 1, 1, 1)\nreturn kernel * t, beta - running_mean * gamma / std\ndef fuse_convs(self):\nif hasattr(self, 'conv'):\nreturn\nkernel, bias = self.get_equivalent_kernel_bias()\nself.conv = nn.Conv2d(in_channels=self.conv1.conv.in_channels,\nout_channels=self.conv1.conv.out_channels,\nkernel_size=self.conv1.conv.kernel_size,\nstride=self.conv1.conv.stride,\npadding=self.conv1.conv.padding,\ndilation=self.conv1.conv.dilation,\ngroups=self.conv1.conv.groups,\nbias=True).requires_grad_(False)\nself.conv.weight.data = kernel\nself.conv.bias.data = bias\nfor para in self.parameters():\npara.detach_()\nself.__delattr__('conv1')\nself.__delattr__('conv2')\nif hasattr(self, 'nm'):\nself.__delattr__('nm')\nif hasattr(self, 'bn'):\nself.__delattr__('bn')\nif hasattr(self, 'id_tensor'):\nself.__delattr__('id_tensor')\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.RepConv.forward","title":"<code>forward(x)</code>","text":"<p>Forward process</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward process\"\"\"\nid_out = 0 if self.bn is None else self.bn(x)\nreturn self.act(self.conv1(x) + self.conv2(x) + id_out)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.RepConv.forward_fuse","title":"<code>forward_fuse(x)</code>","text":"<p>Forward process</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward_fuse(self, x):\n\"\"\"Forward process\"\"\"\nreturn self.act(self.conv(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.ChannelAttention","title":"<code>ultralytics.nn.modules.conv.ChannelAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class ChannelAttention(nn.Module):\n\"\"\"Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet.\"\"\"\ndef __init__(self, channels: int) -&gt; None:\nsuper().__init__()\nself.pool = nn.AdaptiveAvgPool2d(1)\nself.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\nself.act = nn.Sigmoid()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn x * self.act(self.fc(self.pool(x)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.SpatialAttention","title":"<code>ultralytics.nn.modules.conv.SpatialAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>Spatial-attention module.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class SpatialAttention(nn.Module):\n\"\"\"Spatial-attention module.\"\"\"\ndef __init__(self, kernel_size=7):\n\"\"\"Initialize Spatial-attention module with kernel size argument.\"\"\"\nsuper().__init__()\nassert kernel_size in (3, 7), 'kernel size must be 3 or 7'\npadding = 3 if kernel_size == 7 else 1\nself.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\nself.act = nn.Sigmoid()\ndef forward(self, x):\n\"\"\"Apply channel and spatial attention on input for feature recalibration.\"\"\"\nreturn x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.SpatialAttention.__init__","title":"<code>__init__(kernel_size=7)</code>","text":"<p>Initialize Spatial-attention module with kernel size argument.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, kernel_size=7):\n\"\"\"Initialize Spatial-attention module with kernel size argument.\"\"\"\nsuper().__init__()\nassert kernel_size in (3, 7), 'kernel size must be 3 or 7'\npadding = 3 if kernel_size == 7 else 1\nself.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\nself.act = nn.Sigmoid()\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.SpatialAttention.forward","title":"<code>forward(x)</code>","text":"<p>Apply channel and spatial attention on input for feature recalibration.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Apply channel and spatial attention on input for feature recalibration.\"\"\"\nreturn x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.CBAM","title":"<code>ultralytics.nn.modules.conv.CBAM</code>","text":"<p>             Bases: <code>Module</code></p> <p>Convolutional Block Attention Module.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class CBAM(nn.Module):\n\"\"\"Convolutional Block Attention Module.\"\"\"\ndef __init__(self, c1, kernel_size=7):  # ch_in, kernels\nsuper().__init__()\nself.channel_attention = ChannelAttention(c1)\nself.spatial_attention = SpatialAttention(kernel_size)\ndef forward(self, x):\n\"\"\"Applies the forward pass through C1 module.\"\"\"\nreturn self.spatial_attention(self.channel_attention(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.CBAM.forward","title":"<code>forward(x)</code>","text":"<p>Applies the forward pass through C1 module.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Applies the forward pass through C1 module.\"\"\"\nreturn self.spatial_attention(self.channel_attention(x))\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Concat","title":"<code>ultralytics.nn.modules.conv.Concat</code>","text":"<p>             Bases: <code>Module</code></p> <p>Concatenate a list of tensors along dimension.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>class Concat(nn.Module):\n\"\"\"Concatenate a list of tensors along dimension.\"\"\"\ndef __init__(self, dimension=1):\n\"\"\"Concatenates a list of tensors along a specified dimension.\"\"\"\nsuper().__init__()\nself.d = dimension\ndef forward(self, x):\n\"\"\"Forward pass for the YOLOv8 mask Proto module.\"\"\"\nreturn torch.cat(x, self.d)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Concat.__init__","title":"<code>__init__(dimension=1)</code>","text":"<p>Concatenates a list of tensors along a specified dimension.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def __init__(self, dimension=1):\n\"\"\"Concatenates a list of tensors along a specified dimension.\"\"\"\nsuper().__init__()\nself.d = dimension\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.Concat.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass for the YOLOv8 mask Proto module.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass for the YOLOv8 mask Proto module.\"\"\"\nreturn torch.cat(x, self.d)\n</code></pre>"},{"location":"reference/nn/modules/conv/#ultralytics.nn.modules.conv.autopad","title":"<code>ultralytics.nn.modules.conv.autopad(k, p=None, d=1)</code>","text":"<p>Pad to 'same' shape outputs.</p> Source code in <code>ultralytics/nn/modules/conv.py</code> <pre><code>def autopad(k, p=None, d=1):  # kernel, padding, dilation\n\"\"\"Pad to 'same' shape outputs.\"\"\"\nif d &gt; 1:\nk = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\nif p is None:\np = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\nreturn p\n</code></pre>"},{"location":"reference/nn/modules/head/","title":"Reference for <code>ultralytics/nn/modules/head.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/head.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Detect","title":"<code>ultralytics.nn.modules.head.Detect</code>","text":"<p>             Bases: <code>Module</code></p> <p>YOLOv8 Detect head for detection models.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>class Detect(nn.Module):\n\"\"\"YOLOv8 Detect head for detection models.\"\"\"\ndynamic = False  # force grid reconstruction\nexport = False  # export mode\nshape = None\nanchors = torch.empty(0)  # init\nstrides = torch.empty(0)  # init\ndef __init__(self, nc=80, ch=()):  # detection layer\nsuper().__init__()\nself.nc = nc  # number of classes\nself.nl = len(ch)  # number of detection layers\nself.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\nself.no = nc + self.reg_max * 4  # number of outputs per anchor\nself.stride = torch.zeros(self.nl)  # strides computed during build\nc2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channels\nself.cv2 = nn.ModuleList(\nnn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch)\nself.cv3 = nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)\nself.dfl = DFL(self.reg_max) if self.reg_max &gt; 1 else nn.Identity()\ndef forward(self, x):\n\"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\nshape = x[0].shape  # BCHW\nfor i in range(self.nl):\nx[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\nif self.training:\nreturn x\nelif self.dynamic or self.shape != shape:\nself.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\nself.shape = shape\nx_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\nif self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops\nbox = x_cat[:, :self.reg_max * 4]\ncls = x_cat[:, self.reg_max * 4:]\nelse:\nbox, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\ndbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides\nif self.export and self.format in ('tflite', 'edgetpu'):\n# Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:\n# https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309\n# See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695\nimg_h = shape[2] * self.stride[0]\nimg_w = shape[3] * self.stride[0]\nimg_size = torch.tensor([img_w, img_h, img_w, img_h], device=dbox.device).reshape(1, 4, 1)\ndbox /= img_size\ny = torch.cat((dbox, cls.sigmoid()), 1)\nreturn y if self.export else (y, x)\ndef bias_init(self):\n\"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\nm = self  # self.model[-1]  # Detect() module\n# cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n# ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\nfor a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\na[-1].bias.data[:] = 1.0  # box\nb[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Detect.bias_init","title":"<code>bias_init()</code>","text":"<p>Initialize Detect() biases, WARNING: requires stride availability.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def bias_init(self):\n\"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\nm = self  # self.model[-1]  # Detect() module\n# cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n# ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\nfor a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\na[-1].bias.data[:] = 1.0  # box\nb[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Detect.forward","title":"<code>forward(x)</code>","text":"<p>Concatenates and returns predicted bounding boxes and class probabilities.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def forward(self, x):\n\"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\nshape = x[0].shape  # BCHW\nfor i in range(self.nl):\nx[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\nif self.training:\nreturn x\nelif self.dynamic or self.shape != shape:\nself.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\nself.shape = shape\nx_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\nif self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops\nbox = x_cat[:, :self.reg_max * 4]\ncls = x_cat[:, self.reg_max * 4:]\nelse:\nbox, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\ndbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides\nif self.export and self.format in ('tflite', 'edgetpu'):\n# Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:\n# https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309\n# See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695\nimg_h = shape[2] * self.stride[0]\nimg_w = shape[3] * self.stride[0]\nimg_size = torch.tensor([img_w, img_h, img_w, img_h], device=dbox.device).reshape(1, 4, 1)\ndbox /= img_size\ny = torch.cat((dbox, cls.sigmoid()), 1)\nreturn y if self.export else (y, x)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Segment","title":"<code>ultralytics.nn.modules.head.Segment</code>","text":"<p>             Bases: <code>Detect</code></p> <p>YOLOv8 Segment head for segmentation models.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>class Segment(Detect):\n\"\"\"YOLOv8 Segment head for segmentation models.\"\"\"\ndef __init__(self, nc=80, nm=32, npr=256, ch=()):\n\"\"\"Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers.\"\"\"\nsuper().__init__(nc, ch)\nself.nm = nm  # number of masks\nself.npr = npr  # number of protos\nself.proto = Proto(ch[0], self.npr, self.nm)  # protos\nself.detect = Detect.forward\nc4 = max(ch[0] // 4, self.nm)\nself.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\ndef forward(self, x):\n\"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\np = self.proto(x[0])  # mask protos\nbs = p.shape[0]  # batch size\nmc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficients\nx = self.detect(self, x)\nif self.training:\nreturn x, mc, p\nreturn (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Segment.__init__","title":"<code>__init__(nc=80, nm=32, npr=256, ch=())</code>","text":"<p>Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def __init__(self, nc=80, nm=32, npr=256, ch=()):\n\"\"\"Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers.\"\"\"\nsuper().__init__(nc, ch)\nself.nm = nm  # number of masks\nself.npr = npr  # number of protos\nself.proto = Proto(ch[0], self.npr, self.nm)  # protos\nself.detect = Detect.forward\nc4 = max(ch[0] // 4, self.nm)\nself.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Segment.forward","title":"<code>forward(x)</code>","text":"<p>Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def forward(self, x):\n\"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\np = self.proto(x[0])  # mask protos\nbs = p.shape[0]  # batch size\nmc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficients\nx = self.detect(self, x)\nif self.training:\nreturn x, mc, p\nreturn (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Pose","title":"<code>ultralytics.nn.modules.head.Pose</code>","text":"<p>             Bases: <code>Detect</code></p> <p>YOLOv8 Pose head for keypoints models.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>class Pose(Detect):\n\"\"\"YOLOv8 Pose head for keypoints models.\"\"\"\ndef __init__(self, nc=80, kpt_shape=(17, 3), ch=()):\n\"\"\"Initialize YOLO network with default parameters and Convolutional Layers.\"\"\"\nsuper().__init__(nc, ch)\nself.kpt_shape = kpt_shape  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)\nself.nk = kpt_shape[0] * kpt_shape[1]  # number of keypoints total\nself.detect = Detect.forward\nc4 = max(ch[0] // 4, self.nk)\nself.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)\ndef forward(self, x):\n\"\"\"Perform forward pass through YOLO model and return predictions.\"\"\"\nbs = x[0].shape[0]  # batch size\nkpt = torch.cat([self.cv4[i](x[i]).view(bs, self.nk, -1) for i in range(self.nl)], -1)  # (bs, 17*3, h*w)\nx = self.detect(self, x)\nif self.training:\nreturn x, kpt\npred_kpt = self.kpts_decode(bs, kpt)\nreturn torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))\ndef kpts_decode(self, bs, kpts):\n\"\"\"Decodes keypoints.\"\"\"\nndim = self.kpt_shape[1]\nif self.export:  # required for TFLite export to avoid 'PLACEHOLDER_FOR_GREATER_OP_CODES' bug\ny = kpts.view(bs, *self.kpt_shape, -1)\na = (y[:, :, :2] * 2.0 + (self.anchors - 0.5)) * self.strides\nif ndim == 3:\na = torch.cat((a, y[:, :, 2:3].sigmoid()), 2)\nreturn a.view(bs, self.nk, -1)\nelse:\ny = kpts.clone()\nif ndim == 3:\ny[:, 2::3].sigmoid_()  # inplace sigmoid\ny[:, 0::ndim] = (y[:, 0::ndim] * 2.0 + (self.anchors[0] - 0.5)) * self.strides\ny[:, 1::ndim] = (y[:, 1::ndim] * 2.0 + (self.anchors[1] - 0.5)) * self.strides\nreturn y\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Pose.__init__","title":"<code>__init__(nc=80, kpt_shape=(17, 3), ch=())</code>","text":"<p>Initialize YOLO network with default parameters and Convolutional Layers.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):\n\"\"\"Initialize YOLO network with default parameters and Convolutional Layers.\"\"\"\nsuper().__init__(nc, ch)\nself.kpt_shape = kpt_shape  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible)\nself.nk = kpt_shape[0] * kpt_shape[1]  # number of keypoints total\nself.detect = Detect.forward\nc4 = max(ch[0] // 4, self.nk)\nself.cv4 = nn.ModuleList(nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Pose.forward","title":"<code>forward(x)</code>","text":"<p>Perform forward pass through YOLO model and return predictions.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def forward(self, x):\n\"\"\"Perform forward pass through YOLO model and return predictions.\"\"\"\nbs = x[0].shape[0]  # batch size\nkpt = torch.cat([self.cv4[i](x[i]).view(bs, self.nk, -1) for i in range(self.nl)], -1)  # (bs, 17*3, h*w)\nx = self.detect(self, x)\nif self.training:\nreturn x, kpt\npred_kpt = self.kpts_decode(bs, kpt)\nreturn torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Pose.kpts_decode","title":"<code>kpts_decode(bs, kpts)</code>","text":"<p>Decodes keypoints.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def kpts_decode(self, bs, kpts):\n\"\"\"Decodes keypoints.\"\"\"\nndim = self.kpt_shape[1]\nif self.export:  # required for TFLite export to avoid 'PLACEHOLDER_FOR_GREATER_OP_CODES' bug\ny = kpts.view(bs, *self.kpt_shape, -1)\na = (y[:, :, :2] * 2.0 + (self.anchors - 0.5)) * self.strides\nif ndim == 3:\na = torch.cat((a, y[:, :, 2:3].sigmoid()), 2)\nreturn a.view(bs, self.nk, -1)\nelse:\ny = kpts.clone()\nif ndim == 3:\ny[:, 2::3].sigmoid_()  # inplace sigmoid\ny[:, 0::ndim] = (y[:, 0::ndim] * 2.0 + (self.anchors[0] - 0.5)) * self.strides\ny[:, 1::ndim] = (y[:, 1::ndim] * 2.0 + (self.anchors[1] - 0.5)) * self.strides\nreturn y\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Classify","title":"<code>ultralytics.nn.modules.head.Classify</code>","text":"<p>             Bases: <code>Module</code></p> <p>YOLOv8 classification head, i.e. x(b,c1,20,20) to x(b,c2).</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>class Classify(nn.Module):\n\"\"\"YOLOv8 classification head, i.e. x(b,c1,20,20) to x(b,c2).\"\"\"\ndef __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups\nsuper().__init__()\nc_ = 1280  # efficientnet_b0 size\nself.conv = Conv(c1, c_, k, s, p, g)\nself.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)\nself.drop = nn.Dropout(p=0.0, inplace=True)\nself.linear = nn.Linear(c_, c2)  # to x(b,c2)\ndef forward(self, x):\n\"\"\"Performs a forward pass of the YOLO model on input image data.\"\"\"\nif isinstance(x, list):\nx = torch.cat(x, 1)\nx = self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\nreturn x if self.training else x.softmax(1)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.Classify.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass of the YOLO model on input image data.</p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>def forward(self, x):\n\"\"\"Performs a forward pass of the YOLO model on input image data.\"\"\"\nif isinstance(x, list):\nx = torch.cat(x, 1)\nx = self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\nreturn x if self.training else x.softmax(1)\n</code></pre>"},{"location":"reference/nn/modules/head/#ultralytics.nn.modules.head.RTDETRDecoder","title":"<code>ultralytics.nn.modules.head.RTDETRDecoder</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/nn/modules/head.py</code> <pre><code>class RTDETRDecoder(nn.Module):\nexport = False  # export mode\ndef __init__(\nself,\nnc=80,\nch=(512, 1024, 2048),\nhd=256,  # hidden dim\nnq=300,  # num queries\nndp=4,  # num decoder points\nnh=8,  # num head\nndl=6,  # num decoder layers\nd_ffn=1024,  # dim of feedforward\ndropout=0.,\nact=nn.ReLU(),\neval_idx=-1,\n# training args\nnd=100,  # num denoising\nlabel_noise_ratio=0.5,\nbox_noise_scale=1.0,\nlearnt_init_query=False):\nsuper().__init__()\nself.hidden_dim = hd\nself.nhead = nh\nself.nl = len(ch)  # num level\nself.nc = nc\nself.num_queries = nq\nself.num_decoder_layers = ndl\n# backbone feature projection\nself.input_proj = nn.ModuleList(nn.Sequential(nn.Conv2d(x, hd, 1, bias=False), nn.BatchNorm2d(hd)) for x in ch)\n# NOTE: simplified version but it's not consistent with .pt weights.\n# self.input_proj = nn.ModuleList(Conv(x, hd, act=False) for x in ch)\n# Transformer module\ndecoder_layer = DeformableTransformerDecoderLayer(hd, nh, d_ffn, dropout, act, self.nl, ndp)\nself.decoder = DeformableTransformerDecoder(hd, decoder_layer, ndl, eval_idx)\n# denoising part\nself.denoising_class_embed = nn.Embedding(nc, hd)\nself.num_denoising = nd\nself.label_noise_ratio = label_noise_ratio\nself.box_noise_scale = box_noise_scale\n# decoder embedding\nself.learnt_init_query = learnt_init_query\nif learnt_init_query:\nself.tgt_embed = nn.Embedding(nq, hd)\nself.query_pos_head = MLP(4, 2 * hd, hd, num_layers=2)\n# encoder head\nself.enc_output = nn.Sequential(nn.Linear(hd, hd), nn.LayerNorm(hd))\nself.enc_score_head = nn.Linear(hd, nc)\nself.enc_bbox_head = MLP(hd, hd, 4, num_layers=3)\n# decoder head\nself.dec_score_head = nn.ModuleList([nn.Linear(hd, nc) for _ in range(ndl)])\nself.dec_bbox_head = nn.ModuleList([MLP(hd, hd, 4, num_layers=3) for _ in range(ndl)])\nself._reset_parameters()\ndef forward(self, x, batch=None):\nfrom ultralytics.models.utils.ops import get_cdn_group\n# input projection and embedding\nfeats, shapes = self._get_encoder_input(x)\n# prepare denoising training\ndn_embed, dn_bbox, attn_mask, dn_meta = \\\n            get_cdn_group(batch,\nself.nc,\nself.num_queries,\nself.denoising_class_embed.weight,\nself.num_denoising,\nself.label_noise_ratio,\nself.box_noise_scale,\nself.training)\nembed, refer_bbox, enc_bboxes, enc_scores = \\\n            self._get_decoder_input(feats, shapes, dn_embed, dn_bbox)\n# decoder\ndec_bboxes, dec_scores = self.decoder(embed,\nrefer_bbox,\nfeats,\nshapes,\nself.dec_bbox_head,\nself.dec_score_head,\nself.query_pos_head,\nattn_mask=attn_mask)\nx = dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta\nif self.training:\nreturn x\n# (bs, 300, 4+nc)\ny = torch.cat((dec_bboxes.squeeze(0), dec_scores.squeeze(0).sigmoid()), -1)\nreturn y if self.export else (y, x)\ndef _generate_anchors(self, shapes, grid_size=0.05, dtype=torch.float32, device='cpu', eps=1e-2):\nanchors = []\nfor i, (h, w) in enumerate(shapes):\nsy = torch.arange(end=h, dtype=dtype, device=device)\nsx = torch.arange(end=w, dtype=dtype, device=device)\ngrid_y, grid_x = torch.meshgrid(sy, sx, indexing='ij') if TORCH_1_10 else torch.meshgrid(sy, sx)\ngrid_xy = torch.stack([grid_x, grid_y], -1)  # (h, w, 2)\nvalid_WH = torch.tensor([h, w], dtype=dtype, device=device)\ngrid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_WH  # (1, h, w, 2)\nwh = torch.ones_like(grid_xy, dtype=dtype, device=device) * grid_size * (2.0 ** i)\nanchors.append(torch.cat([grid_xy, wh], -1).view(-1, h * w, 4))  # (1, h*w, 4)\nanchors = torch.cat(anchors, 1)  # (1, h*w*nl, 4)\nvalid_mask = ((anchors &gt; eps) * (anchors &lt; 1 - eps)).all(-1, keepdim=True)  # 1, h*w*nl, 1\nanchors = torch.log(anchors / (1 - anchors))\nanchors = anchors.masked_fill(~valid_mask, float('inf'))\nreturn anchors, valid_mask\ndef _get_encoder_input(self, x):\n# get projection features\nx = [self.input_proj[i](feat) for i, feat in enumerate(x)]\n# get encoder inputs\nfeats = []\nshapes = []\nfor feat in x:\nh, w = feat.shape[2:]\n# [b, c, h, w] -&gt; [b, h*w, c]\nfeats.append(feat.flatten(2).permute(0, 2, 1))\n# [nl, 2]\nshapes.append([h, w])\n# [b, h*w, c]\nfeats = torch.cat(feats, 1)\nreturn feats, shapes\ndef _get_decoder_input(self, feats, shapes, dn_embed=None, dn_bbox=None):\nbs = len(feats)\n# prepare input for decoder\nanchors, valid_mask = self._generate_anchors(shapes, dtype=feats.dtype, device=feats.device)\nfeatures = self.enc_output(valid_mask * feats)  # bs, h*w, 256\nenc_outputs_scores = self.enc_score_head(features)  # (bs, h*w, nc)\n# query selection\n# (bs, num_queries)\ntopk_ind = torch.topk(enc_outputs_scores.max(-1).values, self.num_queries, dim=1).indices.view(-1)\n# (bs, num_queries)\nbatch_ind = torch.arange(end=bs, dtype=topk_ind.dtype).unsqueeze(-1).repeat(1, self.num_queries).view(-1)\n# (bs, num_queries, 256)\ntop_k_features = features[batch_ind, topk_ind].view(bs, self.num_queries, -1)\n# (bs, num_queries, 4)\ntop_k_anchors = anchors[:, topk_ind].view(bs, self.num_queries, -1)\n# dynamic anchors + static content\nrefer_bbox = self.enc_bbox_head(top_k_features) + top_k_anchors\nenc_bboxes = refer_bbox.sigmoid()\nif dn_bbox is not None:\nrefer_bbox = torch.cat([dn_bbox, refer_bbox], 1)\nenc_scores = enc_outputs_scores[batch_ind, topk_ind].view(bs, self.num_queries, -1)\nembeddings = self.tgt_embed.weight.unsqueeze(0).repeat(bs, 1, 1) if self.learnt_init_query else top_k_features\nif self.training:\nrefer_bbox = refer_bbox.detach()\nif not self.learnt_init_query:\nembeddings = embeddings.detach()\nif dn_embed is not None:\nembeddings = torch.cat([dn_embed, embeddings], 1)\nreturn embeddings, refer_bbox, enc_bboxes, enc_scores\n# TODO\ndef _reset_parameters(self):\n# class and bbox head init\nbias_cls = bias_init_with_prob(0.01) / 80 * self.nc\n# NOTE: the weight initialization in `linear_init_` would cause NaN when training with custom datasets.\n# linear_init_(self.enc_score_head)\nconstant_(self.enc_score_head.bias, bias_cls)\nconstant_(self.enc_bbox_head.layers[-1].weight, 0.)\nconstant_(self.enc_bbox_head.layers[-1].bias, 0.)\nfor cls_, reg_ in zip(self.dec_score_head, self.dec_bbox_head):\n# linear_init_(cls_)\nconstant_(cls_.bias, bias_cls)\nconstant_(reg_.layers[-1].weight, 0.)\nconstant_(reg_.layers[-1].bias, 0.)\nlinear_init_(self.enc_output[0])\nxavier_uniform_(self.enc_output[0].weight)\nif self.learnt_init_query:\nxavier_uniform_(self.tgt_embed.weight)\nxavier_uniform_(self.query_pos_head.layers[0].weight)\nxavier_uniform_(self.query_pos_head.layers[1].weight)\nfor layer in self.input_proj:\nxavier_uniform_(layer[0].weight)\n</code></pre>"},{"location":"reference/nn/modules/transformer/","title":"Reference for <code>ultralytics/nn/modules/transformer.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/transformer.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerEncoderLayer","title":"<code>ultralytics.nn.modules.transformer.TransformerEncoderLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>Transformer Encoder.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class TransformerEncoderLayer(nn.Module):\n\"\"\"Transformer Encoder.\"\"\"\ndef __init__(self, c1, cm=2048, num_heads=8, dropout=0.0, act=nn.GELU(), normalize_before=False):\nsuper().__init__()\nfrom ...utils.torch_utils import TORCH_1_9\nif not TORCH_1_9:\nraise ModuleNotFoundError(\n'TransformerEncoderLayer() requires torch&gt;=1.9 to use nn.MultiheadAttention(batch_first=True).')\nself.ma = nn.MultiheadAttention(c1, num_heads, dropout=dropout, batch_first=True)\n# Implementation of Feedforward model\nself.fc1 = nn.Linear(c1, cm)\nself.fc2 = nn.Linear(cm, c1)\nself.norm1 = nn.LayerNorm(c1)\nself.norm2 = nn.LayerNorm(c1)\nself.dropout = nn.Dropout(dropout)\nself.dropout1 = nn.Dropout(dropout)\nself.dropout2 = nn.Dropout(dropout)\nself.act = act\nself.normalize_before = normalize_before\ndef with_pos_embed(self, tensor, pos=None):\n\"\"\"Add position embeddings if given.\"\"\"\nreturn tensor if pos is None else tensor + pos\ndef forward_post(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\nq = k = self.with_pos_embed(src, pos)\nsrc2 = self.ma(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\nsrc = src + self.dropout1(src2)\nsrc = self.norm1(src)\nsrc2 = self.fc2(self.dropout(self.act(self.fc1(src))))\nsrc = src + self.dropout2(src2)\nsrc = self.norm2(src)\nreturn src\ndef forward_pre(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\nsrc2 = self.norm1(src)\nq = k = self.with_pos_embed(src2, pos)\nsrc2 = self.ma(q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\nsrc = src + self.dropout1(src2)\nsrc2 = self.norm2(src)\nsrc2 = self.fc2(self.dropout(self.act(self.fc1(src2))))\nsrc = src + self.dropout2(src2)\nreturn src\ndef forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\n\"\"\"Forward propagates the input through the encoder module.\"\"\"\nif self.normalize_before:\nreturn self.forward_pre(src, src_mask, src_key_padding_mask, pos)\nreturn self.forward_post(src, src_mask, src_key_padding_mask, pos)\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerEncoderLayer.forward","title":"<code>forward(src, src_mask=None, src_key_padding_mask=None, pos=None)</code>","text":"<p>Forward propagates the input through the encoder module.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\n\"\"\"Forward propagates the input through the encoder module.\"\"\"\nif self.normalize_before:\nreturn self.forward_pre(src, src_mask, src_key_padding_mask, pos)\nreturn self.forward_post(src, src_mask, src_key_padding_mask, pos)\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerEncoderLayer.with_pos_embed","title":"<code>with_pos_embed(tensor, pos=None)</code>","text":"<p>Add position embeddings if given.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def with_pos_embed(self, tensor, pos=None):\n\"\"\"Add position embeddings if given.\"\"\"\nreturn tensor if pos is None else tensor + pos\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.AIFI","title":"<code>ultralytics.nn.modules.transformer.AIFI</code>","text":"<p>             Bases: <code>TransformerEncoderLayer</code></p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class AIFI(TransformerEncoderLayer):\ndef __init__(self, c1, cm=2048, num_heads=8, dropout=0, act=nn.GELU(), normalize_before=False):\nsuper().__init__(c1, cm, num_heads, dropout, act, normalize_before)\ndef forward(self, x):\nc, h, w = x.shape[1:]\npos_embed = self.build_2d_sincos_position_embedding(w, h, c)\n# flatten [B, C, H, W] to [B, HxW, C]\nx = super().forward(x.flatten(2).permute(0, 2, 1), pos=pos_embed.to(device=x.device, dtype=x.dtype))\nreturn x.permute(0, 2, 1).view([-1, c, h, w]).contiguous()\n@staticmethod\ndef build_2d_sincos_position_embedding(w, h, embed_dim=256, temperature=10000.):\ngrid_w = torch.arange(int(w), dtype=torch.float32)\ngrid_h = torch.arange(int(h), dtype=torch.float32)\ngrid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')\nassert embed_dim % 4 == 0, \\\n            'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\npos_dim = embed_dim // 4\nomega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\nomega = 1. / (temperature ** omega)\nout_w = grid_w.flatten()[..., None] @ omega[None]\nout_h = grid_h.flatten()[..., None] @ omega[None]\nreturn torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], 1)[None]\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerLayer","title":"<code>ultralytics.nn.modules.transformer.TransformerLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance).</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class TransformerLayer(nn.Module):\n\"\"\"Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance).\"\"\"\ndef __init__(self, c, num_heads):\n\"\"\"Initializes a self-attention mechanism using linear transformations and multi-head attention.\"\"\"\nsuper().__init__()\nself.q = nn.Linear(c, c, bias=False)\nself.k = nn.Linear(c, c, bias=False)\nself.v = nn.Linear(c, c, bias=False)\nself.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\nself.fc1 = nn.Linear(c, c, bias=False)\nself.fc2 = nn.Linear(c, c, bias=False)\ndef forward(self, x):\n\"\"\"Apply a transformer block to the input x and return the output.\"\"\"\nx = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\nx = self.fc2(self.fc1(x)) + x\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerLayer.__init__","title":"<code>__init__(c, num_heads)</code>","text":"<p>Initializes a self-attention mechanism using linear transformations and multi-head attention.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def __init__(self, c, num_heads):\n\"\"\"Initializes a self-attention mechanism using linear transformations and multi-head attention.\"\"\"\nsuper().__init__()\nself.q = nn.Linear(c, c, bias=False)\nself.k = nn.Linear(c, c, bias=False)\nself.v = nn.Linear(c, c, bias=False)\nself.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\nself.fc1 = nn.Linear(c, c, bias=False)\nself.fc2 = nn.Linear(c, c, bias=False)\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerLayer.forward","title":"<code>forward(x)</code>","text":"<p>Apply a transformer block to the input x and return the output.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def forward(self, x):\n\"\"\"Apply a transformer block to the input x and return the output.\"\"\"\nx = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\nx = self.fc2(self.fc1(x)) + x\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerBlock","title":"<code>ultralytics.nn.modules.transformer.TransformerBlock</code>","text":"<p>             Bases: <code>Module</code></p> <p>Vision Transformer https://arxiv.org/abs/2010.11929.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class TransformerBlock(nn.Module):\n\"\"\"Vision Transformer https://arxiv.org/abs/2010.11929.\"\"\"\ndef __init__(self, c1, c2, num_heads, num_layers):\n\"\"\"Initialize a Transformer module with position embedding and specified number of heads and layers.\"\"\"\nsuper().__init__()\nself.conv = None\nif c1 != c2:\nself.conv = Conv(c1, c2)\nself.linear = nn.Linear(c2, c2)  # learnable position embedding\nself.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))\nself.c2 = c2\ndef forward(self, x):\n\"\"\"Forward propagates the input through the bottleneck module.\"\"\"\nif self.conv is not None:\nx = self.conv(x)\nb, _, w, h = x.shape\np = x.flatten(2).permute(2, 0, 1)\nreturn self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerBlock.__init__","title":"<code>__init__(c1, c2, num_heads, num_layers)</code>","text":"<p>Initialize a Transformer module with position embedding and specified number of heads and layers.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def __init__(self, c1, c2, num_heads, num_layers):\n\"\"\"Initialize a Transformer module with position embedding and specified number of heads and layers.\"\"\"\nsuper().__init__()\nself.conv = None\nif c1 != c2:\nself.conv = Conv(c1, c2)\nself.linear = nn.Linear(c2, c2)  # learnable position embedding\nself.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))\nself.c2 = c2\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.TransformerBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward propagates the input through the bottleneck module.</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward propagates the input through the bottleneck module.\"\"\"\nif self.conv is not None:\nx = self.conv(x)\nb, _, w, h = x.shape\np = x.flatten(2).permute(2, 0, 1)\nreturn self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.MLPBlock","title":"<code>ultralytics.nn.modules.transformer.MLPBlock</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class MLPBlock(nn.Module):\ndef __init__(self, embedding_dim, mlp_dim, act=nn.GELU):\nsuper().__init__()\nself.lin1 = nn.Linear(embedding_dim, mlp_dim)\nself.lin2 = nn.Linear(mlp_dim, embedding_dim)\nself.act = act()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.lin2(self.act(self.lin1(x)))\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.MLP","title":"<code>ultralytics.nn.modules.transformer.MLP</code>","text":"<p>             Bases: <code>Module</code></p> <p>Very simple multi-layer perceptron (also called FFN)</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class MLP(nn.Module):\n\"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\ndef __init__(self, input_dim, hidden_dim, output_dim, num_layers):\nsuper().__init__()\nself.num_layers = num_layers\nh = [hidden_dim] * (num_layers - 1)\nself.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\ndef forward(self, x):\nfor i, layer in enumerate(self.layers):\nx = F.relu(layer(x)) if i &lt; self.num_layers - 1 else layer(x)\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.LayerNorm2d","title":"<code>ultralytics.nn.modules.transformer.LayerNorm2d</code>","text":"<p>             Bases: <code>Module</code></p> <p>LayerNorm2d module from https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class LayerNorm2d(nn.Module):\n\"\"\"\n    LayerNorm2d module from https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py\n    https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119\n    \"\"\"\ndef __init__(self, num_channels, eps=1e-6):\nsuper().__init__()\nself.weight = nn.Parameter(torch.ones(num_channels))\nself.bias = nn.Parameter(torch.zeros(num_channels))\nself.eps = eps\ndef forward(self, x):\nu = x.mean(1, keepdim=True)\ns = (x - u).pow(2).mean(1, keepdim=True)\nx = (x - u) / torch.sqrt(s + self.eps)\nx = self.weight[:, None, None] * x + self.bias[:, None, None]\nreturn x\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.MSDeformAttn","title":"<code>ultralytics.nn.modules.transformer.MSDeformAttn</code>","text":"<p>             Bases: <code>Module</code></p> <p>Original Multi-Scale Deformable Attention Module. https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/ops/modules/ms_deform_attn.py</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class MSDeformAttn(nn.Module):\n\"\"\"\n    Original Multi-Scale Deformable Attention Module.\n    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/ops/modules/ms_deform_attn.py\n    \"\"\"\ndef __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\nsuper().__init__()\nif d_model % n_heads != 0:\nraise ValueError(f'd_model must be divisible by n_heads, but got {d_model} and {n_heads}')\n_d_per_head = d_model // n_heads\n# you'd better set _d_per_head to a power of 2 which is more efficient in our CUDA implementation\nassert _d_per_head * n_heads == d_model, '`d_model` must be divisible by `n_heads`'\nself.im2col_step = 64\nself.d_model = d_model\nself.n_levels = n_levels\nself.n_heads = n_heads\nself.n_points = n_points\nself.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\nself.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\nself.value_proj = nn.Linear(d_model, d_model)\nself.output_proj = nn.Linear(d_model, d_model)\nself._reset_parameters()\ndef _reset_parameters(self):\nconstant_(self.sampling_offsets.weight.data, 0.)\nthetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\ngrid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\ngrid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(\n1, self.n_levels, self.n_points, 1)\nfor i in range(self.n_points):\ngrid_init[:, :, i, :] *= i + 1\nwith torch.no_grad():\nself.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\nconstant_(self.attention_weights.weight.data, 0.)\nconstant_(self.attention_weights.bias.data, 0.)\nxavier_uniform_(self.value_proj.weight.data)\nconstant_(self.value_proj.bias.data, 0.)\nxavier_uniform_(self.output_proj.weight.data)\nconstant_(self.output_proj.bias.data, 0.)\ndef forward(self, query, refer_bbox, value, value_shapes, value_mask=None):\n\"\"\"\n        https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py\n        Args:\n            query (torch.Tensor): [bs, query_length, C]\n            refer_bbox (torch.Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),\n                bottom-right (1, 1), including padding area\n            value (torch.Tensor): [bs, value_length, C]\n            value_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n            value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements\n        Returns:\n            output (Tensor): [bs, Length_{query}, C]\n        \"\"\"\nbs, len_q = query.shape[:2]\nlen_v = value.shape[1]\nassert sum(s[0] * s[1] for s in value_shapes) == len_v\nvalue = self.value_proj(value)\nif value_mask is not None:\nvalue = value.masked_fill(value_mask[..., None], float(0))\nvalue = value.view(bs, len_v, self.n_heads, self.d_model // self.n_heads)\nsampling_offsets = self.sampling_offsets(query).view(bs, len_q, self.n_heads, self.n_levels, self.n_points, 2)\nattention_weights = self.attention_weights(query).view(bs, len_q, self.n_heads, self.n_levels * self.n_points)\nattention_weights = F.softmax(attention_weights, -1).view(bs, len_q, self.n_heads, self.n_levels, self.n_points)\n# N, Len_q, n_heads, n_levels, n_points, 2\nnum_points = refer_bbox.shape[-1]\nif num_points == 2:\noffset_normalizer = torch.as_tensor(value_shapes, dtype=query.dtype, device=query.device).flip(-1)\nadd = sampling_offsets / offset_normalizer[None, None, None, :, None, :]\nsampling_locations = refer_bbox[:, :, None, :, None, :] + add\nelif num_points == 4:\nadd = sampling_offsets / self.n_points * refer_bbox[:, :, None, :, None, 2:] * 0.5\nsampling_locations = refer_bbox[:, :, None, :, None, :2] + add\nelse:\nraise ValueError(f'Last dim of reference_points must be 2 or 4, but got {num_points}.')\noutput = multi_scale_deformable_attn_pytorch(value, value_shapes, sampling_locations, attention_weights)\noutput = self.output_proj(output)\nreturn output\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.MSDeformAttn.forward","title":"<code>forward(query, refer_bbox, value, value_shapes, value_mask=None)</code>","text":"<p>https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>[bs, query_length, C]</p> required <code>refer_bbox</code> <code>Tensor</code> <p>[bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0), bottom-right (1, 1), including padding area</p> required <code>value</code> <code>Tensor</code> <p>[bs, value_length, C]</p> required <code>value_shapes</code> <code>List</code> <p>[n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]</p> required <code>value_mask</code> <code>Tensor</code> <p>[bs, value_length], True for non-padding elements, False for padding elements</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>[bs, Length_{query}, C]</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>def forward(self, query, refer_bbox, value, value_shapes, value_mask=None):\n\"\"\"\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py\n    Args:\n        query (torch.Tensor): [bs, query_length, C]\n        refer_bbox (torch.Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),\n            bottom-right (1, 1), including padding area\n        value (torch.Tensor): [bs, value_length, C]\n        value_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n        value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements\n    Returns:\n        output (Tensor): [bs, Length_{query}, C]\n    \"\"\"\nbs, len_q = query.shape[:2]\nlen_v = value.shape[1]\nassert sum(s[0] * s[1] for s in value_shapes) == len_v\nvalue = self.value_proj(value)\nif value_mask is not None:\nvalue = value.masked_fill(value_mask[..., None], float(0))\nvalue = value.view(bs, len_v, self.n_heads, self.d_model // self.n_heads)\nsampling_offsets = self.sampling_offsets(query).view(bs, len_q, self.n_heads, self.n_levels, self.n_points, 2)\nattention_weights = self.attention_weights(query).view(bs, len_q, self.n_heads, self.n_levels * self.n_points)\nattention_weights = F.softmax(attention_weights, -1).view(bs, len_q, self.n_heads, self.n_levels, self.n_points)\n# N, Len_q, n_heads, n_levels, n_points, 2\nnum_points = refer_bbox.shape[-1]\nif num_points == 2:\noffset_normalizer = torch.as_tensor(value_shapes, dtype=query.dtype, device=query.device).flip(-1)\nadd = sampling_offsets / offset_normalizer[None, None, None, :, None, :]\nsampling_locations = refer_bbox[:, :, None, :, None, :] + add\nelif num_points == 4:\nadd = sampling_offsets / self.n_points * refer_bbox[:, :, None, :, None, 2:] * 0.5\nsampling_locations = refer_bbox[:, :, None, :, None, :2] + add\nelse:\nraise ValueError(f'Last dim of reference_points must be 2 or 4, but got {num_points}.')\noutput = multi_scale_deformable_attn_pytorch(value, value_shapes, sampling_locations, attention_weights)\noutput = self.output_proj(output)\nreturn output\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.DeformableTransformerDecoderLayer","title":"<code>ultralytics.nn.modules.transformer.DeformableTransformerDecoderLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/deformable_transformer.py</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class DeformableTransformerDecoderLayer(nn.Module):\n\"\"\"\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py\n    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/deformable_transformer.py\n    \"\"\"\ndef __init__(self, d_model=256, n_heads=8, d_ffn=1024, dropout=0., act=nn.ReLU(), n_levels=4, n_points=4):\nsuper().__init__()\n# self attention\nself.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\nself.dropout1 = nn.Dropout(dropout)\nself.norm1 = nn.LayerNorm(d_model)\n# cross attention\nself.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)\nself.dropout2 = nn.Dropout(dropout)\nself.norm2 = nn.LayerNorm(d_model)\n# ffn\nself.linear1 = nn.Linear(d_model, d_ffn)\nself.act = act\nself.dropout3 = nn.Dropout(dropout)\nself.linear2 = nn.Linear(d_ffn, d_model)\nself.dropout4 = nn.Dropout(dropout)\nself.norm3 = nn.LayerNorm(d_model)\n@staticmethod\ndef with_pos_embed(tensor, pos):\nreturn tensor if pos is None else tensor + pos\ndef forward_ffn(self, tgt):\ntgt2 = self.linear2(self.dropout3(self.act(self.linear1(tgt))))\ntgt = tgt + self.dropout4(tgt2)\ntgt = self.norm3(tgt)\nreturn tgt\ndef forward(self, embed, refer_bbox, feats, shapes, padding_mask=None, attn_mask=None, query_pos=None):\n# self attention\nq = k = self.with_pos_embed(embed, query_pos)\ntgt = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), embed.transpose(0, 1),\nattn_mask=attn_mask)[0].transpose(0, 1)\nembed = embed + self.dropout1(tgt)\nembed = self.norm1(embed)\n# cross attention\ntgt = self.cross_attn(self.with_pos_embed(embed, query_pos), refer_bbox.unsqueeze(2), feats, shapes,\npadding_mask)\nembed = embed + self.dropout2(tgt)\nembed = self.norm2(embed)\n# ffn\nembed = self.forward_ffn(embed)\nreturn embed\n</code></pre>"},{"location":"reference/nn/modules/transformer/#ultralytics.nn.modules.transformer.DeformableTransformerDecoder","title":"<code>ultralytics.nn.modules.transformer.DeformableTransformerDecoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py</p> Source code in <code>ultralytics/nn/modules/transformer.py</code> <pre><code>class DeformableTransformerDecoder(nn.Module):\n\"\"\"\n    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py\n    \"\"\"\ndef __init__(self, hidden_dim, decoder_layer, num_layers, eval_idx=-1):\nsuper().__init__()\nself.layers = _get_clones(decoder_layer, num_layers)\nself.num_layers = num_layers\nself.hidden_dim = hidden_dim\nself.eval_idx = eval_idx if eval_idx &gt;= 0 else num_layers + eval_idx\ndef forward(\nself,\nembed,  # decoder embeddings\nrefer_bbox,  # anchor\nfeats,  # image features\nshapes,  # feature shapes\nbbox_head,\nscore_head,\npos_mlp,\nattn_mask=None,\npadding_mask=None):\noutput = embed\ndec_bboxes = []\ndec_cls = []\nlast_refined_bbox = None\nrefer_bbox = refer_bbox.sigmoid()\nfor i, layer in enumerate(self.layers):\noutput = layer(output, refer_bbox, feats, shapes, padding_mask, attn_mask, pos_mlp(refer_bbox))\n# refine bboxes, (bs, num_queries+num_denoising, 4)\nrefined_bbox = torch.sigmoid(bbox_head[i](output) + inverse_sigmoid(refer_bbox))\nif self.training:\ndec_cls.append(score_head[i](output))\nif i == 0:\ndec_bboxes.append(refined_bbox)\nelse:\ndec_bboxes.append(torch.sigmoid(bbox_head[i](output) + inverse_sigmoid(last_refined_bbox)))\nelif i == self.eval_idx:\ndec_cls.append(score_head[i](output))\ndec_bboxes.append(refined_bbox)\nbreak\nlast_refined_bbox = refined_bbox\nrefer_bbox = refined_bbox.detach() if self.training else refined_bbox\nreturn torch.stack(dec_bboxes), torch.stack(dec_cls)\n</code></pre>"},{"location":"reference/nn/modules/utils/","title":"Reference for <code>ultralytics/nn/modules/utils.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/utils.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/nn/modules/utils/#ultralytics.nn.modules.utils._get_clones","title":"<code>ultralytics.nn.modules.utils._get_clones(module, n)</code>","text":"Source code in <code>ultralytics/nn/modules/utils.py</code> <pre><code>def _get_clones(module, n):\nreturn nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n</code></pre>"},{"location":"reference/nn/modules/utils/#ultralytics.nn.modules.utils.bias_init_with_prob","title":"<code>ultralytics.nn.modules.utils.bias_init_with_prob(prior_prob=0.01)</code>","text":"<p>initialize conv/fc bias value according to a given probability value.</p> Source code in <code>ultralytics/nn/modules/utils.py</code> <pre><code>def bias_init_with_prob(prior_prob=0.01):\n\"\"\"initialize conv/fc bias value according to a given probability value.\"\"\"\nreturn float(-np.log((1 - prior_prob) / prior_prob))  # return bias_init\n</code></pre>"},{"location":"reference/nn/modules/utils/#ultralytics.nn.modules.utils.linear_init_","title":"<code>ultralytics.nn.modules.utils.linear_init_(module)</code>","text":"Source code in <code>ultralytics/nn/modules/utils.py</code> <pre><code>def linear_init_(module):\nbound = 1 / math.sqrt(module.weight.shape[0])\nuniform_(module.weight, -bound, bound)\nif hasattr(module, 'bias') and module.bias is not None:\nuniform_(module.bias, -bound, bound)\n</code></pre>"},{"location":"reference/nn/modules/utils/#ultralytics.nn.modules.utils.inverse_sigmoid","title":"<code>ultralytics.nn.modules.utils.inverse_sigmoid(x, eps=1e-05)</code>","text":"Source code in <code>ultralytics/nn/modules/utils.py</code> <pre><code>def inverse_sigmoid(x, eps=1e-5):\nx = x.clamp(min=0, max=1)\nx1 = x.clamp(min=eps)\nx2 = (1 - x).clamp(min=eps)\nreturn torch.log(x1 / x2)\n</code></pre>"},{"location":"reference/nn/modules/utils/#ultralytics.nn.modules.utils.multi_scale_deformable_attn_pytorch","title":"<code>ultralytics.nn.modules.utils.multi_scale_deformable_attn_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights)</code>","text":"<p>Multi-scale deformable attention. https://github.com/IDEA-Research/detrex/blob/main/detrex/layers/multi_scale_deform_attn.py</p> Source code in <code>ultralytics/nn/modules/utils.py</code> <pre><code>def multi_scale_deformable_attn_pytorch(value: torch.Tensor, value_spatial_shapes: torch.Tensor,\nsampling_locations: torch.Tensor,\nattention_weights: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Multi-scale deformable attention.\n    https://github.com/IDEA-Research/detrex/blob/main/detrex/layers/multi_scale_deform_attn.py\n    \"\"\"\nbs, _, num_heads, embed_dims = value.shape\n_, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\nvalue_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\nsampling_grids = 2 * sampling_locations - 1\nsampling_value_list = []\nfor level, (H_, W_) in enumerate(value_spatial_shapes):\n# bs, H_*W_, num_heads, embed_dims -&gt;\n# bs, H_*W_, num_heads*embed_dims -&gt;\n# bs, num_heads*embed_dims, H_*W_ -&gt;\n# bs*num_heads, embed_dims, H_, W_\nvalue_l_ = (value_list[level].flatten(2).transpose(1, 2).reshape(bs * num_heads, embed_dims, H_, W_))\n# bs, num_queries, num_heads, num_points, 2 -&gt;\n# bs, num_heads, num_queries, num_points, 2 -&gt;\n# bs*num_heads, num_queries, num_points, 2\nsampling_grid_l_ = sampling_grids[:, :, :, level].transpose(1, 2).flatten(0, 1)\n# bs*num_heads, embed_dims, num_queries, num_points\nsampling_value_l_ = F.grid_sample(value_l_,\nsampling_grid_l_,\nmode='bilinear',\npadding_mode='zeros',\nalign_corners=False)\nsampling_value_list.append(sampling_value_l_)\n# (bs, num_queries, num_heads, num_levels, num_points) -&gt;\n# (bs, num_heads, num_queries, num_levels, num_points) -&gt;\n# (bs, num_heads, 1, num_queries, num_levels*num_points)\nattention_weights = attention_weights.transpose(1, 2).reshape(bs * num_heads, 1, num_queries,\nnum_levels * num_points)\noutput = ((torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).view(\nbs, num_heads * embed_dims, num_queries))\nreturn output.transpose(1, 2).contiguous()\n</code></pre>"},{"location":"reference/trackers/basetrack/","title":"Reference for <code>ultralytics/trackers/basetrack.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/basetrack.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.TrackState","title":"<code>ultralytics.trackers.basetrack.TrackState</code>","text":"<p>Enumeration of possible object tracking states.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>class TrackState:\n\"\"\"Enumeration of possible object tracking states.\"\"\"\nNew = 0\nTracked = 1\nLost = 2\nRemoved = 3\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack","title":"<code>ultralytics.trackers.basetrack.BaseTrack</code>","text":"<p>Base class for object tracking, handling basic track attributes and operations.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>class BaseTrack:\n\"\"\"Base class for object tracking, handling basic track attributes and operations.\"\"\"\n_count = 0\ntrack_id = 0\nis_activated = False\nstate = TrackState.New\nhistory = OrderedDict()\nfeatures = []\ncurr_feature = None\nscore = 0\nstart_frame = 0\nframe_id = 0\ntime_since_update = 0\n# Multi-camera\nlocation = (np.inf, np.inf)\n@property\ndef end_frame(self):\n\"\"\"Return the last frame ID of the track.\"\"\"\nreturn self.frame_id\n@staticmethod\ndef next_id():\n\"\"\"Increment and return the global track ID counter.\"\"\"\nBaseTrack._count += 1\nreturn BaseTrack._count\ndef activate(self, *args):\n\"\"\"Activate the track with the provided arguments.\"\"\"\nraise NotImplementedError\ndef predict(self):\n\"\"\"Predict the next state of the track.\"\"\"\nraise NotImplementedError\ndef update(self, *args, **kwargs):\n\"\"\"Update the track with new observations.\"\"\"\nraise NotImplementedError\ndef mark_lost(self):\n\"\"\"Mark the track as lost.\"\"\"\nself.state = TrackState.Lost\ndef mark_removed(self):\n\"\"\"Mark the track as removed.\"\"\"\nself.state = TrackState.Removed\n@staticmethod\ndef reset_id():\n\"\"\"Reset the global track ID counter.\"\"\"\nBaseTrack._count = 0\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.end_frame","title":"<code>end_frame</code>  <code>property</code>","text":"<p>Return the last frame ID of the track.</p>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.activate","title":"<code>activate(*args)</code>","text":"<p>Activate the track with the provided arguments.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>def activate(self, *args):\n\"\"\"Activate the track with the provided arguments.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.mark_lost","title":"<code>mark_lost()</code>","text":"<p>Mark the track as lost.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>def mark_lost(self):\n\"\"\"Mark the track as lost.\"\"\"\nself.state = TrackState.Lost\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.mark_removed","title":"<code>mark_removed()</code>","text":"<p>Mark the track as removed.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>def mark_removed(self):\n\"\"\"Mark the track as removed.\"\"\"\nself.state = TrackState.Removed\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.next_id","title":"<code>next_id()</code>  <code>staticmethod</code>","text":"<p>Increment and return the global track ID counter.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>@staticmethod\ndef next_id():\n\"\"\"Increment and return the global track ID counter.\"\"\"\nBaseTrack._count += 1\nreturn BaseTrack._count\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.predict","title":"<code>predict()</code>","text":"<p>Predict the next state of the track.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>def predict(self):\n\"\"\"Predict the next state of the track.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.reset_id","title":"<code>reset_id()</code>  <code>staticmethod</code>","text":"<p>Reset the global track ID counter.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>@staticmethod\ndef reset_id():\n\"\"\"Reset the global track ID counter.\"\"\"\nBaseTrack._count = 0\n</code></pre>"},{"location":"reference/trackers/basetrack/#ultralytics.trackers.basetrack.BaseTrack.update","title":"<code>update(*args, **kwargs)</code>","text":"<p>Update the track with new observations.</p> Source code in <code>ultralytics/trackers/basetrack.py</code> <pre><code>def update(self, *args, **kwargs):\n\"\"\"Update the track with new observations.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/trackers/bot_sort/","title":"Reference for <code>ultralytics/trackers/bot_sort.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/bot_sort.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack","title":"<code>ultralytics.trackers.bot_sort.BOTrack</code>","text":"<p>             Bases: <code>STrack</code></p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>class BOTrack(STrack):\nshared_kalman = KalmanFilterXYWH()\ndef __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\"\"\"Initialize YOLOv8 object with temporal parameters, such as feature history, alpha and current features.\"\"\"\nsuper().__init__(tlwh, score, cls)\nself.smooth_feat = None\nself.curr_feat = None\nif feat is not None:\nself.update_features(feat)\nself.features = deque([], maxlen=feat_history)\nself.alpha = 0.9\ndef update_features(self, feat):\n\"\"\"Update features vector and smooth it using exponential moving average.\"\"\"\nfeat /= np.linalg.norm(feat)\nself.curr_feat = feat\nif self.smooth_feat is None:\nself.smooth_feat = feat\nelse:\nself.smooth_feat = self.alpha * self.smooth_feat + (1 - self.alpha) * feat\nself.features.append(feat)\nself.smooth_feat /= np.linalg.norm(self.smooth_feat)\ndef predict(self):\n\"\"\"Predicts the mean and covariance using Kalman filter.\"\"\"\nmean_state = self.mean.copy()\nif self.state != TrackState.Tracked:\nmean_state[6] = 0\nmean_state[7] = 0\nself.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\ndef re_activate(self, new_track, frame_id, new_id=False):\n\"\"\"Reactivates a track with updated features and optionally assigns a new ID.\"\"\"\nif new_track.curr_feat is not None:\nself.update_features(new_track.curr_feat)\nsuper().re_activate(new_track, frame_id, new_id)\ndef update(self, new_track, frame_id):\n\"\"\"Update the YOLOv8 instance with new track and frame ID.\"\"\"\nif new_track.curr_feat is not None:\nself.update_features(new_track.curr_feat)\nsuper().update(new_track, frame_id)\n@property\ndef tlwh(self):\n\"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n        \"\"\"\nif self.mean is None:\nreturn self._tlwh.copy()\nret = self.mean[:4].copy()\nret[:2] -= ret[2:] / 2\nreturn ret\n@staticmethod\ndef multi_predict(stracks):\n\"\"\"Predicts the mean and covariance of multiple object tracks using shared Kalman filter.\"\"\"\nif len(stracks) &lt;= 0:\nreturn\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nfor i, st in enumerate(stracks):\nif st.state != TrackState.Tracked:\nmulti_mean[i][6] = 0\nmulti_mean[i][7] = 0\nmulti_mean, multi_covariance = BOTrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nstracks[i].mean = mean\nstracks[i].covariance = cov\ndef convert_coords(self, tlwh):\n\"\"\"Converts Top-Left-Width-Height bounding box coordinates to X-Y-Width-Height format.\"\"\"\nreturn self.tlwh_to_xywh(tlwh)\n@staticmethod\ndef tlwh_to_xywh(tlwh):\n\"\"\"Convert bounding box to format `(center x, center y, width,\n        height)`.\n        \"\"\"\nret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nreturn ret\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.tlwh","title":"<code>tlwh</code>  <code>property</code>","text":"<p>Get current position in bounding box format <code>(top left x, top left y, width, height)</code>.</p>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.__init__","title":"<code>__init__(tlwh, score, cls, feat=None, feat_history=50)</code>","text":"<p>Initialize YOLOv8 object with temporal parameters, such as feature history, alpha and current features.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def __init__(self, tlwh, score, cls, feat=None, feat_history=50):\n\"\"\"Initialize YOLOv8 object with temporal parameters, such as feature history, alpha and current features.\"\"\"\nsuper().__init__(tlwh, score, cls)\nself.smooth_feat = None\nself.curr_feat = None\nif feat is not None:\nself.update_features(feat)\nself.features = deque([], maxlen=feat_history)\nself.alpha = 0.9\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.convert_coords","title":"<code>convert_coords(tlwh)</code>","text":"<p>Converts Top-Left-Width-Height bounding box coordinates to X-Y-Width-Height format.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def convert_coords(self, tlwh):\n\"\"\"Converts Top-Left-Width-Height bounding box coordinates to X-Y-Width-Height format.\"\"\"\nreturn self.tlwh_to_xywh(tlwh)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.multi_predict","title":"<code>multi_predict(stracks)</code>  <code>staticmethod</code>","text":"<p>Predicts the mean and covariance of multiple object tracks using shared Kalman filter.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>@staticmethod\ndef multi_predict(stracks):\n\"\"\"Predicts the mean and covariance of multiple object tracks using shared Kalman filter.\"\"\"\nif len(stracks) &lt;= 0:\nreturn\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nfor i, st in enumerate(stracks):\nif st.state != TrackState.Tracked:\nmulti_mean[i][6] = 0\nmulti_mean[i][7] = 0\nmulti_mean, multi_covariance = BOTrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nstracks[i].mean = mean\nstracks[i].covariance = cov\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.predict","title":"<code>predict()</code>","text":"<p>Predicts the mean and covariance using Kalman filter.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def predict(self):\n\"\"\"Predicts the mean and covariance using Kalman filter.\"\"\"\nmean_state = self.mean.copy()\nif self.state != TrackState.Tracked:\nmean_state[6] = 0\nmean_state[7] = 0\nself.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.re_activate","title":"<code>re_activate(new_track, frame_id, new_id=False)</code>","text":"<p>Reactivates a track with updated features and optionally assigns a new ID.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def re_activate(self, new_track, frame_id, new_id=False):\n\"\"\"Reactivates a track with updated features and optionally assigns a new ID.\"\"\"\nif new_track.curr_feat is not None:\nself.update_features(new_track.curr_feat)\nsuper().re_activate(new_track, frame_id, new_id)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.tlwh_to_xywh","title":"<code>tlwh_to_xywh(tlwh)</code>  <code>staticmethod</code>","text":"<p>Convert bounding box to format <code>(center x, center y, width, height)</code>.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>@staticmethod\ndef tlwh_to_xywh(tlwh):\n\"\"\"Convert bounding box to format `(center x, center y, width,\n    height)`.\n    \"\"\"\nret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nreturn ret\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.update","title":"<code>update(new_track, frame_id)</code>","text":"<p>Update the YOLOv8 instance with new track and frame ID.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def update(self, new_track, frame_id):\n\"\"\"Update the YOLOv8 instance with new track and frame ID.\"\"\"\nif new_track.curr_feat is not None:\nself.update_features(new_track.curr_feat)\nsuper().update(new_track, frame_id)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTrack.update_features","title":"<code>update_features(feat)</code>","text":"<p>Update features vector and smooth it using exponential moving average.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def update_features(self, feat):\n\"\"\"Update features vector and smooth it using exponential moving average.\"\"\"\nfeat /= np.linalg.norm(feat)\nself.curr_feat = feat\nif self.smooth_feat is None:\nself.smooth_feat = feat\nelse:\nself.smooth_feat = self.alpha * self.smooth_feat + (1 - self.alpha) * feat\nself.features.append(feat)\nself.smooth_feat /= np.linalg.norm(self.smooth_feat)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT","title":"<code>ultralytics.trackers.bot_sort.BOTSORT</code>","text":"<p>             Bases: <code>BYTETracker</code></p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>class BOTSORT(BYTETracker):\ndef __init__(self, args, frame_rate=30):\n\"\"\"Initialize YOLOv8 object with ReID module and GMC algorithm.\"\"\"\nsuper().__init__(args, frame_rate)\n# ReID module\nself.proximity_thresh = args.proximity_thresh\nself.appearance_thresh = args.appearance_thresh\nif args.with_reid:\n# Haven't supported BoT-SORT(reid) yet\nself.encoder = None\nself.gmc = GMC(method=args.gmc_method)\ndef get_kalmanfilter(self):\n\"\"\"Returns an instance of KalmanFilterXYWH for object tracking.\"\"\"\nreturn KalmanFilterXYWH()\ndef init_track(self, dets, scores, cls, img=None):\n\"\"\"Initialize track with detections, scores, and classes.\"\"\"\nif len(dets) == 0:\nreturn []\nif self.args.with_reid and self.encoder is not None:\nfeatures_keep = self.encoder.inference(img, dets)\nreturn [BOTrack(xyxy, s, c, f) for (xyxy, s, c, f) in zip(dets, scores, cls, features_keep)]  # detections\nelse:\nreturn [BOTrack(xyxy, s, c) for (xyxy, s, c) in zip(dets, scores, cls)]  # detections\ndef get_dists(self, tracks, detections):\n\"\"\"Get distances between tracks and detections using IoU and (optionally) ReID embeddings.\"\"\"\ndists = matching.iou_distance(tracks, detections)\ndists_mask = (dists &gt; self.proximity_thresh)\n# TODO: mot20\n# if not self.args.mot20:\ndists = matching.fuse_score(dists, detections)\nif self.args.with_reid and self.encoder is not None:\nemb_dists = matching.embedding_distance(tracks, detections) / 2.0\nemb_dists[emb_dists &gt; self.appearance_thresh] = 1.0\nemb_dists[dists_mask] = 1.0\ndists = np.minimum(dists, emb_dists)\nreturn dists\ndef multi_predict(self, tracks):\n\"\"\"Predict and track multiple objects with YOLOv8 model.\"\"\"\nBOTrack.multi_predict(tracks)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT.__init__","title":"<code>__init__(args, frame_rate=30)</code>","text":"<p>Initialize YOLOv8 object with ReID module and GMC algorithm.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def __init__(self, args, frame_rate=30):\n\"\"\"Initialize YOLOv8 object with ReID module and GMC algorithm.\"\"\"\nsuper().__init__(args, frame_rate)\n# ReID module\nself.proximity_thresh = args.proximity_thresh\nself.appearance_thresh = args.appearance_thresh\nif args.with_reid:\n# Haven't supported BoT-SORT(reid) yet\nself.encoder = None\nself.gmc = GMC(method=args.gmc_method)\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT.get_dists","title":"<code>get_dists(tracks, detections)</code>","text":"<p>Get distances between tracks and detections using IoU and (optionally) ReID embeddings.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def get_dists(self, tracks, detections):\n\"\"\"Get distances between tracks and detections using IoU and (optionally) ReID embeddings.\"\"\"\ndists = matching.iou_distance(tracks, detections)\ndists_mask = (dists &gt; self.proximity_thresh)\n# TODO: mot20\n# if not self.args.mot20:\ndists = matching.fuse_score(dists, detections)\nif self.args.with_reid and self.encoder is not None:\nemb_dists = matching.embedding_distance(tracks, detections) / 2.0\nemb_dists[emb_dists &gt; self.appearance_thresh] = 1.0\nemb_dists[dists_mask] = 1.0\ndists = np.minimum(dists, emb_dists)\nreturn dists\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT.get_kalmanfilter","title":"<code>get_kalmanfilter()</code>","text":"<p>Returns an instance of KalmanFilterXYWH for object tracking.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def get_kalmanfilter(self):\n\"\"\"Returns an instance of KalmanFilterXYWH for object tracking.\"\"\"\nreturn KalmanFilterXYWH()\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT.init_track","title":"<code>init_track(dets, scores, cls, img=None)</code>","text":"<p>Initialize track with detections, scores, and classes.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def init_track(self, dets, scores, cls, img=None):\n\"\"\"Initialize track with detections, scores, and classes.\"\"\"\nif len(dets) == 0:\nreturn []\nif self.args.with_reid and self.encoder is not None:\nfeatures_keep = self.encoder.inference(img, dets)\nreturn [BOTrack(xyxy, s, c, f) for (xyxy, s, c, f) in zip(dets, scores, cls, features_keep)]  # detections\nelse:\nreturn [BOTrack(xyxy, s, c) for (xyxy, s, c) in zip(dets, scores, cls)]  # detections\n</code></pre>"},{"location":"reference/trackers/bot_sort/#ultralytics.trackers.bot_sort.BOTSORT.multi_predict","title":"<code>multi_predict(tracks)</code>","text":"<p>Predict and track multiple objects with YOLOv8 model.</p> Source code in <code>ultralytics/trackers/bot_sort.py</code> <pre><code>def multi_predict(self, tracks):\n\"\"\"Predict and track multiple objects with YOLOv8 model.\"\"\"\nBOTrack.multi_predict(tracks)\n</code></pre>"},{"location":"reference/trackers/byte_tracker/","title":"Reference for <code>ultralytics/trackers/byte_tracker.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/byte_tracker.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack","title":"<code>ultralytics.trackers.byte_tracker.STrack</code>","text":"<p>             Bases: <code>BaseTrack</code></p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>class STrack(BaseTrack):\nshared_kalman = KalmanFilterXYAH()\ndef __init__(self, tlwh, score, cls):\n\"\"\"wait activate.\"\"\"\nself._tlwh = np.asarray(self.tlbr_to_tlwh(tlwh[:-1]), dtype=np.float32)\nself.kalman_filter = None\nself.mean, self.covariance = None, None\nself.is_activated = False\nself.score = score\nself.tracklet_len = 0\nself.cls = cls\nself.idx = tlwh[-1]\ndef predict(self):\n\"\"\"Predicts mean and covariance using Kalman filter.\"\"\"\nmean_state = self.mean.copy()\nif self.state != TrackState.Tracked:\nmean_state[7] = 0\nself.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n@staticmethod\ndef multi_predict(stracks):\n\"\"\"Perform multi-object predictive tracking using Kalman filter for given stracks.\"\"\"\nif len(stracks) &lt;= 0:\nreturn\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nfor i, st in enumerate(stracks):\nif st.state != TrackState.Tracked:\nmulti_mean[i][7] = 0\nmulti_mean, multi_covariance = STrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nstracks[i].mean = mean\nstracks[i].covariance = cov\n@staticmethod\ndef multi_gmc(stracks, H=np.eye(2, 3)):\n\"\"\"Update state tracks positions and covariances using a homography matrix.\"\"\"\nif len(stracks) &gt; 0:\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nR = H[:2, :2]\nR8x8 = np.kron(np.eye(4, dtype=float), R)\nt = H[:2, 2]\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nmean = R8x8.dot(mean)\nmean[:2] += t\ncov = R8x8.dot(cov).dot(R8x8.transpose())\nstracks[i].mean = mean\nstracks[i].covariance = cov\ndef activate(self, kalman_filter, frame_id):\n\"\"\"Start a new tracklet.\"\"\"\nself.kalman_filter = kalman_filter\nself.track_id = self.next_id()\nself.mean, self.covariance = self.kalman_filter.initiate(self.convert_coords(self._tlwh))\nself.tracklet_len = 0\nself.state = TrackState.Tracked\nif frame_id == 1:\nself.is_activated = True\nself.frame_id = frame_id\nself.start_frame = frame_id\ndef re_activate(self, new_track, frame_id, new_id=False):\n\"\"\"Reactivates a previously lost track with a new detection.\"\"\"\nself.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance,\nself.convert_coords(new_track.tlwh))\nself.tracklet_len = 0\nself.state = TrackState.Tracked\nself.is_activated = True\nself.frame_id = frame_id\nif new_id:\nself.track_id = self.next_id()\nself.score = new_track.score\nself.cls = new_track.cls\nself.idx = new_track.idx\ndef update(self, new_track, frame_id):\n\"\"\"\n        Update a matched track\n        :type new_track: STrack\n        :type frame_id: int\n        :return:\n        \"\"\"\nself.frame_id = frame_id\nself.tracklet_len += 1\nnew_tlwh = new_track.tlwh\nself.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance,\nself.convert_coords(new_tlwh))\nself.state = TrackState.Tracked\nself.is_activated = True\nself.score = new_track.score\nself.cls = new_track.cls\nself.idx = new_track.idx\ndef convert_coords(self, tlwh):\n\"\"\"Convert a bounding box's top-left-width-height format to its x-y-angle-height equivalent.\"\"\"\nreturn self.tlwh_to_xyah(tlwh)\n@property\ndef tlwh(self):\n\"\"\"Get current position in bounding box format `(top left x, top left y,\n        width, height)`.\n        \"\"\"\nif self.mean is None:\nreturn self._tlwh.copy()\nret = self.mean[:4].copy()\nret[2] *= ret[3]\nret[:2] -= ret[2:] / 2\nreturn ret\n@property\ndef tlbr(self):\n\"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n        `(top left, bottom right)`.\n        \"\"\"\nret = self.tlwh.copy()\nret[2:] += ret[:2]\nreturn ret\n@staticmethod\ndef tlwh_to_xyah(tlwh):\n\"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n        height)`, where the aspect ratio is `width / height`.\n        \"\"\"\nret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nret[2] /= ret[3]\nreturn ret\n@staticmethod\ndef tlbr_to_tlwh(tlbr):\n\"\"\"Converts top-left bottom-right format to top-left width height format.\"\"\"\nret = np.asarray(tlbr).copy()\nret[2:] -= ret[:2]\nreturn ret\n@staticmethod\ndef tlwh_to_tlbr(tlwh):\n\"\"\"Converts tlwh bounding box format to tlbr format.\"\"\"\nret = np.asarray(tlwh).copy()\nret[2:] += ret[:2]\nreturn ret\ndef __repr__(self):\n\"\"\"Return a string representation of the BYTETracker object with start and end frames and track ID.\"\"\"\nreturn f'OT_{self.track_id}_({self.start_frame}-{self.end_frame})'\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.tlbr","title":"<code>tlbr</code>  <code>property</code>","text":"<p>Convert bounding box to format <code>(min x, min y, max x, max y)</code>, i.e., <code>(top left, bottom right)</code>.</p>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.tlwh","title":"<code>tlwh</code>  <code>property</code>","text":"<p>Get current position in bounding box format <code>(top left x, top left y, width, height)</code>.</p>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.__init__","title":"<code>__init__(tlwh, score, cls)</code>","text":"<p>wait activate.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def __init__(self, tlwh, score, cls):\n\"\"\"wait activate.\"\"\"\nself._tlwh = np.asarray(self.tlbr_to_tlwh(tlwh[:-1]), dtype=np.float32)\nself.kalman_filter = None\nself.mean, self.covariance = None, None\nself.is_activated = False\nself.score = score\nself.tracklet_len = 0\nself.cls = cls\nself.idx = tlwh[-1]\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the BYTETracker object with start and end frames and track ID.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def __repr__(self):\n\"\"\"Return a string representation of the BYTETracker object with start and end frames and track ID.\"\"\"\nreturn f'OT_{self.track_id}_({self.start_frame}-{self.end_frame})'\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.activate","title":"<code>activate(kalman_filter, frame_id)</code>","text":"<p>Start a new tracklet.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def activate(self, kalman_filter, frame_id):\n\"\"\"Start a new tracklet.\"\"\"\nself.kalman_filter = kalman_filter\nself.track_id = self.next_id()\nself.mean, self.covariance = self.kalman_filter.initiate(self.convert_coords(self._tlwh))\nself.tracklet_len = 0\nself.state = TrackState.Tracked\nif frame_id == 1:\nself.is_activated = True\nself.frame_id = frame_id\nself.start_frame = frame_id\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.convert_coords","title":"<code>convert_coords(tlwh)</code>","text":"<p>Convert a bounding box's top-left-width-height format to its x-y-angle-height equivalent.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def convert_coords(self, tlwh):\n\"\"\"Convert a bounding box's top-left-width-height format to its x-y-angle-height equivalent.\"\"\"\nreturn self.tlwh_to_xyah(tlwh)\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.multi_gmc","title":"<code>multi_gmc(stracks, H=np.eye(2, 3))</code>  <code>staticmethod</code>","text":"<p>Update state tracks positions and covariances using a homography matrix.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef multi_gmc(stracks, H=np.eye(2, 3)):\n\"\"\"Update state tracks positions and covariances using a homography matrix.\"\"\"\nif len(stracks) &gt; 0:\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nR = H[:2, :2]\nR8x8 = np.kron(np.eye(4, dtype=float), R)\nt = H[:2, 2]\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nmean = R8x8.dot(mean)\nmean[:2] += t\ncov = R8x8.dot(cov).dot(R8x8.transpose())\nstracks[i].mean = mean\nstracks[i].covariance = cov\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.multi_predict","title":"<code>multi_predict(stracks)</code>  <code>staticmethod</code>","text":"<p>Perform multi-object predictive tracking using Kalman filter for given stracks.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef multi_predict(stracks):\n\"\"\"Perform multi-object predictive tracking using Kalman filter for given stracks.\"\"\"\nif len(stracks) &lt;= 0:\nreturn\nmulti_mean = np.asarray([st.mean.copy() for st in stracks])\nmulti_covariance = np.asarray([st.covariance for st in stracks])\nfor i, st in enumerate(stracks):\nif st.state != TrackState.Tracked:\nmulti_mean[i][7] = 0\nmulti_mean, multi_covariance = STrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\nfor i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\nstracks[i].mean = mean\nstracks[i].covariance = cov\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.predict","title":"<code>predict()</code>","text":"<p>Predicts mean and covariance using Kalman filter.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def predict(self):\n\"\"\"Predicts mean and covariance using Kalman filter.\"\"\"\nmean_state = self.mean.copy()\nif self.state != TrackState.Tracked:\nmean_state[7] = 0\nself.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.re_activate","title":"<code>re_activate(new_track, frame_id, new_id=False)</code>","text":"<p>Reactivates a previously lost track with a new detection.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def re_activate(self, new_track, frame_id, new_id=False):\n\"\"\"Reactivates a previously lost track with a new detection.\"\"\"\nself.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance,\nself.convert_coords(new_track.tlwh))\nself.tracklet_len = 0\nself.state = TrackState.Tracked\nself.is_activated = True\nself.frame_id = frame_id\nif new_id:\nself.track_id = self.next_id()\nself.score = new_track.score\nself.cls = new_track.cls\nself.idx = new_track.idx\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.tlbr_to_tlwh","title":"<code>tlbr_to_tlwh(tlbr)</code>  <code>staticmethod</code>","text":"<p>Converts top-left bottom-right format to top-left width height format.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef tlbr_to_tlwh(tlbr):\n\"\"\"Converts top-left bottom-right format to top-left width height format.\"\"\"\nret = np.asarray(tlbr).copy()\nret[2:] -= ret[:2]\nreturn ret\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.tlwh_to_tlbr","title":"<code>tlwh_to_tlbr(tlwh)</code>  <code>staticmethod</code>","text":"<p>Converts tlwh bounding box format to tlbr format.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef tlwh_to_tlbr(tlwh):\n\"\"\"Converts tlwh bounding box format to tlbr format.\"\"\"\nret = np.asarray(tlwh).copy()\nret[2:] += ret[:2]\nreturn ret\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.tlwh_to_xyah","title":"<code>tlwh_to_xyah(tlwh)</code>  <code>staticmethod</code>","text":"<p>Convert bounding box to format <code>(center x, center y, aspect ratio, height)</code>, where the aspect ratio is <code>width / height</code>.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef tlwh_to_xyah(tlwh):\n\"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n    height)`, where the aspect ratio is `width / height`.\n    \"\"\"\nret = np.asarray(tlwh).copy()\nret[:2] += ret[2:] / 2\nret[2] /= ret[3]\nreturn ret\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.STrack.update","title":"<code>update(new_track, frame_id)</code>","text":"<p>Update a matched track :type new_track: STrack :type frame_id: int :return:</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def update(self, new_track, frame_id):\n\"\"\"\n    Update a matched track\n    :type new_track: STrack\n    :type frame_id: int\n    :return:\n    \"\"\"\nself.frame_id = frame_id\nself.tracklet_len += 1\nnew_tlwh = new_track.tlwh\nself.mean, self.covariance = self.kalman_filter.update(self.mean, self.covariance,\nself.convert_coords(new_tlwh))\nself.state = TrackState.Tracked\nself.is_activated = True\nself.score = new_track.score\nself.cls = new_track.cls\nself.idx = new_track.idx\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker","title":"<code>ultralytics.trackers.byte_tracker.BYTETracker</code>","text":"Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>class BYTETracker:\ndef __init__(self, args, frame_rate=30):\n\"\"\"Initialize a YOLOv8 object to track objects with given arguments and frame rate.\"\"\"\nself.tracked_stracks = []  # type: list[STrack]\nself.lost_stracks = []  # type: list[STrack]\nself.removed_stracks = []  # type: list[STrack]\nself.frame_id = 0\nself.args = args\nself.max_time_lost = int(frame_rate / 30.0 * args.track_buffer)\nself.kalman_filter = self.get_kalmanfilter()\nself.reset_id()\ndef update(self, results, img=None):\n\"\"\"Updates object tracker with new detections and returns tracked object bounding boxes.\"\"\"\nself.frame_id += 1\nactivated_stracks = []\nrefind_stracks = []\nlost_stracks = []\nremoved_stracks = []\nscores = results.conf\nbboxes = results.xyxy\n# Add index\nbboxes = np.concatenate([bboxes, np.arange(len(bboxes)).reshape(-1, 1)], axis=-1)\ncls = results.cls\nremain_inds = scores &gt; self.args.track_high_thresh\ninds_low = scores &gt; self.args.track_low_thresh\ninds_high = scores &lt; self.args.track_high_thresh\ninds_second = np.logical_and(inds_low, inds_high)\ndets_second = bboxes[inds_second]\ndets = bboxes[remain_inds]\nscores_keep = scores[remain_inds]\nscores_second = scores[inds_second]\ncls_keep = cls[remain_inds]\ncls_second = cls[inds_second]\ndetections = self.init_track(dets, scores_keep, cls_keep, img)\n# Add newly detected tracklets to tracked_stracks\nunconfirmed = []\ntracked_stracks = []  # type: list[STrack]\nfor track in self.tracked_stracks:\nif not track.is_activated:\nunconfirmed.append(track)\nelse:\ntracked_stracks.append(track)\n# Step 2: First association, with high score detection boxes\nstrack_pool = self.joint_stracks(tracked_stracks, self.lost_stracks)\n# Predict the current location with KF\nself.multi_predict(strack_pool)\nif hasattr(self, 'gmc') and img is not None:\nwarp = self.gmc.apply(img, dets)\nSTrack.multi_gmc(strack_pool, warp)\nSTrack.multi_gmc(unconfirmed, warp)\ndists = self.get_dists(strack_pool, detections)\nmatches, u_track, u_detection = matching.linear_assignment(dists, thresh=self.args.match_thresh)\nfor itracked, idet in matches:\ntrack = strack_pool[itracked]\ndet = detections[idet]\nif track.state == TrackState.Tracked:\ntrack.update(det, self.frame_id)\nactivated_stracks.append(track)\nelse:\ntrack.re_activate(det, self.frame_id, new_id=False)\nrefind_stracks.append(track)\n# Step 3: Second association, with low score detection boxes\n# association the untrack to the low score detections\ndetections_second = self.init_track(dets_second, scores_second, cls_second, img)\nr_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n# TODO\ndists = matching.iou_distance(r_tracked_stracks, detections_second)\nmatches, u_track, u_detection_second = matching.linear_assignment(dists, thresh=0.5)\nfor itracked, idet in matches:\ntrack = r_tracked_stracks[itracked]\ndet = detections_second[idet]\nif track.state == TrackState.Tracked:\ntrack.update(det, self.frame_id)\nactivated_stracks.append(track)\nelse:\ntrack.re_activate(det, self.frame_id, new_id=False)\nrefind_stracks.append(track)\nfor it in u_track:\ntrack = r_tracked_stracks[it]\nif track.state != TrackState.Lost:\ntrack.mark_lost()\nlost_stracks.append(track)\n# Deal with unconfirmed tracks, usually tracks with only one beginning frame\ndetections = [detections[i] for i in u_detection]\ndists = self.get_dists(unconfirmed, detections)\nmatches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\nfor itracked, idet in matches:\nunconfirmed[itracked].update(detections[idet], self.frame_id)\nactivated_stracks.append(unconfirmed[itracked])\nfor it in u_unconfirmed:\ntrack = unconfirmed[it]\ntrack.mark_removed()\nremoved_stracks.append(track)\n# Step 4: Init new stracks\nfor inew in u_detection:\ntrack = detections[inew]\nif track.score &lt; self.args.new_track_thresh:\ncontinue\ntrack.activate(self.kalman_filter, self.frame_id)\nactivated_stracks.append(track)\n# Step 5: Update state\nfor track in self.lost_stracks:\nif self.frame_id - track.end_frame &gt; self.max_time_lost:\ntrack.mark_removed()\nremoved_stracks.append(track)\nself.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\nself.tracked_stracks = self.joint_stracks(self.tracked_stracks, activated_stracks)\nself.tracked_stracks = self.joint_stracks(self.tracked_stracks, refind_stracks)\nself.lost_stracks = self.sub_stracks(self.lost_stracks, self.tracked_stracks)\nself.lost_stracks.extend(lost_stracks)\nself.lost_stracks = self.sub_stracks(self.lost_stracks, self.removed_stracks)\nself.tracked_stracks, self.lost_stracks = self.remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\nself.removed_stracks.extend(removed_stracks)\nif len(self.removed_stracks) &gt; 1000:\nself.removed_stracks = self.removed_stracks[-999:]  # clip remove stracks to 1000 maximum\nreturn np.asarray(\n[x.tlbr.tolist() + [x.track_id, x.score, x.cls, x.idx] for x in self.tracked_stracks if x.is_activated],\ndtype=np.float32)\ndef get_kalmanfilter(self):\n\"\"\"Returns a Kalman filter object for tracking bounding boxes.\"\"\"\nreturn KalmanFilterXYAH()\ndef init_track(self, dets, scores, cls, img=None):\n\"\"\"Initialize object tracking with detections and scores using STrack algorithm.\"\"\"\nreturn [STrack(xyxy, s, c) for (xyxy, s, c) in zip(dets, scores, cls)] if len(dets) else []  # detections\ndef get_dists(self, tracks, detections):\n\"\"\"Calculates the distance between tracks and detections using IOU and fuses scores.\"\"\"\ndists = matching.iou_distance(tracks, detections)\n# TODO: mot20\n# if not self.args.mot20:\ndists = matching.fuse_score(dists, detections)\nreturn dists\ndef multi_predict(self, tracks):\n\"\"\"Returns the predicted tracks using the YOLOv8 network.\"\"\"\nSTrack.multi_predict(tracks)\ndef reset_id(self):\n\"\"\"Resets the ID counter of STrack.\"\"\"\nSTrack.reset_id()\n@staticmethod\ndef joint_stracks(tlista, tlistb):\n\"\"\"Combine two lists of stracks into a single one.\"\"\"\nexists = {}\nres = []\nfor t in tlista:\nexists[t.track_id] = 1\nres.append(t)\nfor t in tlistb:\ntid = t.track_id\nif not exists.get(tid, 0):\nexists[tid] = 1\nres.append(t)\nreturn res\n@staticmethod\ndef sub_stracks(tlista, tlistb):\n\"\"\"DEPRECATED CODE in https://github.com/ultralytics/ultralytics/pull/1890/\n        stracks = {t.track_id: t for t in tlista}\n        for t in tlistb:\n            tid = t.track_id\n            if stracks.get(tid, 0):\n                del stracks[tid]\n        return list(stracks.values())\n        \"\"\"\ntrack_ids_b = {t.track_id for t in tlistb}\nreturn [t for t in tlista if t.track_id not in track_ids_b]\n@staticmethod\ndef remove_duplicate_stracks(stracksa, stracksb):\n\"\"\"Remove duplicate stracks with non-maximum IOU distance.\"\"\"\npdist = matching.iou_distance(stracksa, stracksb)\npairs = np.where(pdist &lt; 0.15)\ndupa, dupb = [], []\nfor p, q in zip(*pairs):\ntimep = stracksa[p].frame_id - stracksa[p].start_frame\ntimeq = stracksb[q].frame_id - stracksb[q].start_frame\nif timep &gt; timeq:\ndupb.append(q)\nelse:\ndupa.append(p)\nresa = [t for i, t in enumerate(stracksa) if i not in dupa]\nresb = [t for i, t in enumerate(stracksb) if i not in dupb]\nreturn resa, resb\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.__init__","title":"<code>__init__(args, frame_rate=30)</code>","text":"<p>Initialize a YOLOv8 object to track objects with given arguments and frame rate.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def __init__(self, args, frame_rate=30):\n\"\"\"Initialize a YOLOv8 object to track objects with given arguments and frame rate.\"\"\"\nself.tracked_stracks = []  # type: list[STrack]\nself.lost_stracks = []  # type: list[STrack]\nself.removed_stracks = []  # type: list[STrack]\nself.frame_id = 0\nself.args = args\nself.max_time_lost = int(frame_rate / 30.0 * args.track_buffer)\nself.kalman_filter = self.get_kalmanfilter()\nself.reset_id()\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.get_dists","title":"<code>get_dists(tracks, detections)</code>","text":"<p>Calculates the distance between tracks and detections using IOU and fuses scores.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def get_dists(self, tracks, detections):\n\"\"\"Calculates the distance between tracks and detections using IOU and fuses scores.\"\"\"\ndists = matching.iou_distance(tracks, detections)\n# TODO: mot20\n# if not self.args.mot20:\ndists = matching.fuse_score(dists, detections)\nreturn dists\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.get_kalmanfilter","title":"<code>get_kalmanfilter()</code>","text":"<p>Returns a Kalman filter object for tracking bounding boxes.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def get_kalmanfilter(self):\n\"\"\"Returns a Kalman filter object for tracking bounding boxes.\"\"\"\nreturn KalmanFilterXYAH()\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.init_track","title":"<code>init_track(dets, scores, cls, img=None)</code>","text":"<p>Initialize object tracking with detections and scores using STrack algorithm.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def init_track(self, dets, scores, cls, img=None):\n\"\"\"Initialize object tracking with detections and scores using STrack algorithm.\"\"\"\nreturn [STrack(xyxy, s, c) for (xyxy, s, c) in zip(dets, scores, cls)] if len(dets) else []  # detections\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.joint_stracks","title":"<code>joint_stracks(tlista, tlistb)</code>  <code>staticmethod</code>","text":"<p>Combine two lists of stracks into a single one.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef joint_stracks(tlista, tlistb):\n\"\"\"Combine two lists of stracks into a single one.\"\"\"\nexists = {}\nres = []\nfor t in tlista:\nexists[t.track_id] = 1\nres.append(t)\nfor t in tlistb:\ntid = t.track_id\nif not exists.get(tid, 0):\nexists[tid] = 1\nres.append(t)\nreturn res\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.multi_predict","title":"<code>multi_predict(tracks)</code>","text":"<p>Returns the predicted tracks using the YOLOv8 network.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def multi_predict(self, tracks):\n\"\"\"Returns the predicted tracks using the YOLOv8 network.\"\"\"\nSTrack.multi_predict(tracks)\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.remove_duplicate_stracks","title":"<code>remove_duplicate_stracks(stracksa, stracksb)</code>  <code>staticmethod</code>","text":"<p>Remove duplicate stracks with non-maximum IOU distance.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef remove_duplicate_stracks(stracksa, stracksb):\n\"\"\"Remove duplicate stracks with non-maximum IOU distance.\"\"\"\npdist = matching.iou_distance(stracksa, stracksb)\npairs = np.where(pdist &lt; 0.15)\ndupa, dupb = [], []\nfor p, q in zip(*pairs):\ntimep = stracksa[p].frame_id - stracksa[p].start_frame\ntimeq = stracksb[q].frame_id - stracksb[q].start_frame\nif timep &gt; timeq:\ndupb.append(q)\nelse:\ndupa.append(p)\nresa = [t for i, t in enumerate(stracksa) if i not in dupa]\nresb = [t for i, t in enumerate(stracksb) if i not in dupb]\nreturn resa, resb\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.reset_id","title":"<code>reset_id()</code>","text":"<p>Resets the ID counter of STrack.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def reset_id(self):\n\"\"\"Resets the ID counter of STrack.\"\"\"\nSTrack.reset_id()\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.sub_stracks","title":"<code>sub_stracks(tlista, tlistb)</code>  <code>staticmethod</code>","text":"<p>DEPRECATED CODE in https://github.com/ultralytics/ultralytics/pull/1890/ stracks = {t.track_id: t for t in tlista}</p> for t in tlistb <p>tid = t.track_id if stracks.get(tid, 0):     del stracks[tid]</p> <p>return list(stracks.values())</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>@staticmethod\ndef sub_stracks(tlista, tlistb):\n\"\"\"DEPRECATED CODE in https://github.com/ultralytics/ultralytics/pull/1890/\n    stracks = {t.track_id: t for t in tlista}\n    for t in tlistb:\n        tid = t.track_id\n        if stracks.get(tid, 0):\n            del stracks[tid]\n    return list(stracks.values())\n    \"\"\"\ntrack_ids_b = {t.track_id for t in tlistb}\nreturn [t for t in tlista if t.track_id not in track_ids_b]\n</code></pre>"},{"location":"reference/trackers/byte_tracker/#ultralytics.trackers.byte_tracker.BYTETracker.update","title":"<code>update(results, img=None)</code>","text":"<p>Updates object tracker with new detections and returns tracked object bounding boxes.</p> Source code in <code>ultralytics/trackers/byte_tracker.py</code> <pre><code>def update(self, results, img=None):\n\"\"\"Updates object tracker with new detections and returns tracked object bounding boxes.\"\"\"\nself.frame_id += 1\nactivated_stracks = []\nrefind_stracks = []\nlost_stracks = []\nremoved_stracks = []\nscores = results.conf\nbboxes = results.xyxy\n# Add index\nbboxes = np.concatenate([bboxes, np.arange(len(bboxes)).reshape(-1, 1)], axis=-1)\ncls = results.cls\nremain_inds = scores &gt; self.args.track_high_thresh\ninds_low = scores &gt; self.args.track_low_thresh\ninds_high = scores &lt; self.args.track_high_thresh\ninds_second = np.logical_and(inds_low, inds_high)\ndets_second = bboxes[inds_second]\ndets = bboxes[remain_inds]\nscores_keep = scores[remain_inds]\nscores_second = scores[inds_second]\ncls_keep = cls[remain_inds]\ncls_second = cls[inds_second]\ndetections = self.init_track(dets, scores_keep, cls_keep, img)\n# Add newly detected tracklets to tracked_stracks\nunconfirmed = []\ntracked_stracks = []  # type: list[STrack]\nfor track in self.tracked_stracks:\nif not track.is_activated:\nunconfirmed.append(track)\nelse:\ntracked_stracks.append(track)\n# Step 2: First association, with high score detection boxes\nstrack_pool = self.joint_stracks(tracked_stracks, self.lost_stracks)\n# Predict the current location with KF\nself.multi_predict(strack_pool)\nif hasattr(self, 'gmc') and img is not None:\nwarp = self.gmc.apply(img, dets)\nSTrack.multi_gmc(strack_pool, warp)\nSTrack.multi_gmc(unconfirmed, warp)\ndists = self.get_dists(strack_pool, detections)\nmatches, u_track, u_detection = matching.linear_assignment(dists, thresh=self.args.match_thresh)\nfor itracked, idet in matches:\ntrack = strack_pool[itracked]\ndet = detections[idet]\nif track.state == TrackState.Tracked:\ntrack.update(det, self.frame_id)\nactivated_stracks.append(track)\nelse:\ntrack.re_activate(det, self.frame_id, new_id=False)\nrefind_stracks.append(track)\n# Step 3: Second association, with low score detection boxes\n# association the untrack to the low score detections\ndetections_second = self.init_track(dets_second, scores_second, cls_second, img)\nr_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n# TODO\ndists = matching.iou_distance(r_tracked_stracks, detections_second)\nmatches, u_track, u_detection_second = matching.linear_assignment(dists, thresh=0.5)\nfor itracked, idet in matches:\ntrack = r_tracked_stracks[itracked]\ndet = detections_second[idet]\nif track.state == TrackState.Tracked:\ntrack.update(det, self.frame_id)\nactivated_stracks.append(track)\nelse:\ntrack.re_activate(det, self.frame_id, new_id=False)\nrefind_stracks.append(track)\nfor it in u_track:\ntrack = r_tracked_stracks[it]\nif track.state != TrackState.Lost:\ntrack.mark_lost()\nlost_stracks.append(track)\n# Deal with unconfirmed tracks, usually tracks with only one beginning frame\ndetections = [detections[i] for i in u_detection]\ndists = self.get_dists(unconfirmed, detections)\nmatches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\nfor itracked, idet in matches:\nunconfirmed[itracked].update(detections[idet], self.frame_id)\nactivated_stracks.append(unconfirmed[itracked])\nfor it in u_unconfirmed:\ntrack = unconfirmed[it]\ntrack.mark_removed()\nremoved_stracks.append(track)\n# Step 4: Init new stracks\nfor inew in u_detection:\ntrack = detections[inew]\nif track.score &lt; self.args.new_track_thresh:\ncontinue\ntrack.activate(self.kalman_filter, self.frame_id)\nactivated_stracks.append(track)\n# Step 5: Update state\nfor track in self.lost_stracks:\nif self.frame_id - track.end_frame &gt; self.max_time_lost:\ntrack.mark_removed()\nremoved_stracks.append(track)\nself.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\nself.tracked_stracks = self.joint_stracks(self.tracked_stracks, activated_stracks)\nself.tracked_stracks = self.joint_stracks(self.tracked_stracks, refind_stracks)\nself.lost_stracks = self.sub_stracks(self.lost_stracks, self.tracked_stracks)\nself.lost_stracks.extend(lost_stracks)\nself.lost_stracks = self.sub_stracks(self.lost_stracks, self.removed_stracks)\nself.tracked_stracks, self.lost_stracks = self.remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\nself.removed_stracks.extend(removed_stracks)\nif len(self.removed_stracks) &gt; 1000:\nself.removed_stracks = self.removed_stracks[-999:]  # clip remove stracks to 1000 maximum\nreturn np.asarray(\n[x.tlbr.tolist() + [x.track_id, x.score, x.cls, x.idx] for x in self.tracked_stracks if x.is_activated],\ndtype=np.float32)\n</code></pre>"},{"location":"reference/trackers/track/","title":"Reference for <code>ultralytics/trackers/track.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/track.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/trackers/track/#ultralytics.trackers.track.on_predict_start","title":"<code>ultralytics.trackers.track.on_predict_start(predictor, persist=False)</code>","text":"<p>Initialize trackers for object tracking during prediction.</p> <p>Parameters:</p> Name Type Description Default <code>predictor</code> <code>object</code> <p>The predictor object to initialize trackers for.</p> required <code>persist</code> <code>bool</code> <p>Whether to persist the trackers if they already exist. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the tracker_type is not 'bytetrack' or 'botsort'.</p> Source code in <code>ultralytics/trackers/track.py</code> <pre><code>def on_predict_start(predictor, persist=False):\n\"\"\"\n    Initialize trackers for object tracking during prediction.\n    Args:\n        predictor (object): The predictor object to initialize trackers for.\n        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n    Raises:\n        AssertionError: If the tracker_type is not 'bytetrack' or 'botsort'.\n    \"\"\"\nif hasattr(predictor, 'trackers') and persist:\nreturn\ntracker = check_yaml(predictor.args.tracker)\ncfg = IterableSimpleNamespace(**yaml_load(tracker))\nassert cfg.tracker_type in ['bytetrack', 'botsort'], \\\n        f\"Only support 'bytetrack' and 'botsort' for now, but got '{cfg.tracker_type}'\"\ntrackers = []\nfor _ in range(predictor.dataset.bs):\ntracker = TRACKER_MAP[cfg.tracker_type](args=cfg, frame_rate=30)\ntrackers.append(tracker)\npredictor.trackers = trackers\n</code></pre>"},{"location":"reference/trackers/track/#ultralytics.trackers.track.on_predict_postprocess_end","title":"<code>ultralytics.trackers.track.on_predict_postprocess_end(predictor)</code>","text":"<p>Postprocess detected boxes and update with object tracking.</p> Source code in <code>ultralytics/trackers/track.py</code> <pre><code>def on_predict_postprocess_end(predictor):\n\"\"\"Postprocess detected boxes and update with object tracking.\"\"\"\nbs = predictor.dataset.bs\nim0s = predictor.batch[1]\nfor i in range(bs):\ndet = predictor.results[i].boxes.cpu().numpy()\nif len(det) == 0:\ncontinue\ntracks = predictor.trackers[i].update(det, im0s[i])\nif len(tracks) == 0:\ncontinue\nidx = tracks[:, -1].astype(int)\npredictor.results[i] = predictor.results[i][idx]\npredictor.results[i].update(boxes=torch.as_tensor(tracks[:, :-1]))\n</code></pre>"},{"location":"reference/trackers/track/#ultralytics.trackers.track.register_tracker","title":"<code>ultralytics.trackers.track.register_tracker(model, persist)</code>","text":"<p>Register tracking callbacks to the model for object tracking during prediction.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>The model object to register tracking callbacks for.</p> required <code>persist</code> <code>bool</code> <p>Whether to persist the trackers if they already exist.</p> required Source code in <code>ultralytics/trackers/track.py</code> <pre><code>def register_tracker(model, persist):\n\"\"\"\n    Register tracking callbacks to the model for object tracking during prediction.\n    Args:\n        model (object): The model object to register tracking callbacks for.\n        persist (bool): Whether to persist the trackers if they already exist.\n    \"\"\"\nmodel.add_callback('on_predict_start', partial(on_predict_start, persist=persist))\nmodel.add_callback('on_predict_postprocess_end', on_predict_postprocess_end)\n</code></pre>"},{"location":"reference/trackers/utils/gmc/","title":"Reference for <code>ultralytics/trackers/utils/gmc.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/utils/gmc.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC","title":"<code>ultralytics.trackers.utils.gmc.GMC</code>","text":"Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>class GMC:\ndef __init__(self, method='sparseOptFlow', downscale=2):\n\"\"\"Initialize a video tracker with specified parameters.\"\"\"\nsuper().__init__()\nself.method = method\nself.downscale = max(1, int(downscale))\nif self.method == 'orb':\nself.detector = cv2.FastFeatureDetector_create(20)\nself.extractor = cv2.ORB_create()\nself.matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\nelif self.method == 'sift':\nself.detector = cv2.SIFT_create(nOctaveLayers=3, contrastThreshold=0.02, edgeThreshold=20)\nself.extractor = cv2.SIFT_create(nOctaveLayers=3, contrastThreshold=0.02, edgeThreshold=20)\nself.matcher = cv2.BFMatcher(cv2.NORM_L2)\nelif self.method == 'ecc':\nnumber_of_iterations = 5000\ntermination_eps = 1e-6\nself.warp_mode = cv2.MOTION_EUCLIDEAN\nself.criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)\nelif self.method == 'sparseOptFlow':\nself.feature_params = dict(maxCorners=1000,\nqualityLevel=0.01,\nminDistance=1,\nblockSize=3,\nuseHarrisDetector=False,\nk=0.04)\nelif self.method in ['none', 'None', None]:\nself.method = None\nelse:\nraise ValueError(f'Error: Unknown GMC method:{method}')\nself.prevFrame = None\nself.prevKeyPoints = None\nself.prevDescriptors = None\nself.initializedFirstFrame = False\ndef apply(self, raw_frame, detections=None):\n\"\"\"Apply object detection on a raw frame using specified method.\"\"\"\nif self.method in ['orb', 'sift']:\nreturn self.applyFeatures(raw_frame, detections)\nelif self.method == 'ecc':\nreturn self.applyEcc(raw_frame, detections)\nelif self.method == 'sparseOptFlow':\nreturn self.applySparseOptFlow(raw_frame, detections)\nelse:\nreturn np.eye(2, 3)\ndef applyEcc(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3, dtype=np.float32)\n# Downscale image (TODO: consider using pyramids)\nif self.downscale &gt; 1.0:\nframe = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\nwidth = width // self.downscale\nheight = height // self.downscale\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Run the ECC algorithm. The results are stored in warp_matrix.\n# (cc, H) = cv2.findTransformECC(self.prevFrame, frame, H, self.warp_mode, self.criteria)\ntry:\n(cc, H) = cv2.findTransformECC(self.prevFrame, frame, H, self.warp_mode, self.criteria, None, 1)\nexcept Exception as e:\nLOGGER.warning(f'WARNING: find transform failed. Set warp as identity {e}')\nreturn H\ndef applyFeatures(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3)\n# Downscale image (TODO: consider using pyramids)\nif self.downscale &gt; 1.0:\n# frame = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\nwidth = width // self.downscale\nheight = height // self.downscale\n# Find the keypoints\nmask = np.zeros_like(frame)\n# mask[int(0.05 * height): int(0.95 * height), int(0.05 * width): int(0.95 * width)] = 255\nmask[int(0.02 * height):int(0.98 * height), int(0.02 * width):int(0.98 * width)] = 255\nif detections is not None:\nfor det in detections:\ntlbr = (det[:4] / self.downscale).astype(np.int_)\nmask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 0\nkeypoints = self.detector.detect(frame, mask)\n# Compute the descriptors\nkeypoints, descriptors = self.extractor.compute(frame, keypoints)\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Match descriptors.\nknnMatches = self.matcher.knnMatch(self.prevDescriptors, descriptors, 2)\n# Filtered matches based on smallest spatial distance\nmatches = []\nspatialDistances = []\nmaxSpatialDistance = 0.25 * np.array([width, height])\n# Handle empty matches case\nif len(knnMatches) == 0:\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\nreturn H\nfor m, n in knnMatches:\nif m.distance &lt; 0.9 * n.distance:\nprevKeyPointLocation = self.prevKeyPoints[m.queryIdx].pt\ncurrKeyPointLocation = keypoints[m.trainIdx].pt\nspatialDistance = (prevKeyPointLocation[0] - currKeyPointLocation[0],\nprevKeyPointLocation[1] - currKeyPointLocation[1])\nif (np.abs(spatialDistance[0]) &lt; maxSpatialDistance[0]) and \\\n                        (np.abs(spatialDistance[1]) &lt; maxSpatialDistance[1]):\nspatialDistances.append(spatialDistance)\nmatches.append(m)\nmeanSpatialDistances = np.mean(spatialDistances, 0)\nstdSpatialDistances = np.std(spatialDistances, 0)\ninliers = (spatialDistances - meanSpatialDistances) &lt; 2.5 * stdSpatialDistances\ngoodMatches = []\nprevPoints = []\ncurrPoints = []\nfor i in range(len(matches)):\nif inliers[i, 0] and inliers[i, 1]:\ngoodMatches.append(matches[i])\nprevPoints.append(self.prevKeyPoints[matches[i].queryIdx].pt)\ncurrPoints.append(keypoints[matches[i].trainIdx].pt)\nprevPoints = np.array(prevPoints)\ncurrPoints = np.array(currPoints)\n# Draw the keypoint matches on the output image\n# if False:\n#     import matplotlib.pyplot as plt\n#     matches_img = np.hstack((self.prevFrame, frame))\n#     matches_img = cv2.cvtColor(matches_img, cv2.COLOR_GRAY2BGR)\n#     W = np.size(self.prevFrame, 1)\n#     for m in goodMatches:\n#         prev_pt = np.array(self.prevKeyPoints[m.queryIdx].pt, dtype=np.int_)\n#         curr_pt = np.array(keypoints[m.trainIdx].pt, dtype=np.int_)\n#         curr_pt[0] += W\n#         color = np.random.randint(0, 255, 3)\n#         color = (int(color[0]), int(color[1]), int(color[2]))\n#\n#         matches_img = cv2.line(matches_img, prev_pt, curr_pt, tuple(color), 1, cv2.LINE_AA)\n#         matches_img = cv2.circle(matches_img, prev_pt, 2, tuple(color), -1)\n#         matches_img = cv2.circle(matches_img, curr_pt, 2, tuple(color), -1)\n#\n#     plt.figure()\n#     plt.imshow(matches_img)\n#     plt.show()\n# Find rigid matrix\nif (np.size(prevPoints, 0) &gt; 4) and (np.size(prevPoints, 0) == np.size(prevPoints, 0)):\nH, inliers = cv2.estimateAffinePartial2D(prevPoints, currPoints, cv2.RANSAC)\n# Handle downscale\nif self.downscale &gt; 1.0:\nH[0, 2] *= self.downscale\nH[1, 2] *= self.downscale\nelse:\nLOGGER.warning('WARNING: not enough matching points')\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\nreturn H\ndef applySparseOptFlow(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3)\n# Downscale image\nif self.downscale &gt; 1.0:\n# frame = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\n# Find the keypoints\nkeypoints = cv2.goodFeaturesToTrack(frame, mask=None, **self.feature_params)\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Find correspondences\nmatchedKeypoints, status, err = cv2.calcOpticalFlowPyrLK(self.prevFrame, frame, self.prevKeyPoints, None)\n# Leave good correspondences only\nprevPoints = []\ncurrPoints = []\nfor i in range(len(status)):\nif status[i]:\nprevPoints.append(self.prevKeyPoints[i])\ncurrPoints.append(matchedKeypoints[i])\nprevPoints = np.array(prevPoints)\ncurrPoints = np.array(currPoints)\n# Find rigid matrix\nif (np.size(prevPoints, 0) &gt; 4) and (np.size(prevPoints, 0) == np.size(prevPoints, 0)):\nH, inliers = cv2.estimateAffinePartial2D(prevPoints, currPoints, cv2.RANSAC)\n# Handle downscale\nif self.downscale &gt; 1.0:\nH[0, 2] *= self.downscale\nH[1, 2] *= self.downscale\nelse:\nLOGGER.warning('WARNING: not enough matching points')\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nreturn H\n</code></pre>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC.__init__","title":"<code>__init__(method='sparseOptFlow', downscale=2)</code>","text":"<p>Initialize a video tracker with specified parameters.</p> Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>def __init__(self, method='sparseOptFlow', downscale=2):\n\"\"\"Initialize a video tracker with specified parameters.\"\"\"\nsuper().__init__()\nself.method = method\nself.downscale = max(1, int(downscale))\nif self.method == 'orb':\nself.detector = cv2.FastFeatureDetector_create(20)\nself.extractor = cv2.ORB_create()\nself.matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\nelif self.method == 'sift':\nself.detector = cv2.SIFT_create(nOctaveLayers=3, contrastThreshold=0.02, edgeThreshold=20)\nself.extractor = cv2.SIFT_create(nOctaveLayers=3, contrastThreshold=0.02, edgeThreshold=20)\nself.matcher = cv2.BFMatcher(cv2.NORM_L2)\nelif self.method == 'ecc':\nnumber_of_iterations = 5000\ntermination_eps = 1e-6\nself.warp_mode = cv2.MOTION_EUCLIDEAN\nself.criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)\nelif self.method == 'sparseOptFlow':\nself.feature_params = dict(maxCorners=1000,\nqualityLevel=0.01,\nminDistance=1,\nblockSize=3,\nuseHarrisDetector=False,\nk=0.04)\nelif self.method in ['none', 'None', None]:\nself.method = None\nelse:\nraise ValueError(f'Error: Unknown GMC method:{method}')\nself.prevFrame = None\nself.prevKeyPoints = None\nself.prevDescriptors = None\nself.initializedFirstFrame = False\n</code></pre>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC.apply","title":"<code>apply(raw_frame, detections=None)</code>","text":"<p>Apply object detection on a raw frame using specified method.</p> Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>def apply(self, raw_frame, detections=None):\n\"\"\"Apply object detection on a raw frame using specified method.\"\"\"\nif self.method in ['orb', 'sift']:\nreturn self.applyFeatures(raw_frame, detections)\nelif self.method == 'ecc':\nreturn self.applyEcc(raw_frame, detections)\nelif self.method == 'sparseOptFlow':\nreturn self.applySparseOptFlow(raw_frame, detections)\nelse:\nreturn np.eye(2, 3)\n</code></pre>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC.applyEcc","title":"<code>applyEcc(raw_frame, detections=None)</code>","text":"<p>Initialize.</p> Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>def applyEcc(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3, dtype=np.float32)\n# Downscale image (TODO: consider using pyramids)\nif self.downscale &gt; 1.0:\nframe = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\nwidth = width // self.downscale\nheight = height // self.downscale\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Run the ECC algorithm. The results are stored in warp_matrix.\n# (cc, H) = cv2.findTransformECC(self.prevFrame, frame, H, self.warp_mode, self.criteria)\ntry:\n(cc, H) = cv2.findTransformECC(self.prevFrame, frame, H, self.warp_mode, self.criteria, None, 1)\nexcept Exception as e:\nLOGGER.warning(f'WARNING: find transform failed. Set warp as identity {e}')\nreturn H\n</code></pre>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC.applyFeatures","title":"<code>applyFeatures(raw_frame, detections=None)</code>","text":"<p>Initialize.</p> Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>def applyFeatures(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3)\n# Downscale image (TODO: consider using pyramids)\nif self.downscale &gt; 1.0:\n# frame = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\nwidth = width // self.downscale\nheight = height // self.downscale\n# Find the keypoints\nmask = np.zeros_like(frame)\n# mask[int(0.05 * height): int(0.95 * height), int(0.05 * width): int(0.95 * width)] = 255\nmask[int(0.02 * height):int(0.98 * height), int(0.02 * width):int(0.98 * width)] = 255\nif detections is not None:\nfor det in detections:\ntlbr = (det[:4] / self.downscale).astype(np.int_)\nmask[tlbr[1]:tlbr[3], tlbr[0]:tlbr[2]] = 0\nkeypoints = self.detector.detect(frame, mask)\n# Compute the descriptors\nkeypoints, descriptors = self.extractor.compute(frame, keypoints)\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Match descriptors.\nknnMatches = self.matcher.knnMatch(self.prevDescriptors, descriptors, 2)\n# Filtered matches based on smallest spatial distance\nmatches = []\nspatialDistances = []\nmaxSpatialDistance = 0.25 * np.array([width, height])\n# Handle empty matches case\nif len(knnMatches) == 0:\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\nreturn H\nfor m, n in knnMatches:\nif m.distance &lt; 0.9 * n.distance:\nprevKeyPointLocation = self.prevKeyPoints[m.queryIdx].pt\ncurrKeyPointLocation = keypoints[m.trainIdx].pt\nspatialDistance = (prevKeyPointLocation[0] - currKeyPointLocation[0],\nprevKeyPointLocation[1] - currKeyPointLocation[1])\nif (np.abs(spatialDistance[0]) &lt; maxSpatialDistance[0]) and \\\n                    (np.abs(spatialDistance[1]) &lt; maxSpatialDistance[1]):\nspatialDistances.append(spatialDistance)\nmatches.append(m)\nmeanSpatialDistances = np.mean(spatialDistances, 0)\nstdSpatialDistances = np.std(spatialDistances, 0)\ninliers = (spatialDistances - meanSpatialDistances) &lt; 2.5 * stdSpatialDistances\ngoodMatches = []\nprevPoints = []\ncurrPoints = []\nfor i in range(len(matches)):\nif inliers[i, 0] and inliers[i, 1]:\ngoodMatches.append(matches[i])\nprevPoints.append(self.prevKeyPoints[matches[i].queryIdx].pt)\ncurrPoints.append(keypoints[matches[i].trainIdx].pt)\nprevPoints = np.array(prevPoints)\ncurrPoints = np.array(currPoints)\n# Draw the keypoint matches on the output image\n# if False:\n#     import matplotlib.pyplot as plt\n#     matches_img = np.hstack((self.prevFrame, frame))\n#     matches_img = cv2.cvtColor(matches_img, cv2.COLOR_GRAY2BGR)\n#     W = np.size(self.prevFrame, 1)\n#     for m in goodMatches:\n#         prev_pt = np.array(self.prevKeyPoints[m.queryIdx].pt, dtype=np.int_)\n#         curr_pt = np.array(keypoints[m.trainIdx].pt, dtype=np.int_)\n#         curr_pt[0] += W\n#         color = np.random.randint(0, 255, 3)\n#         color = (int(color[0]), int(color[1]), int(color[2]))\n#\n#         matches_img = cv2.line(matches_img, prev_pt, curr_pt, tuple(color), 1, cv2.LINE_AA)\n#         matches_img = cv2.circle(matches_img, prev_pt, 2, tuple(color), -1)\n#         matches_img = cv2.circle(matches_img, curr_pt, 2, tuple(color), -1)\n#\n#     plt.figure()\n#     plt.imshow(matches_img)\n#     plt.show()\n# Find rigid matrix\nif (np.size(prevPoints, 0) &gt; 4) and (np.size(prevPoints, 0) == np.size(prevPoints, 0)):\nH, inliers = cv2.estimateAffinePartial2D(prevPoints, currPoints, cv2.RANSAC)\n# Handle downscale\nif self.downscale &gt; 1.0:\nH[0, 2] *= self.downscale\nH[1, 2] *= self.downscale\nelse:\nLOGGER.warning('WARNING: not enough matching points')\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nself.prevDescriptors = copy.copy(descriptors)\nreturn H\n</code></pre>"},{"location":"reference/trackers/utils/gmc/#ultralytics.trackers.utils.gmc.GMC.applySparseOptFlow","title":"<code>applySparseOptFlow(raw_frame, detections=None)</code>","text":"<p>Initialize.</p> Source code in <code>ultralytics/trackers/utils/gmc.py</code> <pre><code>def applySparseOptFlow(self, raw_frame, detections=None):\n\"\"\"Initialize.\"\"\"\nheight, width, _ = raw_frame.shape\nframe = cv2.cvtColor(raw_frame, cv2.COLOR_BGR2GRAY)\nH = np.eye(2, 3)\n# Downscale image\nif self.downscale &gt; 1.0:\n# frame = cv2.GaussianBlur(frame, (3, 3), 1.5)\nframe = cv2.resize(frame, (width // self.downscale, height // self.downscale))\n# Find the keypoints\nkeypoints = cv2.goodFeaturesToTrack(frame, mask=None, **self.feature_params)\n# Handle first frame\nif not self.initializedFirstFrame:\n# Initialize data\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\n# Initialization done\nself.initializedFirstFrame = True\nreturn H\n# Find correspondences\nmatchedKeypoints, status, err = cv2.calcOpticalFlowPyrLK(self.prevFrame, frame, self.prevKeyPoints, None)\n# Leave good correspondences only\nprevPoints = []\ncurrPoints = []\nfor i in range(len(status)):\nif status[i]:\nprevPoints.append(self.prevKeyPoints[i])\ncurrPoints.append(matchedKeypoints[i])\nprevPoints = np.array(prevPoints)\ncurrPoints = np.array(currPoints)\n# Find rigid matrix\nif (np.size(prevPoints, 0) &gt; 4) and (np.size(prevPoints, 0) == np.size(prevPoints, 0)):\nH, inliers = cv2.estimateAffinePartial2D(prevPoints, currPoints, cv2.RANSAC)\n# Handle downscale\nif self.downscale &gt; 1.0:\nH[0, 2] *= self.downscale\nH[1, 2] *= self.downscale\nelse:\nLOGGER.warning('WARNING: not enough matching points')\n# Store to next iteration\nself.prevFrame = frame.copy()\nself.prevKeyPoints = copy.copy(keypoints)\nreturn H\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/","title":"Reference for <code>ultralytics/trackers/utils/kalman_filter.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/utils/kalman_filter.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH","title":"<code>ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH</code>","text":"<p>For bytetrack. A simple Kalman filter for tracking bounding boxes in image space.</p> <p>The 8-dimensional state space (x, y, a, h, vx, vy, va, vh) contains the bounding box center position (x, y), aspect ratio a, height h, and their respective velocities.</p> <p>Object motion follows a constant velocity model. The bounding box location (x, y, a, h) is taken as direct observation of the state space (linear observation model).</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>class KalmanFilterXYAH:\n\"\"\"\n    For bytetrack. A simple Kalman filter for tracking bounding boxes in image space.\n    The 8-dimensional state space (x, y, a, h, vx, vy, va, vh) contains the bounding box center position (x, y),\n    aspect ratio a, height h, and their respective velocities.\n    Object motion follows a constant velocity model. The bounding box location (x, y, a, h) is taken as direct\n    observation of the state space (linear observation model).\n    \"\"\"\ndef __init__(self):\n\"\"\"Initialize Kalman filter model matrices with motion and observation uncertainty weights.\"\"\"\nndim, dt = 4, 1.\n# Create Kalman filter model matrices.\nself._motion_mat = np.eye(2 * ndim, 2 * ndim)\nfor i in range(ndim):\nself._motion_mat[i, ndim + i] = dt\nself._update_mat = np.eye(ndim, 2 * ndim)\n# Motion and observation uncertainty are chosen relative to the current state estimate. These weights control\n# the amount of uncertainty in the model. This is a bit hacky.\nself._std_weight_position = 1. / 20\nself._std_weight_velocity = 1. / 160\ndef initiate(self, measurement):\n\"\"\"\n        Create track from unassociated measurement.\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, a, h) with center position (x, y),\n            aspect ratio a, and height h.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n            dimensional) of the new track. Unobserved velocities are initialized\n            to 0 mean.\n        \"\"\"\nmean_pos = measurement\nmean_vel = np.zeros_like(mean_pos)\nmean = np.r_[mean_pos, mean_vel]\nstd = [\n2 * self._std_weight_position * measurement[3], 2 * self._std_weight_position * measurement[3], 1e-2,\n2 * self._std_weight_position * measurement[3], 10 * self._std_weight_velocity * measurement[3],\n10 * self._std_weight_velocity * measurement[3], 1e-5, 10 * self._std_weight_velocity * measurement[3]]\ncovariance = np.diag(np.square(std))\nreturn mean, covariance\ndef predict(self, mean, covariance):\n\"\"\"\n        Run Kalman filter prediction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The 8 dimensional mean vector of the object state at the previous time step.\n        covariance : ndarray\n            The 8x8 dimensional covariance matrix of the object state at the previous time step.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n            initialized to 0 mean.\n        \"\"\"\nstd_pos = [\nself._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-2,\nself._std_weight_position * mean[3]]\nstd_vel = [\nself._std_weight_velocity * mean[3], self._std_weight_velocity * mean[3], 1e-5,\nself._std_weight_velocity * mean[3]]\nmotion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n# mean = np.dot(self._motion_mat, mean)\nmean = np.dot(mean, self._motion_mat.T)\ncovariance = np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\nreturn mean, covariance\ndef project(self, mean, covariance):\n\"\"\"\n        Project state distribution to measurement space.\n        Parameters\n        ----------\n        mean : ndarray\n            The state's mean vector (8 dimensional array).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the projected mean and covariance matrix of the given state estimate.\n        \"\"\"\nstd = [\nself._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-1,\nself._std_weight_position * mean[3]]\ninnovation_cov = np.diag(np.square(std))\nmean = np.dot(self._update_mat, mean)\ncovariance = np.linalg.multi_dot((self._update_mat, covariance, self._update_mat.T))\nreturn mean, covariance + innovation_cov\ndef multi_predict(self, mean, covariance):\n\"\"\"\n        Run Kalman filter prediction step (Vectorized version).\n        Parameters\n        ----------\n        mean : ndarray\n            The Nx8 dimensional mean matrix of the object states at the previous time step.\n        covariance : ndarray\n            The Nx8x8 dimensional covariance matrix of the object states at the previous time step.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n            initialized to 0 mean.\n        \"\"\"\nstd_pos = [\nself._std_weight_position * mean[:, 3], self._std_weight_position * mean[:, 3],\n1e-2 * np.ones_like(mean[:, 3]), self._std_weight_position * mean[:, 3]]\nstd_vel = [\nself._std_weight_velocity * mean[:, 3], self._std_weight_velocity * mean[:, 3],\n1e-5 * np.ones_like(mean[:, 3]), self._std_weight_velocity * mean[:, 3]]\nsqr = np.square(np.r_[std_pos, std_vel]).T\nmotion_cov = [np.diag(sqr[i]) for i in range(len(mean))]\nmotion_cov = np.asarray(motion_cov)\nmean = np.dot(mean, self._motion_mat.T)\nleft = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\ncovariance = np.dot(left, self._motion_mat.T) + motion_cov\nreturn mean, covariance\ndef update(self, mean, covariance, measurement):\n\"\"\"\n        Run Kalman filter correction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The predicted state's mean vector (8 dimensional).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        measurement : ndarray\n            The 4 dimensional measurement vector (x, y, a, h), where (x, y) is the center position, a the aspect\n            ratio, and h the height of the bounding box.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the measurement-corrected state distribution.\n        \"\"\"\nprojected_mean, projected_cov = self.project(mean, covariance)\nchol_factor, lower = scipy.linalg.cho_factor(projected_cov, lower=True, check_finite=False)\nkalman_gain = scipy.linalg.cho_solve((chol_factor, lower),\nnp.dot(covariance, self._update_mat.T).T,\ncheck_finite=False).T\ninnovation = measurement - projected_mean\nnew_mean = mean + np.dot(innovation, kalman_gain.T)\nnew_covariance = covariance - np.linalg.multi_dot((kalman_gain, projected_cov, kalman_gain.T))\nreturn new_mean, new_covariance\ndef gating_distance(self, mean, covariance, measurements, only_position=False, metric='maha'):\n\"\"\"\n        Compute gating distance between state distribution and measurements. A suitable distance threshold can be\n        obtained from `chi2inv95`. If `only_position` is False, the chi-square distribution has 4 degrees of\n        freedom, otherwise 2.\n        Parameters\n        ----------\n        mean : ndarray\n            Mean vector over the state distribution (8 dimensional).\n        covariance : ndarray\n            Covariance of the state distribution (8x8 dimensional).\n        measurements : ndarray\n            An Nx4 dimensional matrix of N measurements, each in format (x, y, a, h) where (x, y) is the bounding box\n            center position, a the aspect ratio, and h the height.\n        only_position : Optional[bool]\n            If True, distance computation is done with respect to the bounding box center position only.\n        Returns\n        -------\n        ndarray\n            Returns an array of length N, where the i-th element contains the squared Mahalanobis distance between\n            (mean, covariance) and `measurements[i]`.\n        \"\"\"\nmean, covariance = self.project(mean, covariance)\nif only_position:\nmean, covariance = mean[:2], covariance[:2, :2]\nmeasurements = measurements[:, :2]\nd = measurements - mean\nif metric == 'gaussian':\nreturn np.sum(d * d, axis=1)\nelif metric == 'maha':\ncholesky_factor = np.linalg.cholesky(covariance)\nz = scipy.linalg.solve_triangular(cholesky_factor, d.T, lower=True, check_finite=False, overwrite_b=True)\nreturn np.sum(z * z, axis=0)  # square maha\nelse:\nraise ValueError('invalid distance metric')\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.__init__","title":"<code>__init__()</code>","text":"<p>Initialize Kalman filter model matrices with motion and observation uncertainty weights.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def __init__(self):\n\"\"\"Initialize Kalman filter model matrices with motion and observation uncertainty weights.\"\"\"\nndim, dt = 4, 1.\n# Create Kalman filter model matrices.\nself._motion_mat = np.eye(2 * ndim, 2 * ndim)\nfor i in range(ndim):\nself._motion_mat[i, ndim + i] = dt\nself._update_mat = np.eye(ndim, 2 * ndim)\n# Motion and observation uncertainty are chosen relative to the current state estimate. These weights control\n# the amount of uncertainty in the model. This is a bit hacky.\nself._std_weight_position = 1. / 20\nself._std_weight_velocity = 1. / 160\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.gating_distance","title":"<code>gating_distance(mean, covariance, measurements, only_position=False, metric='maha')</code>","text":"<p>Compute gating distance between state distribution and measurements. A suitable distance threshold can be obtained from <code>chi2inv95</code>. If <code>only_position</code> is False, the chi-square distribution has 4 degrees of freedom, otherwise 2.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.gating_distance--parameters","title":"Parameters","text":"ndarray <p>Mean vector over the state distribution (8 dimensional).</p> ndarray <p>Covariance of the state distribution (8x8 dimensional).</p> ndarray <p>An Nx4 dimensional matrix of N measurements, each in format (x, y, a, h) where (x, y) is the bounding box center position, a the aspect ratio, and h the height.</p> Optional[bool] <p>If True, distance computation is done with respect to the bounding box center position only.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.gating_distance--returns","title":"Returns","text":"<p>ndarray     Returns an array of length N, where the i-th element contains the squared Mahalanobis distance between     (mean, covariance) and <code>measurements[i]</code>.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def gating_distance(self, mean, covariance, measurements, only_position=False, metric='maha'):\n\"\"\"\n    Compute gating distance between state distribution and measurements. A suitable distance threshold can be\n    obtained from `chi2inv95`. If `only_position` is False, the chi-square distribution has 4 degrees of\n    freedom, otherwise 2.\n    Parameters\n    ----------\n    mean : ndarray\n        Mean vector over the state distribution (8 dimensional).\n    covariance : ndarray\n        Covariance of the state distribution (8x8 dimensional).\n    measurements : ndarray\n        An Nx4 dimensional matrix of N measurements, each in format (x, y, a, h) where (x, y) is the bounding box\n        center position, a the aspect ratio, and h the height.\n    only_position : Optional[bool]\n        If True, distance computation is done with respect to the bounding box center position only.\n    Returns\n    -------\n    ndarray\n        Returns an array of length N, where the i-th element contains the squared Mahalanobis distance between\n        (mean, covariance) and `measurements[i]`.\n    \"\"\"\nmean, covariance = self.project(mean, covariance)\nif only_position:\nmean, covariance = mean[:2], covariance[:2, :2]\nmeasurements = measurements[:, :2]\nd = measurements - mean\nif metric == 'gaussian':\nreturn np.sum(d * d, axis=1)\nelif metric == 'maha':\ncholesky_factor = np.linalg.cholesky(covariance)\nz = scipy.linalg.solve_triangular(cholesky_factor, d.T, lower=True, check_finite=False, overwrite_b=True)\nreturn np.sum(z * z, axis=0)  # square maha\nelse:\nraise ValueError('invalid distance metric')\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.initiate","title":"<code>initiate(measurement)</code>","text":"<p>Create track from unassociated measurement.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.initiate--parameters","title":"Parameters","text":"ndarray <p>Bounding box coordinates (x, y, a, h) with center position (x, y), aspect ratio a, and height h.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.initiate--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector (8 dimensional) and covariance matrix (8x8     dimensional) of the new track. Unobserved velocities are initialized     to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def initiate(self, measurement):\n\"\"\"\n    Create track from unassociated measurement.\n    Parameters\n    ----------\n    measurement : ndarray\n        Bounding box coordinates (x, y, a, h) with center position (x, y),\n        aspect ratio a, and height h.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector (8 dimensional) and covariance matrix (8x8\n        dimensional) of the new track. Unobserved velocities are initialized\n        to 0 mean.\n    \"\"\"\nmean_pos = measurement\nmean_vel = np.zeros_like(mean_pos)\nmean = np.r_[mean_pos, mean_vel]\nstd = [\n2 * self._std_weight_position * measurement[3], 2 * self._std_weight_position * measurement[3], 1e-2,\n2 * self._std_weight_position * measurement[3], 10 * self._std_weight_velocity * measurement[3],\n10 * self._std_weight_velocity * measurement[3], 1e-5, 10 * self._std_weight_velocity * measurement[3]]\ncovariance = np.diag(np.square(std))\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.multi_predict","title":"<code>multi_predict(mean, covariance)</code>","text":"<p>Run Kalman filter prediction step (Vectorized version).</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.multi_predict--parameters","title":"Parameters","text":"ndarray <p>The Nx8 dimensional mean matrix of the object states at the previous time step.</p> ndarray <p>The Nx8x8 dimensional covariance matrix of the object states at the previous time step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.multi_predict--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are     initialized to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def multi_predict(self, mean, covariance):\n\"\"\"\n    Run Kalman filter prediction step (Vectorized version).\n    Parameters\n    ----------\n    mean : ndarray\n        The Nx8 dimensional mean matrix of the object states at the previous time step.\n    covariance : ndarray\n        The Nx8x8 dimensional covariance matrix of the object states at the previous time step.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n        initialized to 0 mean.\n    \"\"\"\nstd_pos = [\nself._std_weight_position * mean[:, 3], self._std_weight_position * mean[:, 3],\n1e-2 * np.ones_like(mean[:, 3]), self._std_weight_position * mean[:, 3]]\nstd_vel = [\nself._std_weight_velocity * mean[:, 3], self._std_weight_velocity * mean[:, 3],\n1e-5 * np.ones_like(mean[:, 3]), self._std_weight_velocity * mean[:, 3]]\nsqr = np.square(np.r_[std_pos, std_vel]).T\nmotion_cov = [np.diag(sqr[i]) for i in range(len(mean))]\nmotion_cov = np.asarray(motion_cov)\nmean = np.dot(mean, self._motion_mat.T)\nleft = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\ncovariance = np.dot(left, self._motion_mat.T) + motion_cov\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.predict","title":"<code>predict(mean, covariance)</code>","text":"<p>Run Kalman filter prediction step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.predict--parameters","title":"Parameters","text":"ndarray <p>The 8 dimensional mean vector of the object state at the previous time step.</p> ndarray <p>The 8x8 dimensional covariance matrix of the object state at the previous time step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.predict--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are     initialized to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def predict(self, mean, covariance):\n\"\"\"\n    Run Kalman filter prediction step.\n    Parameters\n    ----------\n    mean : ndarray\n        The 8 dimensional mean vector of the object state at the previous time step.\n    covariance : ndarray\n        The 8x8 dimensional covariance matrix of the object state at the previous time step.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n        initialized to 0 mean.\n    \"\"\"\nstd_pos = [\nself._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-2,\nself._std_weight_position * mean[3]]\nstd_vel = [\nself._std_weight_velocity * mean[3], self._std_weight_velocity * mean[3], 1e-5,\nself._std_weight_velocity * mean[3]]\nmotion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n# mean = np.dot(self._motion_mat, mean)\nmean = np.dot(mean, self._motion_mat.T)\ncovariance = np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.project","title":"<code>project(mean, covariance)</code>","text":"<p>Project state distribution to measurement space.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.project--parameters","title":"Parameters","text":"ndarray <p>The state's mean vector (8 dimensional array).</p> ndarray <p>The state's covariance matrix (8x8 dimensional).</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.project--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the projected mean and covariance matrix of the given state estimate.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def project(self, mean, covariance):\n\"\"\"\n    Project state distribution to measurement space.\n    Parameters\n    ----------\n    mean : ndarray\n        The state's mean vector (8 dimensional array).\n    covariance : ndarray\n        The state's covariance matrix (8x8 dimensional).\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the projected mean and covariance matrix of the given state estimate.\n    \"\"\"\nstd = [\nself._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-1,\nself._std_weight_position * mean[3]]\ninnovation_cov = np.diag(np.square(std))\nmean = np.dot(self._update_mat, mean)\ncovariance = np.linalg.multi_dot((self._update_mat, covariance, self._update_mat.T))\nreturn mean, covariance + innovation_cov\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.update","title":"<code>update(mean, covariance, measurement)</code>","text":"<p>Run Kalman filter correction step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.update--parameters","title":"Parameters","text":"ndarray <p>The predicted state's mean vector (8 dimensional).</p> ndarray <p>The state's covariance matrix (8x8 dimensional).</p> ndarray <p>The 4 dimensional measurement vector (x, y, a, h), where (x, y) is the center position, a the aspect ratio, and h the height of the bounding box.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH.update--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the measurement-corrected state distribution.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def update(self, mean, covariance, measurement):\n\"\"\"\n    Run Kalman filter correction step.\n    Parameters\n    ----------\n    mean : ndarray\n        The predicted state's mean vector (8 dimensional).\n    covariance : ndarray\n        The state's covariance matrix (8x8 dimensional).\n    measurement : ndarray\n        The 4 dimensional measurement vector (x, y, a, h), where (x, y) is the center position, a the aspect\n        ratio, and h the height of the bounding box.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the measurement-corrected state distribution.\n    \"\"\"\nprojected_mean, projected_cov = self.project(mean, covariance)\nchol_factor, lower = scipy.linalg.cho_factor(projected_cov, lower=True, check_finite=False)\nkalman_gain = scipy.linalg.cho_solve((chol_factor, lower),\nnp.dot(covariance, self._update_mat.T).T,\ncheck_finite=False).T\ninnovation = measurement - projected_mean\nnew_mean = mean + np.dot(innovation, kalman_gain.T)\nnew_covariance = covariance - np.linalg.multi_dot((kalman_gain, projected_cov, kalman_gain.T))\nreturn new_mean, new_covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH","title":"<code>ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH</code>","text":"<p>             Bases: <code>KalmanFilterXYAH</code></p> <p>For BoT-SORT. A simple Kalman filter for tracking bounding boxes in image space.</p> <p>The 8-dimensional state space (x, y, w, h, vx, vy, vw, vh) contains the bounding box center position (x, y), width w, height h, and their respective velocities.</p> <p>Object motion follows a constant velocity model. The bounding box location (x, y, w, h) is taken as direct observation of the state space (linear observation model).</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>class KalmanFilterXYWH(KalmanFilterXYAH):\n\"\"\"\n    For BoT-SORT. A simple Kalman filter for tracking bounding boxes in image space.\n    The 8-dimensional state space (x, y, w, h, vx, vy, vw, vh) contains the bounding box center position (x, y),\n    width w, height h, and their respective velocities.\n    Object motion follows a constant velocity model. The bounding box location (x, y, w, h) is taken as direct\n    observation of the state space (linear observation model).\n    \"\"\"\ndef initiate(self, measurement):\n\"\"\"\n        Create track from unassociated measurement.\n        Parameters\n        ----------\n        measurement : ndarray\n            Bounding box coordinates (x, y, w, h) with center position (x, y), width w, and height h.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector (8 dimensional) and covariance matrix (8x8 dimensional) of the new track.\n            Unobserved velocities are initialized to 0 mean.\n        \"\"\"\nmean_pos = measurement\nmean_vel = np.zeros_like(mean_pos)\nmean = np.r_[mean_pos, mean_vel]\nstd = [\n2 * self._std_weight_position * measurement[2], 2 * self._std_weight_position * measurement[3],\n2 * self._std_weight_position * measurement[2], 2 * self._std_weight_position * measurement[3],\n10 * self._std_weight_velocity * measurement[2], 10 * self._std_weight_velocity * measurement[3],\n10 * self._std_weight_velocity * measurement[2], 10 * self._std_weight_velocity * measurement[3]]\ncovariance = np.diag(np.square(std))\nreturn mean, covariance\ndef predict(self, mean, covariance):\n\"\"\"\n        Run Kalman filter prediction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The 8 dimensional mean vector of the object state at the previous time step.\n        covariance : ndarray\n            The 8x8 dimensional covariance matrix of the object state at the previous time step.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n            initialized to 0 mean.\n        \"\"\"\nstd_pos = [\nself._std_weight_position * mean[2], self._std_weight_position * mean[3],\nself._std_weight_position * mean[2], self._std_weight_position * mean[3]]\nstd_vel = [\nself._std_weight_velocity * mean[2], self._std_weight_velocity * mean[3],\nself._std_weight_velocity * mean[2], self._std_weight_velocity * mean[3]]\nmotion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\nmean = np.dot(mean, self._motion_mat.T)\ncovariance = np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\nreturn mean, covariance\ndef project(self, mean, covariance):\n\"\"\"\n        Project state distribution to measurement space.\n        Parameters\n        ----------\n        mean : ndarray\n            The state's mean vector (8 dimensional array).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the projected mean and covariance matrix of the given state estimate.\n        \"\"\"\nstd = [\nself._std_weight_position * mean[2], self._std_weight_position * mean[3],\nself._std_weight_position * mean[2], self._std_weight_position * mean[3]]\ninnovation_cov = np.diag(np.square(std))\nmean = np.dot(self._update_mat, mean)\ncovariance = np.linalg.multi_dot((self._update_mat, covariance, self._update_mat.T))\nreturn mean, covariance + innovation_cov\ndef multi_predict(self, mean, covariance):\n\"\"\"\n        Run Kalman filter prediction step (Vectorized version).\n        Parameters\n        ----------\n        mean : ndarray\n            The Nx8 dimensional mean matrix of the object states at the previous time step.\n        covariance : ndarray\n            The Nx8x8 dimensional covariance matrix of the object states at the previous time step.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n            initialized to 0 mean.\n        \"\"\"\nstd_pos = [\nself._std_weight_position * mean[:, 2], self._std_weight_position * mean[:, 3],\nself._std_weight_position * mean[:, 2], self._std_weight_position * mean[:, 3]]\nstd_vel = [\nself._std_weight_velocity * mean[:, 2], self._std_weight_velocity * mean[:, 3],\nself._std_weight_velocity * mean[:, 2], self._std_weight_velocity * mean[:, 3]]\nsqr = np.square(np.r_[std_pos, std_vel]).T\nmotion_cov = [np.diag(sqr[i]) for i in range(len(mean))]\nmotion_cov = np.asarray(motion_cov)\nmean = np.dot(mean, self._motion_mat.T)\nleft = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\ncovariance = np.dot(left, self._motion_mat.T) + motion_cov\nreturn mean, covariance\ndef update(self, mean, covariance, measurement):\n\"\"\"\n        Run Kalman filter correction step.\n        Parameters\n        ----------\n        mean : ndarray\n            The predicted state's mean vector (8 dimensional).\n        covariance : ndarray\n            The state's covariance matrix (8x8 dimensional).\n        measurement : ndarray\n            The 4 dimensional measurement vector (x, y, w, h), where (x, y) is the center position, w the width,\n            and h the height of the bounding box.\n        Returns\n        -------\n        (ndarray, ndarray)\n            Returns the measurement-corrected state distribution.\n        \"\"\"\nreturn super().update(mean, covariance, measurement)\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.initiate","title":"<code>initiate(measurement)</code>","text":"<p>Create track from unassociated measurement.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.initiate--parameters","title":"Parameters","text":"ndarray <p>Bounding box coordinates (x, y, w, h) with center position (x, y), width w, and height h.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.initiate--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector (8 dimensional) and covariance matrix (8x8 dimensional) of the new track.     Unobserved velocities are initialized to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def initiate(self, measurement):\n\"\"\"\n    Create track from unassociated measurement.\n    Parameters\n    ----------\n    measurement : ndarray\n        Bounding box coordinates (x, y, w, h) with center position (x, y), width w, and height h.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector (8 dimensional) and covariance matrix (8x8 dimensional) of the new track.\n        Unobserved velocities are initialized to 0 mean.\n    \"\"\"\nmean_pos = measurement\nmean_vel = np.zeros_like(mean_pos)\nmean = np.r_[mean_pos, mean_vel]\nstd = [\n2 * self._std_weight_position * measurement[2], 2 * self._std_weight_position * measurement[3],\n2 * self._std_weight_position * measurement[2], 2 * self._std_weight_position * measurement[3],\n10 * self._std_weight_velocity * measurement[2], 10 * self._std_weight_velocity * measurement[3],\n10 * self._std_weight_velocity * measurement[2], 10 * self._std_weight_velocity * measurement[3]]\ncovariance = np.diag(np.square(std))\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.multi_predict","title":"<code>multi_predict(mean, covariance)</code>","text":"<p>Run Kalman filter prediction step (Vectorized version).</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.multi_predict--parameters","title":"Parameters","text":"ndarray <p>The Nx8 dimensional mean matrix of the object states at the previous time step.</p> ndarray <p>The Nx8x8 dimensional covariance matrix of the object states at the previous time step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.multi_predict--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are     initialized to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def multi_predict(self, mean, covariance):\n\"\"\"\n    Run Kalman filter prediction step (Vectorized version).\n    Parameters\n    ----------\n    mean : ndarray\n        The Nx8 dimensional mean matrix of the object states at the previous time step.\n    covariance : ndarray\n        The Nx8x8 dimensional covariance matrix of the object states at the previous time step.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n        initialized to 0 mean.\n    \"\"\"\nstd_pos = [\nself._std_weight_position * mean[:, 2], self._std_weight_position * mean[:, 3],\nself._std_weight_position * mean[:, 2], self._std_weight_position * mean[:, 3]]\nstd_vel = [\nself._std_weight_velocity * mean[:, 2], self._std_weight_velocity * mean[:, 3],\nself._std_weight_velocity * mean[:, 2], self._std_weight_velocity * mean[:, 3]]\nsqr = np.square(np.r_[std_pos, std_vel]).T\nmotion_cov = [np.diag(sqr[i]) for i in range(len(mean))]\nmotion_cov = np.asarray(motion_cov)\nmean = np.dot(mean, self._motion_mat.T)\nleft = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\ncovariance = np.dot(left, self._motion_mat.T) + motion_cov\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.predict","title":"<code>predict(mean, covariance)</code>","text":"<p>Run Kalman filter prediction step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.predict--parameters","title":"Parameters","text":"ndarray <p>The 8 dimensional mean vector of the object state at the previous time step.</p> ndarray <p>The 8x8 dimensional covariance matrix of the object state at the previous time step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.predict--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are     initialized to 0 mean.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def predict(self, mean, covariance):\n\"\"\"\n    Run Kalman filter prediction step.\n    Parameters\n    ----------\n    mean : ndarray\n        The 8 dimensional mean vector of the object state at the previous time step.\n    covariance : ndarray\n        The 8x8 dimensional covariance matrix of the object state at the previous time step.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the mean vector and covariance matrix of the predicted state. Unobserved velocities are\n        initialized to 0 mean.\n    \"\"\"\nstd_pos = [\nself._std_weight_position * mean[2], self._std_weight_position * mean[3],\nself._std_weight_position * mean[2], self._std_weight_position * mean[3]]\nstd_vel = [\nself._std_weight_velocity * mean[2], self._std_weight_velocity * mean[3],\nself._std_weight_velocity * mean[2], self._std_weight_velocity * mean[3]]\nmotion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\nmean = np.dot(mean, self._motion_mat.T)\ncovariance = np.linalg.multi_dot((self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\nreturn mean, covariance\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.project","title":"<code>project(mean, covariance)</code>","text":"<p>Project state distribution to measurement space.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.project--parameters","title":"Parameters","text":"ndarray <p>The state's mean vector (8 dimensional array).</p> ndarray <p>The state's covariance matrix (8x8 dimensional).</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.project--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the projected mean and covariance matrix of the given state estimate.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def project(self, mean, covariance):\n\"\"\"\n    Project state distribution to measurement space.\n    Parameters\n    ----------\n    mean : ndarray\n        The state's mean vector (8 dimensional array).\n    covariance : ndarray\n        The state's covariance matrix (8x8 dimensional).\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the projected mean and covariance matrix of the given state estimate.\n    \"\"\"\nstd = [\nself._std_weight_position * mean[2], self._std_weight_position * mean[3],\nself._std_weight_position * mean[2], self._std_weight_position * mean[3]]\ninnovation_cov = np.diag(np.square(std))\nmean = np.dot(self._update_mat, mean)\ncovariance = np.linalg.multi_dot((self._update_mat, covariance, self._update_mat.T))\nreturn mean, covariance + innovation_cov\n</code></pre>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.update","title":"<code>update(mean, covariance, measurement)</code>","text":"<p>Run Kalman filter correction step.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.update--parameters","title":"Parameters","text":"ndarray <p>The predicted state's mean vector (8 dimensional).</p> ndarray <p>The state's covariance matrix (8x8 dimensional).</p> ndarray <p>The 4 dimensional measurement vector (x, y, w, h), where (x, y) is the center position, w the width, and h the height of the bounding box.</p>"},{"location":"reference/trackers/utils/kalman_filter/#ultralytics.trackers.utils.kalman_filter.KalmanFilterXYWH.update--returns","title":"Returns","text":"<p>(ndarray, ndarray)     Returns the measurement-corrected state distribution.</p> Source code in <code>ultralytics/trackers/utils/kalman_filter.py</code> <pre><code>def update(self, mean, covariance, measurement):\n\"\"\"\n    Run Kalman filter correction step.\n    Parameters\n    ----------\n    mean : ndarray\n        The predicted state's mean vector (8 dimensional).\n    covariance : ndarray\n        The state's covariance matrix (8x8 dimensional).\n    measurement : ndarray\n        The 4 dimensional measurement vector (x, y, w, h), where (x, y) is the center position, w the width,\n        and h the height of the bounding box.\n    Returns\n    -------\n    (ndarray, ndarray)\n        Returns the measurement-corrected state distribution.\n    \"\"\"\nreturn super().update(mean, covariance, measurement)\n</code></pre>"},{"location":"reference/trackers/utils/matching/","title":"Reference for <code>ultralytics/trackers/utils/matching.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/trackers/utils/matching.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/trackers/utils/matching/#ultralytics.trackers.utils.matching.linear_assignment","title":"<code>ultralytics.trackers.utils.matching.linear_assignment(cost_matrix, thresh, use_lap=True)</code>","text":"<p>Perform linear assignment using scipy or lap.lapjv.</p> <p>Parameters:</p> Name Type Description Default <code>cost_matrix</code> <code>ndarray</code> <p>The matrix containing cost values for assignments.</p> required <code>thresh</code> <code>float</code> <p>Threshold for considering an assignment valid.</p> required <code>use_lap</code> <code>bool</code> <p>Whether to use lap.lapjv. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing matched indices, unmatched indices from 'a', and unmatched indices from 'b'.</p> Source code in <code>ultralytics/trackers/utils/matching.py</code> <pre><code>def linear_assignment(cost_matrix, thresh, use_lap=True):\n\"\"\"\n    Perform linear assignment using scipy or lap.lapjv.\n    Args:\n        cost_matrix (np.ndarray): The matrix containing cost values for assignments.\n        thresh (float): Threshold for considering an assignment valid.\n        use_lap (bool, optional): Whether to use lap.lapjv. Defaults to True.\n    Returns:\n        (tuple): Tuple containing matched indices, unmatched indices from 'a', and unmatched indices from 'b'.\n    \"\"\"\nif cost_matrix.size == 0:\nreturn np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1]))\nif use_lap:\n# https://github.com/gatagat/lap\n_, x, y = lap.lapjv(cost_matrix, extend_cost=True, cost_limit=thresh)\nmatches = [[ix, mx] for ix, mx in enumerate(x) if mx &gt;= 0]\nunmatched_a = np.where(x &lt; 0)[0]\nunmatched_b = np.where(y &lt; 0)[0]\nelse:\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html\nx, y = scipy.optimize.linear_sum_assignment(cost_matrix)  # row x, col y\nmatches = np.asarray([[x[i], y[i]] for i in range(len(x)) if cost_matrix[x[i], y[i]] &lt;= thresh])\nif len(matches) == 0:\nunmatched_a = list(np.arange(cost_matrix.shape[0]))\nunmatched_b = list(np.arange(cost_matrix.shape[1]))\nelse:\nunmatched_a = list(set(np.arange(cost_matrix.shape[0])) - set(matches[:, 0]))\nunmatched_b = list(set(np.arange(cost_matrix.shape[1])) - set(matches[:, 1]))\nreturn matches, unmatched_a, unmatched_b\n</code></pre>"},{"location":"reference/trackers/utils/matching/#ultralytics.trackers.utils.matching.iou_distance","title":"<code>ultralytics.trackers.utils.matching.iou_distance(atracks, btracks)</code>","text":"<p>Compute cost based on Intersection over Union (IoU) between tracks.</p> <p>Parameters:</p> Name Type Description Default <code>atracks</code> <code>list[STrack] | list[numpy.ndarray]</code> <p>List of tracks 'a' or bounding boxes.</p> required <code>btracks</code> <code>list[STrack] | list[numpy.ndarray]</code> <p>List of tracks 'b' or bounding boxes.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cost matrix computed based on IoU.</p> Source code in <code>ultralytics/trackers/utils/matching.py</code> <pre><code>def iou_distance(atracks, btracks):\n\"\"\"\n    Compute cost based on Intersection over Union (IoU) between tracks.\n    Args:\n        atracks (list[STrack] | list[np.ndarray]): List of tracks 'a' or bounding boxes.\n        btracks (list[STrack] | list[np.ndarray]): List of tracks 'b' or bounding boxes.\n    Returns:\n        (np.ndarray): Cost matrix computed based on IoU.\n    \"\"\"\nif (len(atracks) &gt; 0 and isinstance(atracks[0], np.ndarray)) \\\n            or (len(btracks) &gt; 0 and isinstance(btracks[0], np.ndarray)):\natlbrs = atracks\nbtlbrs = btracks\nelse:\natlbrs = [track.tlbr for track in atracks]\nbtlbrs = [track.tlbr for track in btracks]\nious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float32)\nif len(atlbrs) and len(btlbrs):\nious = bbox_ioa(np.ascontiguousarray(atlbrs, dtype=np.float32),\nnp.ascontiguousarray(btlbrs, dtype=np.float32),\niou=True)\nreturn 1 - ious  # cost matrix\n</code></pre>"},{"location":"reference/trackers/utils/matching/#ultralytics.trackers.utils.matching.embedding_distance","title":"<code>ultralytics.trackers.utils.matching.embedding_distance(tracks, detections, metric='cosine')</code>","text":"<p>Compute distance between tracks and detections based on embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>tracks</code> <code>list[STrack]</code> <p>List of tracks.</p> required <code>detections</code> <code>list[BaseTrack]</code> <p>List of detections.</p> required <code>metric</code> <code>str</code> <p>Metric for distance computation. Defaults to 'cosine'.</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Cost matrix computed based on embeddings.</p> Source code in <code>ultralytics/trackers/utils/matching.py</code> <pre><code>def embedding_distance(tracks, detections, metric='cosine'):\n\"\"\"\n    Compute distance between tracks and detections based on embeddings.\n    Args:\n        tracks (list[STrack]): List of tracks.\n        detections (list[BaseTrack]): List of detections.\n        metric (str, optional): Metric for distance computation. Defaults to 'cosine'.\n    Returns:\n        (np.ndarray): Cost matrix computed based on embeddings.\n    \"\"\"\ncost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float32)\nif cost_matrix.size == 0:\nreturn cost_matrix\ndet_features = np.asarray([track.curr_feat for track in detections], dtype=np.float32)\n# for i, track in enumerate(tracks):\n# cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1,-1), det_features, metric))\ntrack_features = np.asarray([track.smooth_feat for track in tracks], dtype=np.float32)\ncost_matrix = np.maximum(0.0, cdist(track_features, det_features, metric))  # Normalized features\nreturn cost_matrix\n</code></pre>"},{"location":"reference/trackers/utils/matching/#ultralytics.trackers.utils.matching.fuse_score","title":"<code>ultralytics.trackers.utils.matching.fuse_score(cost_matrix, detections)</code>","text":"<p>Fuses cost matrix with detection scores to produce a single similarity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>cost_matrix</code> <code>ndarray</code> <p>The matrix containing cost values for assignments.</p> required <code>detections</code> <code>list[BaseTrack]</code> <p>List of detections with scores.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Fused similarity matrix.</p> Source code in <code>ultralytics/trackers/utils/matching.py</code> <pre><code>def fuse_score(cost_matrix, detections):\n\"\"\"\n    Fuses cost matrix with detection scores to produce a single similarity matrix.\n    Args:\n        cost_matrix (np.ndarray): The matrix containing cost values for assignments.\n        detections (list[BaseTrack]): List of detections with scores.\n    Returns:\n        (np.ndarray): Fused similarity matrix.\n    \"\"\"\nif cost_matrix.size == 0:\nreturn cost_matrix\niou_sim = 1 - cost_matrix\ndet_scores = np.array([det.score for det in detections])\ndet_scores = np.expand_dims(det_scores, axis=0).repeat(cost_matrix.shape[0], axis=0)\nfuse_sim = iou_sim * det_scores\nreturn 1 - fuse_sim  # fuse_cost\n</code></pre>"},{"location":"reference/utils/__init__/","title":"Reference for <code>ultralytics/utils/__init__.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/init.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/__init__/#ultralytics.utils.SimpleClass","title":"<code>ultralytics.utils.SimpleClass</code>","text":"<p>Ultralytics SimpleClass is a base class providing helpful string representation, error reporting, and attribute access methods for easier debugging and usage.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class SimpleClass:\n\"\"\"\n    Ultralytics SimpleClass is a base class providing helpful string representation, error reporting, and attribute\n    access methods for easier debugging and usage.\n    \"\"\"\ndef __str__(self):\n\"\"\"Return a human-readable string representation of the object.\"\"\"\nattr = []\nfor a in dir(self):\nv = getattr(self, a)\nif not callable(v) and not a.startswith('_'):\nif isinstance(v, SimpleClass):\n# Display only the module and class name for subclasses\ns = f'{a}: {v.__module__}.{v.__class__.__name__} object'\nelse:\ns = f'{a}: {repr(v)}'\nattr.append(s)\nreturn f'{self.__module__}.{self.__class__.__name__} object with attributes:\\n\\n' + '\\n'.join(attr)\ndef __repr__(self):\n\"\"\"Return a machine-readable string representation of the object.\"\"\"\nreturn self.__str__()\ndef __getattr__(self, attr):\n\"\"\"Custom attribute access error message with helpful information.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SimpleClass.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>Custom attribute access error message with helpful information.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __getattr__(self, attr):\n\"\"\"Custom attribute access error message with helpful information.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SimpleClass.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a machine-readable string representation of the object.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __repr__(self):\n\"\"\"Return a machine-readable string representation of the object.\"\"\"\nreturn self.__str__()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SimpleClass.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the object.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __str__(self):\n\"\"\"Return a human-readable string representation of the object.\"\"\"\nattr = []\nfor a in dir(self):\nv = getattr(self, a)\nif not callable(v) and not a.startswith('_'):\nif isinstance(v, SimpleClass):\n# Display only the module and class name for subclasses\ns = f'{a}: {v.__module__}.{v.__class__.__name__} object'\nelse:\ns = f'{a}: {repr(v)}'\nattr.append(s)\nreturn f'{self.__module__}.{self.__class__.__name__} object with attributes:\\n\\n' + '\\n'.join(attr)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.IterableSimpleNamespace","title":"<code>ultralytics.utils.IterableSimpleNamespace</code>","text":"<p>             Bases: <code>SimpleNamespace</code></p> <p>Ultralytics IterableSimpleNamespace is an extension class of SimpleNamespace that adds iterable functionality and enables usage with dict() and for loops.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class IterableSimpleNamespace(SimpleNamespace):\n\"\"\"\n    Ultralytics IterableSimpleNamespace is an extension class of SimpleNamespace that adds iterable functionality and\n    enables usage with dict() and for loops.\n    \"\"\"\ndef __iter__(self):\n\"\"\"Return an iterator of key-value pairs from the namespace's attributes.\"\"\"\nreturn iter(vars(self).items())\ndef __str__(self):\n\"\"\"Return a human-readable string representation of the object.\"\"\"\nreturn '\\n'.join(f'{k}={v}' for k, v in vars(self).items())\ndef __getattr__(self, attr):\n\"\"\"Custom attribute access error message with helpful information.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"\"\"\n            '{name}' object has no attribute '{attr}'. This may be caused by a modified or out of date ultralytics\n            'default.yaml' file.\\nPlease update your code with 'pip install -U ultralytics' and if necessary replace\n{DEFAULT_CFG_PATH} with the latest version from\n            https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml\n            \"\"\")\ndef get(self, key, default=None):\n\"\"\"Return the value of the specified key if it exists; otherwise, return the default value.\"\"\"\nreturn getattr(self, key, default)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.IterableSimpleNamespace.__getattr__","title":"<code>__getattr__(attr)</code>","text":"<p>Custom attribute access error message with helpful information.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __getattr__(self, attr):\n\"\"\"Custom attribute access error message with helpful information.\"\"\"\nname = self.__class__.__name__\nraise AttributeError(f\"\"\"\n        '{name}' object has no attribute '{attr}'. This may be caused by a modified or out of date ultralytics\n        'default.yaml' file.\\nPlease update your code with 'pip install -U ultralytics' and if necessary replace\n{DEFAULT_CFG_PATH} with the latest version from\n        https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml\n        \"\"\")\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.IterableSimpleNamespace.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator of key-value pairs from the namespace's attributes.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __iter__(self):\n\"\"\"Return an iterator of key-value pairs from the namespace's attributes.\"\"\"\nreturn iter(vars(self).items())\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.IterableSimpleNamespace.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the object.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __str__(self):\n\"\"\"Return a human-readable string representation of the object.\"\"\"\nreturn '\\n'.join(f'{k}={v}' for k, v in vars(self).items())\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.IterableSimpleNamespace.get","title":"<code>get(key, default=None)</code>","text":"<p>Return the value of the specified key if it exists; otherwise, return the default value.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get(self, key, default=None):\n\"\"\"Return the value of the specified key if it exists; otherwise, return the default value.\"\"\"\nreturn getattr(self, key, default)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.EmojiFilter","title":"<code>ultralytics.utils.EmojiFilter</code>","text":"<p>             Bases: <code>Filter</code></p> <p>A custom logging filter class for removing emojis in log messages.</p> <p>This filter is particularly useful for ensuring compatibility with Windows terminals that may not support the display of emojis in log messages.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class EmojiFilter(logging.Filter):\n\"\"\"\n    A custom logging filter class for removing emojis in log messages.\n    This filter is particularly useful for ensuring compatibility with Windows terminals\n    that may not support the display of emojis in log messages.\n    \"\"\"\ndef filter(self, record):\n\"\"\"Filter logs by emoji unicode characters on windows.\"\"\"\nrecord.msg = emojis(record.msg)\nreturn super().filter(record)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.EmojiFilter.filter","title":"<code>filter(record)</code>","text":"<p>Filter logs by emoji unicode characters on windows.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def filter(self, record):\n\"\"\"Filter logs by emoji unicode characters on windows.\"\"\"\nrecord.msg = emojis(record.msg)\nreturn super().filter(record)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.ThreadingLocked","title":"<code>ultralytics.utils.ThreadingLocked</code>","text":"<p>A decorator class for ensuring thread-safe execution of a function or method. This class can be used as a decorator to make sure that if the decorated function is called from multiple threads, only one thread at a time will be able to execute the function.</p> <p>Attributes:</p> Name Type Description <code>lock</code> <code>Lock</code> <p>A lock object used to manage access to the decorated function.</p> Example <pre><code>from ultralytics.utils import ThreadingLocked\n@ThreadingLocked()\ndef my_function():\n# Your code here\npass\n</code></pre> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class ThreadingLocked:\n\"\"\"\n    A decorator class for ensuring thread-safe execution of a function or method.\n    This class can be used as a decorator to make sure that if the decorated function\n    is called from multiple threads, only one thread at a time will be able to execute the function.\n    Attributes:\n        lock (threading.Lock): A lock object used to manage access to the decorated function.\n    Example:\n        ```python\n        from ultralytics.utils import ThreadingLocked\n        @ThreadingLocked()\n        def my_function():\n            # Your code here\n            pass\n        ```\n    \"\"\"\ndef __init__(self):\nself.lock = threading.Lock()\ndef __call__(self, f):\nfrom functools import wraps\n@wraps(f)\ndef decorated(*args, **kwargs):\nwith self.lock:\nreturn f(*args, **kwargs)\nreturn decorated\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.TryExcept","title":"<code>ultralytics.utils.TryExcept</code>","text":"<p>             Bases: <code>ContextDecorator</code></p> <p>YOLOv8 TryExcept class. Usage: @TryExcept() decorator or 'with TryExcept():' context manager.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class TryExcept(contextlib.ContextDecorator):\n\"\"\"YOLOv8 TryExcept class. Usage: @TryExcept() decorator or 'with TryExcept():' context manager.\"\"\"\ndef __init__(self, msg='', verbose=True):\n\"\"\"Initialize TryExcept class with optional message and verbosity settings.\"\"\"\nself.msg = msg\nself.verbose = verbose\ndef __enter__(self):\n\"\"\"Executes when entering TryExcept context, initializes instance.\"\"\"\npass\ndef __exit__(self, exc_type, value, traceback):\n\"\"\"Defines behavior when exiting a 'with' block, prints error message if necessary.\"\"\"\nif self.verbose and value:\nprint(emojis(f\"{self.msg}{': ' if self.msg else ''}{value}\"))\nreturn True\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.TryExcept.__enter__","title":"<code>__enter__()</code>","text":"<p>Executes when entering TryExcept context, initializes instance.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __enter__(self):\n\"\"\"Executes when entering TryExcept context, initializes instance.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.TryExcept.__exit__","title":"<code>__exit__(exc_type, value, traceback)</code>","text":"<p>Defines behavior when exiting a 'with' block, prints error message if necessary.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __exit__(self, exc_type, value, traceback):\n\"\"\"Defines behavior when exiting a 'with' block, prints error message if necessary.\"\"\"\nif self.verbose and value:\nprint(emojis(f\"{self.msg}{': ' if self.msg else ''}{value}\"))\nreturn True\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.TryExcept.__init__","title":"<code>__init__(msg='', verbose=True)</code>","text":"<p>Initialize TryExcept class with optional message and verbosity settings.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __init__(self, msg='', verbose=True):\n\"\"\"Initialize TryExcept class with optional message and verbosity settings.\"\"\"\nself.msg = msg\nself.verbose = verbose\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager","title":"<code>ultralytics.utils.SettingsManager</code>","text":"<p>             Bases: <code>dict</code></p> <p>Manages Ultralytics settings stored in a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path</code> <p>Path to the Ultralytics settings YAML file. Default is USER_CONFIG_DIR / 'settings.yaml'.</p> <code>SETTINGS_YAML</code> <code>version</code> <code>str</code> <p>Settings version. In case of local version mismatch, new default settings will be saved.</p> <code>'0.0.4'</code> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>class SettingsManager(dict):\n\"\"\"\n    Manages Ultralytics settings stored in a YAML file.\n    Args:\n        file (str | Path): Path to the Ultralytics settings YAML file. Default is USER_CONFIG_DIR / 'settings.yaml'.\n        version (str): Settings version. In case of local version mismatch, new default settings will be saved.\n    \"\"\"\ndef __init__(self, file=SETTINGS_YAML, version='0.0.4'):\nimport copy\nimport hashlib\nfrom ultralytics.utils.checks import check_version\nfrom ultralytics.utils.torch_utils import torch_distributed_zero_first\ngit_dir = get_git_dir()\nroot = git_dir or Path()\ndatasets_root = (root.parent if git_dir and is_dir_writeable(root.parent) else root).resolve()\nself.file = Path(file)\nself.version = version\nself.defaults = {\n'settings_version': version,\n'datasets_dir': str(datasets_root / 'datasets'),\n'weights_dir': str(root / 'weights'),\n'runs_dir': str(root / 'runs'),\n'uuid': hashlib.sha256(str(uuid.getnode()).encode()).hexdigest(),\n'sync': True,\n'api_key': '',\n'clearml': True,  # integrations\n'comet': True,\n'dvc': True,\n'hub': True,\n'mlflow': True,\n'neptune': True,\n'raytune': True,\n'tensorboard': True,\n'wandb': True}\nsuper().__init__(copy.deepcopy(self.defaults))\nwith torch_distributed_zero_first(RANK):\nif not self.file.exists():\nself.save()\nself.load()\ncorrect_keys = self.keys() == self.defaults.keys()\ncorrect_types = all(type(a) is type(b) for a, b in zip(self.values(), self.defaults.values()))\ncorrect_version = check_version(self['settings_version'], self.version)\nif not (correct_keys and correct_types and correct_version):\nLOGGER.warning(\n'WARNING \u26a0\ufe0f Ultralytics settings reset to default values. This may be due to a possible problem '\n'with your settings or a recent ultralytics package update. '\nf\"\\nView settings with 'yolo settings' or at '{self.file}'\"\n\"\\nUpdate settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'.\")\nself.reset()\ndef load(self):\n\"\"\"Loads settings from the YAML file.\"\"\"\nsuper().update(yaml_load(self.file))\ndef save(self):\n\"\"\"Saves the current settings to the YAML file.\"\"\"\nyaml_save(self.file, dict(self))\ndef update(self, *args, **kwargs):\n\"\"\"Updates a setting value in the current settings.\"\"\"\nsuper().update(*args, **kwargs)\nself.save()\ndef reset(self):\n\"\"\"Resets the settings to default and saves them.\"\"\"\nself.clear()\nself.update(self.defaults)\nself.save()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager.__init__","title":"<code>__init__(file=SETTINGS_YAML, version='0.0.4')</code>","text":"Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def __init__(self, file=SETTINGS_YAML, version='0.0.4'):\nimport copy\nimport hashlib\nfrom ultralytics.utils.checks import check_version\nfrom ultralytics.utils.torch_utils import torch_distributed_zero_first\ngit_dir = get_git_dir()\nroot = git_dir or Path()\ndatasets_root = (root.parent if git_dir and is_dir_writeable(root.parent) else root).resolve()\nself.file = Path(file)\nself.version = version\nself.defaults = {\n'settings_version': version,\n'datasets_dir': str(datasets_root / 'datasets'),\n'weights_dir': str(root / 'weights'),\n'runs_dir': str(root / 'runs'),\n'uuid': hashlib.sha256(str(uuid.getnode()).encode()).hexdigest(),\n'sync': True,\n'api_key': '',\n'clearml': True,  # integrations\n'comet': True,\n'dvc': True,\n'hub': True,\n'mlflow': True,\n'neptune': True,\n'raytune': True,\n'tensorboard': True,\n'wandb': True}\nsuper().__init__(copy.deepcopy(self.defaults))\nwith torch_distributed_zero_first(RANK):\nif not self.file.exists():\nself.save()\nself.load()\ncorrect_keys = self.keys() == self.defaults.keys()\ncorrect_types = all(type(a) is type(b) for a, b in zip(self.values(), self.defaults.values()))\ncorrect_version = check_version(self['settings_version'], self.version)\nif not (correct_keys and correct_types and correct_version):\nLOGGER.warning(\n'WARNING \u26a0\ufe0f Ultralytics settings reset to default values. This may be due to a possible problem '\n'with your settings or a recent ultralytics package update. '\nf\"\\nView settings with 'yolo settings' or at '{self.file}'\"\n\"\\nUpdate settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'.\")\nself.reset()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager.load","title":"<code>load()</code>","text":"<p>Loads settings from the YAML file.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def load(self):\n\"\"\"Loads settings from the YAML file.\"\"\"\nsuper().update(yaml_load(self.file))\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager.reset","title":"<code>reset()</code>","text":"<p>Resets the settings to default and saves them.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def reset(self):\n\"\"\"Resets the settings to default and saves them.\"\"\"\nself.clear()\nself.update(self.defaults)\nself.save()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager.save","title":"<code>save()</code>","text":"<p>Saves the current settings to the YAML file.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def save(self):\n\"\"\"Saves the current settings to the YAML file.\"\"\"\nyaml_save(self.file, dict(self))\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.SettingsManager.update","title":"<code>update(*args, **kwargs)</code>","text":"<p>Updates a setting value in the current settings.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def update(self, *args, **kwargs):\n\"\"\"Updates a setting value in the current settings.\"\"\"\nsuper().update(*args, **kwargs)\nself.save()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.plt_settings","title":"<code>ultralytics.utils.plt_settings(rcparams=None, backend='Agg')</code>","text":"<p>Decorator to temporarily set rc parameters and the backend for a plotting function.</p> Example <p>decorator: @plt_settings({\"font.size\": 12}) context manager: with plt_settings({\"font.size\": 12}):</p> <p>Parameters:</p> Name Type Description Default <code>rcparams</code> <code>dict</code> <p>Dictionary of rc parameters to set.</p> <code>None</code> <code>backend</code> <code>str</code> <p>Name of the backend to use. Defaults to 'Agg'.</p> <code>'Agg'</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Decorated function with temporarily set rc parameters and backend. This decorator can be applied to any function that needs to have specific matplotlib rc parameters and backend for its execution.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def plt_settings(rcparams=None, backend='Agg'):\n\"\"\"\n    Decorator to temporarily set rc parameters and the backend for a plotting function.\n    Example:\n        decorator: @plt_settings({\"font.size\": 12})\n        context manager: with plt_settings({\"font.size\": 12}):\n    Args:\n        rcparams (dict): Dictionary of rc parameters to set.\n        backend (str, optional): Name of the backend to use. Defaults to 'Agg'.\n    Returns:\n        (Callable): Decorated function with temporarily set rc parameters and backend. This decorator can be\n            applied to any function that needs to have specific matplotlib rc parameters and backend for its execution.\n    \"\"\"\nif rcparams is None:\nrcparams = {'font.size': 11}\ndef decorator(func):\n\"\"\"Decorator to apply temporary rc parameters and backend to a function.\"\"\"\ndef wrapper(*args, **kwargs):\n\"\"\"Sets rc parameters and backend, calls the original function, and restores the settings.\"\"\"\noriginal_backend = plt.get_backend()\nplt.switch_backend(backend)\nwith plt.rc_context(rcparams):\nresult = func(*args, **kwargs)\nplt.switch_backend(original_backend)\nreturn result\nreturn wrapper\nreturn decorator\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.set_logging","title":"<code>ultralytics.utils.set_logging(name=LOGGING_NAME, verbose=True)</code>","text":"<p>Sets up logging for the given name.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def set_logging(name=LOGGING_NAME, verbose=True):\n\"\"\"Sets up logging for the given name.\"\"\"\nrank = int(os.getenv('RANK', -1))  # rank in world for Multi-GPU trainings\nlevel = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR\nlogging.config.dictConfig({\n'version': 1,\n'disable_existing_loggers': False,\n'formatters': {\nname: {\n'format': '%(message)s'}},\n'handlers': {\nname: {\n'class': 'logging.StreamHandler',\n'formatter': name,\n'level': level}},\n'loggers': {\nname: {\n'level': level,\n'handlers': [name],\n'propagate': False}}})\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.emojis","title":"<code>ultralytics.utils.emojis(string='')</code>","text":"<p>Return platform-dependent emoji-safe version of string.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def emojis(string=''):\n\"\"\"Return platform-dependent emoji-safe version of string.\"\"\"\nreturn string.encode().decode('ascii', 'ignore') if WINDOWS else string\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.yaml_save","title":"<code>ultralytics.utils.yaml_save(file='data.yaml', data=None)</code>","text":"<p>Save YAML data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name. Default is 'data.yaml'.</p> <code>'data.yaml'</code> <code>data</code> <code>dict</code> <p>Data to save in YAML format.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Data is saved to the specified file.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def yaml_save(file='data.yaml', data=None):\n\"\"\"\n    Save YAML data to a file.\n    Args:\n        file (str, optional): File name. Default is 'data.yaml'.\n        data (dict): Data to save in YAML format.\n    Returns:\n        (None): Data is saved to the specified file.\n    \"\"\"\nif data is None:\ndata = {}\nfile = Path(file)\nif not file.parent.exists():\n# Create parent directories if they don't exist\nfile.parent.mkdir(parents=True, exist_ok=True)\n# Convert Path objects to strings\nfor k, v in data.items():\nif isinstance(v, Path):\ndata[k] = str(v)\n# Dump data to file in YAML format\nwith open(file, 'w') as f:\nyaml.safe_dump(data, f, sort_keys=False, allow_unicode=True)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.yaml_load","title":"<code>ultralytics.utils.yaml_load(file='data.yaml', append_filename=False)</code>","text":"<p>Load YAML data from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name. Default is 'data.yaml'.</p> <code>'data.yaml'</code> <code>append_filename</code> <code>bool</code> <p>Add the YAML filename to the YAML dictionary. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>YAML data and file name.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def yaml_load(file='data.yaml', append_filename=False):\n\"\"\"\n    Load YAML data from a file.\n    Args:\n        file (str, optional): File name. Default is 'data.yaml'.\n        append_filename (bool): Add the YAML filename to the YAML dictionary. Default is False.\n    Returns:\n        (dict): YAML data and file name.\n    \"\"\"\nwith open(file, errors='ignore', encoding='utf-8') as f:\ns = f.read()  # string\n# Remove special characters\nif not s.isprintable():\ns = re.sub(r'[^\\x09\\x0A\\x0D\\x20-\\x7E\\x85\\xA0-\\uD7FF\\uE000-\\uFFFD\\U00010000-\\U0010ffff]+', '', s)\n# Add YAML filename to dict and return\ndata = yaml.safe_load(s) or {}  # always return a dict (yaml.safe_load() may return None for empty files)\nif append_filename:\ndata['yaml_file'] = str(file)\nreturn data\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.yaml_print","title":"<code>ultralytics.utils.yaml_print(yaml_file)</code>","text":"<p>Pretty prints a YAML file or a YAML-formatted dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Union[str, Path, dict]</code> <p>The file path of the YAML file or a YAML-formatted dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def yaml_print(yaml_file: Union[str, Path, dict]) -&gt; None:\n\"\"\"\n    Pretty prints a YAML file or a YAML-formatted dictionary.\n    Args:\n        yaml_file: The file path of the YAML file or a YAML-formatted dictionary.\n    Returns:\n        None\n    \"\"\"\nyaml_dict = yaml_load(yaml_file) if isinstance(yaml_file, (str, Path)) else yaml_file\ndump = yaml.dump(yaml_dict, sort_keys=False, allow_unicode=True)\nLOGGER.info(f\"Printing '{colorstr('bold', 'black', yaml_file)}'\\n\\n{dump}\")\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_ubuntu","title":"<code>ultralytics.utils.is_ubuntu()</code>","text":"<p>Check if the OS is Ubuntu.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if OS is Ubuntu, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_ubuntu() -&gt; bool:\n\"\"\"\n    Check if the OS is Ubuntu.\n    Returns:\n        (bool): True if OS is Ubuntu, False otherwise.\n    \"\"\"\nwith contextlib.suppress(FileNotFoundError):\nwith open('/etc/os-release') as f:\nreturn 'ID=ubuntu' in f.read()\nreturn False\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_colab","title":"<code>ultralytics.utils.is_colab()</code>","text":"<p>Check if the current script is running inside a Google Colab notebook.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running inside a Colab notebook, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_colab():\n\"\"\"\n    Check if the current script is running inside a Google Colab notebook.\n    Returns:\n        (bool): True if running inside a Colab notebook, False otherwise.\n    \"\"\"\nreturn 'COLAB_RELEASE_TAG' in os.environ or 'COLAB_BACKEND_VERSION' in os.environ\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_kaggle","title":"<code>ultralytics.utils.is_kaggle()</code>","text":"<p>Check if the current script is running inside a Kaggle kernel.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running inside a Kaggle kernel, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_kaggle():\n\"\"\"\n    Check if the current script is running inside a Kaggle kernel.\n    Returns:\n        (bool): True if running inside a Kaggle kernel, False otherwise.\n    \"\"\"\nreturn os.environ.get('PWD') == '/kaggle/working' and os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com'\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_jupyter","title":"<code>ultralytics.utils.is_jupyter()</code>","text":"<p>Check if the current script is running inside a Jupyter Notebook. Verified on Colab, Jupyterlab, Kaggle, Paperspace.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if running inside a Jupyter Notebook, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_jupyter():\n\"\"\"\n    Check if the current script is running inside a Jupyter Notebook.\n    Verified on Colab, Jupyterlab, Kaggle, Paperspace.\n    Returns:\n        (bool): True if running inside a Jupyter Notebook, False otherwise.\n    \"\"\"\nwith contextlib.suppress(Exception):\nfrom IPython import get_ipython\nreturn get_ipython() is not None\nreturn False\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_docker","title":"<code>ultralytics.utils.is_docker()</code>","text":"<p>Determine if the script is running inside a Docker container.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the script is running inside a Docker container, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_docker() -&gt; bool:\n\"\"\"\n    Determine if the script is running inside a Docker container.\n    Returns:\n        (bool): True if the script is running inside a Docker container, False otherwise.\n    \"\"\"\nfile = Path('/proc/self/cgroup')\nif file.exists():\nwith open(file) as f:\nreturn 'docker' in f.read()\nelse:\nreturn False\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_online","title":"<code>ultralytics.utils.is_online()</code>","text":"<p>Check internet connectivity by attempting to connect to a known online host.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connection is successful, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_online() -&gt; bool:\n\"\"\"\n    Check internet connectivity by attempting to connect to a known online host.\n    Returns:\n        (bool): True if connection is successful, False otherwise.\n    \"\"\"\nimport socket\nfor host in '1.1.1.1', '8.8.8.8', '223.5.5.5':  # Cloudflare, Google, AliDNS:\ntry:\ntest_connection = socket.create_connection(address=(host, 53), timeout=2)\nexcept (socket.timeout, socket.gaierror, OSError):\ncontinue\nelse:\n# If the connection was successful, close it to avoid a ResourceWarning\ntest_connection.close()\nreturn True\nreturn False\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_pip_package","title":"<code>ultralytics.utils.is_pip_package(filepath=__name__)</code>","text":"<p>Determines if the file at the given filepath is part of a pip package.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The filepath to check.</p> <code>__name__</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file is part of a pip package, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_pip_package(filepath: str = __name__) -&gt; bool:\n\"\"\"\n    Determines if the file at the given filepath is part of a pip package.\n    Args:\n        filepath (str): The filepath to check.\n    Returns:\n        (bool): True if the file is part of a pip package, False otherwise.\n    \"\"\"\nimport importlib.util\n# Get the spec for the module\nspec = importlib.util.find_spec(filepath)\n# Return whether the spec is not None and the origin is not None (indicating it is a package)\nreturn spec is not None and spec.origin is not None\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_dir_writeable","title":"<code>ultralytics.utils.is_dir_writeable(dir_path)</code>","text":"<p>Check if a directory is writeable.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str | Path</code> <p>The path to the directory.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the directory is writeable, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_dir_writeable(dir_path: Union[str, Path]) -&gt; bool:\n\"\"\"\n    Check if a directory is writeable.\n    Args:\n        dir_path (str | Path): The path to the directory.\n    Returns:\n        (bool): True if the directory is writeable, False otherwise.\n    \"\"\"\nreturn os.access(str(dir_path), os.W_OK)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_pytest_running","title":"<code>ultralytics.utils.is_pytest_running()</code>","text":"<p>Determines whether pytest is currently running or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if pytest is running, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_pytest_running():\n\"\"\"\n    Determines whether pytest is currently running or not.\n    Returns:\n        (bool): True if pytest is running, False otherwise.\n    \"\"\"\nreturn ('PYTEST_CURRENT_TEST' in os.environ) or ('pytest' in sys.modules) or ('pytest' in Path(sys.argv[0]).stem)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_github_actions_ci","title":"<code>ultralytics.utils.is_github_actions_ci()</code>","text":"<p>Determine if the current environment is a GitHub Actions CI Python runner.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current environment is a GitHub Actions CI Python runner, False otherwise.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_github_actions_ci() -&gt; bool:\n\"\"\"\n    Determine if the current environment is a GitHub Actions CI Python runner.\n    Returns:\n        (bool): True if the current environment is a GitHub Actions CI Python runner, False otherwise.\n    \"\"\"\nreturn 'GITHUB_ACTIONS' in os.environ and 'RUNNER_OS' in os.environ and 'RUNNER_TOOL_CACHE' in os.environ\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.is_git_dir","title":"<code>ultralytics.utils.is_git_dir()</code>","text":"<p>Determines whether the current file is part of a git repository. If the current file is not part of a git repository, returns None.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if current file is part of a git repository.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def is_git_dir():\n\"\"\"\n    Determines whether the current file is part of a git repository.\n    If the current file is not part of a git repository, returns None.\n    Returns:\n        (bool): True if current file is part of a git repository.\n    \"\"\"\nreturn get_git_dir() is not None\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_git_dir","title":"<code>ultralytics.utils.get_git_dir()</code>","text":"<p>Determines whether the current file is part of a git repository and if so, returns the repository root directory. If the current file is not part of a git repository, returns None.</p> <p>Returns:</p> Type Description <code>Path | None</code> <p>Git root directory if found or None if not found.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_git_dir():\n\"\"\"\n    Determines whether the current file is part of a git repository and if so, returns the repository root directory.\n    If the current file is not part of a git repository, returns None.\n    Returns:\n        (Path | None): Git root directory if found or None if not found.\n    \"\"\"\nfor d in Path(__file__).parents:\nif (d / '.git').is_dir():\nreturn d\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_git_origin_url","title":"<code>ultralytics.utils.get_git_origin_url()</code>","text":"<p>Retrieves the origin URL of a git repository.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The origin URL of the git repository or None if not git directory.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_git_origin_url():\n\"\"\"\n    Retrieves the origin URL of a git repository.\n    Returns:\n        (str | None): The origin URL of the git repository or None if not git directory.\n    \"\"\"\nif is_git_dir():\nwith contextlib.suppress(subprocess.CalledProcessError):\norigin = subprocess.check_output(['git', 'config', '--get', 'remote.origin.url'])\nreturn origin.decode().strip()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_git_branch","title":"<code>ultralytics.utils.get_git_branch()</code>","text":"<p>Returns the current git branch name. If not in a git repository, returns None.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The current git branch name or None if not a git directory.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_git_branch():\n\"\"\"\n    Returns the current git branch name. If not in a git repository, returns None.\n    Returns:\n        (str | None): The current git branch name or None if not a git directory.\n    \"\"\"\nif is_git_dir():\nwith contextlib.suppress(subprocess.CalledProcessError):\norigin = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\nreturn origin.decode().strip()\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_default_args","title":"<code>ultralytics.utils.get_default_args(func)</code>","text":"<p>Returns a dictionary of default arguments for a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to inspect.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key is a parameter name, and each value is the default value of that parameter.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_default_args(func):\n\"\"\"Returns a dictionary of default arguments for a function.\n    Args:\n        func (callable): The function to inspect.\n    Returns:\n        (dict): A dictionary where each key is a parameter name, and each value is the default value of that parameter.\n    \"\"\"\nsignature = inspect.signature(func)\nreturn {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_ubuntu_version","title":"<code>ultralytics.utils.get_ubuntu_version()</code>","text":"<p>Retrieve the Ubuntu version if the OS is Ubuntu.</p> <p>Returns:</p> Type Description <code>str</code> <p>Ubuntu version or None if not an Ubuntu OS.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_ubuntu_version():\n\"\"\"\n    Retrieve the Ubuntu version if the OS is Ubuntu.\n    Returns:\n        (str): Ubuntu version or None if not an Ubuntu OS.\n    \"\"\"\nif is_ubuntu():\nwith contextlib.suppress(FileNotFoundError, AttributeError):\nwith open('/etc/os-release') as f:\nreturn re.search(r'VERSION_ID=\"(\\d+\\.\\d+)\"', f.read())[1]\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.get_user_config_dir","title":"<code>ultralytics.utils.get_user_config_dir(sub_dir='Ultralytics')</code>","text":"<p>Get the user config directory.</p> <p>Parameters:</p> Name Type Description Default <code>sub_dir</code> <code>str</code> <p>The name of the subdirectory to create.</p> <code>'Ultralytics'</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the user config directory.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def get_user_config_dir(sub_dir='Ultralytics'):\n\"\"\"\n    Get the user config directory.\n    Args:\n        sub_dir (str): The name of the subdirectory to create.\n    Returns:\n        (Path): The path to the user config directory.\n    \"\"\"\n# Return the appropriate config directory for each operating system\nif WINDOWS:\npath = Path.home() / 'AppData' / 'Roaming' / sub_dir\nelif MACOS:  # macOS\npath = Path.home() / 'Library' / 'Application Support' / sub_dir\nelif LINUX:\npath = Path.home() / '.config' / sub_dir\nelse:\nraise ValueError(f'Unsupported operating system: {platform.system()}')\n# GCP and AWS lambda fix, only /tmp is writeable\nif not is_dir_writeable(path.parent):\nLOGGER.warning(f\"WARNING \u26a0\ufe0f user config directory '{path}' is not writeable, defaulting to '/tmp' or CWD.\"\n'Alternatively you can define a YOLO_CONFIG_DIR environment variable for this path.')\npath = Path('/tmp') / sub_dir if is_dir_writeable('/tmp') else Path().cwd() / sub_dir\n# Create the subdirectory if it does not exist\npath.mkdir(parents=True, exist_ok=True)\nreturn path\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.colorstr","title":"<code>ultralytics.utils.colorstr(*input)</code>","text":"<p>Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world').</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def colorstr(*input):\n\"\"\"Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world').\"\"\"\n*args, string = input if len(input) &gt; 1 else ('blue', 'bold', input[0])  # color arguments, string\ncolors = {\n'black': '\\033[30m',  # basic colors\n'red': '\\033[31m',\n'green': '\\033[32m',\n'yellow': '\\033[33m',\n'blue': '\\033[34m',\n'magenta': '\\033[35m',\n'cyan': '\\033[36m',\n'white': '\\033[37m',\n'bright_black': '\\033[90m',  # bright colors\n'bright_red': '\\033[91m',\n'bright_green': '\\033[92m',\n'bright_yellow': '\\033[93m',\n'bright_blue': '\\033[94m',\n'bright_magenta': '\\033[95m',\n'bright_cyan': '\\033[96m',\n'bright_white': '\\033[97m',\n'end': '\\033[0m',  # misc\n'bold': '\\033[1m',\n'underline': '\\033[4m'}\nreturn ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.threaded","title":"<code>ultralytics.utils.threaded(func)</code>","text":"<p>Multi-threads a target function and returns thread. Usage: @threaded decorator.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def threaded(func):\n\"\"\"Multi-threads a target function and returns thread. Usage: @threaded decorator.\"\"\"\ndef wrapper(*args, **kwargs):\n\"\"\"Multi-threads a given function and returns the thread.\"\"\"\nthread = threading.Thread(target=func, args=args, kwargs=kwargs, daemon=True)\nthread.start()\nreturn thread\nreturn wrapper\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.set_sentry","title":"<code>ultralytics.utils.set_sentry()</code>","text":"<p>Initialize the Sentry SDK for error tracking and reporting. Only used if sentry_sdk package is installed and sync=True in settings. Run 'yolo settings' to see and update settings YAML file.</p> <p>Conditions required to send errors (ALL conditions must be met or no errors will be reported):     - sentry_sdk package is installed     - sync=True in YOLO settings     - pytest is not running     - running in a pip package installation     - running in a non-git directory     - running with rank -1 or 0     - online environment     - CLI used to run package (checked with 'yolo' as the name of the main CLI command)</p> <p>The function also configures Sentry SDK to ignore KeyboardInterrupt and FileNotFoundError exceptions and to exclude events with 'out of memory' in their exception message.</p> <p>Additionally, the function sets custom tags and user information for Sentry events.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def set_sentry():\n\"\"\"\n    Initialize the Sentry SDK for error tracking and reporting. Only used if sentry_sdk package is installed and\n    sync=True in settings. Run 'yolo settings' to see and update settings YAML file.\n    Conditions required to send errors (ALL conditions must be met or no errors will be reported):\n        - sentry_sdk package is installed\n        - sync=True in YOLO settings\n        - pytest is not running\n        - running in a pip package installation\n        - running in a non-git directory\n        - running with rank -1 or 0\n        - online environment\n        - CLI used to run package (checked with 'yolo' as the name of the main CLI command)\n    The function also configures Sentry SDK to ignore KeyboardInterrupt and FileNotFoundError\n    exceptions and to exclude events with 'out of memory' in their exception message.\n    Additionally, the function sets custom tags and user information for Sentry events.\n    \"\"\"\ndef before_send(event, hint):\n\"\"\"\n        Modify the event before sending it to Sentry based on specific exception types and messages.\n        Args:\n            event (dict): The event dictionary containing information about the error.\n            hint (dict): A dictionary containing additional information about the error.\n        Returns:\n            dict: The modified event or None if the event should not be sent to Sentry.\n        \"\"\"\nif 'exc_info' in hint:\nexc_type, exc_value, tb = hint['exc_info']\nif exc_type in (KeyboardInterrupt, FileNotFoundError) \\\n                    or 'out of memory' in str(exc_value):\nreturn None  # do not send event\nevent['tags'] = {\n'sys_argv': sys.argv[0],\n'sys_argv_name': Path(sys.argv[0]).name,\n'install': 'git' if is_git_dir() else 'pip' if is_pip_package() else 'other',\n'os': ENVIRONMENT}\nreturn event\nif SETTINGS['sync'] and \\\n            RANK in (-1, 0) and \\\n            Path(sys.argv[0]).name == 'yolo' and \\\n            not TESTS_RUNNING and \\\n            ONLINE and \\\n            is_pip_package() and \\\n            not is_git_dir():\n# If sentry_sdk package is not installed then return and do not use Sentry\ntry:\nimport sentry_sdk  # noqa\nexcept ImportError:\nreturn\nsentry_sdk.init(\ndsn='https://5ff1556b71594bfea135ff0203a0d290@o4504521589325824.ingest.sentry.io/4504521592406016',\ndebug=False,\ntraces_sample_rate=1.0,\nrelease=__version__,\nenvironment='production',  # 'dev' or 'production'\nbefore_send=before_send,\nignore_errors=[KeyboardInterrupt, FileNotFoundError])\nsentry_sdk.set_user({'id': SETTINGS['uuid']})  # SHA-256 anonymized UUID hash\n# Disable all sentry logging\nfor logger in 'sentry_sdk', 'sentry_sdk.errors':\nlogging.getLogger(logger).setLevel(logging.CRITICAL)\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.deprecation_warn","title":"<code>ultralytics.utils.deprecation_warn(arg, new_arg, version=None)</code>","text":"<p>Issue a deprecation warning when a deprecated argument is used, suggesting an updated argument.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def deprecation_warn(arg, new_arg, version=None):\n\"\"\"Issue a deprecation warning when a deprecated argument is used, suggesting an updated argument.\"\"\"\nif not version:\nversion = float(__version__[:3]) + 0.2  # deprecate after 2nd major release\nLOGGER.warning(f\"WARNING \u26a0\ufe0f '{arg}' is deprecated and will be removed in 'ultralytics {version}' in the future. \"\nf\"Please use '{new_arg}' instead.\")\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.clean_url","title":"<code>ultralytics.utils.clean_url(url)</code>","text":"<p>Strip auth from URL, i.e. https://url.com/file.txt?auth -&gt; https://url.com/file.txt.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def clean_url(url):\n\"\"\"Strip auth from URL, i.e. https://url.com/file.txt?auth -&gt; https://url.com/file.txt.\"\"\"\nurl = Path(url).as_posix().replace(':/', '://')  # Pathlib turns :// -&gt; :/, as_posix() for Windows\nreturn urllib.parse.unquote(url).split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\n</code></pre>"},{"location":"reference/utils/__init__/#ultralytics.utils.url2file","title":"<code>ultralytics.utils.url2file(url)</code>","text":"<p>Convert URL to filename, i.e. https://url.com/file.txt?auth -&gt; file.txt.</p> Source code in <code>ultralytics/utils/__init__.py</code> <pre><code>def url2file(url):\n\"\"\"Convert URL to filename, i.e. https://url.com/file.txt?auth -&gt; file.txt.\"\"\"\nreturn Path(clean_url(url)).name\n</code></pre>"},{"location":"reference/utils/autobatch/","title":"Reference for <code>ultralytics/utils/autobatch.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/autobatch.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/utils/autobatch/#ultralytics.utils.autobatch.check_train_batch_size","title":"<code>ultralytics.utils.autobatch.check_train_batch_size(model, imgsz=640, amp=True)</code>","text":"<p>Check YOLO training batch size using the autobatch() function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>YOLO model to check batch size for.</p> required <code>imgsz</code> <code>int</code> <p>Image size used for training.</p> <code>640</code> <code>amp</code> <code>bool</code> <p>If True, use automatic mixed precision (AMP) for training.</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>Optimal batch size computed using the autobatch() function.</p> Source code in <code>ultralytics/utils/autobatch.py</code> <pre><code>def check_train_batch_size(model, imgsz=640, amp=True):\n\"\"\"\n    Check YOLO training batch size using the autobatch() function.\n    Args:\n        model (torch.nn.Module): YOLO model to check batch size for.\n        imgsz (int): Image size used for training.\n        amp (bool): If True, use automatic mixed precision (AMP) for training.\n    Returns:\n        (int): Optimal batch size computed using the autobatch() function.\n    \"\"\"\nwith torch.cuda.amp.autocast(amp):\nreturn autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size\n</code></pre>"},{"location":"reference/utils/autobatch/#ultralytics.utils.autobatch.autobatch","title":"<code>ultralytics.utils.autobatch.autobatch(model, imgsz=640, fraction=0.67, batch_size=DEFAULT_CFG.batch)</code>","text":"<p>Automatically estimate the best YOLO batch size to use a fraction of the available CUDA memory.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>module</code> <p>YOLO model to compute batch size for.</p> required <code>imgsz</code> <code>int</code> <p>The image size used as input for the YOLO model. Defaults to 640.</p> <code>640</code> <code>fraction</code> <code>float</code> <p>The fraction of available CUDA memory to use. Defaults to 0.67.</p> <code>0.67</code> <code>batch_size</code> <code>int</code> <p>The default batch size to use if an error is detected. Defaults to 16.</p> <code>batch</code> <p>Returns:</p> Type Description <code>int</code> <p>The optimal batch size.</p> Source code in <code>ultralytics/utils/autobatch.py</code> <pre><code>def autobatch(model, imgsz=640, fraction=0.67, batch_size=DEFAULT_CFG.batch):\n\"\"\"\n    Automatically estimate the best YOLO batch size to use a fraction of the available CUDA memory.\n    Args:\n        model (torch.nn.module): YOLO model to compute batch size for.\n        imgsz (int, optional): The image size used as input for the YOLO model. Defaults to 640.\n        fraction (float, optional): The fraction of available CUDA memory to use. Defaults to 0.67.\n        batch_size (int, optional): The default batch size to use if an error is detected. Defaults to 16.\n    Returns:\n        (int): The optimal batch size.\n    \"\"\"\n# Check device\nprefix = colorstr('AutoBatch: ')\nLOGGER.info(f'{prefix}Computing optimal batch size for imgsz={imgsz}')\ndevice = next(model.parameters()).device  # get model device\nif device.type == 'cpu':\nLOGGER.info(f'{prefix}CUDA not detected, using default CPU batch-size {batch_size}')\nreturn batch_size\nif torch.backends.cudnn.benchmark:\nLOGGER.info(f'{prefix} \u26a0\ufe0f Requires torch.backends.cudnn.benchmark=False, using default batch-size {batch_size}')\nreturn batch_size\n# Inspect CUDA memory\ngb = 1 &lt;&lt; 30  # bytes to GiB (1024 ** 3)\nd = str(device).upper()  # 'CUDA:0'\nproperties = torch.cuda.get_device_properties(device)  # device properties\nt = properties.total_memory / gb  # GiB total\nr = torch.cuda.memory_reserved(device) / gb  # GiB reserved\na = torch.cuda.memory_allocated(device) / gb  # GiB allocated\nf = t - (r + a)  # GiB free\nLOGGER.info(f'{prefix}{d} ({properties.name}) {t:.2f}G total, {r:.2f}G reserved, {a:.2f}G allocated, {f:.2f}G free')\n# Profile batch sizes\nbatch_sizes = [1, 2, 4, 8, 16]\ntry:\nimg = [torch.empty(b, 3, imgsz, imgsz) for b in batch_sizes]\nresults = profile(img, model, n=3, device=device)\n# Fit a solution\ny = [x[2] for x in results if x]  # memory [2]\np = np.polyfit(batch_sizes[:len(y)], y, deg=1)  # first degree polynomial fit\nb = int((f * fraction - p[1]) / p[0])  # y intercept (optimal batch size)\nif None in results:  # some sizes failed\ni = results.index(None)  # first fail index\nif b &gt;= batch_sizes[i]:  # y intercept above failure point\nb = batch_sizes[max(i - 1, 0)]  # select prior safe point\nif b &lt; 1 or b &gt; 1024:  # b outside of safe range\nb = batch_size\nLOGGER.info(f'{prefix}WARNING \u26a0\ufe0f CUDA anomaly detected, using default batch-size {batch_size}.')\nfraction = (np.polyval(p, b) + r + a) / t  # actual fraction predicted\nLOGGER.info(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) \u2705')\nreturn b\nexcept Exception as e:\nLOGGER.warning(f'{prefix}WARNING \u26a0\ufe0f error detected: {e},  using default batch-size {batch_size}.')\nreturn batch_size\n</code></pre>"},{"location":"reference/utils/benchmarks/","title":"Reference for <code>ultralytics/utils/benchmarks.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/benchmarks.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p>"},{"location":"reference/utils/benchmarks/#ultralytics.utils.benchmarks.ProfileModels","title":"<code>ultralytics.utils.benchmarks.ProfileModels</code>","text":"<p>ProfileModels class for profiling different models on ONNX and TensorRT.</p> <p>This class profiles the performance of different models, provided their paths. The profiling includes parameters such as model speed and FLOPs.</p> <p>Attributes:</p> Name Type Description <code>paths</code> <code>list</code> <p>Paths of the models to profile.</p> <code>num_timed_runs</code> <code>int</code> <p>Number of timed runs for the profiling. Default is 100.</p> <code>num_warmup_runs</code> <code>int</code> <p>Number of warmup runs before profiling. Default is 10.</p> <code>min_time</code> <code>float</code> <p>Minimum number of seconds to profile for. Default is 60.</p> <code>imgsz</code> <code>int</code> <p>Image size used in the models. Default is 640.</p> <p>Methods:</p> Name Description <code>profile</code> <p>Profiles the models and prints the result.</p> Example <pre><code>from ultralytics.utils.benchmarks import ProfileModels\nProfileModels(['yolov8n.yaml', 'yolov8s.yaml'], imgsz=640).profile()\n</code></pre> Source code in <code>ultralytics/utils/benchmarks.py</code> <pre><code>class ProfileModels:\n\"\"\"\n    ProfileModels class for profiling different models on ONNX and TensorRT.\n    This class profiles the performance of different models, provided their paths. The profiling includes parameters such as\n    model speed and FLOPs.\n    Attributes:\n        paths (list): Paths of the models to profile.\n        num_timed_runs (int): Number of timed runs for the profiling. Default is 100.\n        num_warmup_runs (int): Number of warmup runs before profiling. Default is 10.\n        min_time (float): Minimum number of seconds to profile for. Default is 60.\n        imgsz (int): Image size used in the models. Default is 640.\n    Methods:\n        profile(): Profiles the models and prints the result.\n    Example:\n        ```python\n        from ultralytics.utils.benchmarks import ProfileModels\n        ProfileModels(['yolov8n.yaml', 'yolov8s.yaml'], imgsz=640).profile()\n        ```\n    \"\"\"\ndef __init__(self,\npaths: list,\nnum_timed_runs=100,\nnum_warmup_runs=10,\nmin_time=60,\nimgsz=640,\ntrt=True,\ndevice=None):\nself.paths = paths\nself.num_timed_runs = num_timed_runs\nself.num_warmup_runs = num_warmup_runs\nself.min_time = min_time\nself.imgsz = imgsz\nself.trt = trt  # run TensorRT profiling\nself.device = device or torch.device(0 if torch.cuda.is_available() else 'cpu')\ndef profile(self):\nfiles = self.get_files()\nif not files:\nprint('No matching *.pt or *.onnx files found.')\nreturn\ntable_rows = []\noutput = []\nfor file in files:\nengine_file = file.with_suffix('.engine')\nif file.suffix in ('.pt', '.yaml', '.yml'):\nmodel = YOLO(str(file))\nmodel.fuse()  # to report correct params and GFLOPs in model.info()\nmodel_info = model.info()\nif self.trt and self.device.type != 'cpu' and not engine_file.is_file():\nengine_file = model.export(format='engine',\nhalf=True,\nimgsz=self.imgsz,\ndevice=self.device,\nverbose=False)\nonnx_file = model.export(format='onnx',\nhalf=True,\nimgsz=self.imgsz,\nsimplify=True,\ndevice=self.device,\nverbose=False)\nelif file.suffix == '.onnx':\nmodel_info = self.get_onnx_model_info(file)\nonnx_file = file\nelse:\ncontinue\nt_engine = self.profile_tensorrt_model(str(engine_file))\nt_onnx = self.profile_onnx_model(str(onnx_file))\ntable_rows.append(self.generate_table_row(file.stem, t_onnx, t_engine, model_info))\noutput.append(self.generate_results_dict(file.stem, t_onnx, t_engine, model_info))\nself.print_table(table_rows)\nreturn output\ndef get_files(self):\nfiles = []\nfor path in self.paths:\npath = Path(path)\nif path.is_dir():\nextensions = ['*.pt', '*.onnx', '*.yaml']\nfiles.extend([file for ext in extensions for file in glob.glob(str(path / ext))])\nelif path.suffix in ('.pt', '.yaml', '.yml'):  # add non-existing\nfiles.append(str(path))\nelse:\nfiles.extend(glob.glob(str(path)))\nprint(f'Profiling: {sorted(files)}')\nreturn [Path(file) for file in sorted(files)]\ndef get_onnx_model_info(self, onnx_file: str):\n# return (num_layers, num_params, num_gradients, num_flops)\nreturn 0.0, 0.0, 0.0, 0.0\ndef iterative_sigma_clipping(self, data, sigma=2, max_iters=3):\ndata = np.array(data)\nfor _ in range(max_iters):\nmean, std = np.mean(data), np.std(data)\nclipped_data = data[(data &gt; mean - sigma * std) &amp; (data &lt; mean + sigma * std)]\nif len(clipped_data) == len(data):\nbreak\ndata = clipped_data\nreturn data\ndef profile_tensorrt_model(self, engine_file: str):\nif not self.trt or not Path(engine_file).is_file():\nreturn 0.0, 0.0\n# Model and input\nmodel = YOLO(engine_file)\ninput_data = np.random.rand(self.imgsz, self.imgsz, 3).astype(np.float32)  # must be FP32\n# Warmup runs\nelapsed = 0.0\nfor _ in range(3):\nstart_time = time.time()\nfor _ in range(self.num_warmup_runs):\nmodel(input_data, imgsz=self.imgsz, verbose=False)\nelapsed = time.time() - start_time\n# Compute number of runs as higher of min_time or num_timed_runs\nnum_runs = max(round(self.min_time / elapsed * self.num_warmup_runs), self.num_timed_runs * 50)\n# Timed runs\nrun_times = []\nfor _ in tqdm(range(num_runs), desc=engine_file):\nresults = model(input_data, imgsz=self.imgsz, verbose=False)\nrun_times.append(results[0].speed['inference'])  # Convert to milliseconds\nrun_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=3)  # sigma clipping\nreturn np.mean(run_times), np.std(run_times)\ndef profile_onnx_model(self, onnx_file: str):\ncheck_requirements('onnxruntime')\nimport onnxruntime as ort\n# Session with either 'TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'\nsess_options = ort.SessionOptions()\nsess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\nsess_options.intra_op_num_threads = 8  # Limit the number of threads\nsess = ort.InferenceSession(onnx_file, sess_options, providers=['CPUExecutionProvider'])\ninput_tensor = sess.get_inputs()[0]\ninput_type = input_tensor.type\n# Mapping ONNX datatype to numpy datatype\nif 'float16' in input_type:\ninput_dtype = np.float16\nelif 'float' in input_type:\ninput_dtype = np.float32\nelif 'double' in input_type:\ninput_dtype = np.float64\nelif 'int64' in input_type:\ninput_dtype = np.int64\nelif 'int32' in input_type:\ninput_dtype = np.int32\nelse:\nraise ValueError(f'Unsupported ONNX datatype {input_type}')\ninput_data = np.random.rand(*input_tensor.shape).astype(input_dtype)\ninput_name = input_tensor.name\noutput_name = sess.get_outputs()[0].name\n# Warmup runs\nelapsed = 0.0\nfor _ in range(3):\nstart_time = time.time()\nfor _ in range(self.num_warmup_runs):\nsess.run([output_name], {input_name: input_data})\nelapsed = time.time() - start_time\n# Compute number of runs as higher of min_time or num_timed_runs\nnum_runs = max(round(self.min_time / elapsed * self.num_warmup_runs), self.num_timed_runs)\n# Timed runs\nrun_times = []\nfor _ in tqdm(range(num_runs), desc=onnx_file):\nstart_time = time.time()\nsess.run([output_name], {input_name: input_data})\nrun_times.append((time.time() - start_time) * 1000)  # Convert to milliseconds\nrun_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=5)  # sigma clipping\nreturn np.mean(run_times), np.std(run_times)\ndef generate_table_row(self, model_name, t_onnx, t_engine, model_info):\nlayers, params, gradients, flops = model_info\nreturn f'| {model_name:18s} | {self.imgsz} | - | {t_onnx[0]:.2f} \u00b1 {t_onnx[1]:.2f} ms | {t_engine[0]:.2f} \u00b1 {t_engine[1]:.2f} ms | {params / 1e6:.1f} | {flops:.1f} |'\ndef generate_results_dict(self, model_name, t_onnx, t_engine, model_info):\nlayers, params, gradients, flops = model_info\nreturn {\n'model/name': model_name,\n'model/parameters': params,\n'model/GFLOPs': round(flops, 3),\n'model/speed_ONNX(ms)': round(t_onnx[0], 3),\n'model/speed_TensorRT(ms)': round(t_engine[0], 3)}\ndef print_table(self, table_rows):\ngpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'GPU'\nheader = f'| Model | size&lt;br&gt;&lt;sup&gt;(pixels) | mAP&lt;sup&gt;val&lt;br&gt;50-95 | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms) | Speed&lt;br&gt;&lt;sup&gt;{gpu} TensorRT&lt;br&gt;(ms) | params&lt;br&gt;&lt;sup&gt;(M) | FLOPs&lt;br&gt;&lt;sup&gt;(B) |'\nseparator = '|-------------|---------------------|--------------------|------------------------------|-----------------------------------|------------------|-----------------|'\nprint(f'\\n\\n{header}')\nprint(separator)\nfor row in table_rows:\nprint(row)\n</code></pre>"},{"location":"reference/utils/benchmarks/#ultralytics.utils.benchmarks.benchmark","title":"<code>ultralytics.utils.benchmarks.benchmark(model=Path(SETTINGS['weights_dir']) / 'yolov8n.pt', data=None, imgsz=160, half=False, int8=False, device='cpu', verbose=False)</code>","text":"<p>Benchmark a YOLO model across different formats for speed and accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | Path | optional</code> <p>Path to the model file or directory. Default is Path(SETTINGS['weights_dir']) / 'yolov8n.pt'.</p> <code>Path(SETTINGS['weights_dir']) / 'yolov8n.pt'</code> <code>data</code> <code>str</code> <p>Dataset to evaluate on, inherited from TASK2DATA if not passed. Default is None.</p> <code>None</code> <code>imgsz</code> <code>int</code> <p>Image size for the benchmark. Default is 160.</p> <code>160</code> <code>half</code> <code>bool</code> <p>Use half-precision for the model if True. Default is False.</p> <code>False</code> <code>int8</code> <code>bool</code> <p>Use int8-precision for the model if True. Default is False.</p> <code>False</code> <code>device</code> <code>str</code> <p>Device to run the benchmark on, either 'cpu' or 'cuda'. Default is 'cpu'.</p> <code>'cpu'</code> <code>verbose</code> <code>bool | float | optional</code> <p>If True or a float, assert benchmarks pass with given metric. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame with benchmark results for each format, including file size, metric, and inference time.</p> Example <pre><code>from ultralytics.utils.benchmarks import benchmark\nbenchmark(model='yolov8n.pt', imgsz=640)\n</code></pre> Source code in <code>ultralytics/utils/benchmarks.py</code> <pre><code>def benchmark(model=Path(SETTINGS['weights_dir']) / 'yolov8n.pt',\ndata=None,\nimgsz=160,\nhalf=False,\nint8=False,\ndevice='cpu',\nverbose=False):\n\"\"\"\n    Benchmark a YOLO model across different formats for speed and accuracy.\n    Args:\n        model (str | Path | optional): Path to the model file or directory. Default is\n            Path(SETTINGS['weights_dir']) / 'yolov8n.pt'.\n        data (str, optional): Dataset to evaluate on, inherited from TASK2DATA if not passed. Default is None.\n        imgsz (int, optional): Image size for the benchmark. Default is 160.\n        half (bool, optional): Use half-precision for the model if True. Default is False.\n        int8 (bool, optional): Use int8-precision for the model if True. Default is False.\n        device (str, optional): Device to run the benchmark on, either 'cpu' or 'cuda'. Default is 'cpu'.\n        verbose (bool | float | optional): If True or a float, assert benchmarks pass with given metric.\n            Default is False.\n    Returns:\n        df (pandas.DataFrame): A pandas DataFrame with benchmark results for each format, including file size,\n            metric, and inference time.\n    Example:\n        ```python\n        from ultralytics.utils.benchmarks import benchmark\n        benchmark(model='yolov8n.pt', imgsz=640)\n        ```\n    \"\"\"\nimport pandas as pd\npd.options.display.max_columns = 10\npd.options.display.width = 120\ndevice = select_device(device, verbose=False)\nif isinstance(model, (str, Path)):\nmodel = YOLO(model)\ny = []\nt0 = time.time()\nfor i, (name, format, suffix, cpu, gpu) in export_formats().iterrows():  # index, (name, format, suffix, CPU, GPU)\nemoji, filename = '\u274c', None  # export defaults\ntry:\nassert i != 9 or LINUX, 'Edge TPU export only supported on Linux'\nif i == 10:\nassert MACOS or LINUX, 'TF.js export only supported on macOS and Linux'\nelif i == 11:\nassert sys.version_info &lt; (3, 11), 'PaddlePaddle export only supported on Python&lt;=3.10'\nif 'cpu' in device.type:\nassert cpu, 'inference not supported on CPU'\nif 'cuda' in device.type:\nassert gpu, 'inference not supported on GPU'\n# Export\nif format == '-':\nfilename = model.ckpt_path or model.cfg\nexport = model  # PyTorch format\nelse:\nfilename = model.export(imgsz=imgsz, format=format, half=half, int8=int8, device=device, verbose=False)\nexport = YOLO(filename, task=model.task)\nassert suffix in str(filename), 'export failed'\nemoji = '\u274e'  # indicates export succeeded\n# Predict\nassert model.task != 'pose' or i != 7, 'GraphDef Pose inference is not supported'\nassert i not in (9, 10), 'inference not supported'  # Edge TPU and TF.js are unsupported\nassert i != 5 or platform.system() == 'Darwin', 'inference only supported on macOS&gt;=10.13'  # CoreML\nexport.predict(ASSETS / 'bus.jpg', imgsz=imgsz, device=device, half=half)\n# Validate\ndata = data or TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect\nkey = TASK2METRIC[model.task]  # task to metric, i.e. metrics/mAP50-95(B) for task=detect\nresults = export.val(data=data,\nbatch=1,\nimgsz=imgsz,\nplots=False,\ndevice=device,\nhalf=half,\nint8=int8,\nverbose=False)\nmetric, speed = results.results_dict[key], results.speed['inference']\ny.append([name, '\u2705', round(file_size(filename), 1), round(metric, 4), round(speed, 2)])\nexcept Exception as e:\nif verbose:\nassert type(e) is AssertionError, f'Benchmark failure for {name}: {e}'\nLOGGER.warning(f'ERROR \u274c\ufe0f Benchmark failure for {name}: {e}')\ny.append([name, emoji, round(file_size(filename), 1), None, None])  # mAP, t_inference\n# Print results\ncheck_yolo(device=device)  # print system info\ndf = pd.DataFrame(y, columns=['Format', 'Status\u2754', 'Size (MB)', key, 'Inference time (ms/im)'])\nname = Path(model.ckpt_path).name\ns = f'\\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({time.time() - t0:.2f}s)\\n{df}\\n'\nLOGGER.info(s)\nwith open('benchmarks.log', 'a', errors='ignore', encoding='utf-8') as f:\nf.write(s)\nif verbose and isinstance(verbose, float):\nmetrics = df[key].array  # values to compare to floor\nfloor = verbose  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n\nassert all(x &gt; floor for x in metrics if pd.notna(x)), f'Benchmark failure: metric(s) &lt; floor {floor}'\nreturn df\n</code></pre>"},{"location":"reference/utils/checks/","title":"Reference for <code>ultralytics/utils/checks.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/checks.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.is_ascii","title":"<code>ultralytics.utils.checks.is_ascii(s)</code>","text":"<p>Check if a string is composed of only ASCII characters.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>String to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string is composed only of ASCII characters, False otherwise.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def is_ascii(s) -&gt; bool:\n\"\"\"\n    Check if a string is composed of only ASCII characters.\n    Args:\n        s (str): String to be checked.\n    Returns:\n        bool: True if the string is composed only of ASCII characters, False otherwise.\n    \"\"\"\n# Convert list, tuple, None, etc. to string\ns = str(s)\n# Check if the string is composed of only ASCII characters\nreturn all(ord(c) &lt; 128 for c in s)\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_imgsz","title":"<code>ultralytics.utils.checks.check_imgsz(imgsz, stride=32, min_dim=1, max_dim=2, floor=0)</code>","text":"<p>Verify image size is a multiple of the given stride in each dimension. If the image size is not a multiple of the stride, update it to the nearest multiple of the stride that is greater than or equal to the given floor value.</p> <p>Parameters:</p> Name Type Description Default <code>imgsz</code> <code>int | cList[int]</code> <p>Image size.</p> required <code>stride</code> <code>int</code> <p>Stride value.</p> <code>32</code> <code>min_dim</code> <code>int</code> <p>Minimum number of dimensions.</p> <code>1</code> <code>max_dim</code> <code>int</code> <p>Maximum number of dimensions.</p> <code>2</code> <code>floor</code> <code>int</code> <p>Minimum allowed value for image size.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>Updated image size.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_imgsz(imgsz, stride=32, min_dim=1, max_dim=2, floor=0):\n\"\"\"\n    Verify image size is a multiple of the given stride in each dimension. If the image size is not a multiple of the\n    stride, update it to the nearest multiple of the stride that is greater than or equal to the given floor value.\n    Args:\n        imgsz (int | cList[int]): Image size.\n        stride (int): Stride value.\n        min_dim (int): Minimum number of dimensions.\n        max_dim (int): Maximum number of dimensions.\n        floor (int): Minimum allowed value for image size.\n    Returns:\n        (List[int]): Updated image size.\n    \"\"\"\n# Convert stride to integer if it is a tensor\nstride = int(stride.max() if isinstance(stride, torch.Tensor) else stride)\n# Convert image size to list if it is an integer\nif isinstance(imgsz, int):\nimgsz = [imgsz]\nelif isinstance(imgsz, (list, tuple)):\nimgsz = list(imgsz)\nelse:\nraise TypeError(f\"'imgsz={imgsz}' is of invalid type {type(imgsz).__name__}. \"\nf\"Valid imgsz types are int i.e. 'imgsz=640' or list i.e. 'imgsz=[640,640]'\")\n# Apply max_dim\nif len(imgsz) &gt; max_dim:\nmsg = \"'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list \" \\\n              \"or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\"\nif max_dim != 1:\nraise ValueError(f'imgsz={imgsz} is not a valid image size. {msg}')\nLOGGER.warning(f\"WARNING \u26a0\ufe0f updating to 'imgsz={max(imgsz)}'. {msg}\")\nimgsz = [max(imgsz)]\n# Make image size a multiple of the stride\nsz = [max(math.ceil(x / stride) * stride, floor) for x in imgsz]\n# Print warning message if image size was updated\nif sz != imgsz:\nLOGGER.warning(f'WARNING \u26a0\ufe0f imgsz={imgsz} must be multiple of max stride {stride}, updating to {sz}')\n# Add missing dimensions if necessary\nsz = [sz[0], sz[0]] if min_dim == 2 and len(sz) == 1 else sz[0] if min_dim == 1 and len(sz) == 1 else sz\nreturn sz\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_version","title":"<code>ultralytics.utils.checks.check_version(current='0.0.0', required='0.0.0', name='version ', hard=False, verbose=False)</code>","text":"<p>Check current version against the required version or range.</p> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>str</code> <p>Current version.</p> <code>'0.0.0'</code> <code>required</code> <code>str</code> <p>Required version or range (in pip-style format).</p> <code>'0.0.0'</code> <code>name</code> <code>str</code> <p>Name to be used in warning message.</p> <code>'version '</code> <code>hard</code> <code>bool</code> <p>If True, raise an AssertionError if the requirement is not met.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print warning message if requirement is not met.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if requirement is met, False otherwise.</p> Example Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_version(current: str = '0.0.0',\nrequired: str = '0.0.0',\nname: str = 'version ',\nhard: bool = False,\nverbose: bool = False) -&gt; bool:\n\"\"\"\n    Check current version against the required version or range.\n    Args:\n        current (str): Current version.\n        required (str): Required version or range (in pip-style format).\n        name (str): Name to be used in warning message.\n        hard (bool): If True, raise an AssertionError if the requirement is not met.\n        verbose (bool): If True, print warning message if requirement is not met.\n    Returns:\n        (bool): True if requirement is met, False otherwise.\n    Example:\n        # check if current version is exactly 22.04\n        check_version(current='22.04', required='==22.04')\n        # check if current version is greater than or equal to 22.04\n        check_version(current='22.10', required='22.04')  # assumes '&gt;=' inequality if none passed\n        # check if current version is less than or equal to 22.04\n        check_version(current='22.04', required='&lt;=22.04')\n        # check if current version is between 20.04 (inclusive) and 22.04 (exclusive)\n        check_version(current='21.10', required='&gt;20.04,&lt;22.04')\n    \"\"\"\ncurrent = pkg.parse_version(current)\nconstraints = re.findall(r'([&lt;&gt;!=]{1,2}\\s*\\d+\\.\\d+)', required) or [f'&gt;={required}']\nresult = True\nfor constraint in constraints:\nop, version = re.match(r'([&lt;&gt;!=]{1,2})\\s*(\\d+\\.\\d+)', constraint).groups()\nversion = pkg.parse_version(version)\nif op == '==' and current != version:\nresult = False\nelif op == '!=' and current == version:\nresult = False\nelif op == '&gt;=' and not (current &gt;= version):\nresult = False\nelif op == '&lt;=' and not (current &lt;= version):\nresult = False\nelif op == '&gt;' and not (current &gt; version):\nresult = False\nelif op == '&lt;' and not (current &lt; version):\nresult = False\nif not result:\nwarning_message = f'WARNING \u26a0\ufe0f {name}{required} is required, but {name}{current} is currently installed'\nif hard:\nraise ModuleNotFoundError(emojis(warning_message))  # assert version requirements met\nif verbose:\nLOGGER.warning(warning_message)\nreturn result\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_version--check-if-current-version-is-exactly-2204","title":"check if current version is exactly 22.04","text":"<p>check_version(current='22.04', required='==22.04')</p>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_version--check-if-current-version-is-greater-than-or-equal-to-2204","title":"check if current version is greater than or equal to 22.04","text":"<p>check_version(current='22.10', required='22.04')  # assumes '&gt;=' inequality if none passed</p>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_version--check-if-current-version-is-less-than-or-equal-to-2204","title":"check if current version is less than or equal to 22.04","text":"<p>check_version(current='22.04', required='&lt;=22.04')</p>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_version--check-if-current-version-is-between-2004-inclusive-and-2204-exclusive","title":"check if current version is between 20.04 (inclusive) and 22.04 (exclusive)","text":"<p>check_version(current='21.10', required='&gt;20.04,&lt;22.04')</p>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_latest_pypi_version","title":"<code>ultralytics.utils.checks.check_latest_pypi_version(package_name='ultralytics')</code>","text":"<p>Returns the latest version of a PyPI package without downloading or installing it.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>The name of the package to find the latest version for.</p> <code>'ultralytics'</code> <p>Returns:</p> Type Description <code>str</code> <p>The latest version of the package.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_latest_pypi_version(package_name='ultralytics'):\n\"\"\"\n    Returns the latest version of a PyPI package without downloading or installing it.\n    Parameters:\n        package_name (str): The name of the package to find the latest version for.\n    Returns:\n        (str): The latest version of the package.\n    \"\"\"\nwith contextlib.suppress(Exception):\nrequests.packages.urllib3.disable_warnings()  # Disable the InsecureRequestWarning\nresponse = requests.get(f'https://pypi.org/pypi/{package_name}/json', timeout=3)\nif response.status_code == 200:\nreturn response.json()['info']['version']\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_pip_update_available","title":"<code>ultralytics.utils.checks.check_pip_update_available()</code>","text":"<p>Checks if a new version of the ultralytics package is available on PyPI.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if an update is available, False otherwise.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_pip_update_available():\n\"\"\"\n    Checks if a new version of the ultralytics package is available on PyPI.\n    Returns:\n        (bool): True if an update is available, False otherwise.\n    \"\"\"\nif ONLINE and is_pip_package():\nwith contextlib.suppress(Exception):\nfrom ultralytics import __version__\nlatest = check_latest_pypi_version()\nif pkg.parse_version(__version__) &lt; pkg.parse_version(latest):  # update is available\nLOGGER.info(f'New https://pypi.org/project/ultralytics/{latest} available \ud83d\ude03 '\nf\"Update with 'pip install -U ultralytics'\")\nreturn True\nreturn False\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_font","title":"<code>ultralytics.utils.checks.check_font(font='Arial.ttf')</code>","text":"<p>Find font locally or download to user's configuration directory if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>font</code> <code>str</code> <p>Path or name of font.</p> <code>'Arial.ttf'</code> <p>Returns:</p> Name Type Description <code>file</code> <code>Path</code> <p>Resolved font file path.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>@ThreadingLocked()\ndef check_font(font='Arial.ttf'):\n\"\"\"\n    Find font locally or download to user's configuration directory if it does not already exist.\n    Args:\n        font (str): Path or name of font.\n    Returns:\n        file (Path): Resolved font file path.\n    \"\"\"\nname = Path(font).name\n# Check USER_CONFIG_DIR\nfile = USER_CONFIG_DIR / name\nif file.exists():\nreturn file\n# Check system fonts\nmatches = [s for s in font_manager.findSystemFonts() if font in s]\nif any(matches):\nreturn matches[0]\n# Download to USER_CONFIG_DIR if missing\nurl = f'https://ultralytics.com/assets/{name}'\nif downloads.is_url(url):\ndownloads.safe_download(url=url, file=file)\nreturn file\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_python","title":"<code>ultralytics.utils.checks.check_python(minimum='3.8.0')</code>","text":"<p>Check current python version against the required minimum version.</p> <p>Parameters:</p> Name Type Description Default <code>minimum</code> <code>str</code> <p>Required minimum version of python.</p> <code>'3.8.0'</code> <p>Returns:</p> Type Description <code>bool</code> <p>None</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_python(minimum: str = '3.8.0') -&gt; bool:\n\"\"\"\n    Check current python version against the required minimum version.\n    Args:\n        minimum (str): Required minimum version of python.\n    Returns:\n        None\n    \"\"\"\nreturn check_version(platform.python_version(), minimum, name='Python ', hard=True)\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_requirements","title":"<code>ultralytics.utils.checks.check_requirements(requirements=ROOT.parent / 'requirements.txt', exclude=(), install=True, cmds='')</code>","text":"<p>Check if installed dependencies meet YOLOv8 requirements and attempt to auto-update if needed.</p> <p>Parameters:</p> Name Type Description Default <code>requirements</code> <code>Union[Path, str, List[str]]</code> <p>Path to a requirements.txt file, a single package requirement as a string, or a list of package requirements as strings.</p> <code>parent / 'requirements.txt'</code> <code>exclude</code> <code>Tuple[str]</code> <p>Tuple of package names to exclude from checking.</p> <code>()</code> <code>install</code> <code>bool</code> <p>If True, attempt to auto-update packages that don't meet requirements.</p> <code>True</code> <code>cmds</code> <code>str</code> <p>Additional commands to pass to the pip install command when auto-updating.</p> <code>''</code> Example <pre><code>from ultralytics.utils.checks import check_requirements\n# Check a requirements.txt file\ncheck_requirements('path/to/requirements.txt')\n# Check a single package\ncheck_requirements('ultralytics&gt;=8.0.0')\n# Check multiple packages\ncheck_requirements(['numpy', 'ultralytics&gt;=8.0.0'])\n</code></pre> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>@TryExcept()\ndef check_requirements(requirements=ROOT.parent / 'requirements.txt', exclude=(), install=True, cmds=''):\n\"\"\"\n    Check if installed dependencies meet YOLOv8 requirements and attempt to auto-update if needed.\n    Args:\n        requirements (Union[Path, str, List[str]]): Path to a requirements.txt file, a single package requirement as a\n            string, or a list of package requirements as strings.\n        exclude (Tuple[str]): Tuple of package names to exclude from checking.\n        install (bool): If True, attempt to auto-update packages that don't meet requirements.\n        cmds (str): Additional commands to pass to the pip install command when auto-updating.\n    Example:\n        ```python\n        from ultralytics.utils.checks import check_requirements\n        # Check a requirements.txt file\n        check_requirements('path/to/requirements.txt')\n        # Check a single package\n        check_requirements('ultralytics&gt;=8.0.0')\n        # Check multiple packages\n        check_requirements(['numpy', 'ultralytics&gt;=8.0.0'])\n        ```\n    \"\"\"\nprefix = colorstr('red', 'bold', 'requirements:')\ncheck_python()  # check python version\ncheck_torchvision()  # check torch-torchvision compatibility\nif isinstance(requirements, Path):  # requirements.txt file\nfile = requirements.resolve()\nassert file.exists(), f'{prefix} {file} not found, check failed.'\nwith file.open() as f:\nrequirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(f) if x.name not in exclude]\nelif isinstance(requirements, str):\nrequirements = [requirements]\npkgs = []\nfor r in requirements:\nr_stripped = r.split('/')[-1].replace('.git', '')  # replace git+https://org/repo.git -&gt; 'repo'\ntry:\npkg.require(r_stripped)  # exception if requirements not met\nexcept pkg.DistributionNotFound:\ntry:  # attempt to import (slower but more accurate)\nimport importlib\nimportlib.import_module(next(pkg.parse_requirements(r_stripped)).name)\nexcept ImportError:\npkgs.append(r)\nexcept pkg.VersionConflict:\npkgs.append(r)\ns = ' '.join(f'\"{x}\"' for x in pkgs)  # console string\nif s:\nif install and AUTOINSTALL:  # check environment variable\nn = len(pkgs)  # number of packages updates\nLOGGER.info(f\"{prefix} Ultralytics requirement{'s' * (n &gt; 1)} {pkgs} not found, attempting AutoUpdate...\")\ntry:\nt = time.time()\nassert is_online(), 'AutoUpdate skipped (offline)'\nLOGGER.info(subprocess.check_output(f'pip install --no-cache {s} {cmds}', shell=True).decode())\ndt = time.time() - t\nLOGGER.info(\nf\"{prefix} AutoUpdate success \u2705 {dt:.1f}s, installed {n} package{'s' * (n &gt; 1)}: {pkgs}\\n\"\nf\"{prefix} \u26a0\ufe0f {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\\n\")\nexcept Exception as e:\nLOGGER.warning(f'{prefix} \u274c {e}')\nreturn False\nelse:\nreturn False\nreturn True\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_torchvision","title":"<code>ultralytics.utils.checks.check_torchvision()</code>","text":"<p>Checks the installed versions of PyTorch and Torchvision to ensure they're compatible.</p> <p>This function checks the installed versions of PyTorch and Torchvision, and warns if they're incompatible according to the provided compatibility table based on https://github.com/pytorch/vision#installation. The compatibility table is a dictionary where the keys are PyTorch versions and the values are lists of compatible Torchvision versions.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_torchvision():\n\"\"\"\n    Checks the installed versions of PyTorch and Torchvision to ensure they're compatible.\n    This function checks the installed versions of PyTorch and Torchvision, and warns if they're incompatible according\n    to the provided compatibility table based on https://github.com/pytorch/vision#installation. The\n    compatibility table is a dictionary where the keys are PyTorch versions and the values are lists of compatible\n    Torchvision versions.\n    \"\"\"\nimport torchvision\n# Compatibility table\ncompatibility_table = {'2.0': ['0.15'], '1.13': ['0.14'], '1.12': ['0.13']}\n# Extract only the major and minor versions\nv_torch = '.'.join(torch.__version__.split('+')[0].split('.')[:2])\nv_torchvision = '.'.join(torchvision.__version__.split('+')[0].split('.')[:2])\nif v_torch in compatibility_table:\ncompatible_versions = compatibility_table[v_torch]\nif all(pkg.parse_version(v_torchvision) != pkg.parse_version(v) for v in compatible_versions):\nprint(f'WARNING \u26a0\ufe0f torchvision=={v_torchvision} is incompatible with torch=={v_torch}.\\n'\nf\"Run 'pip install torchvision=={compatible_versions[0]}' to fix torchvision or \"\n\"'pip install -U torch torchvision' to update both.\\n\"\n'For a full compatibility table see https://github.com/pytorch/vision#installation')\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_suffix","title":"<code>ultralytics.utils.checks.check_suffix(file='yolov8n.pt', suffix='.pt', msg='')</code>","text":"<p>Check file(s) for acceptable suffix.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_suffix(file='yolov8n.pt', suffix='.pt', msg=''):\n\"\"\"Check file(s) for acceptable suffix.\"\"\"\nif file and suffix:\nif isinstance(suffix, str):\nsuffix = (suffix, )\nfor f in file if isinstance(file, (list, tuple)) else [file]:\ns = Path(f).suffix.lower().strip()  # file suffix\nif len(s):\nassert s in suffix, f'{msg}{f} acceptable suffix is {suffix}, not {s}'\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_yolov5u_filename","title":"<code>ultralytics.utils.checks.check_yolov5u_filename(file, verbose=True)</code>","text":"<p>Replace legacy YOLOv5 filenames with updated YOLOv5u filenames.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_yolov5u_filename(file: str, verbose: bool = True):\n\"\"\"Replace legacy YOLOv5 filenames with updated YOLOv5u filenames.\"\"\"\nif 'yolov3' in file or 'yolov5' in file:\nif 'u.yaml' in file:\nfile = file.replace('u.yaml', '.yaml')  # i.e. yolov5nu.yaml -&gt; yolov5n.yaml\nelif '.pt' in file and 'u' not in file:\noriginal_file = file\nfile = re.sub(r'(.*yolov5([nsmlx]))\\.pt', '\\\\1u.pt', file)  # i.e. yolov5n.pt -&gt; yolov5nu.pt\nfile = re.sub(r'(.*yolov5([nsmlx])6)\\.pt', '\\\\1u.pt', file)  # i.e. yolov5n6.pt -&gt; yolov5n6u.pt\nfile = re.sub(r'(.*yolov3(|-tiny|-spp))\\.pt', '\\\\1u.pt', file)  # i.e. yolov3-spp.pt -&gt; yolov3-sppu.pt\nif file != original_file and verbose:\nLOGGER.info(\nf\"PRO TIP \ud83d\udca1 Replace 'model={original_file}' with new 'model={file}'.\\nYOLOv5 'u' models are \"\nf'trained with https://github.com/ultralytics/ultralytics and feature improved performance vs '\nf'standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\\n')\nreturn file\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_file","title":"<code>ultralytics.utils.checks.check_file(file, suffix='', download=True, hard=True)</code>","text":"<p>Search/download file (if necessary) and return path.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_file(file, suffix='', download=True, hard=True):\n\"\"\"Search/download file (if necessary) and return path.\"\"\"\ncheck_suffix(file, suffix)  # optional\nfile = str(file).strip()  # convert to string and strip spaces\nfile = check_yolov5u_filename(file)  # yolov5n -&gt; yolov5nu\nif not file or ('://' not in file and Path(file).exists()):  # exists ('://' check required in Windows Python&lt;3.10)\nreturn file\nelif download and file.lower().startswith(('https://', 'http://', 'rtsp://', 'rtmp://')):  # download\nurl = file  # warning: Pathlib turns :// -&gt; :/\nfile = url2file(file)  # '%2F' to '/', split https://url.com/file.txt?auth\nif Path(file).exists():\nLOGGER.info(f'Found {clean_url(url)} locally at {file}')  # file already exists\nelse:\ndownloads.safe_download(url=url, file=file, unzip=False)\nreturn file\nelse:  # search\nfiles = glob.glob(str(ROOT / 'cfg' / '**' / file), recursive=True)  # find file\nif not files and hard:\nraise FileNotFoundError(f\"'{file}' does not exist\")\nelif len(files) &gt; 1 and hard:\nraise FileNotFoundError(f\"Multiple files match '{file}', specify exact path: {files}\")\nreturn files[0] if len(files) else []  # return file\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_yaml","title":"<code>ultralytics.utils.checks.check_yaml(file, suffix=('.yaml', '.yml'), hard=True)</code>","text":"<p>Search/download YAML file (if necessary) and return path, checking suffix.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_yaml(file, suffix=('.yaml', '.yml'), hard=True):\n\"\"\"Search/download YAML file (if necessary) and return path, checking suffix.\"\"\"\nreturn check_file(file, suffix, hard=hard)\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_imshow","title":"<code>ultralytics.utils.checks.check_imshow(warn=False)</code>","text":"<p>Check if environment supports image displays.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_imshow(warn=False):\n\"\"\"Check if environment supports image displays.\"\"\"\ntry:\nif LINUX:\nassert 'DISPLAY' in os.environ and not is_docker() and not is_colab() and not is_kaggle()\ncv2.imshow('test', np.zeros((8, 8, 3), dtype=np.uint8))  # show a small 8-pixel image\ncv2.waitKey(1)\ncv2.destroyAllWindows()\ncv2.waitKey(1)\nreturn True\nexcept Exception as e:\nif warn:\nLOGGER.warning(f'WARNING \u26a0\ufe0f Environment does not support cv2.imshow() or PIL Image.show()\\n{e}')\nreturn False\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_yolo","title":"<code>ultralytics.utils.checks.check_yolo(verbose=True, device='')</code>","text":"<p>Return a human-readable YOLO software and hardware summary.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_yolo(verbose=True, device=''):\n\"\"\"Return a human-readable YOLO software and hardware summary.\"\"\"\nfrom ultralytics.utils.torch_utils import select_device\nif is_jupyter():\nif check_requirements('wandb', install=False):\nos.system('pip uninstall -y wandb')  # uninstall wandb: unwanted account creation prompt with infinite hang\nif is_colab():\nshutil.rmtree('sample_data', ignore_errors=True)  # remove colab /sample_data directory\nif verbose:\n# System info\ngib = 1 &lt;&lt; 30  # bytes per GiB\nram = psutil.virtual_memory().total\ntotal, used, free = shutil.disk_usage('/')\ns = f'({os.cpu_count()} CPUs, {ram / gib:.1f} GB RAM, {(total - free) / gib:.1f}/{total / gib:.1f} GB disk)'\nwith contextlib.suppress(Exception):  # clear display if ipython is installed\nfrom IPython import display\ndisplay.clear_output()\nelse:\ns = ''\nselect_device(device=device, newline=False)\nLOGGER.info(f'Setup complete \u2705 {s}')\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.check_amp","title":"<code>ultralytics.utils.checks.check_amp(model)</code>","text":"<p>This function checks the PyTorch Automatic Mixed Precision (AMP) functionality of a YOLOv8 model. If the checks fail, it means there are anomalies with AMP on the system that may cause NaN losses or zero-mAP results, so AMP will be disabled during training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>A YOLOv8 model instance.</p> required Example <pre><code>from ultralytics import YOLO\nfrom ultralytics.utils.checks import check_amp\nmodel = YOLO('yolov8n.pt').model.cuda()\ncheck_amp(model)\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Returns True if the AMP functionality works correctly with YOLOv8 model, else False.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def check_amp(model):\n\"\"\"\n    This function checks the PyTorch Automatic Mixed Precision (AMP) functionality of a YOLOv8 model.\n    If the checks fail, it means there are anomalies with AMP on the system that may cause NaN losses or zero-mAP\n    results, so AMP will be disabled during training.\n    Args:\n        model (nn.Module): A YOLOv8 model instance.\n    Example:\n        ```python\n        from ultralytics import YOLO\n        from ultralytics.utils.checks import check_amp\n        model = YOLO('yolov8n.pt').model.cuda()\n        check_amp(model)\n        ```\n    Returns:\n        (bool): Returns True if the AMP functionality works correctly with YOLOv8 model, else False.\n    \"\"\"\ndevice = next(model.parameters()).device  # get model device\nif device.type in ('cpu', 'mps'):\nreturn False  # AMP only used on CUDA devices\ndef amp_allclose(m, im):\n\"\"\"All close FP32 vs AMP results.\"\"\"\na = m(im, device=device, verbose=False)[0].boxes.data  # FP32 inference\nwith torch.cuda.amp.autocast(True):\nb = m(im, device=device, verbose=False)[0].boxes.data  # AMP inference\ndel m\nreturn a.shape == b.shape and torch.allclose(a, b.float(), atol=0.5)  # close to 0.5 absolute tolerance\nim = ASSETS / 'bus.jpg'  # image to check\nprefix = colorstr('AMP: ')\nLOGGER.info(f'{prefix}running Automatic Mixed Precision (AMP) checks with YOLOv8n...')\nwarning_msg = \"Setting 'amp=True'. If you experience zero-mAP or NaN losses you can disable AMP with amp=False.\"\ntry:\nfrom ultralytics import YOLO\nassert amp_allclose(YOLO('yolov8n.pt'), im)\nLOGGER.info(f'{prefix}checks passed \u2705')\nexcept ConnectionError:\nLOGGER.warning(f'{prefix}checks skipped \u26a0\ufe0f, offline and unable to download YOLOv8n. {warning_msg}')\nexcept (AttributeError, ModuleNotFoundError):\nLOGGER.warning(\nf'{prefix}checks skipped \u26a0\ufe0f. Unable to load YOLOv8n due to possible Ultralytics package modifications. {warning_msg}'\n)\nexcept AssertionError:\nLOGGER.warning(f'{prefix}checks failed \u274c. Anomalies were detected with AMP on your system that may lead to '\nf'NaN losses or zero-mAP results, so AMP will be disabled during training.')\nreturn False\nreturn True\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.git_describe","title":"<code>ultralytics.utils.checks.git_describe(path=ROOT)</code>","text":"<p>Return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe.</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def git_describe(path=ROOT):  # path must be a directory\n\"\"\"Return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe.\"\"\"\nwith contextlib.suppress(Exception):\nreturn subprocess.check_output(f'git -C {path} describe --tags --long --always', shell=True).decode()[:-1]\nreturn ''\n</code></pre>"},{"location":"reference/utils/checks/#ultralytics.utils.checks.print_args","title":"<code>ultralytics.utils.checks.print_args(args=None, show_file=True, show_func=False)</code>","text":"<p>Print function arguments (optional args dict).</p> Source code in <code>ultralytics/utils/checks.py</code> <pre><code>def print_args(args: Optional[dict] = None, show_file=True, show_func=False):\n\"\"\"Print function arguments (optional args dict).\"\"\"\ndef strip_auth(v):\n\"\"\"Clean longer Ultralytics HUB URLs by stripping potential authentication information.\"\"\"\nreturn clean_url(v) if (isinstance(v, str) and v.startswith('http') and len(v) &gt; 100) else v\nx = inspect.currentframe().f_back  # previous frame\nfile, _, func, _, _ = inspect.getframeinfo(x)\nif args is None:  # get args automatically\nargs, _, _, frm = inspect.getargvalues(x)\nargs = {k: v for k, v in frm.items() if k in args}\ntry:\nfile = Path(file).resolve().relative_to(ROOT).with_suffix('')\nexcept ValueError:\nfile = Path(file).stem\ns = (f'{file}: ' if show_file else '') + (f'{func}: ' if show_func else '')\nLOGGER.info(colorstr(s) + ', '.join(f'{k}={strip_auth(v)}' for k, v in args.items()))\n</code></pre>"},{"location":"reference/utils/dist/","title":"Reference for <code>ultralytics/utils/dist.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/dist.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/dist/#ultralytics.utils.dist.find_free_network_port","title":"<code>ultralytics.utils.dist.find_free_network_port()</code>","text":"<p>Finds a free port on localhost.</p> <p>It is useful in single-node training when we don't want to connect to a real main node but have to set the <code>MASTER_PORT</code> environment variable.</p> Source code in <code>ultralytics/utils/dist.py</code> <pre><code>def find_free_network_port() -&gt; int:\n\"\"\"Finds a free port on localhost.\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\n    `MASTER_PORT` environment variable.\n    \"\"\"\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\ns.bind(('127.0.0.1', 0))\nreturn s.getsockname()[1]  # port\n</code></pre>"},{"location":"reference/utils/dist/#ultralytics.utils.dist.generate_ddp_file","title":"<code>ultralytics.utils.dist.generate_ddp_file(trainer)</code>","text":"<p>Generates a DDP file and returns its file name.</p> Source code in <code>ultralytics/utils/dist.py</code> <pre><code>def generate_ddp_file(trainer):\n\"\"\"Generates a DDP file and returns its file name.\"\"\"\nmodule, name = f'{trainer.__class__.__module__}.{trainer.__class__.__name__}'.rsplit('.', 1)\ncontent = f'''overrides = {vars(trainer.args)} \\nif __name__ == \"__main__\":\n    from {module} import {name}\n    from ultralytics.utils import DEFAULT_CFG_DICT\n    cfg = DEFAULT_CFG_DICT.copy()\n    cfg.update(save_dir='')   # handle the extra key 'save_dir'\n    trainer = {name}(cfg=cfg, overrides=overrides)\n    trainer.train()'''\n(USER_CONFIG_DIR / 'DDP').mkdir(exist_ok=True)\nwith tempfile.NamedTemporaryFile(prefix='_temp_',\nsuffix=f'{id(trainer)}.py',\nmode='w+',\nencoding='utf-8',\ndir=USER_CONFIG_DIR / 'DDP',\ndelete=False) as file:\nfile.write(content)\nreturn file.name\n</code></pre>"},{"location":"reference/utils/dist/#ultralytics.utils.dist.generate_ddp_command","title":"<code>ultralytics.utils.dist.generate_ddp_command(world_size, trainer)</code>","text":"<p>Generates and returns command for distributed training.</p> Source code in <code>ultralytics/utils/dist.py</code> <pre><code>def generate_ddp_command(world_size, trainer):\n\"\"\"Generates and returns command for distributed training.\"\"\"\nimport __main__  # noqa local import to avoid https://github.com/Lightning-AI/lightning/issues/15218\nif not trainer.resume:\nshutil.rmtree(trainer.save_dir)  # remove the save_dir\nfile = str(Path(sys.argv[0]).resolve())\nsafe_pattern = re.compile(r'^[a-zA-Z0-9_. /\\\\-]{1,128}$')  # allowed characters and maximum of 100 characters\nif not (safe_pattern.match(file) and Path(file).exists() and file.endswith('.py')):  # using CLI\nfile = generate_ddp_file(trainer)\ndist_cmd = 'torch.distributed.run' if TORCH_1_9 else 'torch.distributed.launch'\nport = find_free_network_port()\ncmd = [sys.executable, '-m', dist_cmd, '--nproc_per_node', f'{world_size}', '--master_port', f'{port}', file]\nreturn cmd, file\n</code></pre>"},{"location":"reference/utils/dist/#ultralytics.utils.dist.ddp_cleanup","title":"<code>ultralytics.utils.dist.ddp_cleanup(trainer, file)</code>","text":"<p>Delete temp file if created.</p> Source code in <code>ultralytics/utils/dist.py</code> <pre><code>def ddp_cleanup(trainer, file):\n\"\"\"Delete temp file if created.\"\"\"\nif f'{id(trainer)}.py' in file:  # if temp_file suffix in file\nos.remove(file)\n</code></pre>"},{"location":"reference/utils/downloads/","title":"Reference for <code>ultralytics/utils/downloads.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/downloads.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.is_url","title":"<code>ultralytics.utils.downloads.is_url(url, check=True)</code>","text":"<p>Check if string is URL and check if URL exists.</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def is_url(url, check=True):\n\"\"\"Check if string is URL and check if URL exists.\"\"\"\nwith contextlib.suppress(Exception):\nurl = str(url)\nresult = parse.urlparse(url)\nassert all([result.scheme, result.netloc])  # check if is url\nif check:\nwith request.urlopen(url) as response:\nreturn response.getcode() == 200  # check if exists online\nreturn True\nreturn False\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.delete_dsstore","title":"<code>ultralytics.utils.downloads.delete_dsstore(path, files_to_delete=('.DS_Store', '__MACOSX'))</code>","text":"<p>Deletes all \".DS_store\" files under a specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the \".DS_store\" files should be deleted.</p> required <code>files_to_delete</code> <code>tuple</code> <p>The files to be deleted.</p> <code>('.DS_Store', '__MACOSX')</code> Example <pre><code>from ultralytics.utils.downloads import delete_dsstore\ndelete_dsstore('path/to/dir')\n</code></pre> Note <p>\".DS_store\" files are created by the Apple operating system and contain metadata about folders and files. They are hidden system files and can cause issues when transferring files between different operating systems.</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def delete_dsstore(path, files_to_delete=('.DS_Store', '__MACOSX')):\n\"\"\"\n    Deletes all \".DS_store\" files under a specified directory.\n    Args:\n        path (str, optional): The directory path where the \".DS_store\" files should be deleted.\n        files_to_delete (tuple): The files to be deleted.\n    Example:\n        ```python\n        from ultralytics.utils.downloads import delete_dsstore\n        delete_dsstore('path/to/dir')\n        ```\n    Note:\n        \".DS_store\" files are created by the Apple operating system and contain metadata about folders and files. They\n        are hidden system files and can cause issues when transferring files between different operating systems.\n    \"\"\"\n# Delete Apple .DS_store files\nfor file in files_to_delete:\nmatches = list(Path(path).rglob(file))\nLOGGER.info(f'Deleting {file} files: {matches}')\nfor f in matches:\nf.unlink()\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.zip_directory","title":"<code>ultralytics.utils.downloads.zip_directory(directory, compress=True, exclude=('.DS_Store', '__MACOSX'), progress=True)</code>","text":"<p>Zips the contents of a directory, excluding files containing strings in the exclude list. The resulting zip file is named after the directory and placed alongside it.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | Path</code> <p>The path to the directory to be zipped.</p> required <code>compress</code> <code>bool</code> <p>Whether to compress the files while zipping. Default is True.</p> <code>True</code> <code>exclude</code> <code>tuple</code> <p>A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').</p> <code>('.DS_Store', '__MACOSX')</code> <code>progress</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the resulting zip file.</p> Example <pre><code>from ultralytics.utils.downloads import zip_directory\nfile = zip_directory('path/to/dir')\n</code></pre> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def zip_directory(directory, compress=True, exclude=('.DS_Store', '__MACOSX'), progress=True):\n\"\"\"\n    Zips the contents of a directory, excluding files containing strings in the exclude list.\n    The resulting zip file is named after the directory and placed alongside it.\n    Args:\n        directory (str | Path): The path to the directory to be zipped.\n        compress (bool): Whether to compress the files while zipping. Default is True.\n        exclude (tuple, optional): A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').\n        progress (bool, optional): Whether to display a progress bar. Defaults to True.\n    Returns:\n        (Path): The path to the resulting zip file.\n    Example:\n        ```python\n        from ultralytics.utils.downloads import zip_directory\n        file = zip_directory('path/to/dir')\n        ```\n    \"\"\"\nfrom zipfile import ZIP_DEFLATED, ZIP_STORED, ZipFile\ndelete_dsstore(directory)\ndirectory = Path(directory)\nif not directory.is_dir():\nraise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n# Unzip with progress bar\nfiles_to_zip = [f for f in directory.rglob('*') if f.is_file() and all(x not in f.name for x in exclude)]\nzip_file = directory.with_suffix('.zip')\ncompression = ZIP_DEFLATED if compress else ZIP_STORED\nwith ZipFile(zip_file, 'w', compression) as f:\nfor file in tqdm(files_to_zip, desc=f'Zipping {directory} to {zip_file}...', unit='file', disable=not progress):\nf.write(file, file.relative_to(directory))\nreturn zip_file  # return path to zip file\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.unzip_file","title":"<code>ultralytics.utils.downloads.unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX'), exist_ok=False, progress=True)</code>","text":"<p>Unzips a *.zip file to the specified path, excluding files containing strings in the exclude list.</p> <p>If the zipfile does not contain a single top-level directory, the function will create a new directory with the same name as the zipfile (without the extension) to extract its contents. If a path is not provided, the function will use the parent directory of the zipfile as the default path.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the zipfile to be extracted.</p> required <code>path</code> <code>str</code> <p>The path to extract the zipfile to. Defaults to None.</p> <code>None</code> <code>exclude</code> <code>tuple</code> <p>A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').</p> <code>('.DS_Store', '__MACOSX')</code> <code>exist_ok</code> <code>bool</code> <p>Whether to overwrite existing contents if they exist. Defaults to False.</p> <code>False</code> <code>progress</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>BadZipFile</code> <p>If the provided file does not exist or is not a valid zipfile.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the directory where the zipfile was extracted.</p> Example <pre><code>from ultralytics.utils.downloads import unzip_file\ndir = unzip_file('path/to/file.zip')\n</code></pre> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX'), exist_ok=False, progress=True):\n\"\"\"\n    Unzips a *.zip file to the specified path, excluding files containing strings in the exclude list.\n    If the zipfile does not contain a single top-level directory, the function will create a new\n    directory with the same name as the zipfile (without the extension) to extract its contents.\n    If a path is not provided, the function will use the parent directory of the zipfile as the default path.\n    Args:\n        file (str): The path to the zipfile to be extracted.\n        path (str, optional): The path to extract the zipfile to. Defaults to None.\n        exclude (tuple, optional): A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').\n        exist_ok (bool, optional): Whether to overwrite existing contents if they exist. Defaults to False.\n        progress (bool, optional): Whether to display a progress bar. Defaults to True.\n    Raises:\n        BadZipFile: If the provided file does not exist or is not a valid zipfile.\n    Returns:\n        (Path): The path to the directory where the zipfile was extracted.\n    Example:\n        ```python\n        from ultralytics.utils.downloads import unzip_file\n        dir = unzip_file('path/to/file.zip')\n        ```\n    \"\"\"\nfrom zipfile import BadZipFile, ZipFile, is_zipfile\nif not (Path(file).exists() and is_zipfile(file)):\nraise BadZipFile(f\"File '{file}' does not exist or is a bad zip file.\")\nif path is None:\npath = Path(file).parent  # default path\n# Unzip the file contents\nwith ZipFile(file) as zipObj:\nfiles = [f for f in zipObj.namelist() if all(x not in f for x in exclude)]\ntop_level_dirs = {Path(f).parts[0] for f in files}\nif len(top_level_dirs) &gt; 1 or not files[0].endswith('/'):  # zip has multiple files at top level\npath = extract_path = Path(path) / Path(file).stem  # i.e. ../datasets/coco8\nelse:  # zip has 1 top-level directory\nextract_path = path  # i.e. ../datasets\npath = Path(path) / list(top_level_dirs)[0]  # i.e. ../datasets/coco8\n# Check if destination directory already exists and contains files\nif path.exists() and any(path.iterdir()) and not exist_ok:\n# If it exists and is not empty, return the path without unzipping\nLOGGER.info(f'Skipping {file} unzip (already unzipped)')\nreturn path\nfor f in tqdm(files, desc=f'Unzipping {file} to {Path(path).resolve()}...', unit='file', disable=not progress):\nzipObj.extract(f, path=extract_path)\nreturn path  # return unzip dir\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.check_disk_space","title":"<code>ultralytics.utils.downloads.check_disk_space(url='https://ultralytics.com/assets/coco128.zip', sf=1.5, hard=True)</code>","text":"<p>Check if there is sufficient disk space to download and store a file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to the file. Defaults to 'https://ultralytics.com/assets/coco128.zip'.</p> <code>'https://ultralytics.com/assets/coco128.zip'</code> <code>sf</code> <code>float</code> <p>Safety factor, the multiplier for the required free space. Defaults to 2.0.</p> <code>1.5</code> <code>hard</code> <code>bool</code> <p>Whether to throw an error or not on insufficient disk space. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if there is sufficient disk space, False otherwise.</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def check_disk_space(url='https://ultralytics.com/assets/coco128.zip', sf=1.5, hard=True):\n\"\"\"\n    Check if there is sufficient disk space to download and store a file.\n    Args:\n        url (str, optional): The URL to the file. Defaults to 'https://ultralytics.com/assets/coco128.zip'.\n        sf (float, optional): Safety factor, the multiplier for the required free space. Defaults to 2.0.\n        hard (bool, optional): Whether to throw an error or not on insufficient disk space. Defaults to True.\n    Returns:\n        (bool): True if there is sufficient disk space, False otherwise.\n    \"\"\"\nwith contextlib.suppress(Exception):\ngib = 1 &lt;&lt; 30  # bytes per GiB\ndata = int(requests.head(url).headers['Content-Length']) / gib  # file size (GB)\ntotal, used, free = (x / gib for x in shutil.disk_usage('/'))  # bytes\nif data * sf &lt; free:\nreturn True  # sufficient space\n# Insufficient space\ntext = (f'WARNING \u26a0\ufe0f Insufficient free disk space {free:.1f} GB &lt; {data * sf:.3f} GB required, '\nf'Please free {data * sf - free:.1f} GB additional disk space and try again.')\nif hard:\nraise MemoryError(text)\nLOGGER.warning(text)\nreturn False\nreturn True\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.get_google_drive_file_info","title":"<code>ultralytics.utils.downloads.get_google_drive_file_info(link)</code>","text":"<p>Retrieves the direct download link and filename for a shareable Google Drive file link.</p> <p>Parameters:</p> Name Type Description Default <code>link</code> <code>str</code> <p>The shareable link of the Google Drive file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Direct download URL for the Google Drive file.</p> <code>str</code> <p>Original filename of the Google Drive file. If filename extraction fails, returns None.</p> Example <pre><code>from ultralytics.utils.downloads import get_google_drive_file_info\nlink = \"https://drive.google.com/file/d/1cqT-cJgANNrhIHCrEufUYhQ4RqiWG_lJ/view?usp=drive_link\"\nurl, filename = get_google_drive_file_info(link)\n</code></pre> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def get_google_drive_file_info(link):\n\"\"\"\n    Retrieves the direct download link and filename for a shareable Google Drive file link.\n    Args:\n        link (str): The shareable link of the Google Drive file.\n    Returns:\n        (str): Direct download URL for the Google Drive file.\n        (str): Original filename of the Google Drive file. If filename extraction fails, returns None.\n    Example:\n        ```python\n        from ultralytics.utils.downloads import get_google_drive_file_info\n        link = \"https://drive.google.com/file/d/1cqT-cJgANNrhIHCrEufUYhQ4RqiWG_lJ/view?usp=drive_link\"\n        url, filename = get_google_drive_file_info(link)\n        ```\n    \"\"\"\nfile_id = link.split('/d/')[1].split('/view')[0]\ndrive_url = f'https://drive.google.com/uc?export=download&amp;id={file_id}'\nfilename = None\n# Start session\nwith requests.Session() as session:\nresponse = session.get(drive_url, stream=True)\nif 'quota exceeded' in str(response.content.lower()):\nraise ConnectionError(\nemojis(f'\u274c  Google Drive file download quota exceeded. '\nf'Please try again later or download this file manually at {link}.'))\nfor k, v in response.cookies.items():\nif k.startswith('download_warning'):\ndrive_url += f'&amp;confirm={v}'  # v is token\ncd = response.headers.get('content-disposition')\nif cd:\nfilename = re.findall('filename=\"(.+)\"', cd)[0]\nreturn drive_url, filename\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.safe_download","title":"<code>ultralytics.utils.downloads.safe_download(url, file=None, dir=None, unzip=True, delete=False, curl=False, retry=3, min_bytes=1.0, progress=True)</code>","text":"<p>Downloads files from a URL, with options for retrying, unzipping, and deleting the downloaded file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be downloaded.</p> required <code>file</code> <code>str</code> <p>The filename of the downloaded file. If not provided, the file will be saved with the same name as the URL.</p> <code>None</code> <code>dir</code> <code>str</code> <p>The directory to save the downloaded file. If not provided, the file will be saved in the current working directory.</p> <code>None</code> <code>unzip</code> <code>bool</code> <p>Whether to unzip the downloaded file. Default: True.</p> <code>True</code> <code>delete</code> <code>bool</code> <p>Whether to delete the downloaded file after unzipping. Default: False.</p> <code>False</code> <code>curl</code> <code>bool</code> <p>Whether to use curl command line tool for downloading. Default: False.</p> <code>False</code> <code>retry</code> <code>int</code> <p>The number of times to retry the download in case of failure. Default: 3.</p> <code>3</code> <code>min_bytes</code> <code>float</code> <p>The minimum number of bytes that the downloaded file should have, to be considered a successful download. Default: 1E0.</p> <code>1.0</code> <code>progress</code> <code>bool</code> <p>Whether to display a progress bar during the download. Default: True.</p> <code>True</code> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def safe_download(url,\nfile=None,\ndir=None,\nunzip=True,\ndelete=False,\ncurl=False,\nretry=3,\nmin_bytes=1E0,\nprogress=True):\n\"\"\"\n    Downloads files from a URL, with options for retrying, unzipping, and deleting the downloaded file.\n    Args:\n        url (str): The URL of the file to be downloaded.\n        file (str, optional): The filename of the downloaded file.\n            If not provided, the file will be saved with the same name as the URL.\n        dir (str, optional): The directory to save the downloaded file.\n            If not provided, the file will be saved in the current working directory.\n        unzip (bool, optional): Whether to unzip the downloaded file. Default: True.\n        delete (bool, optional): Whether to delete the downloaded file after unzipping. Default: False.\n        curl (bool, optional): Whether to use curl command line tool for downloading. Default: False.\n        retry (int, optional): The number of times to retry the download in case of failure. Default: 3.\n        min_bytes (float, optional): The minimum number of bytes that the downloaded file should have, to be considered\n            a successful download. Default: 1E0.\n        progress (bool, optional): Whether to display a progress bar during the download. Default: True.\n    \"\"\"\n# Check if the URL is a Google Drive link\ngdrive = 'drive.google.com' in url\nif gdrive:\nurl, file = get_google_drive_file_info(url)\nf = dir / (file if gdrive else url2file(url)) if dir else Path(file)  # URL converted to filename\nif '://' not in str(url) and Path(url).is_file():  # URL exists ('://' check required in Windows Python&lt;3.10)\nf = Path(url)  # filename\nelif not f.is_file():  # URL and file do not exist\nassert dir or file, 'dir or file required for download'\ndesc = f\"Downloading {url if gdrive else clean_url(url)} to '{f}'\"\nLOGGER.info(f'{desc}...')\nf.parent.mkdir(parents=True, exist_ok=True)  # make directory if missing\ncheck_disk_space(url)\nfor i in range(retry + 1):\ntry:\nif curl or i &gt; 0:  # curl download with retry, continue\ns = 'sS' * (not progress)  # silent\nr = subprocess.run(['curl', '-#', f'-{s}L', url, '-o', f, '--retry', '3', '-C', '-']).returncode\nassert r == 0, f'Curl return value {r}'\nelse:  # urllib download\nmethod = 'torch'\nif method == 'torch':\ntorch.hub.download_url_to_file(url, f, progress=progress)\nelse:\nwith request.urlopen(url) as response, tqdm(total=int(response.getheader('Content-Length', 0)),\ndesc=desc,\ndisable=not progress,\nunit='B',\nunit_scale=True,\nunit_divisor=1024,\nbar_format=TQDM_BAR_FORMAT) as pbar:\nwith open(f, 'wb') as f_opened:\nfor data in response:\nf_opened.write(data)\npbar.update(len(data))\nif f.exists():\nif f.stat().st_size &gt; min_bytes:\nbreak  # success\nf.unlink()  # remove partial downloads\nexcept Exception as e:\nif i == 0 and not is_online():\nraise ConnectionError(emojis(f'\u274c  Download failure for {url}. Environment is not online.')) from e\nelif i &gt;= retry:\nraise ConnectionError(emojis(f'\u274c  Download failure for {url}. Retry limit reached.')) from e\nLOGGER.warning(f'\u26a0\ufe0f Download failure, retrying {i + 1}/{retry} {url}...')\nif unzip and f.exists() and f.suffix in ('', '.zip', '.tar', '.gz'):\nfrom zipfile import is_zipfile\nunzip_dir = dir or f.parent  # unzip to dir if provided else unzip in place\nif is_zipfile(f):\nunzip_dir = unzip_file(file=f, path=unzip_dir, progress=progress)  # unzip\nelif f.suffix in ('.tar', '.gz'):\nLOGGER.info(f'Unzipping {f} to {unzip_dir.resolve()}...')\nsubprocess.run(['tar', 'xf' if f.suffix == '.tar' else 'xfz', f, '--directory', unzip_dir], check=True)\nif delete:\nf.unlink()  # remove zip\nreturn unzip_dir\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.get_github_assets","title":"<code>ultralytics.utils.downloads.get_github_assets(repo='ultralytics/assets', version='latest', retry=False)</code>","text":"<p>Return GitHub repo tag and assets (i.e. ['yolov8n.pt', 'yolov8s.pt', ...]).</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def get_github_assets(repo='ultralytics/assets', version='latest', retry=False):\n\"\"\"Return GitHub repo tag and assets (i.e. ['yolov8n.pt', 'yolov8s.pt', ...]).\"\"\"\nif version != 'latest':\nversion = f'tags/{version}'  # i.e. tags/v6.2\nurl = f'https://api.github.com/repos/{repo}/releases/{version}'\nr = requests.get(url)  # github api\nif r.status_code != 200 and r.reason != 'rate limit exceeded' and retry:  # failed and not 403 rate limit exceeded\nr = requests.get(url)  # try again\nif r.status_code != 200:\nLOGGER.warning(f'\u26a0\ufe0f GitHub assets check failure for {url}: {r.status_code} {r.reason}')\nreturn '', []\ndata = r.json()\nreturn data['tag_name'], [x['name'] for x in data['assets']]  # tag, assets\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.attempt_download_asset","title":"<code>ultralytics.utils.downloads.attempt_download_asset(file, repo='ultralytics/assets', release='v0.0.0')</code>","text":"<p>Attempt file download from GitHub release assets if not found locally. release = 'latest', 'v6.2', etc.</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def attempt_download_asset(file, repo='ultralytics/assets', release='v0.0.0'):\n\"\"\"Attempt file download from GitHub release assets if not found locally. release = 'latest', 'v6.2', etc.\"\"\"\nfrom ultralytics.utils import SETTINGS  # scoped for circular import\n# YOLOv3/5u updates\nfile = str(file)\nfile = checks.check_yolov5u_filename(file)\nfile = Path(file.strip().replace(\"'\", ''))\nif file.exists():\nreturn str(file)\nelif (SETTINGS['weights_dir'] / file).exists():\nreturn str(SETTINGS['weights_dir'] / file)\nelse:\n# URL specified\nname = Path(parse.unquote(str(file))).name  # decode '%2F' to '/' etc.\nif str(file).startswith(('http:/', 'https:/')):  # download\nurl = str(file).replace(':/', '://')  # Pathlib turns :// -&gt; :/\nfile = url2file(name)  # parse authentication https://url.com/file.txt?auth...\nif Path(file).is_file():\nLOGGER.info(f'Found {clean_url(url)} locally at {file}')  # file already exists\nelse:\nsafe_download(url=url, file=file, min_bytes=1E5)\nelif repo == GITHUB_ASSETS_REPO and name in GITHUB_ASSETS_NAMES:\nsafe_download(url=f'https://github.com/{repo}/releases/download/{release}/{name}', file=file, min_bytes=1E5)\nelse:\ntag, assets = get_github_assets(repo, release)\nif not assets:\ntag, assets = get_github_assets(repo)  # latest release\nif name in assets:\nsafe_download(url=f'https://github.com/{repo}/releases/download/{tag}/{name}', file=file, min_bytes=1E5)\nreturn str(file)\n</code></pre>"},{"location":"reference/utils/downloads/#ultralytics.utils.downloads.download","title":"<code>ultralytics.utils.downloads.download(url, dir=Path.cwd(), unzip=True, delete=False, curl=False, threads=1, retry=3)</code>","text":"<p>Downloads and unzips files concurrently if threads &gt; 1, else sequentially.</p> Source code in <code>ultralytics/utils/downloads.py</code> <pre><code>def download(url, dir=Path.cwd(), unzip=True, delete=False, curl=False, threads=1, retry=3):\n\"\"\"Downloads and unzips files concurrently if threads &gt; 1, else sequentially.\"\"\"\ndir = Path(dir)\ndir.mkdir(parents=True, exist_ok=True)  # make directory\nif threads &gt; 1:\nwith ThreadPool(threads) as pool:\npool.map(\nlambda x: safe_download(\nurl=x[0], dir=x[1], unzip=unzip, delete=delete, curl=curl, retry=retry, progress=threads &lt;= 1),\nzip(url, repeat(dir)))\npool.close()\npool.join()\nelse:\nfor u in [url] if isinstance(url, (str, Path)) else url:\nsafe_download(url=u, dir=dir, unzip=unzip, delete=delete, curl=curl, retry=retry)\n</code></pre>"},{"location":"reference/utils/errors/","title":"Reference for <code>ultralytics/utils/errors.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/errors.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/utils/errors/#ultralytics.utils.errors.HUBModelError","title":"<code>ultralytics.utils.errors.HUBModelError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>ultralytics/utils/errors.py</code> <pre><code>class HUBModelError(Exception):\ndef __init__(self, message='Model not found. Please check model URL and try again.'):\n\"\"\"Create an exception for when a model is not found.\"\"\"\nsuper().__init__(emojis(message))\n</code></pre>"},{"location":"reference/utils/errors/#ultralytics.utils.errors.HUBModelError.__init__","title":"<code>__init__(message='Model not found. Please check model URL and try again.')</code>","text":"<p>Create an exception for when a model is not found.</p> Source code in <code>ultralytics/utils/errors.py</code> <pre><code>def __init__(self, message='Model not found. Please check model URL and try again.'):\n\"\"\"Create an exception for when a model is not found.\"\"\"\nsuper().__init__(emojis(message))\n</code></pre>"},{"location":"reference/utils/files/","title":"Reference for <code>ultralytics/utils/files.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/files.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/files/#ultralytics.utils.files.WorkingDirectory","title":"<code>ultralytics.utils.files.WorkingDirectory</code>","text":"<p>             Bases: <code>ContextDecorator</code></p> <p>Usage: @WorkingDirectory(dir) decorator or 'with WorkingDirectory(dir):' context manager.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>class WorkingDirectory(contextlib.ContextDecorator):\n\"\"\"Usage: @WorkingDirectory(dir) decorator or 'with WorkingDirectory(dir):' context manager.\"\"\"\ndef __init__(self, new_dir):\n\"\"\"Sets the working directory to 'new_dir' upon instantiation.\"\"\"\nself.dir = new_dir  # new dir\nself.cwd = Path.cwd().resolve()  # current dir\ndef __enter__(self):\n\"\"\"Changes the current directory to the specified directory.\"\"\"\nos.chdir(self.dir)\ndef __exit__(self, exc_type, exc_val, exc_tb):  # noqa\n\"\"\"Restore the current working directory on context exit.\"\"\"\nos.chdir(self.cwd)\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.WorkingDirectory.__enter__","title":"<code>__enter__()</code>","text":"<p>Changes the current directory to the specified directory.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def __enter__(self):\n\"\"\"Changes the current directory to the specified directory.\"\"\"\nos.chdir(self.dir)\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.WorkingDirectory.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Restore the current working directory on context exit.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):  # noqa\n\"\"\"Restore the current working directory on context exit.\"\"\"\nos.chdir(self.cwd)\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.WorkingDirectory.__init__","title":"<code>__init__(new_dir)</code>","text":"<p>Sets the working directory to 'new_dir' upon instantiation.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def __init__(self, new_dir):\n\"\"\"Sets the working directory to 'new_dir' upon instantiation.\"\"\"\nself.dir = new_dir  # new dir\nself.cwd = Path.cwd().resolve()  # current dir\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.spaces_in_path","title":"<code>ultralytics.utils.files.spaces_in_path(path)</code>","text":"<p>Context manager to handle paths with spaces in their names. If a path contains spaces, it replaces them with underscores, copies the file/directory to the new path, executes the context code block, then copies the file/directory back to its original location.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The original path.</p> required <p>Yields:</p> Type Description <code>Path</code> <p>Temporary path with spaces replaced by underscores if spaces were present, otherwise the original path.</p> Example <pre><code>with ultralytics.utils.files import spaces_in_path\nwith spaces_in_path('/path/with spaces') as new_path:\n# your code here\n</code></pre> Source code in <code>ultralytics/utils/files.py</code> <pre><code>@contextmanager\ndef spaces_in_path(path):\n\"\"\"\n    Context manager to handle paths with spaces in their names.\n    If a path contains spaces, it replaces them with underscores, copies the file/directory to the new path,\n    executes the context code block, then copies the file/directory back to its original location.\n    Args:\n        path (str | Path): The original path.\n    Yields:\n        (Path): Temporary path with spaces replaced by underscores if spaces were present, otherwise the original path.\n    Example:\n        ```python\n        with ultralytics.utils.files import spaces_in_path\n        with spaces_in_path('/path/with spaces') as new_path:\n            # your code here\n        ```\n    \"\"\"\n# If path has spaces, replace them with underscores\nif ' ' in str(path):\nstring = isinstance(path, str)  # input type\npath = Path(path)\n# Create a temporary directory and construct the new path\nwith tempfile.TemporaryDirectory() as tmp_dir:\ntmp_path = Path(tmp_dir) / path.name.replace(' ', '_')\n# Copy file/directory\nif path.is_dir():\n# tmp_path.mkdir(parents=True, exist_ok=True)\nshutil.copytree(path, tmp_path)\nelif path.is_file():\ntmp_path.parent.mkdir(parents=True, exist_ok=True)\nshutil.copy2(path, tmp_path)\ntry:\n# Yield the temporary path\nyield str(tmp_path) if string else tmp_path\nfinally:\n# Copy file/directory back\nif tmp_path.is_dir():\nshutil.copytree(tmp_path, path, dirs_exist_ok=True)\nelif tmp_path.is_file():\nshutil.copy2(tmp_path, path)  # Copy back the file\nelse:\n# If there are no spaces, just yield the original path\nyield path\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.increment_path","title":"<code>ultralytics.utils.files.increment_path(path, exist_ok=False, sep='', mkdir=False)</code>","text":"<p>Increments a file or directory path, i.e. runs/exp --&gt; runs/exp{sep}2, runs/exp{sep}3, ... etc.</p> <p>If the path exists and exist_ok is not set to True, the path will be incremented by appending a number and sep to the end of the path. If the path is a file, the file extension will be preserved. If the path is a directory, the number will be appended directly to the end of the path. If mkdir is set to True, the path will be created as a directory if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>(str, Path)</code> <p>Path to increment.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, the path will not be incremented and returned as-is. Defaults to False.</p> <code>False</code> <code>sep</code> <code>str</code> <p>Separator to use between the path and the incrementation number. Defaults to ''.</p> <code>''</code> <code>mkdir</code> <code>bool</code> <p>Create a directory if it does not exist. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Incremented path.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def increment_path(path, exist_ok=False, sep='', mkdir=False):\n\"\"\"\n    Increments a file or directory path, i.e. runs/exp --&gt; runs/exp{sep}2, runs/exp{sep}3, ... etc.\n    If the path exists and exist_ok is not set to True, the path will be incremented by appending a number and sep to\n    the end of the path. If the path is a file, the file extension will be preserved. If the path is a directory, the\n    number will be appended directly to the end of the path. If mkdir is set to True, the path will be created as a\n    directory if it does not already exist.\n    Args:\n        path (str, pathlib.Path): Path to increment.\n        exist_ok (bool, optional): If True, the path will not be incremented and returned as-is. Defaults to False.\n        sep (str, optional): Separator to use between the path and the incrementation number. Defaults to ''.\n        mkdir (bool, optional): Create a directory if it does not exist. Defaults to False.\n    Returns:\n        (pathlib.Path): Incremented path.\n    \"\"\"\npath = Path(path)  # os-agnostic\nif path.exists() and not exist_ok:\npath, suffix = (path.with_suffix(''), path.suffix) if path.is_file() else (path, '')\n# Method 1\nfor n in range(2, 9999):\np = f'{path}{sep}{n}{suffix}'  # increment path\nif not os.path.exists(p):  #\nbreak\npath = Path(p)\nif mkdir:\npath.mkdir(parents=True, exist_ok=True)  # make directory\nreturn path\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.file_age","title":"<code>ultralytics.utils.files.file_age(path=__file__)</code>","text":"<p>Return days since last file update.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def file_age(path=__file__):\n\"\"\"Return days since last file update.\"\"\"\ndt = (datetime.now() - datetime.fromtimestamp(Path(path).stat().st_mtime))  # delta\nreturn dt.days  # + dt.seconds / 86400  # fractional days\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.file_date","title":"<code>ultralytics.utils.files.file_date(path=__file__)</code>","text":"<p>Return human-readable file modification date, i.e. '2021-3-26'.</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def file_date(path=__file__):\n\"\"\"Return human-readable file modification date, i.e. '2021-3-26'.\"\"\"\nt = datetime.fromtimestamp(Path(path).stat().st_mtime)\nreturn f'{t.year}-{t.month}-{t.day}'\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.file_size","title":"<code>ultralytics.utils.files.file_size(path)</code>","text":"<p>Return file/dir size (MB).</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def file_size(path):\n\"\"\"Return file/dir size (MB).\"\"\"\nif isinstance(path, (str, Path)):\nmb = 1 &lt;&lt; 20  # bytes to MiB (1024 ** 2)\npath = Path(path)\nif path.is_file():\nreturn path.stat().st_size / mb\nelif path.is_dir():\nreturn sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / mb\nreturn 0.0\n</code></pre>"},{"location":"reference/utils/files/#ultralytics.utils.files.get_latest_run","title":"<code>ultralytics.utils.files.get_latest_run(search_dir='.')</code>","text":"<p>Return path to most recent 'last.pt' in /runs (i.e. to --resume from).</p> Source code in <code>ultralytics/utils/files.py</code> <pre><code>def get_latest_run(search_dir='.'):\n\"\"\"Return path to most recent 'last.pt' in /runs (i.e. to --resume from).\"\"\"\nlast_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\nreturn max(last_list, key=os.path.getctime) if last_list else ''\n</code></pre>"},{"location":"reference/utils/instance/","title":"Reference for <code>ultralytics/utils/instance.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/instance.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes","title":"<code>ultralytics.utils.instance.Bboxes</code>","text":"<p>Bounding Boxes class. Only numpy variables are supported.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>class Bboxes:\n\"\"\"Bounding Boxes class. Only numpy variables are supported.\"\"\"\ndef __init__(self, bboxes, format='xyxy') -&gt; None:\nassert format in _formats, f'Invalid bounding box format: {format}, format must be one of {_formats}'\nbboxes = bboxes[None, :] if bboxes.ndim == 1 else bboxes\nassert bboxes.ndim == 2\nassert bboxes.shape[1] == 4\nself.bboxes = bboxes\nself.format = format\n# self.normalized = normalized\ndef convert(self, format):\n\"\"\"Converts bounding box format from one type to another.\"\"\"\nassert format in _formats, f'Invalid bounding box format: {format}, format must be one of {_formats}'\nif self.format == format:\nreturn\nelif self.format == 'xyxy':\nfunc = xyxy2xywh if format == 'xywh' else xyxy2ltwh\nelif self.format == 'xywh':\nfunc = xywh2xyxy if format == 'xyxy' else xywh2ltwh\nelse:\nfunc = ltwh2xyxy if format == 'xyxy' else ltwh2xywh\nself.bboxes = func(self.bboxes)\nself.format = format\ndef areas(self):\n\"\"\"Return box areas.\"\"\"\nself.convert('xyxy')\nreturn (self.bboxes[:, 2] - self.bboxes[:, 0]) * (self.bboxes[:, 3] - self.bboxes[:, 1])\n# def denormalize(self, w, h):\n#    if not self.normalized:\n#         return\n#     assert (self.bboxes &lt;= 1.0).all()\n#     self.bboxes[:, 0::2] *= w\n#     self.bboxes[:, 1::2] *= h\n#     self.normalized = False\n#\n# def normalize(self, w, h):\n#     if self.normalized:\n#         return\n#     assert (self.bboxes &gt; 1.0).any()\n#     self.bboxes[:, 0::2] /= w\n#     self.bboxes[:, 1::2] /= h\n#     self.normalized = True\ndef mul(self, scale):\n\"\"\"\n        Args:\n            scale (tuple | list | int): the scale for four coords.\n        \"\"\"\nif isinstance(scale, Number):\nscale = to_4tuple(scale)\nassert isinstance(scale, (tuple, list))\nassert len(scale) == 4\nself.bboxes[:, 0] *= scale[0]\nself.bboxes[:, 1] *= scale[1]\nself.bboxes[:, 2] *= scale[2]\nself.bboxes[:, 3] *= scale[3]\ndef add(self, offset):\n\"\"\"\n        Args:\n            offset (tuple | list | int): the offset for four coords.\n        \"\"\"\nif isinstance(offset, Number):\noffset = to_4tuple(offset)\nassert isinstance(offset, (tuple, list))\nassert len(offset) == 4\nself.bboxes[:, 0] += offset[0]\nself.bboxes[:, 1] += offset[1]\nself.bboxes[:, 2] += offset[2]\nself.bboxes[:, 3] += offset[3]\ndef __len__(self):\n\"\"\"Return the number of boxes.\"\"\"\nreturn len(self.bboxes)\n@classmethod\ndef concatenate(cls, boxes_list: List['Bboxes'], axis=0) -&gt; 'Bboxes':\n\"\"\"\n        Concatenate a list of Bboxes objects into a single Bboxes object.\n        Args:\n            boxes_list (List[Bboxes]): A list of Bboxes objects to concatenate.\n            axis (int, optional): The axis along which to concatenate the bounding boxes.\n                                   Defaults to 0.\n        Returns:\n            Bboxes: A new Bboxes object containing the concatenated bounding boxes.\n        Note:\n            The input should be a list or tuple of Bboxes objects.\n        \"\"\"\nassert isinstance(boxes_list, (list, tuple))\nif not boxes_list:\nreturn cls(np.empty(0))\nassert all(isinstance(box, Bboxes) for box in boxes_list)\nif len(boxes_list) == 1:\nreturn boxes_list[0]\nreturn cls(np.concatenate([b.bboxes for b in boxes_list], axis=axis))\ndef __getitem__(self, index) -&gt; 'Bboxes':\n\"\"\"\n        Retrieve a specific bounding box or a set of bounding boxes using indexing.\n        Args:\n            index (int, slice, or np.ndarray): The index, slice, or boolean array to select\n                                               the desired bounding boxes.\n        Returns:\n            Bboxes: A new Bboxes object containing the selected bounding boxes.\n        Raises:\n            AssertionError: If the indexed bounding boxes do not form a 2-dimensional matrix.\n        Note:\n            When using boolean indexing, make sure to provide a boolean array with the same\n            length as the number of bounding boxes.\n        \"\"\"\nif isinstance(index, int):\nreturn Bboxes(self.bboxes[index].view(1, -1))\nb = self.bboxes[index]\nassert b.ndim == 2, f'Indexing on Bboxes with {index} failed to return a matrix!'\nreturn Bboxes(b)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve a specific bounding box or a set of bounding boxes using indexing.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int, slice, or np.ndarray</code> <p>The index, slice, or boolean array to select                                the desired bounding boxes.</p> required <p>Returns:</p> Name Type Description <code>Bboxes</code> <code>Bboxes</code> <p>A new Bboxes object containing the selected bounding boxes.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the indexed bounding boxes do not form a 2-dimensional matrix.</p> Note <p>When using boolean indexing, make sure to provide a boolean array with the same length as the number of bounding boxes.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def __getitem__(self, index) -&gt; 'Bboxes':\n\"\"\"\n    Retrieve a specific bounding box or a set of bounding boxes using indexing.\n    Args:\n        index (int, slice, or np.ndarray): The index, slice, or boolean array to select\n                                           the desired bounding boxes.\n    Returns:\n        Bboxes: A new Bboxes object containing the selected bounding boxes.\n    Raises:\n        AssertionError: If the indexed bounding boxes do not form a 2-dimensional matrix.\n    Note:\n        When using boolean indexing, make sure to provide a boolean array with the same\n        length as the number of bounding boxes.\n    \"\"\"\nif isinstance(index, int):\nreturn Bboxes(self.bboxes[index].view(1, -1))\nb = self.bboxes[index]\nassert b.ndim == 2, f'Indexing on Bboxes with {index} failed to return a matrix!'\nreturn Bboxes(b)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of boxes.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def __len__(self):\n\"\"\"Return the number of boxes.\"\"\"\nreturn len(self.bboxes)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.add","title":"<code>add(offset)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>offset</code> <code>tuple | list | int</code> <p>the offset for four coords.</p> required Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def add(self, offset):\n\"\"\"\n    Args:\n        offset (tuple | list | int): the offset for four coords.\n    \"\"\"\nif isinstance(offset, Number):\noffset = to_4tuple(offset)\nassert isinstance(offset, (tuple, list))\nassert len(offset) == 4\nself.bboxes[:, 0] += offset[0]\nself.bboxes[:, 1] += offset[1]\nself.bboxes[:, 2] += offset[2]\nself.bboxes[:, 3] += offset[3]\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.areas","title":"<code>areas()</code>","text":"<p>Return box areas.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def areas(self):\n\"\"\"Return box areas.\"\"\"\nself.convert('xyxy')\nreturn (self.bboxes[:, 2] - self.bboxes[:, 0]) * (self.bboxes[:, 3] - self.bboxes[:, 1])\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.concatenate","title":"<code>concatenate(boxes_list, axis=0)</code>  <code>classmethod</code>","text":"<p>Concatenate a list of Bboxes objects into a single Bboxes object.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_list</code> <code>List[Bboxes]</code> <p>A list of Bboxes objects to concatenate.</p> required <code>axis</code> <code>int</code> <p>The axis along which to concatenate the bounding boxes.                    Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Bboxes</code> <code>Bboxes</code> <p>A new Bboxes object containing the concatenated bounding boxes.</p> Note <p>The input should be a list or tuple of Bboxes objects.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>@classmethod\ndef concatenate(cls, boxes_list: List['Bboxes'], axis=0) -&gt; 'Bboxes':\n\"\"\"\n    Concatenate a list of Bboxes objects into a single Bboxes object.\n    Args:\n        boxes_list (List[Bboxes]): A list of Bboxes objects to concatenate.\n        axis (int, optional): The axis along which to concatenate the bounding boxes.\n                               Defaults to 0.\n    Returns:\n        Bboxes: A new Bboxes object containing the concatenated bounding boxes.\n    Note:\n        The input should be a list or tuple of Bboxes objects.\n    \"\"\"\nassert isinstance(boxes_list, (list, tuple))\nif not boxes_list:\nreturn cls(np.empty(0))\nassert all(isinstance(box, Bboxes) for box in boxes_list)\nif len(boxes_list) == 1:\nreturn boxes_list[0]\nreturn cls(np.concatenate([b.bboxes for b in boxes_list], axis=axis))\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.convert","title":"<code>convert(format)</code>","text":"<p>Converts bounding box format from one type to another.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def convert(self, format):\n\"\"\"Converts bounding box format from one type to another.\"\"\"\nassert format in _formats, f'Invalid bounding box format: {format}, format must be one of {_formats}'\nif self.format == format:\nreturn\nelif self.format == 'xyxy':\nfunc = xyxy2xywh if format == 'xywh' else xyxy2ltwh\nelif self.format == 'xywh':\nfunc = xywh2xyxy if format == 'xyxy' else xywh2ltwh\nelse:\nfunc = ltwh2xyxy if format == 'xyxy' else ltwh2xywh\nself.bboxes = func(self.bboxes)\nself.format = format\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Bboxes.mul","title":"<code>mul(scale)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>scale</code> <code>tuple | list | int</code> <p>the scale for four coords.</p> required Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def mul(self, scale):\n\"\"\"\n    Args:\n        scale (tuple | list | int): the scale for four coords.\n    \"\"\"\nif isinstance(scale, Number):\nscale = to_4tuple(scale)\nassert isinstance(scale, (tuple, list))\nassert len(scale) == 4\nself.bboxes[:, 0] *= scale[0]\nself.bboxes[:, 1] *= scale[1]\nself.bboxes[:, 2] *= scale[2]\nself.bboxes[:, 3] *= scale[3]\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances","title":"<code>ultralytics.utils.instance.Instances</code>","text":"Source code in <code>ultralytics/utils/instance.py</code> <pre><code>class Instances:\ndef __init__(self, bboxes, segments=None, keypoints=None, bbox_format='xywh', normalized=True) -&gt; None:\n\"\"\"\n        Args:\n            bboxes (ndarray): bboxes with shape [N, 4].\n            segments (list | ndarray): segments.\n            keypoints (ndarray): keypoints(x, y, visible) with shape [N, 17, 3].\n        \"\"\"\nif segments is None:\nsegments = []\nself._bboxes = Bboxes(bboxes=bboxes, format=bbox_format)\nself.keypoints = keypoints\nself.normalized = normalized\nif len(segments) &gt; 0:\n# list[np.array(1000, 2)] * num_samples\nsegments = resample_segments(segments)\n# (N, 1000, 2)\nsegments = np.stack(segments, axis=0)\nelse:\nsegments = np.zeros((0, 1000, 2), dtype=np.float32)\nself.segments = segments\ndef convert_bbox(self, format):\n\"\"\"Convert bounding box format.\"\"\"\nself._bboxes.convert(format=format)\n@property\ndef bbox_areas(self):\n\"\"\"Calculate the area of bounding boxes.\"\"\"\nreturn self._bboxes.areas()\ndef scale(self, scale_w, scale_h, bbox_only=False):\n\"\"\"this might be similar with denormalize func but without normalized sign.\"\"\"\nself._bboxes.mul(scale=(scale_w, scale_h, scale_w, scale_h))\nif bbox_only:\nreturn\nself.segments[..., 0] *= scale_w\nself.segments[..., 1] *= scale_h\nif self.keypoints is not None:\nself.keypoints[..., 0] *= scale_w\nself.keypoints[..., 1] *= scale_h\ndef denormalize(self, w, h):\n\"\"\"Denormalizes boxes, segments, and keypoints from normalized coordinates.\"\"\"\nif not self.normalized:\nreturn\nself._bboxes.mul(scale=(w, h, w, h))\nself.segments[..., 0] *= w\nself.segments[..., 1] *= h\nif self.keypoints is not None:\nself.keypoints[..., 0] *= w\nself.keypoints[..., 1] *= h\nself.normalized = False\ndef normalize(self, w, h):\n\"\"\"Normalize bounding boxes, segments, and keypoints to image dimensions.\"\"\"\nif self.normalized:\nreturn\nself._bboxes.mul(scale=(1 / w, 1 / h, 1 / w, 1 / h))\nself.segments[..., 0] /= w\nself.segments[..., 1] /= h\nif self.keypoints is not None:\nself.keypoints[..., 0] /= w\nself.keypoints[..., 1] /= h\nself.normalized = True\ndef add_padding(self, padw, padh):\n\"\"\"Handle rect and mosaic situation.\"\"\"\nassert not self.normalized, 'you should add padding with absolute coordinates.'\nself._bboxes.add(offset=(padw, padh, padw, padh))\nself.segments[..., 0] += padw\nself.segments[..., 1] += padh\nif self.keypoints is not None:\nself.keypoints[..., 0] += padw\nself.keypoints[..., 1] += padh\ndef __getitem__(self, index) -&gt; 'Instances':\n\"\"\"\n        Retrieve a specific instance or a set of instances using indexing.\n        Args:\n            index (int, slice, or np.ndarray): The index, slice, or boolean array to select\n                                               the desired instances.\n        Returns:\n            Instances: A new Instances object containing the selected bounding boxes,\n                       segments, and keypoints if present.\n        Note:\n            When using boolean indexing, make sure to provide a boolean array with the same\n            length as the number of instances.\n        \"\"\"\nsegments = self.segments[index] if len(self.segments) else self.segments\nkeypoints = self.keypoints[index] if self.keypoints is not None else None\nbboxes = self.bboxes[index]\nbbox_format = self._bboxes.format\nreturn Instances(\nbboxes=bboxes,\nsegments=segments,\nkeypoints=keypoints,\nbbox_format=bbox_format,\nnormalized=self.normalized,\n)\ndef flipud(self, h):\n\"\"\"Flips the coordinates of bounding boxes, segments, and keypoints vertically.\"\"\"\nif self._bboxes.format == 'xyxy':\ny1 = self.bboxes[:, 1].copy()\ny2 = self.bboxes[:, 3].copy()\nself.bboxes[:, 1] = h - y2\nself.bboxes[:, 3] = h - y1\nelse:\nself.bboxes[:, 1] = h - self.bboxes[:, 1]\nself.segments[..., 1] = h - self.segments[..., 1]\nif self.keypoints is not None:\nself.keypoints[..., 1] = h - self.keypoints[..., 1]\ndef fliplr(self, w):\n\"\"\"Reverses the order of the bounding boxes and segments horizontally.\"\"\"\nif self._bboxes.format == 'xyxy':\nx1 = self.bboxes[:, 0].copy()\nx2 = self.bboxes[:, 2].copy()\nself.bboxes[:, 0] = w - x2\nself.bboxes[:, 2] = w - x1\nelse:\nself.bboxes[:, 0] = w - self.bboxes[:, 0]\nself.segments[..., 0] = w - self.segments[..., 0]\nif self.keypoints is not None:\nself.keypoints[..., 0] = w - self.keypoints[..., 0]\ndef clip(self, w, h):\n\"\"\"Clips bounding boxes, segments, and keypoints values to stay within image boundaries.\"\"\"\nori_format = self._bboxes.format\nself.convert_bbox(format='xyxy')\nself.bboxes[:, [0, 2]] = self.bboxes[:, [0, 2]].clip(0, w)\nself.bboxes[:, [1, 3]] = self.bboxes[:, [1, 3]].clip(0, h)\nif ori_format != 'xyxy':\nself.convert_bbox(format=ori_format)\nself.segments[..., 0] = self.segments[..., 0].clip(0, w)\nself.segments[..., 1] = self.segments[..., 1].clip(0, h)\nif self.keypoints is not None:\nself.keypoints[..., 0] = self.keypoints[..., 0].clip(0, w)\nself.keypoints[..., 1] = self.keypoints[..., 1].clip(0, h)\ndef remove_zero_area_boxes(self):\n\"\"\"Remove zero-area boxes, i.e. after clipping some boxes may have zero width or height. This removes them.\"\"\"\ngood = self.bbox_areas &gt; 0\nif not all(good):\nself._bboxes = self._bboxes[good]\nif len(self.segments):\nself.segments = self.segments[good]\nif self.keypoints is not None:\nself.keypoints = self.keypoints[good]\nreturn good\ndef update(self, bboxes, segments=None, keypoints=None):\n\"\"\"Updates instance variables.\"\"\"\nself._bboxes = Bboxes(bboxes, format=self._bboxes.format)\nif segments is not None:\nself.segments = segments\nif keypoints is not None:\nself.keypoints = keypoints\ndef __len__(self):\n\"\"\"Return the length of the instance list.\"\"\"\nreturn len(self.bboxes)\n@classmethod\ndef concatenate(cls, instances_list: List['Instances'], axis=0) -&gt; 'Instances':\n\"\"\"\n        Concatenates a list of Instances objects into a single Instances object.\n        Args:\n            instances_list (List[Instances]): A list of Instances objects to concatenate.\n            axis (int, optional): The axis along which the arrays will be concatenated. Defaults to 0.\n        Returns:\n            Instances: A new Instances object containing the concatenated bounding boxes,\n                       segments, and keypoints if present.\n        Note:\n            The `Instances` objects in the list should have the same properties, such as\n            the format of the bounding boxes, whether keypoints are present, and if the\n            coordinates are normalized.\n        \"\"\"\nassert isinstance(instances_list, (list, tuple))\nif not instances_list:\nreturn cls(np.empty(0))\nassert all(isinstance(instance, Instances) for instance in instances_list)\nif len(instances_list) == 1:\nreturn instances_list[0]\nuse_keypoint = instances_list[0].keypoints is not None\nbbox_format = instances_list[0]._bboxes.format\nnormalized = instances_list[0].normalized\ncat_boxes = np.concatenate([ins.bboxes for ins in instances_list], axis=axis)\ncat_segments = np.concatenate([b.segments for b in instances_list], axis=axis)\ncat_keypoints = np.concatenate([b.keypoints for b in instances_list], axis=axis) if use_keypoint else None\nreturn cls(cat_boxes, cat_segments, cat_keypoints, bbox_format, normalized)\n@property\ndef bboxes(self):\n\"\"\"Return bounding boxes.\"\"\"\nreturn self._bboxes.bboxes\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.bbox_areas","title":"<code>bbox_areas</code>  <code>property</code>","text":"<p>Calculate the area of bounding boxes.</p>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.bboxes","title":"<code>bboxes</code>  <code>property</code>","text":"<p>Return bounding boxes.</p>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve a specific instance or a set of instances using indexing.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int, slice, or np.ndarray</code> <p>The index, slice, or boolean array to select                                the desired instances.</p> required <p>Returns:</p> Name Type Description <code>Instances</code> <code>Instances</code> <p>A new Instances object containing the selected bounding boxes,        segments, and keypoints if present.</p> Note <p>When using boolean indexing, make sure to provide a boolean array with the same length as the number of instances.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def __getitem__(self, index) -&gt; 'Instances':\n\"\"\"\n    Retrieve a specific instance or a set of instances using indexing.\n    Args:\n        index (int, slice, or np.ndarray): The index, slice, or boolean array to select\n                                           the desired instances.\n    Returns:\n        Instances: A new Instances object containing the selected bounding boxes,\n                   segments, and keypoints if present.\n    Note:\n        When using boolean indexing, make sure to provide a boolean array with the same\n        length as the number of instances.\n    \"\"\"\nsegments = self.segments[index] if len(self.segments) else self.segments\nkeypoints = self.keypoints[index] if self.keypoints is not None else None\nbboxes = self.bboxes[index]\nbbox_format = self._bboxes.format\nreturn Instances(\nbboxes=bboxes,\nsegments=segments,\nkeypoints=keypoints,\nbbox_format=bbox_format,\nnormalized=self.normalized,\n)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.__init__","title":"<code>__init__(bboxes, segments=None, keypoints=None, bbox_format='xywh', normalized=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>bboxes</code> <code>ndarray</code> <p>bboxes with shape [N, 4].</p> required <code>segments</code> <code>list | ndarray</code> <p>segments.</p> <code>None</code> <code>keypoints</code> <code>ndarray</code> <p>keypoints(x, y, visible) with shape [N, 17, 3].</p> <code>None</code> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def __init__(self, bboxes, segments=None, keypoints=None, bbox_format='xywh', normalized=True) -&gt; None:\n\"\"\"\n    Args:\n        bboxes (ndarray): bboxes with shape [N, 4].\n        segments (list | ndarray): segments.\n        keypoints (ndarray): keypoints(x, y, visible) with shape [N, 17, 3].\n    \"\"\"\nif segments is None:\nsegments = []\nself._bboxes = Bboxes(bboxes=bboxes, format=bbox_format)\nself.keypoints = keypoints\nself.normalized = normalized\nif len(segments) &gt; 0:\n# list[np.array(1000, 2)] * num_samples\nsegments = resample_segments(segments)\n# (N, 1000, 2)\nsegments = np.stack(segments, axis=0)\nelse:\nsegments = np.zeros((0, 1000, 2), dtype=np.float32)\nself.segments = segments\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the instance list.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def __len__(self):\n\"\"\"Return the length of the instance list.\"\"\"\nreturn len(self.bboxes)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.add_padding","title":"<code>add_padding(padw, padh)</code>","text":"<p>Handle rect and mosaic situation.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def add_padding(self, padw, padh):\n\"\"\"Handle rect and mosaic situation.\"\"\"\nassert not self.normalized, 'you should add padding with absolute coordinates.'\nself._bboxes.add(offset=(padw, padh, padw, padh))\nself.segments[..., 0] += padw\nself.segments[..., 1] += padh\nif self.keypoints is not None:\nself.keypoints[..., 0] += padw\nself.keypoints[..., 1] += padh\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.clip","title":"<code>clip(w, h)</code>","text":"<p>Clips bounding boxes, segments, and keypoints values to stay within image boundaries.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def clip(self, w, h):\n\"\"\"Clips bounding boxes, segments, and keypoints values to stay within image boundaries.\"\"\"\nori_format = self._bboxes.format\nself.convert_bbox(format='xyxy')\nself.bboxes[:, [0, 2]] = self.bboxes[:, [0, 2]].clip(0, w)\nself.bboxes[:, [1, 3]] = self.bboxes[:, [1, 3]].clip(0, h)\nif ori_format != 'xyxy':\nself.convert_bbox(format=ori_format)\nself.segments[..., 0] = self.segments[..., 0].clip(0, w)\nself.segments[..., 1] = self.segments[..., 1].clip(0, h)\nif self.keypoints is not None:\nself.keypoints[..., 0] = self.keypoints[..., 0].clip(0, w)\nself.keypoints[..., 1] = self.keypoints[..., 1].clip(0, h)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.concatenate","title":"<code>concatenate(instances_list, axis=0)</code>  <code>classmethod</code>","text":"<p>Concatenates a list of Instances objects into a single Instances object.</p> <p>Parameters:</p> Name Type Description Default <code>instances_list</code> <code>List[Instances]</code> <p>A list of Instances objects to concatenate.</p> required <code>axis</code> <code>int</code> <p>The axis along which the arrays will be concatenated. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Instances</code> <code>Instances</code> <p>A new Instances object containing the concatenated bounding boxes,        segments, and keypoints if present.</p> Note <p>The <code>Instances</code> objects in the list should have the same properties, such as the format of the bounding boxes, whether keypoints are present, and if the coordinates are normalized.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>@classmethod\ndef concatenate(cls, instances_list: List['Instances'], axis=0) -&gt; 'Instances':\n\"\"\"\n    Concatenates a list of Instances objects into a single Instances object.\n    Args:\n        instances_list (List[Instances]): A list of Instances objects to concatenate.\n        axis (int, optional): The axis along which the arrays will be concatenated. Defaults to 0.\n    Returns:\n        Instances: A new Instances object containing the concatenated bounding boxes,\n                   segments, and keypoints if present.\n    Note:\n        The `Instances` objects in the list should have the same properties, such as\n        the format of the bounding boxes, whether keypoints are present, and if the\n        coordinates are normalized.\n    \"\"\"\nassert isinstance(instances_list, (list, tuple))\nif not instances_list:\nreturn cls(np.empty(0))\nassert all(isinstance(instance, Instances) for instance in instances_list)\nif len(instances_list) == 1:\nreturn instances_list[0]\nuse_keypoint = instances_list[0].keypoints is not None\nbbox_format = instances_list[0]._bboxes.format\nnormalized = instances_list[0].normalized\ncat_boxes = np.concatenate([ins.bboxes for ins in instances_list], axis=axis)\ncat_segments = np.concatenate([b.segments for b in instances_list], axis=axis)\ncat_keypoints = np.concatenate([b.keypoints for b in instances_list], axis=axis) if use_keypoint else None\nreturn cls(cat_boxes, cat_segments, cat_keypoints, bbox_format, normalized)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.convert_bbox","title":"<code>convert_bbox(format)</code>","text":"<p>Convert bounding box format.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def convert_bbox(self, format):\n\"\"\"Convert bounding box format.\"\"\"\nself._bboxes.convert(format=format)\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.denormalize","title":"<code>denormalize(w, h)</code>","text":"<p>Denormalizes boxes, segments, and keypoints from normalized coordinates.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def denormalize(self, w, h):\n\"\"\"Denormalizes boxes, segments, and keypoints from normalized coordinates.\"\"\"\nif not self.normalized:\nreturn\nself._bboxes.mul(scale=(w, h, w, h))\nself.segments[..., 0] *= w\nself.segments[..., 1] *= h\nif self.keypoints is not None:\nself.keypoints[..., 0] *= w\nself.keypoints[..., 1] *= h\nself.normalized = False\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.fliplr","title":"<code>fliplr(w)</code>","text":"<p>Reverses the order of the bounding boxes and segments horizontally.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def fliplr(self, w):\n\"\"\"Reverses the order of the bounding boxes and segments horizontally.\"\"\"\nif self._bboxes.format == 'xyxy':\nx1 = self.bboxes[:, 0].copy()\nx2 = self.bboxes[:, 2].copy()\nself.bboxes[:, 0] = w - x2\nself.bboxes[:, 2] = w - x1\nelse:\nself.bboxes[:, 0] = w - self.bboxes[:, 0]\nself.segments[..., 0] = w - self.segments[..., 0]\nif self.keypoints is not None:\nself.keypoints[..., 0] = w - self.keypoints[..., 0]\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.flipud","title":"<code>flipud(h)</code>","text":"<p>Flips the coordinates of bounding boxes, segments, and keypoints vertically.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def flipud(self, h):\n\"\"\"Flips the coordinates of bounding boxes, segments, and keypoints vertically.\"\"\"\nif self._bboxes.format == 'xyxy':\ny1 = self.bboxes[:, 1].copy()\ny2 = self.bboxes[:, 3].copy()\nself.bboxes[:, 1] = h - y2\nself.bboxes[:, 3] = h - y1\nelse:\nself.bboxes[:, 1] = h - self.bboxes[:, 1]\nself.segments[..., 1] = h - self.segments[..., 1]\nif self.keypoints is not None:\nself.keypoints[..., 1] = h - self.keypoints[..., 1]\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.normalize","title":"<code>normalize(w, h)</code>","text":"<p>Normalize bounding boxes, segments, and keypoints to image dimensions.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def normalize(self, w, h):\n\"\"\"Normalize bounding boxes, segments, and keypoints to image dimensions.\"\"\"\nif self.normalized:\nreturn\nself._bboxes.mul(scale=(1 / w, 1 / h, 1 / w, 1 / h))\nself.segments[..., 0] /= w\nself.segments[..., 1] /= h\nif self.keypoints is not None:\nself.keypoints[..., 0] /= w\nself.keypoints[..., 1] /= h\nself.normalized = True\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.remove_zero_area_boxes","title":"<code>remove_zero_area_boxes()</code>","text":"<p>Remove zero-area boxes, i.e. after clipping some boxes may have zero width or height. This removes them.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def remove_zero_area_boxes(self):\n\"\"\"Remove zero-area boxes, i.e. after clipping some boxes may have zero width or height. This removes them.\"\"\"\ngood = self.bbox_areas &gt; 0\nif not all(good):\nself._bboxes = self._bboxes[good]\nif len(self.segments):\nself.segments = self.segments[good]\nif self.keypoints is not None:\nself.keypoints = self.keypoints[good]\nreturn good\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.scale","title":"<code>scale(scale_w, scale_h, bbox_only=False)</code>","text":"<p>this might be similar with denormalize func but without normalized sign.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def scale(self, scale_w, scale_h, bbox_only=False):\n\"\"\"this might be similar with denormalize func but without normalized sign.\"\"\"\nself._bboxes.mul(scale=(scale_w, scale_h, scale_w, scale_h))\nif bbox_only:\nreturn\nself.segments[..., 0] *= scale_w\nself.segments[..., 1] *= scale_h\nif self.keypoints is not None:\nself.keypoints[..., 0] *= scale_w\nself.keypoints[..., 1] *= scale_h\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance.Instances.update","title":"<code>update(bboxes, segments=None, keypoints=None)</code>","text":"<p>Updates instance variables.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def update(self, bboxes, segments=None, keypoints=None):\n\"\"\"Updates instance variables.\"\"\"\nself._bboxes = Bboxes(bboxes, format=self._bboxes.format)\nif segments is not None:\nself.segments = segments\nif keypoints is not None:\nself.keypoints = keypoints\n</code></pre>"},{"location":"reference/utils/instance/#ultralytics.utils.instance._ntuple","title":"<code>ultralytics.utils.instance._ntuple(n)</code>","text":"<p>From PyTorch internals.</p> Source code in <code>ultralytics/utils/instance.py</code> <pre><code>def _ntuple(n):\n\"\"\"From PyTorch internals.\"\"\"\ndef parse(x):\n\"\"\"Parse bounding boxes format between XYWH and LTWH.\"\"\"\nreturn x if isinstance(x, abc.Iterable) else tuple(repeat(x, n))\nreturn parse\n</code></pre>"},{"location":"reference/utils/loss/","title":"Reference for <code>ultralytics/utils/loss.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/loss.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.VarifocalLoss","title":"<code>ultralytics.utils.loss.VarifocalLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class VarifocalLoss(nn.Module):\n\"\"\"Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367.\"\"\"\ndef __init__(self):\n\"\"\"Initialize the VarifocalLoss class.\"\"\"\nsuper().__init__()\ndef forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):\n\"\"\"Computes varfocal loss.\"\"\"\nweight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\nwith torch.cuda.amp.autocast(enabled=False):\nloss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *\nweight).mean(1).sum()\nreturn loss\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.VarifocalLoss.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the VarifocalLoss class.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __init__(self):\n\"\"\"Initialize the VarifocalLoss class.\"\"\"\nsuper().__init__()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.VarifocalLoss.forward","title":"<code>forward(pred_score, gt_score, label, alpha=0.75, gamma=2.0)</code>","text":"<p>Computes varfocal loss.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):\n\"\"\"Computes varfocal loss.\"\"\"\nweight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\nwith torch.cuda.amp.autocast(enabled=False):\nloss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *\nweight).mean(1).sum()\nreturn loss\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.FocalLoss","title":"<code>ultralytics.utils.loss.FocalLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5).</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class FocalLoss(nn.Module):\n\"\"\"Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5).\"\"\"\ndef __init__(self, ):\nsuper().__init__()\ndef forward(self, pred, label, gamma=1.5, alpha=0.25):\n\"\"\"Calculates and updates confusion matrix for object detection/classification tasks.\"\"\"\nloss = F.binary_cross_entropy_with_logits(pred, label, reduction='none')\n# p_t = torch.exp(-loss)\n# loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability\n# TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py\npred_prob = pred.sigmoid()  # prob from logits\np_t = label * pred_prob + (1 - label) * (1 - pred_prob)\nmodulating_factor = (1.0 - p_t) ** gamma\nloss *= modulating_factor\nif alpha &gt; 0:\nalpha_factor = label * alpha + (1 - label) * (1 - alpha)\nloss *= alpha_factor\nreturn loss.mean(1).sum()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.FocalLoss.forward","title":"<code>forward(pred, label, gamma=1.5, alpha=0.25)</code>","text":"<p>Calculates and updates confusion matrix for object detection/classification tasks.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def forward(self, pred, label, gamma=1.5, alpha=0.25):\n\"\"\"Calculates and updates confusion matrix for object detection/classification tasks.\"\"\"\nloss = F.binary_cross_entropy_with_logits(pred, label, reduction='none')\n# p_t = torch.exp(-loss)\n# loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability\n# TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py\npred_prob = pred.sigmoid()  # prob from logits\np_t = label * pred_prob + (1 - label) * (1 - pred_prob)\nmodulating_factor = (1.0 - p_t) ** gamma\nloss *= modulating_factor\nif alpha &gt; 0:\nalpha_factor = label * alpha + (1 - label) * (1 - alpha)\nloss *= alpha_factor\nreturn loss.mean(1).sum()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.BboxLoss","title":"<code>ultralytics.utils.loss.BboxLoss</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class BboxLoss(nn.Module):\ndef __init__(self, reg_max, use_dfl=False):\n\"\"\"Initialize the BboxLoss module with regularization maximum and DFL settings.\"\"\"\nsuper().__init__()\nself.reg_max = reg_max\nself.use_dfl = use_dfl\ndef forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n\"\"\"IoU loss.\"\"\"\nweight = target_scores.sum(-1)[fg_mask].unsqueeze(-1)\niou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\nloss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n# DFL loss\nif self.use_dfl:\ntarget_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\nloss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\nloss_dfl = loss_dfl.sum() / target_scores_sum\nelse:\nloss_dfl = torch.tensor(0.0).to(pred_dist.device)\nreturn loss_iou, loss_dfl\n@staticmethod\ndef _df_loss(pred_dist, target):\n\"\"\"Return sum of left and right DFL losses.\"\"\"\n# Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\ntl = target.long()  # target left\ntr = tl + 1  # target right\nwl = tr - target  # weight left\nwr = 1 - wl  # weight right\nreturn (F.cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape) * wl +\nF.cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.BboxLoss.__init__","title":"<code>__init__(reg_max, use_dfl=False)</code>","text":"<p>Initialize the BboxLoss module with regularization maximum and DFL settings.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __init__(self, reg_max, use_dfl=False):\n\"\"\"Initialize the BboxLoss module with regularization maximum and DFL settings.\"\"\"\nsuper().__init__()\nself.reg_max = reg_max\nself.use_dfl = use_dfl\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.BboxLoss.forward","title":"<code>forward(pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask)</code>","text":"<p>IoU loss.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n\"\"\"IoU loss.\"\"\"\nweight = target_scores.sum(-1)[fg_mask].unsqueeze(-1)\niou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\nloss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n# DFL loss\nif self.use_dfl:\ntarget_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\nloss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\nloss_dfl = loss_dfl.sum() / target_scores_sum\nelse:\nloss_dfl = torch.tensor(0.0).to(pred_dist.device)\nreturn loss_iou, loss_dfl\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.KeypointLoss","title":"<code>ultralytics.utils.loss.KeypointLoss</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class KeypointLoss(nn.Module):\ndef __init__(self, sigmas) -&gt; None:\nsuper().__init__()\nself.sigmas = sigmas\ndef forward(self, pred_kpts, gt_kpts, kpt_mask, area):\n\"\"\"Calculates keypoint loss factor and Euclidean distance loss for predicted and actual keypoints.\"\"\"\nd = (pred_kpts[..., 0] - gt_kpts[..., 0]) ** 2 + (pred_kpts[..., 1] - gt_kpts[..., 1]) ** 2\nkpt_loss_factor = (torch.sum(kpt_mask != 0) + torch.sum(kpt_mask == 0)) / (torch.sum(kpt_mask != 0) + 1e-9)\n# e = d / (2 * (area * self.sigmas) ** 2 + 1e-9)  # from formula\ne = d / (2 * self.sigmas) ** 2 / (area + 1e-9) / 2  # from cocoeval\nreturn kpt_loss_factor * ((1 - torch.exp(-e)) * kpt_mask).mean()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.KeypointLoss.forward","title":"<code>forward(pred_kpts, gt_kpts, kpt_mask, area)</code>","text":"<p>Calculates keypoint loss factor and Euclidean distance loss for predicted and actual keypoints.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def forward(self, pred_kpts, gt_kpts, kpt_mask, area):\n\"\"\"Calculates keypoint loss factor and Euclidean distance loss for predicted and actual keypoints.\"\"\"\nd = (pred_kpts[..., 0] - gt_kpts[..., 0]) ** 2 + (pred_kpts[..., 1] - gt_kpts[..., 1]) ** 2\nkpt_loss_factor = (torch.sum(kpt_mask != 0) + torch.sum(kpt_mask == 0)) / (torch.sum(kpt_mask != 0) + 1e-9)\n# e = d / (2 * (area * self.sigmas) ** 2 + 1e-9)  # from formula\ne = d / (2 * self.sigmas) ** 2 / (area + 1e-9) / 2  # from cocoeval\nreturn kpt_loss_factor * ((1 - torch.exp(-e)) * kpt_mask).mean()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8DetectionLoss","title":"<code>ultralytics.utils.loss.v8DetectionLoss</code>","text":"Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class v8DetectionLoss:\ndef __init__(self, model):  # model must be de-paralleled\ndevice = next(model.parameters()).device  # get model device\nh = model.args  # hyperparameters\nm = model.model[-1]  # Detect() module\nself.bce = nn.BCEWithLogitsLoss(reduction='none')\nself.hyp = h\nself.stride = m.stride  # model strides\nself.nc = m.nc  # number of classes\nself.no = m.no\nself.reg_max = m.reg_max\nself.device = device\nself.use_dfl = m.reg_max &gt; 1\nself.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\nself.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)\nself.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\ndef preprocess(self, targets, batch_size, scale_tensor):\n\"\"\"Preprocesses the target counts and matches with the input batch size to output a tensor.\"\"\"\nif targets.shape[0] == 0:\nout = torch.zeros(batch_size, 0, 5, device=self.device)\nelse:\ni = targets[:, 0]  # image index\n_, counts = i.unique(return_counts=True)\ncounts = counts.to(dtype=torch.int32)\nout = torch.zeros(batch_size, counts.max(), 5, device=self.device)\nfor j in range(batch_size):\nmatches = i == j\nn = matches.sum()\nif n:\nout[j, :n] = targets[matches, 1:]\nout[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\nreturn out\ndef bbox_decode(self, anchor_points, pred_dist):\n\"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\nif self.use_dfl:\nb, a, c = pred_dist.shape  # batch, anchors, channels\npred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n# pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n# pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\nreturn dist2bbox(pred_dist, anchor_points, xywh=False)\ndef __call__(self, preds, batch):\n\"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\nloss = torch.zeros(3, device=self.device)  # box, cls, dfl\nfeats = preds[1] if isinstance(preds, tuple) else preds\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nbatch_size = pred_scores.shape[0]\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\ntargets = torch.cat((batch['batch_idx'].view(-1, 1), batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n_, target_bboxes, target_scores, fg_mask, _ = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n# bbox loss\nif fg_mask.sum():\ntarget_bboxes /= stride_tensor\nloss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\ntarget_scores_sum, fg_mask)\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.cls  # cls gain\nloss[2] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8DetectionLoss.__call__","title":"<code>__call__(preds, batch)</code>","text":"<p>Calculate the sum of the loss for box, cls and dfl multiplied by batch size.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __call__(self, preds, batch):\n\"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\nloss = torch.zeros(3, device=self.device)  # box, cls, dfl\nfeats = preds[1] if isinstance(preds, tuple) else preds\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nbatch_size = pred_scores.shape[0]\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\ntargets = torch.cat((batch['batch_idx'].view(-1, 1), batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n_, target_bboxes, target_scores, fg_mask, _ = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n# bbox loss\nif fg_mask.sum():\ntarget_bboxes /= stride_tensor\nloss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\ntarget_scores_sum, fg_mask)\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.cls  # cls gain\nloss[2] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8DetectionLoss.bbox_decode","title":"<code>bbox_decode(anchor_points, pred_dist)</code>","text":"<p>Decode predicted object bounding box coordinates from anchor points and distribution.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def bbox_decode(self, anchor_points, pred_dist):\n\"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\nif self.use_dfl:\nb, a, c = pred_dist.shape  # batch, anchors, channels\npred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n# pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n# pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\nreturn dist2bbox(pred_dist, anchor_points, xywh=False)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8DetectionLoss.preprocess","title":"<code>preprocess(targets, batch_size, scale_tensor)</code>","text":"<p>Preprocesses the target counts and matches with the input batch size to output a tensor.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def preprocess(self, targets, batch_size, scale_tensor):\n\"\"\"Preprocesses the target counts and matches with the input batch size to output a tensor.\"\"\"\nif targets.shape[0] == 0:\nout = torch.zeros(batch_size, 0, 5, device=self.device)\nelse:\ni = targets[:, 0]  # image index\n_, counts = i.unique(return_counts=True)\ncounts = counts.to(dtype=torch.int32)\nout = torch.zeros(batch_size, counts.max(), 5, device=self.device)\nfor j in range(batch_size):\nmatches = i == j\nn = matches.sum()\nif n:\nout[j, :n] = targets[matches, 1:]\nout[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\nreturn out\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8SegmentationLoss","title":"<code>ultralytics.utils.loss.v8SegmentationLoss</code>","text":"<p>             Bases: <code>v8DetectionLoss</code></p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class v8SegmentationLoss(v8DetectionLoss):\ndef __init__(self, model):  # model must be de-paralleled\nsuper().__init__(model)\nself.nm = model.model[-1].nm  # number of masks\nself.overlap = model.args.overlap_mask\ndef __call__(self, preds, batch):\n\"\"\"Calculate and return the loss for the YOLO model.\"\"\"\nloss = torch.zeros(4, device=self.device)  # box, cls, dfl\nfeats, pred_masks, proto = preds if len(preds) == 3 else preds[1]\nbatch_size, _, mask_h, mask_w = proto.shape  # batch size, number of masks, mask height, mask width\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\n# b, grids, ..\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\npred_masks = pred_masks.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\ntry:\nbatch_idx = batch['batch_idx'].view(-1, 1)\ntargets = torch.cat((batch_idx, batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\nexcept RuntimeError as e:\nraise TypeError('ERROR \u274c segment dataset incorrectly formatted or not a segment dataset.\\n'\n\"This error can occur when incorrectly training a 'segment' model on a 'detect' dataset, \"\n\"i.e. 'yolo train model=yolov8n-seg.pt data=coco128.yaml'.\\nVerify your dataset is a \"\n\"correctly formatted 'segment' dataset using 'data=coco128-seg.yaml' \"\n'as an example.\\nSee https://docs.ultralytics.com/tasks/segment/ for help.') from e\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n_, target_bboxes, target_scores, fg_mask, target_gt_idx = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[2] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\nif fg_mask.sum():\n# bbox loss\nloss[0], loss[3] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes / stride_tensor,\ntarget_scores, target_scores_sum, fg_mask)\n# masks loss\nmasks = batch['masks'].to(self.device).float()\nif tuple(masks.shape[-2:]) != (mask_h, mask_w):  # downsample\nmasks = F.interpolate(masks[None], (mask_h, mask_w), mode='nearest')[0]\nfor i in range(batch_size):\nif fg_mask[i].sum():\nmask_idx = target_gt_idx[i][fg_mask[i]]\nif self.overlap:\ngt_mask = torch.where(masks[[i]] == (mask_idx + 1).view(-1, 1, 1), 1.0, 0.0)\nelse:\ngt_mask = masks[batch_idx.view(-1) == i][mask_idx]\nxyxyn = target_bboxes[i][fg_mask[i]] / imgsz[[1, 0, 1, 0]]\nmarea = xyxy2xywh(xyxyn)[:, 2:].prod(1)\nmxyxy = xyxyn * torch.tensor([mask_w, mask_h, mask_w, mask_h], device=self.device)\nloss[1] += self.single_mask_loss(gt_mask, pred_masks[i][fg_mask[i]], proto[i], mxyxy, marea)  # seg\n# WARNING: lines below prevents Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\nelse:\nloss[1] += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\n# WARNING: lines below prevent Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\nelse:\nloss[1] += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.box / batch_size  # seg gain\nloss[2] *= self.hyp.cls  # cls gain\nloss[3] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\ndef single_mask_loss(self, gt_mask, pred, proto, xyxy, area):\n\"\"\"Mask loss for one image.\"\"\"\npred_mask = (pred @ proto.view(self.nm, -1)).view(-1, *proto.shape[1:])  # (n, 32) @ (32,80,80) -&gt; (n,80,80)\nloss = F.binary_cross_entropy_with_logits(pred_mask, gt_mask, reduction='none')\nreturn (crop_mask(loss, xyxy).mean(dim=(1, 2)) / area).mean()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8SegmentationLoss.__call__","title":"<code>__call__(preds, batch)</code>","text":"<p>Calculate and return the loss for the YOLO model.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __call__(self, preds, batch):\n\"\"\"Calculate and return the loss for the YOLO model.\"\"\"\nloss = torch.zeros(4, device=self.device)  # box, cls, dfl\nfeats, pred_masks, proto = preds if len(preds) == 3 else preds[1]\nbatch_size, _, mask_h, mask_w = proto.shape  # batch size, number of masks, mask height, mask width\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\n# b, grids, ..\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\npred_masks = pred_masks.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\ntry:\nbatch_idx = batch['batch_idx'].view(-1, 1)\ntargets = torch.cat((batch_idx, batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\nexcept RuntimeError as e:\nraise TypeError('ERROR \u274c segment dataset incorrectly formatted or not a segment dataset.\\n'\n\"This error can occur when incorrectly training a 'segment' model on a 'detect' dataset, \"\n\"i.e. 'yolo train model=yolov8n-seg.pt data=coco128.yaml'.\\nVerify your dataset is a \"\n\"correctly formatted 'segment' dataset using 'data=coco128-seg.yaml' \"\n'as an example.\\nSee https://docs.ultralytics.com/tasks/segment/ for help.') from e\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n_, target_bboxes, target_scores, fg_mask, target_gt_idx = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[2] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\nif fg_mask.sum():\n# bbox loss\nloss[0], loss[3] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes / stride_tensor,\ntarget_scores, target_scores_sum, fg_mask)\n# masks loss\nmasks = batch['masks'].to(self.device).float()\nif tuple(masks.shape[-2:]) != (mask_h, mask_w):  # downsample\nmasks = F.interpolate(masks[None], (mask_h, mask_w), mode='nearest')[0]\nfor i in range(batch_size):\nif fg_mask[i].sum():\nmask_idx = target_gt_idx[i][fg_mask[i]]\nif self.overlap:\ngt_mask = torch.where(masks[[i]] == (mask_idx + 1).view(-1, 1, 1), 1.0, 0.0)\nelse:\ngt_mask = masks[batch_idx.view(-1) == i][mask_idx]\nxyxyn = target_bboxes[i][fg_mask[i]] / imgsz[[1, 0, 1, 0]]\nmarea = xyxy2xywh(xyxyn)[:, 2:].prod(1)\nmxyxy = xyxyn * torch.tensor([mask_w, mask_h, mask_w, mask_h], device=self.device)\nloss[1] += self.single_mask_loss(gt_mask, pred_masks[i][fg_mask[i]], proto[i], mxyxy, marea)  # seg\n# WARNING: lines below prevents Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\nelse:\nloss[1] += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\n# WARNING: lines below prevent Multi-GPU DDP 'unused gradient' PyTorch errors, do not remove\nelse:\nloss[1] += (proto * 0).sum() + (pred_masks * 0).sum()  # inf sums may lead to nan loss\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.box / batch_size  # seg gain\nloss[2] *= self.hyp.cls  # cls gain\nloss[3] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8SegmentationLoss.single_mask_loss","title":"<code>single_mask_loss(gt_mask, pred, proto, xyxy, area)</code>","text":"<p>Mask loss for one image.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def single_mask_loss(self, gt_mask, pred, proto, xyxy, area):\n\"\"\"Mask loss for one image.\"\"\"\npred_mask = (pred @ proto.view(self.nm, -1)).view(-1, *proto.shape[1:])  # (n, 32) @ (32,80,80) -&gt; (n,80,80)\nloss = F.binary_cross_entropy_with_logits(pred_mask, gt_mask, reduction='none')\nreturn (crop_mask(loss, xyxy).mean(dim=(1, 2)) / area).mean()\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8PoseLoss","title":"<code>ultralytics.utils.loss.v8PoseLoss</code>","text":"<p>             Bases: <code>v8DetectionLoss</code></p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class v8PoseLoss(v8DetectionLoss):\ndef __init__(self, model):  # model must be de-paralleled\nsuper().__init__(model)\nself.kpt_shape = model.model[-1].kpt_shape\nself.bce_pose = nn.BCEWithLogitsLoss()\nis_pose = self.kpt_shape == [17, 3]\nnkpt = self.kpt_shape[0]  # number of keypoints\nsigmas = torch.from_numpy(OKS_SIGMA).to(self.device) if is_pose else torch.ones(nkpt, device=self.device) / nkpt\nself.keypoint_loss = KeypointLoss(sigmas=sigmas)\ndef __call__(self, preds, batch):\n\"\"\"Calculate the total loss and detach it.\"\"\"\nloss = torch.zeros(5, device=self.device)  # box, cls, dfl, kpt_location, kpt_visibility\nfeats, pred_kpts = preds if isinstance(preds[0], list) else preds[1]\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\n# b, grids, ..\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\npred_kpts = pred_kpts.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\nbatch_size = pred_scores.shape[0]\nbatch_idx = batch['batch_idx'].view(-1, 1)\ntargets = torch.cat((batch_idx, batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\npred_kpts = self.kpts_decode(anchor_points, pred_kpts.view(batch_size, -1, *self.kpt_shape))  # (b, h*w, 17, 3)\n_, target_bboxes, target_scores, fg_mask, target_gt_idx = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[3] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n# bbox loss\nif fg_mask.sum():\ntarget_bboxes /= stride_tensor\nloss[0], loss[4] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\ntarget_scores_sum, fg_mask)\nkeypoints = batch['keypoints'].to(self.device).float().clone()\nkeypoints[..., 0] *= imgsz[1]\nkeypoints[..., 1] *= imgsz[0]\nfor i in range(batch_size):\nif fg_mask[i].sum():\nidx = target_gt_idx[i][fg_mask[i]]\ngt_kpt = keypoints[batch_idx.view(-1) == i][idx]  # (n, 51)\ngt_kpt[..., 0] /= stride_tensor[fg_mask[i]]\ngt_kpt[..., 1] /= stride_tensor[fg_mask[i]]\narea = xyxy2xywh(target_bboxes[i][fg_mask[i]])[:, 2:].prod(1, keepdim=True)\npred_kpt = pred_kpts[i][fg_mask[i]]\nkpt_mask = gt_kpt[..., 2] != 0\nloss[1] += self.keypoint_loss(pred_kpt, gt_kpt, kpt_mask, area)  # pose loss\n# kpt_score loss\nif pred_kpt.shape[-1] == 3:\nloss[2] += self.bce_pose(pred_kpt[..., 2], kpt_mask.float())  # keypoint obj loss\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.pose / batch_size  # pose gain\nloss[2] *= self.hyp.kobj / batch_size  # kobj gain\nloss[3] *= self.hyp.cls  # cls gain\nloss[4] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\ndef kpts_decode(self, anchor_points, pred_kpts):\n\"\"\"Decodes predicted keypoints to image coordinates.\"\"\"\ny = pred_kpts.clone()\ny[..., :2] *= 2.0\ny[..., 0] += anchor_points[:, [0]] - 0.5\ny[..., 1] += anchor_points[:, [1]] - 0.5\nreturn y\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8PoseLoss.__call__","title":"<code>__call__(preds, batch)</code>","text":"<p>Calculate the total loss and detach it.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __call__(self, preds, batch):\n\"\"\"Calculate the total loss and detach it.\"\"\"\nloss = torch.zeros(5, device=self.device)  # box, cls, dfl, kpt_location, kpt_visibility\nfeats, pred_kpts = preds if isinstance(preds[0], list) else preds[1]\npred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n(self.reg_max * 4, self.nc), 1)\n# b, grids, ..\npred_scores = pred_scores.permute(0, 2, 1).contiguous()\npred_distri = pred_distri.permute(0, 2, 1).contiguous()\npred_kpts = pred_kpts.permute(0, 2, 1).contiguous()\ndtype = pred_scores.dtype\nimgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\nanchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n# targets\nbatch_size = pred_scores.shape[0]\nbatch_idx = batch['batch_idx'].view(-1, 1)\ntargets = torch.cat((batch_idx, batch['cls'].view(-1, 1), batch['bboxes']), 1)\ntargets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\ngt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\nmask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n# pboxes\npred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\npred_kpts = self.kpts_decode(anchor_points, pred_kpts.view(batch_size, -1, *self.kpt_shape))  # (b, h*w, 17, 3)\n_, target_bboxes, target_scores, fg_mask, target_gt_idx = self.assigner(\npred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\nanchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\ntarget_scores_sum = max(target_scores.sum(), 1)\n# cls loss\n# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\nloss[3] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n# bbox loss\nif fg_mask.sum():\ntarget_bboxes /= stride_tensor\nloss[0], loss[4] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\ntarget_scores_sum, fg_mask)\nkeypoints = batch['keypoints'].to(self.device).float().clone()\nkeypoints[..., 0] *= imgsz[1]\nkeypoints[..., 1] *= imgsz[0]\nfor i in range(batch_size):\nif fg_mask[i].sum():\nidx = target_gt_idx[i][fg_mask[i]]\ngt_kpt = keypoints[batch_idx.view(-1) == i][idx]  # (n, 51)\ngt_kpt[..., 0] /= stride_tensor[fg_mask[i]]\ngt_kpt[..., 1] /= stride_tensor[fg_mask[i]]\narea = xyxy2xywh(target_bboxes[i][fg_mask[i]])[:, 2:].prod(1, keepdim=True)\npred_kpt = pred_kpts[i][fg_mask[i]]\nkpt_mask = gt_kpt[..., 2] != 0\nloss[1] += self.keypoint_loss(pred_kpt, gt_kpt, kpt_mask, area)  # pose loss\n# kpt_score loss\nif pred_kpt.shape[-1] == 3:\nloss[2] += self.bce_pose(pred_kpt[..., 2], kpt_mask.float())  # keypoint obj loss\nloss[0] *= self.hyp.box  # box gain\nloss[1] *= self.hyp.pose / batch_size  # pose gain\nloss[2] *= self.hyp.kobj / batch_size  # kobj gain\nloss[3] *= self.hyp.cls  # cls gain\nloss[4] *= self.hyp.dfl  # dfl gain\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8PoseLoss.kpts_decode","title":"<code>kpts_decode(anchor_points, pred_kpts)</code>","text":"<p>Decodes predicted keypoints to image coordinates.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def kpts_decode(self, anchor_points, pred_kpts):\n\"\"\"Decodes predicted keypoints to image coordinates.\"\"\"\ny = pred_kpts.clone()\ny[..., :2] *= 2.0\ny[..., 0] += anchor_points[:, [0]] - 0.5\ny[..., 1] += anchor_points[:, [1]] - 0.5\nreturn y\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8ClassificationLoss","title":"<code>ultralytics.utils.loss.v8ClassificationLoss</code>","text":"Source code in <code>ultralytics/utils/loss.py</code> <pre><code>class v8ClassificationLoss:\ndef __call__(self, preds, batch):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nloss = torch.nn.functional.cross_entropy(preds, batch['cls'], reduction='sum') / 64\nloss_items = loss.detach()\nreturn loss, loss_items\n</code></pre>"},{"location":"reference/utils/loss/#ultralytics.utils.loss.v8ClassificationLoss.__call__","title":"<code>__call__(preds, batch)</code>","text":"<p>Compute the classification loss between predictions and true labels.</p> Source code in <code>ultralytics/utils/loss.py</code> <pre><code>def __call__(self, preds, batch):\n\"\"\"Compute the classification loss between predictions and true labels.\"\"\"\nloss = torch.nn.functional.cross_entropy(preds, batch['cls'], reduction='sum') / 64\nloss_items = loss.detach()\nreturn loss, loss_items\n</code></pre>"},{"location":"reference/utils/metrics/","title":"Reference for <code>ultralytics/utils/metrics.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/metrics.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix","title":"<code>ultralytics.utils.metrics.ConfusionMatrix</code>","text":"<p>A class for calculating and updating a confusion matrix for object detection and classification tasks.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>The type of task, either 'detect' or 'classify'.</p> <code>matrix</code> <code>array</code> <p>The confusion matrix, with dimensions depending on the task.</p> <code>nc</code> <code>int</code> <p>The number of classes.</p> <code>conf</code> <code>float</code> <p>The confidence threshold for detections.</p> <code>iou_thres</code> <code>float</code> <p>The Intersection over Union threshold.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class ConfusionMatrix:\n\"\"\"\n    A class for calculating and updating a confusion matrix for object detection and classification tasks.\n    Attributes:\n        task (str): The type of task, either 'detect' or 'classify'.\n        matrix (np.array): The confusion matrix, with dimensions depending on the task.\n        nc (int): The number of classes.\n        conf (float): The confidence threshold for detections.\n        iou_thres (float): The Intersection over Union threshold.\n    \"\"\"\ndef __init__(self, nc, conf=0.25, iou_thres=0.45, task='detect'):\n\"\"\"Initialize attributes for the YOLO model.\"\"\"\nself.task = task\nself.matrix = np.zeros((nc + 1, nc + 1)) if self.task == 'detect' else np.zeros((nc, nc))\nself.nc = nc  # number of classes\nself.conf = conf\nself.iou_thres = iou_thres\ndef process_cls_preds(self, preds, targets):\n\"\"\"\n        Update confusion matrix for classification task\n        Args:\n            preds (Array[N, min(nc,5)]): Predicted class labels.\n            targets (Array[N, 1]): Ground truth class labels.\n        \"\"\"\npreds, targets = torch.cat(preds)[:, 0], torch.cat(targets)\nfor p, t in zip(preds.cpu().numpy(), targets.cpu().numpy()):\nself.matrix[p][t] += 1\ndef process_batch(self, detections, labels):\n\"\"\"\n        Update confusion matrix for object detection task.\n        Args:\n            detections (Array[N, 6]): Detected bounding boxes and their associated information.\n                                      Each row should contain (x1, y1, x2, y2, conf, class).\n            labels (Array[M, 5]): Ground truth bounding boxes and their associated class labels.\n                                  Each row should contain (class, x1, y1, x2, y2).\n        \"\"\"\nif detections is None:\ngt_classes = labels.int()\nfor gc in gt_classes:\nself.matrix[self.nc, gc] += 1  # background FN\nreturn\ndetections = detections[detections[:, 4] &gt; self.conf]\ngt_classes = labels[:, 0].int()\ndetection_classes = detections[:, 5].int()\niou = box_iou(labels[:, 1:], detections[:, :4])\nx = torch.where(iou &gt; self.iou_thres)\nif x[0].shape[0]:\nmatches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\nif x[0].shape[0] &gt; 1:\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 1], return_index=True)[1]]\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 0], return_index=True)[1]]\nelse:\nmatches = np.zeros((0, 3))\nn = matches.shape[0] &gt; 0\nm0, m1, _ = matches.transpose().astype(int)\nfor i, gc in enumerate(gt_classes):\nj = m0 == i\nif n and sum(j) == 1:\nself.matrix[detection_classes[m1[j]], gc] += 1  # correct\nelse:\nself.matrix[self.nc, gc] += 1  # true background\nif n:\nfor i, dc in enumerate(detection_classes):\nif not any(m1 == i):\nself.matrix[dc, self.nc] += 1  # predicted background\ndef matrix(self):\n\"\"\"Returns the confusion matrix.\"\"\"\nreturn self.matrix\ndef tp_fp(self):\n\"\"\"Returns true positives and false positives.\"\"\"\ntp = self.matrix.diagonal()  # true positives\nfp = self.matrix.sum(1) - tp  # false positives\n# fn = self.matrix.sum(0) - tp  # false negatives (missed detections)\nreturn (tp[:-1], fp[:-1]) if self.task == 'detect' else (tp, fp)  # remove background class if task=detect\n@TryExcept('WARNING \u26a0\ufe0f ConfusionMatrix plot failure')\n@plt_settings()\ndef plot(self, normalize=True, save_dir='', names=(), on_plot=None):\n\"\"\"\n        Plot the confusion matrix using seaborn and save it to a file.\n        Args:\n            normalize (bool): Whether to normalize the confusion matrix.\n            save_dir (str): Directory where the plot will be saved.\n            names (tuple): Names of classes, used as labels on the plot.\n            on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        \"\"\"\nimport seaborn as sn\narray = self.matrix / ((self.matrix.sum(0).reshape(1, -1) + 1E-9) if normalize else 1)  # normalize columns\narray[array &lt; 0.005] = np.nan  # don't annotate (would appear as 0.00)\nfig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\nnc, nn = self.nc, len(names)  # number of classes, names\nsn.set(font_scale=1.0 if nc &lt; 50 else 0.8)  # for label size\nlabels = (0 &lt; nn &lt; 99) and (nn == nc)  # apply names to ticklabels\nticklabels = (list(names) + ['background']) if labels else 'auto'\nwith warnings.catch_warnings():\nwarnings.simplefilter('ignore')  # suppress empty matrix RuntimeWarning: All-NaN slice encountered\nsn.heatmap(array,\nax=ax,\nannot=nc &lt; 30,\nannot_kws={\n'size': 8},\ncmap='Blues',\nfmt='.2f' if normalize else '.0f',\nsquare=True,\nvmin=0.0,\nxticklabels=ticklabels,\nyticklabels=ticklabels).set_facecolor((1, 1, 1))\ntitle = 'Confusion Matrix' + ' Normalized' * normalize\nax.set_xlabel('True')\nax.set_ylabel('Predicted')\nax.set_title(title)\nplot_fname = Path(save_dir) / f'{title.lower().replace(\" \", \"_\")}.png'\nfig.savefig(plot_fname, dpi=250)\nplt.close(fig)\nif on_plot:\non_plot(plot_fname)\ndef print(self):\n\"\"\"\n        Print the confusion matrix to the console.\n        \"\"\"\nfor i in range(self.nc + 1):\nLOGGER.info(' '.join(map(str, self.matrix[i])))\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.__init__","title":"<code>__init__(nc, conf=0.25, iou_thres=0.45, task='detect')</code>","text":"<p>Initialize attributes for the YOLO model.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def __init__(self, nc, conf=0.25, iou_thres=0.45, task='detect'):\n\"\"\"Initialize attributes for the YOLO model.\"\"\"\nself.task = task\nself.matrix = np.zeros((nc + 1, nc + 1)) if self.task == 'detect' else np.zeros((nc, nc))\nself.nc = nc  # number of classes\nself.conf = conf\nself.iou_thres = iou_thres\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.matrix","title":"<code>matrix()</code>","text":"<p>Returns the confusion matrix.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def matrix(self):\n\"\"\"Returns the confusion matrix.\"\"\"\nreturn self.matrix\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.plot","title":"<code>plot(normalize=True, save_dir='', names=(), on_plot=None)</code>","text":"<p>Plot the confusion matrix using seaborn and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether to normalize the confusion matrix.</p> <code>True</code> <code>save_dir</code> <code>str</code> <p>Directory where the plot will be saved.</p> <code>''</code> <code>names</code> <code>tuple</code> <p>Names of classes, used as labels on the plot.</p> <code>()</code> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered.</p> <code>None</code> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>@TryExcept('WARNING \u26a0\ufe0f ConfusionMatrix plot failure')\n@plt_settings()\ndef plot(self, normalize=True, save_dir='', names=(), on_plot=None):\n\"\"\"\n    Plot the confusion matrix using seaborn and save it to a file.\n    Args:\n        normalize (bool): Whether to normalize the confusion matrix.\n        save_dir (str): Directory where the plot will be saved.\n        names (tuple): Names of classes, used as labels on the plot.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n    \"\"\"\nimport seaborn as sn\narray = self.matrix / ((self.matrix.sum(0).reshape(1, -1) + 1E-9) if normalize else 1)  # normalize columns\narray[array &lt; 0.005] = np.nan  # don't annotate (would appear as 0.00)\nfig, ax = plt.subplots(1, 1, figsize=(12, 9), tight_layout=True)\nnc, nn = self.nc, len(names)  # number of classes, names\nsn.set(font_scale=1.0 if nc &lt; 50 else 0.8)  # for label size\nlabels = (0 &lt; nn &lt; 99) and (nn == nc)  # apply names to ticklabels\nticklabels = (list(names) + ['background']) if labels else 'auto'\nwith warnings.catch_warnings():\nwarnings.simplefilter('ignore')  # suppress empty matrix RuntimeWarning: All-NaN slice encountered\nsn.heatmap(array,\nax=ax,\nannot=nc &lt; 30,\nannot_kws={\n'size': 8},\ncmap='Blues',\nfmt='.2f' if normalize else '.0f',\nsquare=True,\nvmin=0.0,\nxticklabels=ticklabels,\nyticklabels=ticklabels).set_facecolor((1, 1, 1))\ntitle = 'Confusion Matrix' + ' Normalized' * normalize\nax.set_xlabel('True')\nax.set_ylabel('Predicted')\nax.set_title(title)\nplot_fname = Path(save_dir) / f'{title.lower().replace(\" \", \"_\")}.png'\nfig.savefig(plot_fname, dpi=250)\nplt.close(fig)\nif on_plot:\non_plot(plot_fname)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.print","title":"<code>print()</code>","text":"<p>Print the confusion matrix to the console.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def print(self):\n\"\"\"\n    Print the confusion matrix to the console.\n    \"\"\"\nfor i in range(self.nc + 1):\nLOGGER.info(' '.join(map(str, self.matrix[i])))\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.process_batch","title":"<code>process_batch(detections, labels)</code>","text":"<p>Update confusion matrix for object detection task.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Array[N, 6]</code> <p>Detected bounding boxes and their associated information.                       Each row should contain (x1, y1, x2, y2, conf, class).</p> required <code>labels</code> <code>Array[M, 5]</code> <p>Ground truth bounding boxes and their associated class labels.                   Each row should contain (class, x1, y1, x2, y2).</p> required Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process_batch(self, detections, labels):\n\"\"\"\n    Update confusion matrix for object detection task.\n    Args:\n        detections (Array[N, 6]): Detected bounding boxes and their associated information.\n                                  Each row should contain (x1, y1, x2, y2, conf, class).\n        labels (Array[M, 5]): Ground truth bounding boxes and their associated class labels.\n                              Each row should contain (class, x1, y1, x2, y2).\n    \"\"\"\nif detections is None:\ngt_classes = labels.int()\nfor gc in gt_classes:\nself.matrix[self.nc, gc] += 1  # background FN\nreturn\ndetections = detections[detections[:, 4] &gt; self.conf]\ngt_classes = labels[:, 0].int()\ndetection_classes = detections[:, 5].int()\niou = box_iou(labels[:, 1:], detections[:, :4])\nx = torch.where(iou &gt; self.iou_thres)\nif x[0].shape[0]:\nmatches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\nif x[0].shape[0] &gt; 1:\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 1], return_index=True)[1]]\nmatches = matches[matches[:, 2].argsort()[::-1]]\nmatches = matches[np.unique(matches[:, 0], return_index=True)[1]]\nelse:\nmatches = np.zeros((0, 3))\nn = matches.shape[0] &gt; 0\nm0, m1, _ = matches.transpose().astype(int)\nfor i, gc in enumerate(gt_classes):\nj = m0 == i\nif n and sum(j) == 1:\nself.matrix[detection_classes[m1[j]], gc] += 1  # correct\nelse:\nself.matrix[self.nc, gc] += 1  # true background\nif n:\nfor i, dc in enumerate(detection_classes):\nif not any(m1 == i):\nself.matrix[dc, self.nc] += 1  # predicted background\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.process_cls_preds","title":"<code>process_cls_preds(preds, targets)</code>","text":"<p>Update confusion matrix for classification task</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Array[N, min(nc, 5)]</code> <p>Predicted class labels.</p> required <code>targets</code> <code>Array[N, 1]</code> <p>Ground truth class labels.</p> required Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process_cls_preds(self, preds, targets):\n\"\"\"\n    Update confusion matrix for classification task\n    Args:\n        preds (Array[N, min(nc,5)]): Predicted class labels.\n        targets (Array[N, 1]): Ground truth class labels.\n    \"\"\"\npreds, targets = torch.cat(preds)[:, 0], torch.cat(targets)\nfor p, t in zip(preds.cpu().numpy(), targets.cpu().numpy()):\nself.matrix[p][t] += 1\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ConfusionMatrix.tp_fp","title":"<code>tp_fp()</code>","text":"<p>Returns true positives and false positives.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def tp_fp(self):\n\"\"\"Returns true positives and false positives.\"\"\"\ntp = self.matrix.diagonal()  # true positives\nfp = self.matrix.sum(1) - tp  # false positives\n# fn = self.matrix.sum(0) - tp  # false negatives (missed detections)\nreturn (tp[:-1], fp[:-1]) if self.task == 'detect' else (tp, fp)  # remove background class if task=detect\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric","title":"<code>ultralytics.utils.metrics.Metric</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>Class for computing evaluation metrics for YOLOv8 model.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>list</code> <p>Precision for each class. Shape: (nc,).</p> <code>r</code> <code>list</code> <p>Recall for each class. Shape: (nc,).</p> <code>f1</code> <code>list</code> <p>F1 score for each class. Shape: (nc,).</p> <code>all_ap</code> <code>list</code> <p>AP scores for all classes and all IoU thresholds. Shape: (nc, 10).</p> <code>ap_class_index</code> <code>list</code> <p>Index of class for each AP score. Shape: (nc,).</p> <code>nc</code> <code>int</code> <p>Number of classes.</p> <p>Methods:</p> Name Description <code>ap50</code> <p>AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].</p> <code>ap</code> <p>AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].</p> <code>mp</code> <p>Mean precision of all classes. Returns: Float.</p> <code>mr</code> <p>Mean recall of all classes. Returns: Float.</p> <code>map50</code> <p>Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.</p> <code>map75</code> <p>Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.</p> <code>map</code> <p>Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.</p> <code>mean_results</code> <p>Mean of results, returns mp, mr, map50, map.</p> <code>class_result</code> <p>Class-aware result, returns p[i], r[i], ap50[i], ap[i].</p> <code>maps</code> <p>mAP of each class. Returns: Array of mAP scores, shape: (nc,).</p> <code>fitness</code> <p>Model fitness as a weighted combination of metrics. Returns: Float.</p> <code>update</code> <p>Update metric attributes with new evaluation results.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class Metric(SimpleClass):\n\"\"\"\n        Class for computing evaluation metrics for YOLOv8 model.\n        Attributes:\n            p (list): Precision for each class. Shape: (nc,).\n            r (list): Recall for each class. Shape: (nc,).\n            f1 (list): F1 score for each class. Shape: (nc,).\n            all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n            ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n            nc (int): Number of classes.\n        Methods:\n            ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n            ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n            mp(): Mean precision of all classes. Returns: Float.\n            mr(): Mean recall of all classes. Returns: Float.\n            map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n            map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n            map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n            mean_results(): Mean of results, returns mp, mr, map50, map.\n            class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n            maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n            fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n            update(results): Update metric attributes with new evaluation results.\n        \"\"\"\ndef __init__(self) -&gt; None:\nself.p = []  # (nc, )\nself.r = []  # (nc, )\nself.f1 = []  # (nc, )\nself.all_ap = []  # (nc, 10)\nself.ap_class_index = []  # (nc, )\nself.nc = 0\n@property\ndef ap50(self):\n\"\"\"\n        Returns the Average Precision (AP) at an IoU threshold of 0.5 for all classes.\n        Returns:\n            (np.ndarray, list): Array of shape (nc,) with AP50 values per class, or an empty list if not available.\n        \"\"\"\nreturn self.all_ap[:, 0] if len(self.all_ap) else []\n@property\ndef ap(self):\n\"\"\"\n        Returns the Average Precision (AP) at an IoU threshold of 0.5-0.95 for all classes.\n        Returns:\n            (np.ndarray, list): Array of shape (nc,) with AP50-95 values per class, or an empty list if not available.\n        \"\"\"\nreturn self.all_ap.mean(1) if len(self.all_ap) else []\n@property\ndef mp(self):\n\"\"\"\n        Returns the Mean Precision of all classes.\n        Returns:\n            (float): The mean precision of all classes.\n        \"\"\"\nreturn self.p.mean() if len(self.p) else 0.0\n@property\ndef mr(self):\n\"\"\"\n        Returns the Mean Recall of all classes.\n        Returns:\n            (float): The mean recall of all classes.\n        \"\"\"\nreturn self.r.mean() if len(self.r) else 0.0\n@property\ndef map50(self):\n\"\"\"\n        Returns the mean Average Precision (mAP) at an IoU threshold of 0.5.\n        Returns:\n            (float): The mAP50 at an IoU threshold of 0.5.\n        \"\"\"\nreturn self.all_ap[:, 0].mean() if len(self.all_ap) else 0.0\n@property\ndef map75(self):\n\"\"\"\n        Returns the mean Average Precision (mAP) at an IoU threshold of 0.75.\n        Returns:\n            (float): The mAP50 at an IoU threshold of 0.75.\n        \"\"\"\nreturn self.all_ap[:, 5].mean() if len(self.all_ap) else 0.0\n@property\ndef map(self):\n\"\"\"\n        Returns the mean Average Precision (mAP) over IoU thresholds of 0.5 - 0.95 in steps of 0.05.\n        Returns:\n            (float): The mAP over IoU thresholds of 0.5 - 0.95 in steps of 0.05.\n        \"\"\"\nreturn self.all_ap.mean() if len(self.all_ap) else 0.0\ndef mean_results(self):\n\"\"\"Mean of results, return mp, mr, map50, map.\"\"\"\nreturn [self.mp, self.mr, self.map50, self.map]\ndef class_result(self, i):\n\"\"\"class-aware result, return p[i], r[i], ap50[i], ap[i].\"\"\"\nreturn self.p[i], self.r[i], self.ap50[i], self.ap[i]\n@property\ndef maps(self):\n\"\"\"mAP of each class.\"\"\"\nmaps = np.zeros(self.nc) + self.map\nfor i, c in enumerate(self.ap_class_index):\nmaps[c] = self.ap[i]\nreturn maps\ndef fitness(self):\n\"\"\"Model fitness as a weighted combination of metrics.\"\"\"\nw = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\nreturn (np.array(self.mean_results()) * w).sum()\ndef update(self, results):\n\"\"\"\n        Args:\n            results (tuple): A tuple of (p, r, ap, f1, ap_class)\n        \"\"\"\nself.p, self.r, self.f1, self.all_ap, self.ap_class_index = results\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.ap","title":"<code>ap</code>  <code>property</code>","text":"<p>Returns the Average Precision (AP) at an IoU threshold of 0.5-0.95 for all classes.</p> <p>Returns:</p> Type Description <code>(ndarray, list)</code> <p>Array of shape (nc,) with AP50-95 values per class, or an empty list if not available.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.ap50","title":"<code>ap50</code>  <code>property</code>","text":"<p>Returns the Average Precision (AP) at an IoU threshold of 0.5 for all classes.</p> <p>Returns:</p> Type Description <code>(ndarray, list)</code> <p>Array of shape (nc,) with AP50 values per class, or an empty list if not available.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.map","title":"<code>map</code>  <code>property</code>","text":"<p>Returns the mean Average Precision (mAP) over IoU thresholds of 0.5 - 0.95 in steps of 0.05.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mAP over IoU thresholds of 0.5 - 0.95 in steps of 0.05.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.map50","title":"<code>map50</code>  <code>property</code>","text":"<p>Returns the mean Average Precision (mAP) at an IoU threshold of 0.5.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mAP50 at an IoU threshold of 0.5.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.map75","title":"<code>map75</code>  <code>property</code>","text":"<p>Returns the mean Average Precision (mAP) at an IoU threshold of 0.75.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mAP50 at an IoU threshold of 0.75.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.maps","title":"<code>maps</code>  <code>property</code>","text":"<p>mAP of each class.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.mp","title":"<code>mp</code>  <code>property</code>","text":"<p>Returns the Mean Precision of all classes.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean precision of all classes.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.mr","title":"<code>mr</code>  <code>property</code>","text":"<p>Returns the Mean Recall of all classes.</p> <p>Returns:</p> Type Description <code>float</code> <p>The mean recall of all classes.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.class_result","title":"<code>class_result(i)</code>","text":"<p>class-aware result, return p[i], r[i], ap50[i], ap[i].</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def class_result(self, i):\n\"\"\"class-aware result, return p[i], r[i], ap50[i], ap[i].\"\"\"\nreturn self.p[i], self.r[i], self.ap50[i], self.ap[i]\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.fitness","title":"<code>fitness()</code>","text":"<p>Model fitness as a weighted combination of metrics.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def fitness(self):\n\"\"\"Model fitness as a weighted combination of metrics.\"\"\"\nw = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\nreturn (np.array(self.mean_results()) * w).sum()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.mean_results","title":"<code>mean_results()</code>","text":"<p>Mean of results, return mp, mr, map50, map.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def mean_results(self):\n\"\"\"Mean of results, return mp, mr, map50, map.\"\"\"\nreturn [self.mp, self.mr, self.map50, self.map]\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.Metric.update","title":"<code>update(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>tuple</code> <p>A tuple of (p, r, ap, f1, ap_class)</p> required Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def update(self, results):\n\"\"\"\n    Args:\n        results (tuple): A tuple of (p, r, ap, f1, ap_class)\n    \"\"\"\nself.p, self.r, self.f1, self.all_ap, self.ap_class_index = results\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics","title":"<code>ultralytics.utils.metrics.DetMetrics</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>This class is a utility class for computing detection metrics such as precision, recall, and mean average precision (mAP) of an object detection model.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>Path</code> <p>A path to the directory where the output plots will be saved. Defaults to current directory.</p> <code>Path('.')</code> <code>plot</code> <code>bool</code> <p>A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.</p> <code>False</code> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered. Defaults to None.</p> <code>None</code> <code>names</code> <code>tuple of str</code> <p>A tuple of strings that represents the names of the classes. Defaults to an empty tuple.</p> <code>()</code> <p>Attributes:</p> Name Type Description <code>save_dir</code> <code>Path</code> <p>A path to the directory where the output plots will be saved.</p> <code>plot</code> <code>bool</code> <p>A flag that indicates whether to plot the precision-recall curves for each class.</p> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered.</p> <code>names</code> <code>tuple of str</code> <p>A tuple of strings that represents the names of the classes.</p> <code>box</code> <code>Metric</code> <p>An instance of the Metric class for storing the results of the detection metrics.</p> <code>speed</code> <code>dict</code> <p>A dictionary for storing the execution time of different parts of the detection process.</p> <p>Methods:</p> Name Description <code>process</code> <p>Updates the metric results with the latest batch of predictions.</p> <code>keys</code> <p>Returns a list of keys for accessing the computed detection metrics.</p> <code>mean_results</code> <p>Returns a list of mean values for the computed detection metrics.</p> <code>class_result</code> <p>Returns a list of values for the computed detection metrics for a specific class.</p> <code>maps</code> <p>Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.</p> <code>fitness</code> <p>Computes the fitness score based on the computed detection metrics.</p> <code>ap_class_index</code> <p>Returns a list of class indices sorted by their average precision (AP) values.</p> <code>results_dict</code> <p>Returns a dictionary that maps detection metric keys to their computed values.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class DetMetrics(SimpleClass):\n\"\"\"\n    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n    (mAP) of an object detection model.\n    Args:\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (tuple of str): A tuple of strings that represents the names of the classes.\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n    Methods:\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n        keys: Returns a list of keys for accessing the computed detection metrics.\n        mean_results: Returns a list of mean values for the computed detection metrics.\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n        fitness: Computes the fitness score based on the computed detection metrics.\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n    \"\"\"\ndef __init__(self, save_dir=Path('.'), plot=False, on_plot=None, names=()) -&gt; None:\nself.save_dir = save_dir\nself.plot = plot\nself.on_plot = on_plot\nself.names = names\nself.box = Metric()\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\ndef process(self, tp, conf, pred_cls, target_cls):\n\"\"\"Process predicted results for object detection and update metrics.\"\"\"\nresults = ap_per_class(tp,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\nsave_dir=self.save_dir,\nnames=self.names,\non_plot=self.on_plot)[2:]\nself.box.nc = len(self.names)\nself.box.update(results)\n@property\ndef keys(self):\n\"\"\"Returns a list of keys for accessing specific metrics.\"\"\"\nreturn ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\ndef mean_results(self):\n\"\"\"Calculate mean of detected objects &amp; return precision, recall, mAP50, and mAP50-95.\"\"\"\nreturn self.box.mean_results()\ndef class_result(self, i):\n\"\"\"Return the result of evaluating the performance of an object detection model on a specific class.\"\"\"\nreturn self.box.class_result(i)\n@property\ndef maps(self):\n\"\"\"Returns mean Average Precision (mAP) scores per class.\"\"\"\nreturn self.box.maps\n@property\ndef fitness(self):\n\"\"\"Returns the fitness of box object.\"\"\"\nreturn self.box.fitness()\n@property\ndef ap_class_index(self):\n\"\"\"Returns the average precision index per class.\"\"\"\nreturn self.box.ap_class_index\n@property\ndef results_dict(self):\n\"\"\"Returns dictionary of computed performance metrics and statistics.\"\"\"\nreturn dict(zip(self.keys + ['fitness'], self.mean_results() + [self.fitness]))\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.ap_class_index","title":"<code>ap_class_index</code>  <code>property</code>","text":"<p>Returns the average precision index per class.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.fitness","title":"<code>fitness</code>  <code>property</code>","text":"<p>Returns the fitness of box object.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Returns a list of keys for accessing specific metrics.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.maps","title":"<code>maps</code>  <code>property</code>","text":"<p>Returns mean Average Precision (mAP) scores per class.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.results_dict","title":"<code>results_dict</code>  <code>property</code>","text":"<p>Returns dictionary of computed performance metrics and statistics.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.class_result","title":"<code>class_result(i)</code>","text":"<p>Return the result of evaluating the performance of an object detection model on a specific class.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def class_result(self, i):\n\"\"\"Return the result of evaluating the performance of an object detection model on a specific class.\"\"\"\nreturn self.box.class_result(i)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.mean_results","title":"<code>mean_results()</code>","text":"<p>Calculate mean of detected objects &amp; return precision, recall, mAP50, and mAP50-95.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def mean_results(self):\n\"\"\"Calculate mean of detected objects &amp; return precision, recall, mAP50, and mAP50-95.\"\"\"\nreturn self.box.mean_results()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.DetMetrics.process","title":"<code>process(tp, conf, pred_cls, target_cls)</code>","text":"<p>Process predicted results for object detection and update metrics.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process(self, tp, conf, pred_cls, target_cls):\n\"\"\"Process predicted results for object detection and update metrics.\"\"\"\nresults = ap_per_class(tp,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\nsave_dir=self.save_dir,\nnames=self.names,\non_plot=self.on_plot)[2:]\nself.box.nc = len(self.names)\nself.box.update(results)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics","title":"<code>ultralytics.utils.metrics.SegmentMetrics</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>Calculates and aggregates detection and segmentation metrics over a given set of classes.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>Path</code> <p>Path to the directory where the output plots should be saved. Default is the current directory.</p> <code>Path('.')</code> <code>plot</code> <code>bool</code> <p>Whether to save the detection and segmentation plots. Default is False.</p> <code>False</code> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered. Defaults to None.</p> <code>None</code> <code>names</code> <code>list</code> <p>List of class names. Default is an empty list.</p> <code>()</code> <p>Attributes:</p> Name Type Description <code>save_dir</code> <code>Path</code> <p>Path to the directory where the output plots should be saved.</p> <code>plot</code> <code>bool</code> <p>Whether to save the detection and segmentation plots.</p> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered.</p> <code>names</code> <code>list</code> <p>List of class names.</p> <code>box</code> <code>Metric</code> <p>An instance of the Metric class to calculate box detection metrics.</p> <code>seg</code> <code>Metric</code> <p>An instance of the Metric class to calculate mask segmentation metrics.</p> <code>speed</code> <code>dict</code> <p>Dictionary to store the time taken in different phases of inference.</p> <p>Methods:</p> Name Description <code>process</code> <p>Processes metrics over the given set of predictions.</p> <code>mean_results</code> <p>Returns the mean of the detection and segmentation metrics over all the classes.</p> <code>class_result</code> <p>Returns the detection and segmentation metrics of class <code>i</code>.</p> <code>maps</code> <p>Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.</p> <code>fitness</code> <p>Returns the fitness scores, which are a single weighted combination of metrics.</p> <code>ap_class_index</code> <p>Returns the list of indices of classes used to compute Average Precision (AP).</p> <code>results_dict</code> <p>Returns the dictionary containing all the detection and segmentation metrics and fitness score.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class SegmentMetrics(SimpleClass):\n\"\"\"\n    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    \"\"\"\ndef __init__(self, save_dir=Path('.'), plot=False, on_plot=None, names=()) -&gt; None:\nself.save_dir = save_dir\nself.plot = plot\nself.on_plot = on_plot\nself.names = names\nself.box = Metric()\nself.seg = Metric()\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\ndef process(self, tp_b, tp_m, conf, pred_cls, target_cls):\n\"\"\"\n        Processes the detection and segmentation metrics over the given set of predictions.\n        Args:\n            tp_b (list): List of True Positive boxes.\n            tp_m (list): List of True Positive masks.\n            conf (list): List of confidence scores.\n            pred_cls (list): List of predicted classes.\n            target_cls (list): List of target classes.\n        \"\"\"\nresults_mask = ap_per_class(tp_m,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Mask')[2:]\nself.seg.nc = len(self.names)\nself.seg.update(results_mask)\nresults_box = ap_per_class(tp_b,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Box')[2:]\nself.box.nc = len(self.names)\nself.box.update(results_box)\n@property\ndef keys(self):\n\"\"\"Returns a list of keys for accessing metrics.\"\"\"\nreturn [\n'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)',\n'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)', 'metrics/mAP50-95(M)']\ndef mean_results(self):\n\"\"\"Return the mean metrics for bounding box and segmentation results.\"\"\"\nreturn self.box.mean_results() + self.seg.mean_results()\ndef class_result(self, i):\n\"\"\"Returns classification results for a specified class index.\"\"\"\nreturn self.box.class_result(i) + self.seg.class_result(i)\n@property\ndef maps(self):\n\"\"\"Returns mAP scores for object detection and semantic segmentation models.\"\"\"\nreturn self.box.maps + self.seg.maps\n@property\ndef fitness(self):\n\"\"\"Get the fitness score for both segmentation and bounding box models.\"\"\"\nreturn self.seg.fitness() + self.box.fitness()\n@property\ndef ap_class_index(self):\n\"\"\"Boxes and masks have the same ap_class_index.\"\"\"\nreturn self.box.ap_class_index\n@property\ndef results_dict(self):\n\"\"\"Returns results of object detection model for evaluation.\"\"\"\nreturn dict(zip(self.keys + ['fitness'], self.mean_results() + [self.fitness]))\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.ap_class_index","title":"<code>ap_class_index</code>  <code>property</code>","text":"<p>Boxes and masks have the same ap_class_index.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.fitness","title":"<code>fitness</code>  <code>property</code>","text":"<p>Get the fitness score for both segmentation and bounding box models.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Returns a list of keys for accessing metrics.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.maps","title":"<code>maps</code>  <code>property</code>","text":"<p>Returns mAP scores for object detection and semantic segmentation models.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.results_dict","title":"<code>results_dict</code>  <code>property</code>","text":"<p>Returns results of object detection model for evaluation.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.class_result","title":"<code>class_result(i)</code>","text":"<p>Returns classification results for a specified class index.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def class_result(self, i):\n\"\"\"Returns classification results for a specified class index.\"\"\"\nreturn self.box.class_result(i) + self.seg.class_result(i)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.mean_results","title":"<code>mean_results()</code>","text":"<p>Return the mean metrics for bounding box and segmentation results.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def mean_results(self):\n\"\"\"Return the mean metrics for bounding box and segmentation results.\"\"\"\nreturn self.box.mean_results() + self.seg.mean_results()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.SegmentMetrics.process","title":"<code>process(tp_b, tp_m, conf, pred_cls, target_cls)</code>","text":"<p>Processes the detection and segmentation metrics over the given set of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>tp_b</code> <code>list</code> <p>List of True Positive boxes.</p> required <code>tp_m</code> <code>list</code> <p>List of True Positive masks.</p> required <code>conf</code> <code>list</code> <p>List of confidence scores.</p> required <code>pred_cls</code> <code>list</code> <p>List of predicted classes.</p> required <code>target_cls</code> <code>list</code> <p>List of target classes.</p> required Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process(self, tp_b, tp_m, conf, pred_cls, target_cls):\n\"\"\"\n    Processes the detection and segmentation metrics over the given set of predictions.\n    Args:\n        tp_b (list): List of True Positive boxes.\n        tp_m (list): List of True Positive masks.\n        conf (list): List of confidence scores.\n        pred_cls (list): List of predicted classes.\n        target_cls (list): List of target classes.\n    \"\"\"\nresults_mask = ap_per_class(tp_m,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Mask')[2:]\nself.seg.nc = len(self.names)\nself.seg.update(results_mask)\nresults_box = ap_per_class(tp_b,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Box')[2:]\nself.box.nc = len(self.names)\nself.box.update(results_box)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics","title":"<code>ultralytics.utils.metrics.PoseMetrics</code>","text":"<p>             Bases: <code>SegmentMetrics</code></p> <p>Calculates and aggregates detection and pose metrics over a given set of classes.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>Path</code> <p>Path to the directory where the output plots should be saved. Default is the current directory.</p> <code>Path('.')</code> <code>plot</code> <code>bool</code> <p>Whether to save the detection and segmentation plots. Default is False.</p> <code>False</code> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered. Defaults to None.</p> <code>None</code> <code>names</code> <code>list</code> <p>List of class names. Default is an empty list.</p> <code>()</code> <p>Attributes:</p> Name Type Description <code>save_dir</code> <code>Path</code> <p>Path to the directory where the output plots should be saved.</p> <code>plot</code> <code>bool</code> <p>Whether to save the detection and segmentation plots.</p> <code>on_plot</code> <code>func</code> <p>An optional callback to pass plots path and data when they are rendered.</p> <code>names</code> <code>list</code> <p>List of class names.</p> <code>box</code> <code>Metric</code> <p>An instance of the Metric class to calculate box detection metrics.</p> <code>pose</code> <code>Metric</code> <p>An instance of the Metric class to calculate mask segmentation metrics.</p> <code>speed</code> <code>dict</code> <p>Dictionary to store the time taken in different phases of inference.</p> <p>Methods:</p> Name Description <code>process</code> <p>Processes metrics over the given set of predictions.</p> <code>mean_results</code> <p>Returns the mean of the detection and segmentation metrics over all the classes.</p> <code>class_result</code> <p>Returns the detection and segmentation metrics of class <code>i</code>.</p> <code>maps</code> <p>Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.</p> <code>fitness</code> <p>Returns the fitness scores, which are a single weighted combination of metrics.</p> <code>ap_class_index</code> <p>Returns the list of indices of classes used to compute Average Precision (AP).</p> <code>results_dict</code> <p>Returns the dictionary containing all the detection and segmentation metrics and fitness score.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class PoseMetrics(SegmentMetrics):\n\"\"\"\n    Calculates and aggregates detection and pose metrics over a given set of classes.\n    Args:\n        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (list): List of class names. Default is an empty list.\n    Attributes:\n        save_dir (Path): Path to the directory where the output plots should be saved.\n        plot (bool): Whether to save the detection and segmentation plots.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (list): List of class names.\n        box (Metric): An instance of the Metric class to calculate box detection metrics.\n        pose (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n        speed (dict): Dictionary to store the time taken in different phases of inference.\n    Methods:\n        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n        class_result(i): Returns the detection and segmentation metrics of class `i`.\n        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score.\n    \"\"\"\ndef __init__(self, save_dir=Path('.'), plot=False, on_plot=None, names=()) -&gt; None:\nsuper().__init__(save_dir, plot, names)\nself.save_dir = save_dir\nself.plot = plot\nself.on_plot = on_plot\nself.names = names\nself.box = Metric()\nself.pose = Metric()\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\ndef process(self, tp_b, tp_p, conf, pred_cls, target_cls):\n\"\"\"\n        Processes the detection and pose metrics over the given set of predictions.\n        Args:\n            tp_b (list): List of True Positive boxes.\n            tp_p (list): List of True Positive keypoints.\n            conf (list): List of confidence scores.\n            pred_cls (list): List of predicted classes.\n            target_cls (list): List of target classes.\n        \"\"\"\nresults_pose = ap_per_class(tp_p,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Pose')[2:]\nself.pose.nc = len(self.names)\nself.pose.update(results_pose)\nresults_box = ap_per_class(tp_b,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Box')[2:]\nself.box.nc = len(self.names)\nself.box.update(results_box)\n@property\ndef keys(self):\n\"\"\"Returns list of evaluation metric keys.\"\"\"\nreturn [\n'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)',\n'metrics/precision(P)', 'metrics/recall(P)', 'metrics/mAP50(P)', 'metrics/mAP50-95(P)']\ndef mean_results(self):\n\"\"\"Return the mean results of box and pose.\"\"\"\nreturn self.box.mean_results() + self.pose.mean_results()\ndef class_result(self, i):\n\"\"\"Return the class-wise detection results for a specific class i.\"\"\"\nreturn self.box.class_result(i) + self.pose.class_result(i)\n@property\ndef maps(self):\n\"\"\"Returns the mean average precision (mAP) per class for both box and pose detections.\"\"\"\nreturn self.box.maps + self.pose.maps\n@property\ndef fitness(self):\n\"\"\"Computes classification metrics and speed using the `targets` and `pred` inputs.\"\"\"\nreturn self.pose.fitness() + self.box.fitness()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.fitness","title":"<code>fitness</code>  <code>property</code>","text":"<p>Computes classification metrics and speed using the <code>targets</code> and <code>pred</code> inputs.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Returns list of evaluation metric keys.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.maps","title":"<code>maps</code>  <code>property</code>","text":"<p>Returns the mean average precision (mAP) per class for both box and pose detections.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.class_result","title":"<code>class_result(i)</code>","text":"<p>Return the class-wise detection results for a specific class i.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def class_result(self, i):\n\"\"\"Return the class-wise detection results for a specific class i.\"\"\"\nreturn self.box.class_result(i) + self.pose.class_result(i)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.mean_results","title":"<code>mean_results()</code>","text":"<p>Return the mean results of box and pose.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def mean_results(self):\n\"\"\"Return the mean results of box and pose.\"\"\"\nreturn self.box.mean_results() + self.pose.mean_results()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.PoseMetrics.process","title":"<code>process(tp_b, tp_p, conf, pred_cls, target_cls)</code>","text":"<p>Processes the detection and pose metrics over the given set of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>tp_b</code> <code>list</code> <p>List of True Positive boxes.</p> required <code>tp_p</code> <code>list</code> <p>List of True Positive keypoints.</p> required <code>conf</code> <code>list</code> <p>List of confidence scores.</p> required <code>pred_cls</code> <code>list</code> <p>List of predicted classes.</p> required <code>target_cls</code> <code>list</code> <p>List of target classes.</p> required Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process(self, tp_b, tp_p, conf, pred_cls, target_cls):\n\"\"\"\n    Processes the detection and pose metrics over the given set of predictions.\n    Args:\n        tp_b (list): List of True Positive boxes.\n        tp_p (list): List of True Positive keypoints.\n        conf (list): List of confidence scores.\n        pred_cls (list): List of predicted classes.\n        target_cls (list): List of target classes.\n    \"\"\"\nresults_pose = ap_per_class(tp_p,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Pose')[2:]\nself.pose.nc = len(self.names)\nself.pose.update(results_pose)\nresults_box = ap_per_class(tp_b,\nconf,\npred_cls,\ntarget_cls,\nplot=self.plot,\non_plot=self.on_plot,\nsave_dir=self.save_dir,\nnames=self.names,\nprefix='Box')[2:]\nself.box.nc = len(self.names)\nself.box.update(results_box)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ClassifyMetrics","title":"<code>ultralytics.utils.metrics.ClassifyMetrics</code>","text":"<p>             Bases: <code>SimpleClass</code></p> <p>Class for computing classification metrics including top-1 and top-5 accuracy.</p> <p>Attributes:</p> Name Type Description <code>top1</code> <code>float</code> <p>The top-1 accuracy.</p> <code>top5</code> <code>float</code> <p>The top-5 accuracy.</p> <code>speed</code> <code>Dict[str, float]</code> <p>A dictionary containing the time taken for each step in the pipeline.</p> Properties <p>fitness (float): The fitness of the model, which is equal to top-5 accuracy. results_dict (Dict[str, Union[float, str]]): A dictionary containing the classification metrics and fitness. keys (List[str]): A list of keys for the results_dict.</p> <p>Methods:</p> Name Description <code>process</code> <p>Processes the targets and predictions to compute classification metrics.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>class ClassifyMetrics(SimpleClass):\n\"\"\"\n    Class for computing classification metrics including top-1 and top-5 accuracy.\n    Attributes:\n        top1 (float): The top-1 accuracy.\n        top5 (float): The top-5 accuracy.\n        speed (Dict[str, float]): A dictionary containing the time taken for each step in the pipeline.\n    Properties:\n        fitness (float): The fitness of the model, which is equal to top-5 accuracy.\n        results_dict (Dict[str, Union[float, str]]): A dictionary containing the classification metrics and fitness.\n        keys (List[str]): A list of keys for the results_dict.\n    Methods:\n        process(targets, pred): Processes the targets and predictions to compute classification metrics.\n    \"\"\"\ndef __init__(self) -&gt; None:\nself.top1 = 0\nself.top5 = 0\nself.speed = {'preprocess': 0.0, 'inference': 0.0, 'loss': 0.0, 'postprocess': 0.0}\ndef process(self, targets, pred):\n\"\"\"Target classes and predicted classes.\"\"\"\npred, targets = torch.cat(pred), torch.cat(targets)\ncorrect = (targets[:, None] == pred).float()\nacc = torch.stack((correct[:, 0], correct.max(1).values), dim=1)  # (top1, top5) accuracy\nself.top1, self.top5 = acc.mean(0).tolist()\n@property\ndef fitness(self):\n\"\"\"Returns mean of top-1 and top-5 accuracies as fitness score.\"\"\"\nreturn (self.top1 + self.top5) / 2\n@property\ndef results_dict(self):\n\"\"\"Returns a dictionary with model's performance metrics and fitness score.\"\"\"\nreturn dict(zip(self.keys + ['fitness'], [self.top1, self.top5, self.fitness]))\n@property\ndef keys(self):\n\"\"\"Returns a list of keys for the results_dict property.\"\"\"\nreturn ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ClassifyMetrics.fitness","title":"<code>fitness</code>  <code>property</code>","text":"<p>Returns mean of top-1 and top-5 accuracies as fitness score.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ClassifyMetrics.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Returns a list of keys for the results_dict property.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ClassifyMetrics.results_dict","title":"<code>results_dict</code>  <code>property</code>","text":"<p>Returns a dictionary with model's performance metrics and fitness score.</p>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ClassifyMetrics.process","title":"<code>process(targets, pred)</code>","text":"<p>Target classes and predicted classes.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def process(self, targets, pred):\n\"\"\"Target classes and predicted classes.\"\"\"\npred, targets = torch.cat(pred), torch.cat(targets)\ncorrect = (targets[:, None] == pred).float()\nacc = torch.stack((correct[:, 0], correct.max(1).values), dim=1)  # (top1, top5) accuracy\nself.top1, self.top5 = acc.mean(0).tolist()\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.bbox_ioa","title":"<code>ultralytics.utils.metrics.bbox_ioa(box1, box2, iou=False, eps=1e-07)</code>","text":"<p>Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.</p> <p>Parameters:</p> Name Type Description Default <code>box1</code> <code>array</code> <p>A numpy array of shape (n, 4) representing n bounding boxes.</p> required <code>box2</code> <code>array</code> <p>A numpy array of shape (m, 4) representing m bounding boxes.</p> required <code>iou</code> <code>bool</code> <p>Calculate the standard iou if True else return inter_area/box2_area.</p> <code>False</code> <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-7.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>array</code> <p>A numpy array of shape (n, m) representing the intersection over box2 area.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def bbox_ioa(box1, box2, iou=False, eps=1e-7):\n\"\"\"\n    Calculate the intersection over box2 area given box1 and box2. Boxes are in x1y1x2y2 format.\n    Args:\n        box1 (np.array): A numpy array of shape (n, 4) representing n bounding boxes.\n        box2 (np.array): A numpy array of shape (m, 4) representing m bounding boxes.\n        iou (bool): Calculate the standard iou if True else return inter_area/box2_area.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n    Returns:\n        (np.array): A numpy array of shape (n, m) representing the intersection over box2 area.\n    \"\"\"\n# Get the coordinates of bounding boxes\nb1_x1, b1_y1, b1_x2, b1_y2 = box1.T\nb2_x1, b2_y1, b2_x2, b2_y2 = box2.T\n# Intersection area\ninter_area = (np.minimum(b1_x2[:, None], b2_x2) - np.maximum(b1_x1[:, None], b2_x1)).clip(0) * \\\n                 (np.minimum(b1_y2[:, None], b2_y2) - np.maximum(b1_y1[:, None], b2_y1)).clip(0)\n# box2 area\narea = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\nif iou:\nbox1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\narea = area + box1_area[:, None] - inter_area\n# Intersection over box2 area\nreturn inter_area / (area + eps)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.box_iou","title":"<code>ultralytics.utils.metrics.box_iou(box1, box2, eps=1e-07)</code>","text":"<p>Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format. Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py</p> <p>Parameters:</p> Name Type Description Default <code>box1</code> <code>Tensor</code> <p>A tensor of shape (N, 4) representing N bounding boxes.</p> required <code>box2</code> <code>Tensor</code> <p>A tensor of shape (M, 4) representing M bounding boxes.</p> required <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-7.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>An NxM tensor containing the pairwise IoU values for every element in box1 and box2.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def box_iou(box1, box2, eps=1e-7):\n\"\"\"\n    Calculate intersection-over-union (IoU) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    Args:\n        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n    Returns:\n        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n    \"\"\"\n# inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n(a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\ninter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n# IoU = inter / (area1 + area2 - inter)\nreturn inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.bbox_iou","title":"<code>ultralytics.utils.metrics.bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-07)</code>","text":"<p>Calculate Intersection over Union (IoU) of box1(1, 4) to box2(n, 4).</p> <p>Parameters:</p> Name Type Description Default <code>box1</code> <code>Tensor</code> <p>A tensor representing a single bounding box with shape (1, 4).</p> required <code>box2</code> <code>Tensor</code> <p>A tensor representing n bounding boxes with shape (n, 4).</p> required <code>xywh</code> <code>bool</code> <p>If True, input boxes are in (x, y, w, h) format. If False, input boxes are in                    (x1, y1, x2, y2) format. Defaults to True.</p> <code>True</code> <code>GIoU</code> <code>bool</code> <p>If True, calculate Generalized IoU. Defaults to False.</p> <code>False</code> <code>DIoU</code> <code>bool</code> <p>If True, calculate Distance IoU. Defaults to False.</p> <code>False</code> <code>CIoU</code> <code>bool</code> <p>If True, calculate Complete IoU. Defaults to False.</p> <code>False</code> <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-7.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>IoU, GIoU, DIoU, or CIoU values depending on the specified flags.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n\"\"\"\n    Calculate Intersection over Union (IoU) of box1(1, 4) to box2(n, 4).\n    Args:\n        box1 (torch.Tensor): A tensor representing a single bounding box with shape (1, 4).\n        box2 (torch.Tensor): A tensor representing n bounding boxes with shape (n, 4).\n        xywh (bool, optional): If True, input boxes are in (x, y, w, h) format. If False, input boxes are in\n                               (x1, y1, x2, y2) format. Defaults to True.\n        GIoU (bool, optional): If True, calculate Generalized IoU. Defaults to False.\n        DIoU (bool, optional): If True, calculate Distance IoU. Defaults to False.\n        CIoU (bool, optional): If True, calculate Complete IoU. Defaults to False.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n    Returns:\n        (torch.Tensor): IoU, GIoU, DIoU, or CIoU values depending on the specified flags.\n    \"\"\"\n# Get the coordinates of bounding boxes\nif xywh:  # transform from xywh to xyxy\n(x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, -1), box2.chunk(4, -1)\nw1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\nb1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\nb2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\nelse:  # x1, y1, x2, y2 = box1\nb1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\nb2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\nw1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\nw2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n# Intersection area\ninter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp_(0) * \\\n            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp_(0)\n# Union Area\nunion = w1 * h1 + w2 * h2 - inter + eps\n# IoU\niou = inter / union\nif CIoU or DIoU or GIoU:\ncw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\nch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\nif CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\nc2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\nrho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\nif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\nv = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\nwith torch.no_grad():\nalpha = v / (v - iou + (1 + eps))\nreturn iou - (rho2 / c2 + v * alpha)  # CIoU\nreturn iou - rho2 / c2  # DIoU\nc_area = cw * ch + eps  # convex area\nreturn iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\nreturn iou  # IoU\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.mask_iou","title":"<code>ultralytics.utils.metrics.mask_iou(mask1, mask2, eps=1e-07)</code>","text":"<p>Calculate masks IoU.</p> <p>Parameters:</p> Name Type Description Default <code>mask1</code> <code>Tensor</code> <p>A tensor of shape (N, n) where N is the number of ground truth objects and n is the             product of image width and height.</p> required <code>mask2</code> <code>Tensor</code> <p>A tensor of shape (M, n) where M is the number of predicted objects and n is the             product of image width and height.</p> required <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-7.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (N, M) representing masks IoU.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def mask_iou(mask1, mask2, eps=1e-7):\n\"\"\"\n    Calculate masks IoU.\n    Args:\n        mask1 (torch.Tensor): A tensor of shape (N, n) where N is the number of ground truth objects and n is the\n                        product of image width and height.\n        mask2 (torch.Tensor): A tensor of shape (M, n) where M is the number of predicted objects and n is the\n                        product of image width and height.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing masks IoU.\n    \"\"\"\nintersection = torch.matmul(mask1, mask2.T).clamp_(0)\nunion = (mask1.sum(1)[:, None] + mask2.sum(1)[None]) - intersection  # (area1 + area2) - intersection\nreturn intersection / (union + eps)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.kpt_iou","title":"<code>ultralytics.utils.metrics.kpt_iou(kpt1, kpt2, area, sigma, eps=1e-07)</code>","text":"<p>Calculate Object Keypoint Similarity (OKS).</p> <p>Parameters:</p> Name Type Description Default <code>kpt1</code> <code>Tensor</code> <p>A tensor of shape (N, 17, 3) representing ground truth keypoints.</p> required <code>kpt2</code> <code>Tensor</code> <p>A tensor of shape (M, 17, 3) representing predicted keypoints.</p> required <code>area</code> <code>Tensor</code> <p>A tensor of shape (N,) representing areas from ground truth.</p> required <code>sigma</code> <code>list</code> <p>A list containing 17 values representing keypoint scales.</p> required <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-7.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (N, M) representing keypoint similarities.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def kpt_iou(kpt1, kpt2, area, sigma, eps=1e-7):\n\"\"\"\n    Calculate Object Keypoint Similarity (OKS).\n    Args:\n        kpt1 (torch.Tensor): A tensor of shape (N, 17, 3) representing ground truth keypoints.\n        kpt2 (torch.Tensor): A tensor of shape (M, 17, 3) representing predicted keypoints.\n        area (torch.Tensor): A tensor of shape (N,) representing areas from ground truth.\n        sigma (list): A list containing 17 values representing keypoint scales.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n    Returns:\n        (torch.Tensor): A tensor of shape (N, M) representing keypoint similarities.\n    \"\"\"\nd = (kpt1[:, None, :, 0] - kpt2[..., 0]) ** 2 + (kpt1[:, None, :, 1] - kpt2[..., 1]) ** 2  # (N, M, 17)\nsigma = torch.tensor(sigma, device=kpt1.device, dtype=kpt1.dtype)  # (17, )\nkpt_mask = kpt1[..., 2] != 0  # (N, 17)\ne = d / (2 * sigma) ** 2 / (area[:, None, None] + eps) / 2  # from cocoeval\n# e = d / ((area[None, :, None] + eps) * sigma) ** 2 / 2  # from formula\nreturn (torch.exp(-e) * kpt_mask[:, None]).sum(-1) / (kpt_mask.sum(-1)[:, None] + eps)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.smooth_BCE","title":"<code>ultralytics.utils.metrics.smooth_BCE(eps=0.1)</code>","text":"Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n# return positive, negative label smoothing BCE targets\nreturn 1.0 - 0.5 * eps, 0.5 * eps\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.smooth","title":"<code>ultralytics.utils.metrics.smooth(y, f=0.05)</code>","text":"<p>Box filter of fraction f.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def smooth(y, f=0.05):\n\"\"\"Box filter of fraction f.\"\"\"\nnf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\np = np.ones(nf // 2)  # ones padding\nyp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\nreturn np.convolve(yp, np.ones(nf) / nf, mode='valid')  # y-smoothed\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.plot_pr_curve","title":"<code>ultralytics.utils.metrics.plot_pr_curve(px, py, ap, save_dir=Path('pr_curve.png'), names=(), on_plot=None)</code>","text":"<p>Plots a precision-recall curve.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>@plt_settings()\ndef plot_pr_curve(px, py, ap, save_dir=Path('pr_curve.png'), names=(), on_plot=None):\n\"\"\"Plots a precision-recall curve.\"\"\"\nfig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\npy = np.stack(py, axis=1)\nif 0 &lt; len(names) &lt; 21:  # display per-class legend if &lt; 21 classes\nfor i, y in enumerate(py.T):\nax.plot(px, y, linewidth=1, label=f'{names[i]} {ap[i, 0]:.3f}')  # plot(recall, precision)\nelse:\nax.plot(px, py, linewidth=1, color='grey')  # plot(recall, precision)\nax.plot(px, py.mean(1), linewidth=3, color='blue', label='all classes %.3f mAP@0.5' % ap[:, 0].mean())\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.legend(bbox_to_anchor=(1.04, 1), loc='upper left')\nax.set_title('Precision-Recall Curve')\nfig.savefig(save_dir, dpi=250)\nplt.close(fig)\nif on_plot:\non_plot(save_dir)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.plot_mc_curve","title":"<code>ultralytics.utils.metrics.plot_mc_curve(px, py, save_dir=Path('mc_curve.png'), names=(), xlabel='Confidence', ylabel='Metric', on_plot=None)</code>","text":"<p>Plots a metric-confidence curve.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>@plt_settings()\ndef plot_mc_curve(px, py, save_dir=Path('mc_curve.png'), names=(), xlabel='Confidence', ylabel='Metric', on_plot=None):\n\"\"\"Plots a metric-confidence curve.\"\"\"\nfig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\nif 0 &lt; len(names) &lt; 21:  # display per-class legend if &lt; 21 classes\nfor i, y in enumerate(py):\nax.plot(px, y, linewidth=1, label=f'{names[i]}')  # plot(confidence, metric)\nelse:\nax.plot(px, py.T, linewidth=1, color='grey')  # plot(confidence, metric)\ny = smooth(py.mean(0), 0.05)\nax.plot(px, y, linewidth=3, color='blue', label=f'all classes {y.max():.2f} at {px[y.argmax()]:.3f}')\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.legend(bbox_to_anchor=(1.04, 1), loc='upper left')\nax.set_title(f'{ylabel}-Confidence Curve')\nfig.savefig(save_dir, dpi=250)\nplt.close(fig)\nif on_plot:\non_plot(save_dir)\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.compute_ap","title":"<code>ultralytics.utils.metrics.compute_ap(recall, precision)</code>","text":"<p>Compute the average precision (AP) given the recall and precision curves.</p> <p>Parameters:</p> Name Type Description Default <code>recall</code> <code>list</code> <p>The recall curve.</p> required <code>precision</code> <code>list</code> <p>The precision curve.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average precision.</p> <code>ndarray</code> <p>Precision envelope curve.</p> <code>ndarray</code> <p>Modified recall curve with sentinel values added at the beginning and end.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def compute_ap(recall, precision):\n\"\"\"\n    Compute the average precision (AP) given the recall and precision curves.\n    Args:\n        recall (list): The recall curve.\n        precision (list): The precision curve.\n    Returns:\n        (float): Average precision.\n        (np.ndarray): Precision envelope curve.\n        (np.ndarray): Modified recall curve with sentinel values added at the beginning and end.\n    \"\"\"\n# Append sentinel values to beginning and end\nmrec = np.concatenate(([0.0], recall, [1.0]))\nmpre = np.concatenate(([1.0], precision, [0.0]))\n# Compute the precision envelope\nmpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n# Integrate area under curve\nmethod = 'interp'  # methods: 'continuous', 'interp'\nif method == 'interp':\nx = np.linspace(0, 1, 101)  # 101-point interp (COCO)\nap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\nelse:  # 'continuous'\ni = np.where(mrec[1:] != mrec[:-1])[0]  # points where x-axis (recall) changes\nap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\nreturn ap, mpre, mrec\n</code></pre>"},{"location":"reference/utils/metrics/#ultralytics.utils.metrics.ap_per_class","title":"<code>ultralytics.utils.metrics.ap_per_class(tp, conf, pred_cls, target_cls, plot=False, on_plot=None, save_dir=Path(), names=(), eps=1e-16, prefix='')</code>","text":"<p>Computes the average precision per class for object detection evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>tp</code> <code>ndarray</code> <p>Binary array indicating whether the detection is correct (True) or not (False).</p> required <code>conf</code> <code>ndarray</code> <p>Array of confidence scores of the detections.</p> required <code>pred_cls</code> <code>ndarray</code> <p>Array of predicted classes of the detections.</p> required <code>target_cls</code> <code>ndarray</code> <p>Array of true classes of the detections.</p> required <code>plot</code> <code>bool</code> <p>Whether to plot PR curves or not. Defaults to False.</p> <code>False</code> <code>on_plot</code> <code>func</code> <p>A callback to pass plots path and data when they are rendered. Defaults to None.</p> <code>None</code> <code>save_dir</code> <code>Path</code> <p>Directory to save the PR curves. Defaults to an empty path.</p> <code>Path()</code> <code>names</code> <code>tuple</code> <p>Tuple of class names to plot PR curves. Defaults to an empty tuple.</p> <code>()</code> <code>eps</code> <code>float</code> <p>A small value to avoid division by zero. Defaults to 1e-16.</p> <code>1e-16</code> <code>prefix</code> <code>str</code> <p>A prefix string for saving the plot files. Defaults to an empty string.</p> <code>''</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of six arrays and one array of unique classes, where: tp (np.ndarray): True positive counts for each class. fp (np.ndarray): False positive counts for each class. p (np.ndarray): Precision values at each confidence threshold. r (np.ndarray): Recall values at each confidence threshold. f1 (np.ndarray): F1-score values at each confidence threshold. ap (np.ndarray): Average precision for each class at different IoU thresholds. unique_classes (np.ndarray): An array of unique classes that have data.</p> Source code in <code>ultralytics/utils/metrics.py</code> <pre><code>def ap_per_class(tp,\nconf,\npred_cls,\ntarget_cls,\nplot=False,\non_plot=None,\nsave_dir=Path(),\nnames=(),\neps=1e-16,\nprefix=''):\n\"\"\"\n    Computes the average precision per class for object detection evaluation.\n    Args:\n        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n        conf (np.ndarray): Array of confidence scores of the detections.\n        pred_cls (np.ndarray): Array of predicted classes of the detections.\n        target_cls (np.ndarray): Array of true classes of the detections.\n        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n    Returns:\n        (tuple): A tuple of six arrays and one array of unique classes, where:\n            tp (np.ndarray): True positive counts for each class.\n            fp (np.ndarray): False positive counts for each class.\n            p (np.ndarray): Precision values at each confidence threshold.\n            r (np.ndarray): Recall values at each confidence threshold.\n            f1 (np.ndarray): F1-score values at each confidence threshold.\n            ap (np.ndarray): Average precision for each class at different IoU thresholds.\n            unique_classes (np.ndarray): An array of unique classes that have data.\n    \"\"\"\n# Sort by objectness\ni = np.argsort(-conf)\ntp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n# Find unique classes\nunique_classes, nt = np.unique(target_cls, return_counts=True)\nnc = unique_classes.shape[0]  # number of classes, number of detections\n# Create Precision-Recall curve and compute AP for each class\npx, py = np.linspace(0, 1, 1000), []  # for plotting\nap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\nfor ci, c in enumerate(unique_classes):\ni = pred_cls == c\nn_l = nt[ci]  # number of labels\nn_p = i.sum()  # number of predictions\nif n_p == 0 or n_l == 0:\ncontinue\n# Accumulate FPs and TPs\nfpc = (1 - tp[i]).cumsum(0)\ntpc = tp[i].cumsum(0)\n# Recall\nrecall = tpc / (n_l + eps)  # recall curve\nr[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n# Precision\nprecision = tpc / (tpc + fpc)  # precision curve\np[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n# AP from recall-precision curve\nfor j in range(tp.shape[1]):\nap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\nif plot and j == 0:\npy.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n# Compute F1 (harmonic mean of precision and recall)\nf1 = 2 * p * r / (p + r + eps)\nnames = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\nnames = dict(enumerate(names))  # to dict\nif plot:\nplot_pr_curve(px, py, ap, save_dir / f'{prefix}PR_curve.png', names, on_plot=on_plot)\nplot_mc_curve(px, f1, save_dir / f'{prefix}F1_curve.png', names, ylabel='F1', on_plot=on_plot)\nplot_mc_curve(px, p, save_dir / f'{prefix}P_curve.png', names, ylabel='Precision', on_plot=on_plot)\nplot_mc_curve(px, r, save_dir / f'{prefix}R_curve.png', names, ylabel='Recall', on_plot=on_plot)\ni = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\np, r, f1 = p[:, i], r[:, i], f1[:, i]\ntp = (r * nt).round()  # true positives\nfp = (tp / (p + eps) - tp).round()  # false positives\nreturn tp, fp, p, r, f1, ap, unique_classes.astype(int)\n</code></pre>"},{"location":"reference/utils/ops/","title":"Reference for <code>ultralytics/utils/ops.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/ops.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.Profile","title":"<code>ultralytics.utils.ops.Profile</code>","text":"<p>             Bases: <code>ContextDecorator</code></p> <p>YOLOv8 Profile class. Use as a decorator with @Profile() or as a context manager with 'with Profile():'.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>class Profile(contextlib.ContextDecorator):\n\"\"\"\n    YOLOv8 Profile class. Use as a decorator with @Profile() or as a context manager with 'with Profile():'.\n    \"\"\"\ndef __init__(self, t=0.0):\n\"\"\"\n        Initialize the Profile class.\n        Args:\n            t (float): Initial time. Defaults to 0.0.\n        \"\"\"\nself.t = t\nself.cuda = torch.cuda.is_available()\ndef __enter__(self):\n\"\"\"Start timing.\"\"\"\nself.start = self.time()\nreturn self\ndef __exit__(self, type, value, traceback):  # noqa\n\"\"\"Stop timing.\"\"\"\nself.dt = self.time() - self.start  # delta-time\nself.t += self.dt  # accumulate dt\ndef time(self):\n\"\"\"Get current time.\"\"\"\nif self.cuda:\ntorch.cuda.synchronize()\nreturn time.time()\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.Profile.__enter__","title":"<code>__enter__()</code>","text":"<p>Start timing.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def __enter__(self):\n\"\"\"Start timing.\"\"\"\nself.start = self.time()\nreturn self\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.Profile.__exit__","title":"<code>__exit__(type, value, traceback)</code>","text":"<p>Stop timing.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def __exit__(self, type, value, traceback):  # noqa\n\"\"\"Stop timing.\"\"\"\nself.dt = self.time() - self.start  # delta-time\nself.t += self.dt  # accumulate dt\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.Profile.__init__","title":"<code>__init__(t=0.0)</code>","text":"<p>Initialize the Profile class.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>Initial time. Defaults to 0.0.</p> <code>0.0</code> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def __init__(self, t=0.0):\n\"\"\"\n    Initialize the Profile class.\n    Args:\n        t (float): Initial time. Defaults to 0.0.\n    \"\"\"\nself.t = t\nself.cuda = torch.cuda.is_available()\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.Profile.time","title":"<code>time()</code>","text":"<p>Get current time.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def time(self):\n\"\"\"Get current time.\"\"\"\nif self.cuda:\ntorch.cuda.synchronize()\nreturn time.time()\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.segment2box","title":"<code>ultralytics.utils.ops.segment2box(segment, width=640, height=640)</code>","text":"<p>Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy).</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>Tensor</code> <p>the segment label</p> required <code>width</code> <code>int</code> <p>the width of the image. Defaults to 640</p> <code>640</code> <code>height</code> <code>int</code> <p>The height of the image. Defaults to 640</p> <code>640</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>the minimum and maximum x and y values of the segment.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def segment2box(segment, width=640, height=640):\n\"\"\"\n    Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy).\n    Args:\n        segment (torch.Tensor): the segment label\n        width (int): the width of the image. Defaults to 640\n        height (int): The height of the image. Defaults to 640\n    Returns:\n        (np.ndarray): the minimum and maximum x and y values of the segment.\n    \"\"\"\n# Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\nx, y = segment.T  # segment xy\ninside = (x &gt;= 0) &amp; (y &gt;= 0) &amp; (x &lt;= width) &amp; (y &lt;= height)\nx, y, = x[inside], y[inside]\nreturn np.array([x.min(), y.min(), x.max(), y.max()], dtype=segment.dtype) if any(x) else np.zeros(\n4, dtype=segment.dtype)  # xyxy\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.scale_boxes","title":"<code>ultralytics.utils.ops.scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True)</code>","text":"<p>Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in (img1_shape) to the shape of a different image (img0_shape).</p> <p>Parameters:</p> Name Type Description Default <code>img1_shape</code> <code>tuple</code> <p>The shape of the image that the bounding boxes are for, in the format of (height, width).</p> required <code>boxes</code> <code>Tensor</code> <p>the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)</p> required <code>img0_shape</code> <code>tuple</code> <p>the shape of the target image, in the format of (height, width).</p> required <code>ratio_pad</code> <code>tuple</code> <p>a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be calculated based on the size difference between the two images.</p> <code>None</code> <code>padding</code> <code>bool</code> <p>If True, assuming the boxes is based on image augmented by yolo style. If False then do regular rescaling.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>boxes</code> <code>Tensor</code> <p>The scaled bounding boxes, in the format of (x1, y1, x2, y2)</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True):\n\"\"\"\n    Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in\n    (img1_shape) to the shape of a different image (img0_shape).\n    Args:\n        img1_shape (tuple): The shape of the image that the bounding boxes are for, in the format of (height, width).\n        boxes (torch.Tensor): the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)\n        img0_shape (tuple): the shape of the target image, in the format of (height, width).\n        ratio_pad (tuple): a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be\n            calculated based on the size difference between the two images.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    Returns:\n        boxes (torch.Tensor): The scaled bounding boxes, in the format of (x1, y1, x2, y2)\n    \"\"\"\nif ratio_pad is None:  # calculate from img0_shape\ngain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\npad = round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1), round(\n(img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1)  # wh padding\nelse:\ngain = ratio_pad[0][0]\npad = ratio_pad[1]\nif padding:\nboxes[..., [0, 2]] -= pad[0]  # x padding\nboxes[..., [1, 3]] -= pad[1]  # y padding\nboxes[..., :4] /= gain\nclip_boxes(boxes, img0_shape)\nreturn boxes\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.make_divisible","title":"<code>ultralytics.utils.ops.make_divisible(x, divisor)</code>","text":"<p>Returns the nearest number that is divisible by the given divisor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The number to make divisible.</p> required <code>divisor</code> <code>int | Tensor</code> <p>The divisor.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The nearest number divisible by the divisor.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def make_divisible(x, divisor):\n\"\"\"\n    Returns the nearest number that is divisible by the given divisor.\n    Args:\n        x (int): The number to make divisible.\n        divisor (int | torch.Tensor): The divisor.\n    Returns:\n        (int): The nearest number divisible by the divisor.\n    \"\"\"\nif isinstance(divisor, torch.Tensor):\ndivisor = int(divisor.max())  # to int\nreturn math.ceil(x / divisor) * divisor\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.non_max_suppression","title":"<code>ultralytics.utils.ops.non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680)</code>","text":"<p>Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes) containing the predicted boxes, classes, and masks. The tensor should be in the format output by a model, such as YOLO.</p> required <code>conf_thres</code> <code>float</code> <p>The confidence threshold below which boxes will be filtered out. Valid values are between 0.0 and 1.0.</p> <code>0.25</code> <code>iou_thres</code> <code>float</code> <p>The IoU threshold below which boxes will be filtered out during NMS. Valid values are between 0.0 and 1.0.</p> <code>0.45</code> <code>classes</code> <code>List[int]</code> <p>A list of class indices to consider. If None, all classes will be considered.</p> <code>None</code> <code>agnostic</code> <code>bool</code> <p>If True, the model is agnostic to the number of classes, and all classes will be considered as one.</p> <code>False</code> <code>multi_label</code> <code>bool</code> <p>If True, each box may have multiple labels.</p> <code>False</code> <code>labels</code> <code>List[List[Union[int, float, Tensor]]]</code> <p>A list of lists, where each inner list contains the apriori labels for a given image. The list should be in the format output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).</p> <code>()</code> <code>max_det</code> <code>int</code> <p>The maximum number of boxes to keep after NMS.</p> <code>300</code> <code>nc</code> <code>int</code> <p>The number of classes output by the model. Any indices after this will be considered masks.</p> <code>0</code> <code>max_time_img</code> <code>float</code> <p>The maximum time (seconds) for processing one image.</p> <code>0.05</code> <code>max_nms</code> <code>int</code> <p>The maximum number of boxes into torchvision.ops.nms().</p> <code>30000</code> <code>max_wh</code> <code>int</code> <p>The maximum box width and height in pixels</p> <code>7680</code> <p>Returns:</p> Type Description <code>List[torch.Tensor]</code> <p>A list of length batch_size, where each element is a tensor of shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def non_max_suppression(\nprediction,\nconf_thres=0.25,\niou_thres=0.45,\nclasses=None,\nagnostic=False,\nmulti_label=False,\nlabels=(),\nmax_det=300,\nnc=0,  # number of classes (optional)\nmax_time_img=0.05,\nmax_nms=30000,\nmax_wh=7680,\n):\n\"\"\"\n    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n    Args:\n        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)\n            containing the predicted boxes, classes, and masks. The tensor should be in the format\n            output by a model, such as YOLO.\n        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n            Valid values are between 0.0 and 1.0.\n        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n            Valid values are between 0.0 and 1.0.\n        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n            classes will be considered as one.\n        multi_label (bool): If True, each box may have multiple labels.\n        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n            list contains the apriori labels for a given image. The list should be in the format\n            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n        max_det (int): The maximum number of boxes to keep after NMS.\n        nc (int, optional): The number of classes output by the model. Any indices after this will be considered masks.\n        max_time_img (float): The maximum time (seconds) for processing one image.\n        max_nms (int): The maximum number of boxes into torchvision.ops.nms().\n        max_wh (int): The maximum box width and height in pixels\n    Returns:\n        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns\n            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).\n    \"\"\"\n# Checks\nassert 0 &lt;= conf_thres &lt;= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\nassert 0 &lt;= iou_thres &lt;= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\nif isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\nprediction = prediction[0]  # select only inference output\ndevice = prediction.device\nmps = 'mps' in device.type  # Apple MPS\nif mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\nprediction = prediction.cpu()\nbs = prediction.shape[0]  # batch size\nnc = nc or (prediction.shape[1] - 4)  # number of classes\nnm = prediction.shape[1] - nc - 4\nmi = 4 + nc  # mask start index\nxc = prediction[:, 4:mi].amax(1) &gt; conf_thres  # candidates\n# Settings\n# min_wh = 2  # (pixels) minimum box width and height\ntime_limit = 0.5 + max_time_img * bs  # seconds to quit after\nmulti_label &amp;= nc &gt; 1  # multiple labels per box (adds 0.5ms/img)\nprediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\nprediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy\nt = time.time()\noutput = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\nfor xi, x in enumerate(prediction):  # image index, image inference\n# Apply constraints\n# x[((x[:, 2:4] &lt; min_wh) | (x[:, 2:4] &gt; max_wh)).any(1), 4] = 0  # width-height\nx = x[xc[xi]]  # confidence\n# Cat apriori labels if autolabelling\nif labels and len(labels[xi]):\nlb = labels[xi]\nv = torch.zeros((len(lb), nc + nm + 4), device=x.device)\nv[:, :4] = xywh2xyxy(lb[:, 1:5])  # box\nv[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\nx = torch.cat((x, v), 0)\n# If none remain process next image\nif not x.shape[0]:\ncontinue\n# Detections matrix nx6 (xyxy, conf, cls)\nbox, cls, mask = x.split((4, nc, nm), 1)\nif multi_label:\ni, j = torch.where(cls &gt; conf_thres)\nx = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\nelse:  # best class only\nconf, j = cls.max(1, keepdim=True)\nx = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) &gt; conf_thres]\n# Filter by class\nif classes is not None:\nx = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n# Check shape\nn = x.shape[0]  # number of boxes\nif not n:  # no boxes\ncontinue\nif n &gt; max_nms:  # excess boxes\nx = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n# Batched NMS\nc = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\nboxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\ni = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\ni = i[:max_det]  # limit detections\n# # Experimental\n# merge = False  # use merge-NMS\n# if merge and (1 &lt; n &lt; 3E3):  # Merge NMS (boxes merged using weighted mean)\n#     # Update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n#     from .metrics import box_iou\n#     iou = box_iou(boxes[i], boxes) &gt; iou_thres  # iou matrix\n#     weights = iou * scores[None]  # box weights\n#     x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n#     redundant = True  # require redundant detections\n#     if redundant:\n#         i = i[iou.sum(1) &gt; 1]  # require redundancy\noutput[xi] = x[i]\nif mps:\noutput[xi] = output[xi].to(device)\nif (time.time() - t) &gt; time_limit:\nLOGGER.warning(f'WARNING \u26a0\ufe0f NMS time limit {time_limit:.3f}s exceeded')\nbreak  # time limit exceeded\nreturn output\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.clip_boxes","title":"<code>ultralytics.utils.ops.clip_boxes(boxes, shape)</code>","text":"<p>Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>Tensor</code> <p>the bounding boxes to clip</p> required <code>shape</code> <code>tuple</code> <p>the shape of the image</p> required Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def clip_boxes(boxes, shape):\n\"\"\"\n    Takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\n    Args:\n      boxes (torch.Tensor): the bounding boxes to clip\n      shape (tuple): the shape of the image\n    \"\"\"\nif isinstance(boxes, torch.Tensor):  # faster individually\nboxes[..., 0].clamp_(0, shape[1])  # x1\nboxes[..., 1].clamp_(0, shape[0])  # y1\nboxes[..., 2].clamp_(0, shape[1])  # x2\nboxes[..., 3].clamp_(0, shape[0])  # y2\nelse:  # np.array (faster grouped)\nboxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\nboxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.clip_coords","title":"<code>ultralytics.utils.ops.clip_coords(coords, shape)</code>","text":"<p>Clip line coordinates to the image boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Tensor | ndarray</code> <p>A list of line coordinates.</p> required <code>shape</code> <code>tuple</code> <p>A tuple of integers representing the size of the image in the format (height, width).</p> required <p>Returns:</p> Type Description <code>None</code> <p>The function modifies the input <code>coordinates</code> in place, by clipping each coordinate to the image boundaries.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def clip_coords(coords, shape):\n\"\"\"\n    Clip line coordinates to the image boundaries.\n    Args:\n        coords (torch.Tensor | numpy.ndarray): A list of line coordinates.\n        shape (tuple): A tuple of integers representing the size of the image in the format (height, width).\n    Returns:\n        (None): The function modifies the input `coordinates` in place, by clipping each coordinate to the image boundaries.\n    \"\"\"\nif isinstance(coords, torch.Tensor):  # faster individually\ncoords[..., 0].clamp_(0, shape[1])  # x\ncoords[..., 1].clamp_(0, shape[0])  # y\nelse:  # np.array (faster grouped)\ncoords[..., 0] = coords[..., 0].clip(0, shape[1])  # x\ncoords[..., 1] = coords[..., 1].clip(0, shape[0])  # y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.scale_image","title":"<code>ultralytics.utils.ops.scale_image(masks, im0_shape, ratio_pad=None)</code>","text":"<p>Takes a mask, and resizes it to the original image size</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>resized and padded masks/images, [h, w, num]/[h, w, 3].</p> required <code>im0_shape</code> <code>tuple</code> <p>the original image shape</p> required <code>ratio_pad</code> <code>tuple</code> <p>the ratio of the padding to the original image.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>masks</code> <code>Tensor</code> <p>The masks that are being returned.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def scale_image(masks, im0_shape, ratio_pad=None):\n\"\"\"\n    Takes a mask, and resizes it to the original image size\n    Args:\n        masks (np.ndarray): resized and padded masks/images, [h, w, num]/[h, w, 3].\n        im0_shape (tuple): the original image shape\n        ratio_pad (tuple): the ratio of the padding to the original image.\n    Returns:\n        masks (torch.Tensor): The masks that are being returned.\n    \"\"\"\n# Rescale coordinates (xyxy) from im1_shape to im0_shape\nim1_shape = masks.shape\nif im1_shape[:2] == im0_shape[:2]:\nreturn masks\nif ratio_pad is None:  # calculate from im0_shape\ngain = min(im1_shape[0] / im0_shape[0], im1_shape[1] / im0_shape[1])  # gain  = old / new\npad = (im1_shape[1] - im0_shape[1] * gain) / 2, (im1_shape[0] - im0_shape[0] * gain) / 2  # wh padding\nelse:\ngain = ratio_pad[0][0]\npad = ratio_pad[1]\ntop, left = int(pad[1]), int(pad[0])  # y, x\nbottom, right = int(im1_shape[0] - pad[1]), int(im1_shape[1] - pad[0])\nif len(masks.shape) &lt; 2:\nraise ValueError(f'\"len of masks shape\" should be 2 or 3, but got {len(masks.shape)}')\nmasks = masks[top:bottom, left:right]\nmasks = cv2.resize(masks, (im0_shape[1], im0_shape[0]))\nif len(masks.shape) == 2:\nmasks = masks[:, :, None]\nreturn masks\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xyxy2xywh","title":"<code>ultralytics.utils.ops.xyxy2xywh(x)</code>","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The input bounding box coordinates in (x1, y1, x2, y2) format.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in (x, y, width, height) format.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xyxy2xywh(x):\n\"\"\"\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height) format.\n    \"\"\"\nassert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\ny = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\ny[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\ny[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\ny[..., 2] = x[..., 2] - x[..., 0]  # width\ny[..., 3] = x[..., 3] - x[..., 1]  # height\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xywh2xyxy","title":"<code>ultralytics.utils.ops.xywh2xyxy(x)</code>","text":"<p>Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The input bounding box coordinates in (x, y, width, height) format.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in (x1, y1, x2, y2) format.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xywh2xyxy(x):\n\"\"\"\n    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n    top-left corner and (x2, y2) is the bottom-right corner.\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n    \"\"\"\nassert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\ny = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\ndw = x[..., 2] / 2  # half-width\ndh = x[..., 3] / 2  # half-height\ny[..., 0] = x[..., 0] - dw  # top left x\ny[..., 1] = x[..., 1] - dh  # top left y\ny[..., 2] = x[..., 0] + dw  # bottom right x\ny[..., 3] = x[..., 1] + dh  # bottom right y\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xywhn2xyxy","title":"<code>ultralytics.utils.ops.xywhn2xyxy(x, w=640, h=640, padw=0, padh=0)</code>","text":"<p>Convert normalized bounding box coordinates to pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates.</p> required <code>w</code> <code>int</code> <p>Width of the image. Defaults to 640</p> <code>640</code> <code>h</code> <code>int</code> <p>Height of the image. Defaults to 640</p> <code>640</code> <code>padw</code> <code>int</code> <p>Padding width. Defaults to 0</p> <code>0</code> <code>padh</code> <code>int</code> <p>Padding height. Defaults to 0</p> <code>0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The coordinates of the bounding box in the format [x1, y1, x2, y2] where x1,y1 is the top-left corner, x2,y2 is the bottom-right corner of the bounding box.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n\"\"\"\n    Convert normalized bounding box coordinates to pixel coordinates.\n    Args:\n        x (np.ndarray | torch.Tensor): The bounding box coordinates.\n        w (int): Width of the image. Defaults to 640\n        h (int): Height of the image. Defaults to 640\n        padw (int): Padding width. Defaults to 0\n        padh (int): Padding height. Defaults to 0\n    Returns:\n        y (np.ndarray | torch.Tensor): The coordinates of the bounding box in the format [x1, y1, x2, y2] where\n            x1,y1 is the top-left corner, x2,y2 is the bottom-right corner of the bounding box.\n    \"\"\"\nassert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\ny = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\ny[..., 0] = w * (x[..., 0] - x[..., 2] / 2) + padw  # top left x\ny[..., 1] = h * (x[..., 1] - x[..., 3] / 2) + padh  # top left y\ny[..., 2] = w * (x[..., 0] + x[..., 2] / 2) + padw  # bottom right x\ny[..., 3] = h * (x[..., 1] + x[..., 3] / 2) + padh  # bottom right y\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xyxy2xywhn","title":"<code>ultralytics.utils.ops.xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0)</code>","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y, width and height are normalized to image dimensions</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The input bounding box coordinates in (x1, y1, x2, y2) format.</p> required <code>w</code> <code>int</code> <p>The width of the image. Defaults to 640</p> <code>640</code> <code>h</code> <code>int</code> <p>The height of the image. Defaults to 640</p> <code>640</code> <code>clip</code> <code>bool</code> <p>If True, the boxes will be clipped to the image boundaries. Defaults to False</p> <code>False</code> <code>eps</code> <code>float</code> <p>The minimum value of the box's width and height. Defaults to 0.0</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in (x, y, width, height, normalized) format</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n\"\"\"\n    Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format.\n    x, y, width and height are normalized to image dimensions\n    Args:\n        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x1, y1, x2, y2) format.\n        w (int): The width of the image. Defaults to 640\n        h (int): The height of the image. Defaults to 640\n        clip (bool): If True, the boxes will be clipped to the image boundaries. Defaults to False\n        eps (float): The minimum value of the box's width and height. Defaults to 0.0\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x, y, width, height, normalized) format\n    \"\"\"\nif clip:\nclip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\nassert x.shape[-1] == 4, f'input shape last dimension expected 4 but input shape is {x.shape}'\ny = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\ny[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\ny[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\ny[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\ny[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xywh2ltwh","title":"<code>ultralytics.utils.ops.xywh2ltwh(x)</code>","text":"<p>Convert the bounding box format from [x, y, w, h] to [x1, y1, w, h], where x1, y1 are the top-left coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The input tensor with the bounding box coordinates in the xywh format</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in the xyltwh format</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xywh2ltwh(x):\n\"\"\"\n    Convert the bounding box format from [x, y, w, h] to [x1, y1, w, h], where x1, y1 are the top-left coordinates.\n    Args:\n        x (np.ndarray | torch.Tensor): The input tensor with the bounding box coordinates in the xywh format\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in the xyltwh format\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\ny[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xyxy2ltwh","title":"<code>ultralytics.utils.ops.xyxy2ltwh(x)</code>","text":"<p>Convert nx4 bounding boxes from [x1, y1, x2, y2] to [x1, y1, w, h], where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>The input tensor with the bounding boxes coordinates in the xyxy format</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in the xyltwh format.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xyxy2ltwh(x):\n\"\"\"\n    Convert nx4 bounding boxes from [x1, y1, x2, y2] to [x1, y1, w, h], where xy1=top-left, xy2=bottom-right\n    Args:\n        x (np.ndarray | torch.Tensor): The input tensor with the bounding boxes coordinates in the xyxy format\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in the xyltwh format.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 2] = x[..., 2] - x[..., 0]  # width\ny[..., 3] = x[..., 3] - x[..., 1]  # height\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.ltwh2xywh","title":"<code>ultralytics.utils.ops.ltwh2xywh(x)</code>","text":"<p>Convert nx4 boxes from [x1, y1, w, h] to [x, y, w, h] where xy1=top-left, xy=center</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>the input tensor</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>The bounding box coordinates in the xywh format.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def ltwh2xywh(x):\n\"\"\"\n    Convert nx4 boxes from [x1, y1, w, h] to [x, y, w, h] where xy1=top-left, xy=center\n    Args:\n        x (torch.Tensor): the input tensor\n    Returns:\n        y (np.ndarray | torch.Tensor): The bounding box coordinates in the xywh format.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 0] = x[..., 0] + x[..., 2] / 2  # center x\ny[..., 1] = x[..., 1] + x[..., 3] / 2  # center y\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xyxyxyxy2xywhr","title":"<code>ultralytics.utils.ops.xyxyxyxy2xywhr(corners)</code>","text":"<p>Convert batched Oriented Bounding Boxes (OBB) from [xy1, xy2, xy3, xy4] to [xywh, rotation].</p> <p>Parameters:</p> Name Type Description Default <code>corners</code> <code>ndarray | Tensor</code> <p>Input corners of shape (n, 8).</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p>Converted data in [cx, cy, w, h, rotation] format of shape (n, 5).</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xyxyxyxy2xywhr(corners):\n\"\"\"\n    Convert batched Oriented Bounding Boxes (OBB) from [xy1, xy2, xy3, xy4] to [xywh, rotation].\n    Args:\n        corners (numpy.ndarray | torch.Tensor): Input corners of shape (n, 8).\n    Returns:\n        (numpy.ndarray | torch.Tensor): Converted data in [cx, cy, w, h, rotation] format of shape (n, 5).\n    \"\"\"\nis_numpy = isinstance(corners, np.ndarray)\natan2, sqrt = (np.arctan2, np.sqrt) if is_numpy else (torch.atan2, torch.sqrt)\nx1, y1, x2, y2, x3, y3, x4, y4 = corners.T\ncx = (x1 + x3) / 2\ncy = (y1 + y3) / 2\ndx21 = x2 - x1\ndy21 = y2 - y1\nw = sqrt(dx21 ** 2 + dy21 ** 2)\nh = sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2)\nrotation = atan2(-dy21, dx21)\nrotation *= 180.0 / math.pi  # radians to degrees\nreturn np.vstack((cx, cy, w, h, rotation)).T if is_numpy else torch.stack((cx, cy, w, h, rotation), dim=1)\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.xywhr2xyxyxyxy","title":"<code>ultralytics.utils.ops.xywhr2xyxyxyxy(center)</code>","text":"<p>Convert batched Oriented Bounding Boxes (OBB) from [xywh, rotation] to [xy1, xy2, xy3, xy4].</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>ndarray | Tensor</code> <p>Input data in [cx, cy, w, h, rotation] format of shape (n, 5).</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p>Converted corner points of shape (n, 8).</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def xywhr2xyxyxyxy(center):\n\"\"\"\n    Convert batched Oriented Bounding Boxes (OBB) from [xywh, rotation] to [xy1, xy2, xy3, xy4].\n    Args:\n        center (numpy.ndarray | torch.Tensor): Input data in [cx, cy, w, h, rotation] format of shape (n, 5).\n    Returns:\n        (numpy.ndarray | torch.Tensor): Converted corner points of shape (n, 8).\n    \"\"\"\nis_numpy = isinstance(center, np.ndarray)\ncos, sin = (np.cos, np.sin) if is_numpy else (torch.cos, torch.sin)\ncx, cy, w, h, rotation = center.T\nrotation *= math.pi / 180.0  # degrees to radians\ndx = w / 2\ndy = h / 2\ncos_rot = cos(rotation)\nsin_rot = sin(rotation)\ndx_cos_rot = dx * cos_rot\ndx_sin_rot = dx * sin_rot\ndy_cos_rot = dy * cos_rot\ndy_sin_rot = dy * sin_rot\nx1 = cx - dx_cos_rot - dy_sin_rot\ny1 = cy + dx_sin_rot - dy_cos_rot\nx2 = cx + dx_cos_rot - dy_sin_rot\ny2 = cy - dx_sin_rot - dy_cos_rot\nx3 = cx + dx_cos_rot + dy_sin_rot\ny3 = cy - dx_sin_rot + dy_cos_rot\nx4 = cx - dx_cos_rot + dy_sin_rot\ny4 = cy + dx_sin_rot + dy_cos_rot\nreturn np.vstack((x1, y1, x2, y2, x3, y3, x4, y4)).T if is_numpy else torch.stack(\n(x1, y1, x2, y2, x3, y3, x4, y4), dim=1)\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.ltwh2xyxy","title":"<code>ultralytics.utils.ops.ltwh2xyxy(x)</code>","text":"<p>It converts the bounding box from [x1, y1, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | Tensor</code> <p>the input image</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray | Tensor</code> <p>the xyxy coordinates of the bounding boxes.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def ltwh2xyxy(x):\n\"\"\"\n    It converts the bounding box from [x1, y1, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    Args:\n        x (np.ndarray | torch.Tensor): the input image\n    Returns:\n        y (np.ndarray | torch.Tensor): the xyxy coordinates of the bounding boxes.\n    \"\"\"\ny = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\ny[..., 2] = x[..., 2] + x[..., 0]  # width\ny[..., 3] = x[..., 3] + x[..., 1]  # height\nreturn y\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.segments2boxes","title":"<code>ultralytics.utils.ops.segments2boxes(segments)</code>","text":"<p>It converts segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>list</code> <p>list of segments, each segment is a list of points, each point is a list of x, y coordinates</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the xywh coordinates of the bounding boxes.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def segments2boxes(segments):\n\"\"\"\n    It converts segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n    Args:\n        segments (list): list of segments, each segment is a list of points, each point is a list of x, y coordinates\n    Returns:\n        (np.ndarray): the xywh coordinates of the bounding boxes.\n    \"\"\"\nboxes = []\nfor s in segments:\nx, y = s.T  # segment xy\nboxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\nreturn xyxy2xywh(np.array(boxes))  # cls, xywh\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.resample_segments","title":"<code>ultralytics.utils.ops.resample_segments(segments, n=1000)</code>","text":"<p>Inputs a list of segments (n,2) and returns a list of segments (n,2) up-sampled to n points each.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>list</code> <p>a list of (n,2) arrays, where n is the number of points in the segment.</p> required <code>n</code> <code>int</code> <p>number of points to resample the segment to. Defaults to 1000</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>list</code> <p>the resampled segments.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def resample_segments(segments, n=1000):\n\"\"\"\n    Inputs a list of segments (n,2) and returns a list of segments (n,2) up-sampled to n points each.\n    Args:\n        segments (list): a list of (n,2) arrays, where n is the number of points in the segment.\n        n (int): number of points to resample the segment to. Defaults to 1000\n    Returns:\n        segments (list): the resampled segments.\n    \"\"\"\nfor i, s in enumerate(segments):\ns = np.concatenate((s, s[0:1, :]), axis=0)\nx = np.linspace(0, len(s) - 1, n)\nxp = np.arange(len(s))\nsegments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)],\ndtype=np.float32).reshape(2, -1).T  # segment xy\nreturn segments\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.crop_mask","title":"<code>ultralytics.utils.ops.crop_mask(masks, boxes)</code>","text":"<p>It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Tensor</code> <p>[n, h, w] tensor of masks</p> required <code>boxes</code> <code>Tensor</code> <p>[n, 4] tensor of bbox coordinates in relative point form</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The masks are being cropped to the bounding box.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def crop_mask(masks, boxes):\n\"\"\"\n    It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box.\n    Args:\n        masks (torch.Tensor): [n, h, w] tensor of masks\n        boxes (torch.Tensor): [n, 4] tensor of bbox coordinates in relative point form\n    Returns:\n        (torch.Tensor): The masks are being cropped to the bounding box.\n    \"\"\"\nn, h, w = masks.shape\nx1, y1, x2, y2 = torch.chunk(boxes[:, :, None], 4, 1)  # x1 shape(n,1,1)\nr = torch.arange(w, device=masks.device, dtype=x1.dtype)[None, None, :]  # rows shape(1,1,w)\nc = torch.arange(h, device=masks.device, dtype=x1.dtype)[None, :, None]  # cols shape(1,h,1)\nreturn masks * ((r &gt;= x1) * (r &lt; x2) * (c &gt;= y1) * (c &lt; y2))\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.process_mask_upsample","title":"<code>ultralytics.utils.ops.process_mask_upsample(protos, masks_in, bboxes, shape)</code>","text":"<p>Takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher quality but is slower.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>Tensor</code> <p>[mask_dim, mask_h, mask_w]</p> required <code>masks_in</code> <code>Tensor</code> <p>[n, mask_dim], n is number of masks after nms</p> required <code>bboxes</code> <code>Tensor</code> <p>[n, 4], n is number of masks after nms</p> required <code>shape</code> <code>tuple</code> <p>the size of the input image (h,w)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The upsampled masks.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def process_mask_upsample(protos, masks_in, bboxes, shape):\n\"\"\"\n    Takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher\n    quality but is slower.\n    Args:\n        protos (torch.Tensor): [mask_dim, mask_h, mask_w]\n        masks_in (torch.Tensor): [n, mask_dim], n is number of masks after nms\n        bboxes (torch.Tensor): [n, 4], n is number of masks after nms\n        shape (tuple): the size of the input image (h,w)\n    Returns:\n        (torch.Tensor): The upsampled masks.\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)\nmasks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW\nmasks = crop_mask(masks, bboxes)  # CHW\nreturn masks.gt_(0.5)\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.process_mask","title":"<code>ultralytics.utils.ops.process_mask(protos, masks_in, bboxes, shape, upsample=False)</code>","text":"<p>Apply masks to bounding boxes using the output of the mask head.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>Tensor</code> <p>A tensor of shape [mask_dim, mask_h, mask_w].</p> required <code>masks_in</code> <code>Tensor</code> <p>A tensor of shape [n, mask_dim], where n is the number of masks after NMS.</p> required <code>bboxes</code> <code>Tensor</code> <p>A tensor of shape [n, 4], where n is the number of masks after NMS.</p> required <code>shape</code> <code>tuple</code> <p>A tuple of integers representing the size of the input image in the format (h, w).</p> required <code>upsample</code> <code>bool</code> <p>A flag to indicate whether to upsample the mask to the original image size. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A binary mask tensor of shape [n, h, w], where n is the number of masks after NMS, and h and w are the height and width of the input image. The mask is applied to the bounding boxes.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def process_mask(protos, masks_in, bboxes, shape, upsample=False):\n\"\"\"\n    Apply masks to bounding boxes using the output of the mask head.\n    Args:\n        protos (torch.Tensor): A tensor of shape [mask_dim, mask_h, mask_w].\n        masks_in (torch.Tensor): A tensor of shape [n, mask_dim], where n is the number of masks after NMS.\n        bboxes (torch.Tensor): A tensor of shape [n, 4], where n is the number of masks after NMS.\n        shape (tuple): A tuple of integers representing the size of the input image in the format (h, w).\n        upsample (bool): A flag to indicate whether to upsample the mask to the original image size. Default is False.\n    Returns:\n        (torch.Tensor): A binary mask tensor of shape [n, h, w], where n is the number of masks after NMS, and h and w\n            are the height and width of the input image. The mask is applied to the bounding boxes.\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nih, iw = shape\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)  # CHW\ndownsampled_bboxes = bboxes.clone()\ndownsampled_bboxes[:, 0] *= mw / iw\ndownsampled_bboxes[:, 2] *= mw / iw\ndownsampled_bboxes[:, 3] *= mh / ih\ndownsampled_bboxes[:, 1] *= mh / ih\nmasks = crop_mask(masks, downsampled_bboxes)  # CHW\nif upsample:\nmasks = F.interpolate(masks[None], shape, mode='bilinear', align_corners=False)[0]  # CHW\nreturn masks.gt_(0.5)\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.process_mask_native","title":"<code>ultralytics.utils.ops.process_mask_native(protos, masks_in, bboxes, shape)</code>","text":"<p>It takes the output of the mask head, and crops it after upsampling to the bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>Tensor</code> <p>[mask_dim, mask_h, mask_w]</p> required <code>masks_in</code> <code>Tensor</code> <p>[n, mask_dim], n is number of masks after nms</p> required <code>bboxes</code> <code>Tensor</code> <p>[n, 4], n is number of masks after nms</p> required <code>shape</code> <code>tuple</code> <p>the size of the input image (h,w)</p> required <p>Returns:</p> Name Type Description <code>masks</code> <code>Tensor</code> <p>The returned masks with dimensions [h, w, n]</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def process_mask_native(protos, masks_in, bboxes, shape):\n\"\"\"\n    It takes the output of the mask head, and crops it after upsampling to the bounding boxes.\n    Args:\n        protos (torch.Tensor): [mask_dim, mask_h, mask_w]\n        masks_in (torch.Tensor): [n, mask_dim], n is number of masks after nms\n        bboxes (torch.Tensor): [n, 4], n is number of masks after nms\n        shape (tuple): the size of the input image (h,w)\n    Returns:\n        masks (torch.Tensor): The returned masks with dimensions [h, w, n]\n    \"\"\"\nc, mh, mw = protos.shape  # CHW\nmasks = (masks_in @ protos.float().view(c, -1)).sigmoid().view(-1, mh, mw)\nmasks = scale_masks(masks[None], shape)[0]  # CHW\nmasks = crop_mask(masks, bboxes)  # CHW\nreturn masks.gt_(0.5)\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.scale_masks","title":"<code>ultralytics.utils.ops.scale_masks(masks, shape, padding=True)</code>","text":"<p>Rescale segment masks to shape.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Tensor</code> <p>(N, C, H, W).</p> required <code>shape</code> <code>tuple</code> <p>Height and width.</p> required <code>padding</code> <code>bool</code> <p>If True, assuming the boxes is based on image augmented by yolo style. If False then do regular rescaling.</p> <code>True</code> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def scale_masks(masks, shape, padding=True):\n\"\"\"\n    Rescale segment masks to shape.\n    Args:\n        masks (torch.Tensor): (N, C, H, W).\n        shape (tuple): Height and width.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    \"\"\"\nmh, mw = masks.shape[2:]\ngain = min(mh / shape[0], mw / shape[1])  # gain  = old / new\npad = [mw - shape[1] * gain, mh - shape[0] * gain]  # wh padding\nif padding:\npad[0] /= 2\npad[1] /= 2\ntop, left = (int(pad[1]), int(pad[0])) if padding else (0, 0)  # y, x\nbottom, right = (int(mh - pad[1]), int(mw - pad[0]))\nmasks = masks[..., top:bottom, left:right]\nmasks = F.interpolate(masks, shape, mode='bilinear', align_corners=False)  # NCHW\nreturn masks\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.scale_coords","title":"<code>ultralytics.utils.ops.scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False, padding=True)</code>","text":"<p>Rescale segment coordinates (xy) from img1_shape to img0_shape</p> <p>Parameters:</p> Name Type Description Default <code>img1_shape</code> <code>tuple</code> <p>The shape of the image that the coords are from.</p> required <code>coords</code> <code>Tensor</code> <p>the coords to be scaled of shape n,2.</p> required <code>img0_shape</code> <code>tuple</code> <p>the shape of the image that the segmentation is being applied to.</p> required <code>ratio_pad</code> <code>tuple</code> <p>the ratio of the image size to the padded image size.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, the coordinates will be normalized to the range [0, 1]. Defaults to False.</p> <code>False</code> <code>padding</code> <code>bool</code> <p>If True, assuming the boxes is based on image augmented by yolo style. If False then do regular rescaling.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>coords</code> <code>Tensor</code> <p>The scaled coordinates.</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None, normalize=False, padding=True):\n\"\"\"\n    Rescale segment coordinates (xy) from img1_shape to img0_shape\n    Args:\n        img1_shape (tuple): The shape of the image that the coords are from.\n        coords (torch.Tensor): the coords to be scaled of shape n,2.\n        img0_shape (tuple): the shape of the image that the segmentation is being applied to.\n        ratio_pad (tuple): the ratio of the image size to the padded image size.\n        normalize (bool): If True, the coordinates will be normalized to the range [0, 1]. Defaults to False.\n        padding (bool): If True, assuming the boxes is based on image augmented by yolo style. If False then do regular\n            rescaling.\n    Returns:\n        coords (torch.Tensor): The scaled coordinates.\n    \"\"\"\nif ratio_pad is None:  # calculate from img0_shape\ngain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\npad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\nelse:\ngain = ratio_pad[0][0]\npad = ratio_pad[1]\nif padding:\ncoords[..., 0] -= pad[0]  # x padding\ncoords[..., 1] -= pad[1]  # y padding\ncoords[..., 0] /= gain\ncoords[..., 1] /= gain\nclip_coords(coords, img0_shape)\nif normalize:\ncoords[..., 0] /= img0_shape[1]  # width\ncoords[..., 1] /= img0_shape[0]  # height\nreturn coords\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.masks2segments","title":"<code>ultralytics.utils.ops.masks2segments(masks, strategy='largest')</code>","text":"<p>It takes a list of masks(n,h,w) and returns a list of segments(n,xy)</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Tensor</code> <p>the output of the model, which is a tensor of shape (batch_size, 160, 160)</p> required <code>strategy</code> <code>str</code> <p>'concat' or 'largest'. Defaults to largest</p> <code>'largest'</code> <p>Returns:</p> Name Type Description <code>segments</code> <code>List</code> <p>list of segment masks</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def masks2segments(masks, strategy='largest'):\n\"\"\"\n    It takes a list of masks(n,h,w) and returns a list of segments(n,xy)\n    Args:\n        masks (torch.Tensor): the output of the model, which is a tensor of shape (batch_size, 160, 160)\n        strategy (str): 'concat' or 'largest'. Defaults to largest\n    Returns:\n        segments (List): list of segment masks\n    \"\"\"\nsegments = []\nfor x in masks.int().cpu().numpy().astype('uint8'):\nc = cv2.findContours(x, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\nif c:\nif strategy == 'concat':  # concatenate all segments\nc = np.concatenate([x.reshape(-1, 2) for x in c])\nelif strategy == 'largest':  # select largest segment\nc = np.array(c[np.array([len(x) for x in c]).argmax()]).reshape(-1, 2)\nelse:\nc = np.zeros((0, 2))  # no segments found\nsegments.append(c.astype('float32'))\nreturn segments\n</code></pre>"},{"location":"reference/utils/ops/#ultralytics.utils.ops.clean_str","title":"<code>ultralytics.utils.ops.clean_str(s)</code>","text":"<p>Cleans a string by replacing special characters with underscore _</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>a string needing special characters replaced</p> required <p>Returns:</p> Type Description <code>str</code> <p>a string with special characters replaced by an underscore _</p> Source code in <code>ultralytics/utils/ops.py</code> <pre><code>def clean_str(s):\n\"\"\"\n    Cleans a string by replacing special characters with underscore _\n    Args:\n        s (str): a string needing special characters replaced\n    Returns:\n        (str): a string with special characters replaced by an underscore _\n    \"\"\"\nreturn re.sub(pattern='[|@#!\u00a1\u00b7$\u20ac%&amp;()=?\u00bf^*;:,\u00a8\u00b4&gt;&lt;+]', repl='_', string=s)\n</code></pre>"},{"location":"reference/utils/patches/","title":"Reference for <code>ultralytics/utils/patches.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/patches.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/patches/#ultralytics.utils.patches.imread","title":"<code>ultralytics.utils.patches.imread(filename, flags=cv2.IMREAD_COLOR)</code>","text":"Source code in <code>ultralytics/utils/patches.py</code> <pre><code>def imread(filename, flags=cv2.IMREAD_COLOR):\nreturn cv2.imdecode(np.fromfile(filename, np.uint8), flags)\n</code></pre>"},{"location":"reference/utils/patches/#ultralytics.utils.patches.imwrite","title":"<code>ultralytics.utils.patches.imwrite(filename, img)</code>","text":"Source code in <code>ultralytics/utils/patches.py</code> <pre><code>def imwrite(filename, img):\ntry:\ncv2.imencode(Path(filename).suffix, img)[1].tofile(filename)\nreturn True\nexcept Exception:\nreturn False\n</code></pre>"},{"location":"reference/utils/patches/#ultralytics.utils.patches.imshow","title":"<code>ultralytics.utils.patches.imshow(path, im)</code>","text":"Source code in <code>ultralytics/utils/patches.py</code> <pre><code>def imshow(path, im):\n_imshow(path.encode('unicode_escape').decode(), im)\n</code></pre>"},{"location":"reference/utils/patches/#ultralytics.utils.patches.torch_save","title":"<code>ultralytics.utils.patches.torch_save(*args, **kwargs)</code>","text":"<p>Use dill (if exists) to serialize the lambda functions where pickle does not do this.</p> Source code in <code>ultralytics/utils/patches.py</code> <pre><code>def torch_save(*args, **kwargs):\n\"\"\"Use dill (if exists) to serialize the lambda functions where pickle does not do this.\"\"\"\ntry:\nimport dill as pickle\nexcept ImportError:\nimport pickle\nif 'pickle_module' not in kwargs:\nkwargs['pickle_module'] = pickle\nreturn _torch_save(*args, **kwargs)\n</code></pre>"},{"location":"reference/utils/plotting/","title":"Reference for <code>ultralytics/utils/plotting.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/plotting.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Colors","title":"<code>ultralytics.utils.plotting.Colors</code>","text":"<p>Ultralytics default color palette https://ultralytics.com/.</p> <p>This class provides methods to work with the Ultralytics color palette, including converting hex color codes to RGB values.</p> <p>Attributes:</p> Name Type Description <code>palette</code> <code>list of tuple</code> <p>List of RGB color values.</p> <code>n</code> <code>int</code> <p>The number of colors in the palette.</p> <code>pose_palette</code> <code>array</code> <p>A specific color palette array with dtype np.uint8.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>class Colors:\n\"\"\"\n    Ultralytics default color palette https://ultralytics.com/.\n    This class provides methods to work with the Ultralytics color palette, including converting hex color codes to\n    RGB values.\n    Attributes:\n        palette (list of tuple): List of RGB color values.\n        n (int): The number of colors in the palette.\n        pose_palette (np.array): A specific color palette array with dtype np.uint8.\n    \"\"\"\ndef __init__(self):\n\"\"\"Initialize colors as hex = matplotlib.colors.TABLEAU_COLORS.values().\"\"\"\nhexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n'2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\nself.palette = [self.hex2rgb(f'#{c}') for c in hexs]\nself.n = len(self.palette)\nself.pose_palette = np.array([[255, 128, 0], [255, 153, 51], [255, 178, 102], [230, 230, 0], [255, 153, 255],\n[153, 204, 255], [255, 102, 255], [255, 51, 255], [102, 178, 255], [51, 153, 255],\n[255, 153, 153], [255, 102, 102], [255, 51, 51], [153, 255, 153], [102, 255, 102],\n[51, 255, 51], [0, 255, 0], [0, 0, 255], [255, 0, 0], [255, 255, 255]],\ndtype=np.uint8)\ndef __call__(self, i, bgr=False):\n\"\"\"Converts hex color codes to RGB values.\"\"\"\nc = self.palette[int(i) % self.n]\nreturn (c[2], c[1], c[0]) if bgr else c\n@staticmethod\ndef hex2rgb(h):\n\"\"\"Converts hex color codes to RGB values (i.e. default PIL order).\"\"\"\nreturn tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Colors.__call__","title":"<code>__call__(i, bgr=False)</code>","text":"<p>Converts hex color codes to RGB values.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def __call__(self, i, bgr=False):\n\"\"\"Converts hex color codes to RGB values.\"\"\"\nc = self.palette[int(i) % self.n]\nreturn (c[2], c[1], c[0]) if bgr else c\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Colors.__init__","title":"<code>__init__()</code>","text":"<p>Initialize colors as hex = matplotlib.colors.TABLEAU_COLORS.values().</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def __init__(self):\n\"\"\"Initialize colors as hex = matplotlib.colors.TABLEAU_COLORS.values().\"\"\"\nhexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n'2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\nself.palette = [self.hex2rgb(f'#{c}') for c in hexs]\nself.n = len(self.palette)\nself.pose_palette = np.array([[255, 128, 0], [255, 153, 51], [255, 178, 102], [230, 230, 0], [255, 153, 255],\n[153, 204, 255], [255, 102, 255], [255, 51, 255], [102, 178, 255], [51, 153, 255],\n[255, 153, 153], [255, 102, 102], [255, 51, 51], [153, 255, 153], [102, 255, 102],\n[51, 255, 51], [0, 255, 0], [0, 0, 255], [255, 0, 0], [255, 255, 255]],\ndtype=np.uint8)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Colors.hex2rgb","title":"<code>hex2rgb(h)</code>  <code>staticmethod</code>","text":"<p>Converts hex color codes to RGB values (i.e. default PIL order).</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>@staticmethod\ndef hex2rgb(h):\n\"\"\"Converts hex color codes to RGB values (i.e. default PIL order).\"\"\"\nreturn tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator","title":"<code>ultralytics.utils.plotting.Annotator</code>","text":"<p>Ultralytics Annotator for train/val mosaics and JPGs and predictions annotations.</p> <p>Attributes:</p> Name Type Description <code>im</code> <code>Image.Image or numpy array</code> <p>The image to annotate.</p> <code>pil</code> <code>bool</code> <p>Whether to use PIL or cv2 for drawing annotations.</p> <code>font</code> <code>truetype or load_default</code> <p>Font used for text annotations.</p> <code>lw</code> <code>float</code> <p>Line width for drawing.</p> <code>skeleton</code> <code>List[List[int]]</code> <p>Skeleton structure for keypoints.</p> <code>limb_color</code> <code>List[int]</code> <p>Color palette for limbs.</p> <code>kpt_color</code> <code>List[int]</code> <p>Color palette for keypoints.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>class Annotator:\n\"\"\"\n    Ultralytics Annotator for train/val mosaics and JPGs and predictions annotations.\n    Attributes:\n        im (Image.Image or numpy array): The image to annotate.\n        pil (bool): Whether to use PIL or cv2 for drawing annotations.\n        font (ImageFont.truetype or ImageFont.load_default): Font used for text annotations.\n        lw (float): Line width for drawing.\n        skeleton (List[List[int]]): Skeleton structure for keypoints.\n        limb_color (List[int]): Color palette for limbs.\n        kpt_color (List[int]): Color palette for keypoints.\n    \"\"\"\ndef __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):\n\"\"\"Initialize the Annotator class with image and line width along with color palette for keypoints and limbs.\"\"\"\nassert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\nnon_ascii = not is_ascii(example)  # non-latin labels, i.e. asian, arabic, cyrillic\nself.pil = pil or non_ascii\nif self.pil:  # use PIL\nself.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\nself.draw = ImageDraw.Draw(self.im)\ntry:\nfont = check_font('Arial.Unicode.ttf' if non_ascii else font)\nsize = font_size or max(round(sum(self.im.size) / 2 * 0.035), 12)\nself.font = ImageFont.truetype(str(font), size)\nexcept Exception:\nself.font = ImageFont.load_default()\n# Deprecation fix for w, h = getsize(string) -&gt; _, _, w, h = getbox(string)\nif check_version(pil_version, '9.2.0'):\nself.font.getsize = lambda x: self.font.getbbox(x)[2:4]  # text width, height\nelse:  # use cv2\nself.im = im\nself.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n# Pose\nself.skeleton = [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n[8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]]\nself.limb_color = colors.pose_palette[[9, 9, 9, 9, 7, 7, 7, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 16, 16]]\nself.kpt_color = colors.pose_palette[[16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9]]\ndef box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n\"\"\"Add one xyxy box to image with label.\"\"\"\nif isinstance(box, torch.Tensor):\nbox = box.tolist()\nif self.pil or not is_ascii(label):\nself.draw.rectangle(box, width=self.lw, outline=color)  # box\nif label:\nw, h = self.font.getsize(label)  # text width, height\noutside = box[1] - h &gt;= 0  # label fits outside box\nself.draw.rectangle(\n(box[0], box[1] - h if outside else box[1], box[0] + w + 1,\nbox[1] + 1 if outside else box[1] + h + 1),\nfill=color,\n)\n# self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL&gt;8.0\nself.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\nelse:  # cv2\np1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\ncv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\nif label:\ntf = max(self.lw - 1, 1)  # font thickness\nw, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\noutside = p1[1] - h &gt;= 3\np2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\ncv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\ncv2.putText(self.im,\nlabel, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n0,\nself.lw / 3,\ntxt_color,\nthickness=tf,\nlineType=cv2.LINE_AA)\ndef masks(self, masks, colors, im_gpu, alpha=0.5, retina_masks=False):\n\"\"\"\n        Plot masks on image.\n        Args:\n            masks (tensor): Predicted masks on cuda, shape: [n, h, w]\n            colors (List[List[Int]]): Colors for predicted masks, [[r, g, b] * n]\n            im_gpu (tensor): Image is in cuda, shape: [3, h, w], range: [0, 1]\n            alpha (float): Mask transparency: 0.0 fully transparent, 1.0 opaque\n            retina_masks (bool): Whether to use high resolution masks or not. Defaults to False.\n        \"\"\"\nif self.pil:\n# Convert to numpy first\nself.im = np.asarray(self.im).copy()\nif len(masks) == 0:\nself.im[:] = im_gpu.permute(1, 2, 0).contiguous().cpu().numpy() * 255\nif im_gpu.device != masks.device:\nim_gpu = im_gpu.to(masks.device)\ncolors = torch.tensor(colors, device=masks.device, dtype=torch.float32) / 255.0  # shape(n,3)\ncolors = colors[:, None, None]  # shape(n,1,1,3)\nmasks = masks.unsqueeze(3)  # shape(n,h,w,1)\nmasks_color = masks * (colors * alpha)  # shape(n,h,w,3)\ninv_alph_masks = (1 - masks * alpha).cumprod(0)  # shape(n,h,w,1)\nmcs = masks_color.max(dim=0).values  # shape(n,h,w,3)\nim_gpu = im_gpu.flip(dims=[0])  # flip channel\nim_gpu = im_gpu.permute(1, 2, 0).contiguous()  # shape(h,w,3)\nim_gpu = im_gpu * inv_alph_masks[-1] + mcs\nim_mask = (im_gpu * 255)\nim_mask_np = im_mask.byte().cpu().numpy()\nself.im[:] = im_mask_np if retina_masks else scale_image(im_mask_np, self.im.shape)\nif self.pil:\n# Convert im back to PIL and update draw\nself.fromarray(self.im)\ndef kpts(self, kpts, shape=(640, 640), radius=5, kpt_line=True):\n\"\"\"\n        Plot keypoints on the image.\n        Args:\n            kpts (tensor): Predicted keypoints with shape [17, 3]. Each keypoint has (x, y, confidence).\n            shape (tuple): Image shape as a tuple (h, w), where h is the height and w is the width.\n            radius (int, optional): Radius of the drawn keypoints. Default is 5.\n            kpt_line (bool, optional): If True, the function will draw lines connecting keypoints\n                                       for human pose. Default is True.\n        Note: `kpt_line=True` currently only supports human pose plotting.\n        \"\"\"\nif self.pil:\n# Convert to numpy first\nself.im = np.asarray(self.im).copy()\nnkpt, ndim = kpts.shape\nis_pose = nkpt == 17 and ndim == 3\nkpt_line &amp;= is_pose  # `kpt_line=True` for now only supports human pose plotting\nfor i, k in enumerate(kpts):\ncolor_k = [int(x) for x in self.kpt_color[i]] if is_pose else colors(i)\nx_coord, y_coord = k[0], k[1]\nif x_coord % shape[1] != 0 and y_coord % shape[0] != 0:\nif len(k) == 3:\nconf = k[2]\nif conf &lt; 0.5:\ncontinue\ncv2.circle(self.im, (int(x_coord), int(y_coord)), radius, color_k, -1, lineType=cv2.LINE_AA)\nif kpt_line:\nndim = kpts.shape[-1]\nfor i, sk in enumerate(self.skeleton):\npos1 = (int(kpts[(sk[0] - 1), 0]), int(kpts[(sk[0] - 1), 1]))\npos2 = (int(kpts[(sk[1] - 1), 0]), int(kpts[(sk[1] - 1), 1]))\nif ndim == 3:\nconf1 = kpts[(sk[0] - 1), 2]\nconf2 = kpts[(sk[1] - 1), 2]\nif conf1 &lt; 0.5 or conf2 &lt; 0.5:\ncontinue\nif pos1[0] % shape[1] == 0 or pos1[1] % shape[0] == 0 or pos1[0] &lt; 0 or pos1[1] &lt; 0:\ncontinue\nif pos2[0] % shape[1] == 0 or pos2[1] % shape[0] == 0 or pos2[0] &lt; 0 or pos2[1] &lt; 0:\ncontinue\ncv2.line(self.im, pos1, pos2, [int(x) for x in self.limb_color[i]], thickness=2, lineType=cv2.LINE_AA)\nif self.pil:\n# Convert im back to PIL and update draw\nself.fromarray(self.im)\ndef rectangle(self, xy, fill=None, outline=None, width=1):\n\"\"\"Add rectangle to image (PIL-only).\"\"\"\nself.draw.rectangle(xy, fill, outline, width)\ndef text(self, xy, text, txt_color=(255, 255, 255), anchor='top', box_style=False):\n\"\"\"Adds text to an image using PIL or cv2.\"\"\"\nif anchor == 'bottom':  # start y from font bottom\nw, h = self.font.getsize(text)  # text width, height\nxy[1] += 1 - h\nif self.pil:\nif box_style:\nw, h = self.font.getsize(text)\nself.draw.rectangle((xy[0], xy[1], xy[0] + w + 1, xy[1] + h + 1), fill=txt_color)\n# Using `txt_color` for background and draw fg with white color\ntxt_color = (255, 255, 255)\nif '\\n' in text:\nlines = text.split('\\n')\n_, h = self.font.getsize(text)\nfor line in lines:\nself.draw.text(xy, line, fill=txt_color, font=self.font)\nxy[1] += h\nelse:\nself.draw.text(xy, text, fill=txt_color, font=self.font)\nelse:\nif box_style:\ntf = max(self.lw - 1, 1)  # font thickness\nw, h = cv2.getTextSize(text, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\noutside = xy[1] - h &gt;= 3\np2 = xy[0] + w, xy[1] - h - 3 if outside else xy[1] + h + 3\ncv2.rectangle(self.im, xy, p2, txt_color, -1, cv2.LINE_AA)  # filled\n# Using `txt_color` for background and draw fg with white color\ntxt_color = (255, 255, 255)\ntf = max(self.lw - 1, 1)  # font thickness\ncv2.putText(self.im, text, xy, 0, self.lw / 3, txt_color, thickness=tf, lineType=cv2.LINE_AA)\ndef fromarray(self, im):\n\"\"\"Update self.im from a numpy array.\"\"\"\nself.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\nself.draw = ImageDraw.Draw(self.im)\ndef result(self):\n\"\"\"Return annotated image as array.\"\"\"\nreturn np.asarray(self.im)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.__init__","title":"<code>__init__(im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc')</code>","text":"<p>Initialize the Annotator class with image and line width along with color palette for keypoints and limbs.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):\n\"\"\"Initialize the Annotator class with image and line width along with color palette for keypoints and limbs.\"\"\"\nassert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\nnon_ascii = not is_ascii(example)  # non-latin labels, i.e. asian, arabic, cyrillic\nself.pil = pil or non_ascii\nif self.pil:  # use PIL\nself.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\nself.draw = ImageDraw.Draw(self.im)\ntry:\nfont = check_font('Arial.Unicode.ttf' if non_ascii else font)\nsize = font_size or max(round(sum(self.im.size) / 2 * 0.035), 12)\nself.font = ImageFont.truetype(str(font), size)\nexcept Exception:\nself.font = ImageFont.load_default()\n# Deprecation fix for w, h = getsize(string) -&gt; _, _, w, h = getbox(string)\nif check_version(pil_version, '9.2.0'):\nself.font.getsize = lambda x: self.font.getbbox(x)[2:4]  # text width, height\nelse:  # use cv2\nself.im = im\nself.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n# Pose\nself.skeleton = [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n[8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]]\nself.limb_color = colors.pose_palette[[9, 9, 9, 9, 7, 7, 7, 0, 0, 0, 0, 0, 16, 16, 16, 16, 16, 16, 16]]\nself.kpt_color = colors.pose_palette[[16, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9]]\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.box_label","title":"<code>box_label(box, label='', color=(128, 128, 128), txt_color=(255, 255, 255))</code>","text":"<p>Add one xyxy box to image with label.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n\"\"\"Add one xyxy box to image with label.\"\"\"\nif isinstance(box, torch.Tensor):\nbox = box.tolist()\nif self.pil or not is_ascii(label):\nself.draw.rectangle(box, width=self.lw, outline=color)  # box\nif label:\nw, h = self.font.getsize(label)  # text width, height\noutside = box[1] - h &gt;= 0  # label fits outside box\nself.draw.rectangle(\n(box[0], box[1] - h if outside else box[1], box[0] + w + 1,\nbox[1] + 1 if outside else box[1] + h + 1),\nfill=color,\n)\n# self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL&gt;8.0\nself.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\nelse:  # cv2\np1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\ncv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\nif label:\ntf = max(self.lw - 1, 1)  # font thickness\nw, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\noutside = p1[1] - h &gt;= 3\np2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\ncv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\ncv2.putText(self.im,\nlabel, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n0,\nself.lw / 3,\ntxt_color,\nthickness=tf,\nlineType=cv2.LINE_AA)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.fromarray","title":"<code>fromarray(im)</code>","text":"<p>Update self.im from a numpy array.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def fromarray(self, im):\n\"\"\"Update self.im from a numpy array.\"\"\"\nself.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\nself.draw = ImageDraw.Draw(self.im)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.kpts","title":"<code>kpts(kpts, shape=(640, 640), radius=5, kpt_line=True)</code>","text":"<p>Plot keypoints on the image.</p> <p>Parameters:</p> Name Type Description Default <code>kpts</code> <code>tensor</code> <p>Predicted keypoints with shape [17, 3]. Each keypoint has (x, y, confidence).</p> required <code>shape</code> <code>tuple</code> <p>Image shape as a tuple (h, w), where h is the height and w is the width.</p> <code>(640, 640)</code> <code>radius</code> <code>int</code> <p>Radius of the drawn keypoints. Default is 5.</p> <code>5</code> <code>kpt_line</code> <code>bool</code> <p>If True, the function will draw lines connecting keypoints                        for human pose. Default is True.</p> <code>True</code> <p>Note: <code>kpt_line=True</code> currently only supports human pose plotting.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def kpts(self, kpts, shape=(640, 640), radius=5, kpt_line=True):\n\"\"\"\n    Plot keypoints on the image.\n    Args:\n        kpts (tensor): Predicted keypoints with shape [17, 3]. Each keypoint has (x, y, confidence).\n        shape (tuple): Image shape as a tuple (h, w), where h is the height and w is the width.\n        radius (int, optional): Radius of the drawn keypoints. Default is 5.\n        kpt_line (bool, optional): If True, the function will draw lines connecting keypoints\n                                   for human pose. Default is True.\n    Note: `kpt_line=True` currently only supports human pose plotting.\n    \"\"\"\nif self.pil:\n# Convert to numpy first\nself.im = np.asarray(self.im).copy()\nnkpt, ndim = kpts.shape\nis_pose = nkpt == 17 and ndim == 3\nkpt_line &amp;= is_pose  # `kpt_line=True` for now only supports human pose plotting\nfor i, k in enumerate(kpts):\ncolor_k = [int(x) for x in self.kpt_color[i]] if is_pose else colors(i)\nx_coord, y_coord = k[0], k[1]\nif x_coord % shape[1] != 0 and y_coord % shape[0] != 0:\nif len(k) == 3:\nconf = k[2]\nif conf &lt; 0.5:\ncontinue\ncv2.circle(self.im, (int(x_coord), int(y_coord)), radius, color_k, -1, lineType=cv2.LINE_AA)\nif kpt_line:\nndim = kpts.shape[-1]\nfor i, sk in enumerate(self.skeleton):\npos1 = (int(kpts[(sk[0] - 1), 0]), int(kpts[(sk[0] - 1), 1]))\npos2 = (int(kpts[(sk[1] - 1), 0]), int(kpts[(sk[1] - 1), 1]))\nif ndim == 3:\nconf1 = kpts[(sk[0] - 1), 2]\nconf2 = kpts[(sk[1] - 1), 2]\nif conf1 &lt; 0.5 or conf2 &lt; 0.5:\ncontinue\nif pos1[0] % shape[1] == 0 or pos1[1] % shape[0] == 0 or pos1[0] &lt; 0 or pos1[1] &lt; 0:\ncontinue\nif pos2[0] % shape[1] == 0 or pos2[1] % shape[0] == 0 or pos2[0] &lt; 0 or pos2[1] &lt; 0:\ncontinue\ncv2.line(self.im, pos1, pos2, [int(x) for x in self.limb_color[i]], thickness=2, lineType=cv2.LINE_AA)\nif self.pil:\n# Convert im back to PIL and update draw\nself.fromarray(self.im)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.masks","title":"<code>masks(masks, colors, im_gpu, alpha=0.5, retina_masks=False)</code>","text":"<p>Plot masks on image.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>tensor</code> <p>Predicted masks on cuda, shape: [n, h, w]</p> required <code>colors</code> <code>List[List[Int]]</code> <p>Colors for predicted masks, [[r, g, b] * n]</p> required <code>im_gpu</code> <code>tensor</code> <p>Image is in cuda, shape: [3, h, w], range: [0, 1]</p> required <code>alpha</code> <code>float</code> <p>Mask transparency: 0.0 fully transparent, 1.0 opaque</p> <code>0.5</code> <code>retina_masks</code> <code>bool</code> <p>Whether to use high resolution masks or not. Defaults to False.</p> <code>False</code> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def masks(self, masks, colors, im_gpu, alpha=0.5, retina_masks=False):\n\"\"\"\n    Plot masks on image.\n    Args:\n        masks (tensor): Predicted masks on cuda, shape: [n, h, w]\n        colors (List[List[Int]]): Colors for predicted masks, [[r, g, b] * n]\n        im_gpu (tensor): Image is in cuda, shape: [3, h, w], range: [0, 1]\n        alpha (float): Mask transparency: 0.0 fully transparent, 1.0 opaque\n        retina_masks (bool): Whether to use high resolution masks or not. Defaults to False.\n    \"\"\"\nif self.pil:\n# Convert to numpy first\nself.im = np.asarray(self.im).copy()\nif len(masks) == 0:\nself.im[:] = im_gpu.permute(1, 2, 0).contiguous().cpu().numpy() * 255\nif im_gpu.device != masks.device:\nim_gpu = im_gpu.to(masks.device)\ncolors = torch.tensor(colors, device=masks.device, dtype=torch.float32) / 255.0  # shape(n,3)\ncolors = colors[:, None, None]  # shape(n,1,1,3)\nmasks = masks.unsqueeze(3)  # shape(n,h,w,1)\nmasks_color = masks * (colors * alpha)  # shape(n,h,w,3)\ninv_alph_masks = (1 - masks * alpha).cumprod(0)  # shape(n,h,w,1)\nmcs = masks_color.max(dim=0).values  # shape(n,h,w,3)\nim_gpu = im_gpu.flip(dims=[0])  # flip channel\nim_gpu = im_gpu.permute(1, 2, 0).contiguous()  # shape(h,w,3)\nim_gpu = im_gpu * inv_alph_masks[-1] + mcs\nim_mask = (im_gpu * 255)\nim_mask_np = im_mask.byte().cpu().numpy()\nself.im[:] = im_mask_np if retina_masks else scale_image(im_mask_np, self.im.shape)\nif self.pil:\n# Convert im back to PIL and update draw\nself.fromarray(self.im)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.rectangle","title":"<code>rectangle(xy, fill=None, outline=None, width=1)</code>","text":"<p>Add rectangle to image (PIL-only).</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def rectangle(self, xy, fill=None, outline=None, width=1):\n\"\"\"Add rectangle to image (PIL-only).\"\"\"\nself.draw.rectangle(xy, fill, outline, width)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.result","title":"<code>result()</code>","text":"<p>Return annotated image as array.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def result(self):\n\"\"\"Return annotated image as array.\"\"\"\nreturn np.asarray(self.im)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.Annotator.text","title":"<code>text(xy, text, txt_color=(255, 255, 255), anchor='top', box_style=False)</code>","text":"<p>Adds text to an image using PIL or cv2.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def text(self, xy, text, txt_color=(255, 255, 255), anchor='top', box_style=False):\n\"\"\"Adds text to an image using PIL or cv2.\"\"\"\nif anchor == 'bottom':  # start y from font bottom\nw, h = self.font.getsize(text)  # text width, height\nxy[1] += 1 - h\nif self.pil:\nif box_style:\nw, h = self.font.getsize(text)\nself.draw.rectangle((xy[0], xy[1], xy[0] + w + 1, xy[1] + h + 1), fill=txt_color)\n# Using `txt_color` for background and draw fg with white color\ntxt_color = (255, 255, 255)\nif '\\n' in text:\nlines = text.split('\\n')\n_, h = self.font.getsize(text)\nfor line in lines:\nself.draw.text(xy, line, fill=txt_color, font=self.font)\nxy[1] += h\nelse:\nself.draw.text(xy, text, fill=txt_color, font=self.font)\nelse:\nif box_style:\ntf = max(self.lw - 1, 1)  # font thickness\nw, h = cv2.getTextSize(text, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\noutside = xy[1] - h &gt;= 3\np2 = xy[0] + w, xy[1] - h - 3 if outside else xy[1] + h + 3\ncv2.rectangle(self.im, xy, p2, txt_color, -1, cv2.LINE_AA)  # filled\n# Using `txt_color` for background and draw fg with white color\ntxt_color = (255, 255, 255)\ntf = max(self.lw - 1, 1)  # font thickness\ncv2.putText(self.im, text, xy, 0, self.lw / 3, txt_color, thickness=tf, lineType=cv2.LINE_AA)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.plot_labels","title":"<code>ultralytics.utils.plotting.plot_labels(boxes, cls, names=(), save_dir=Path(''), on_plot=None)</code>","text":"<p>Plot training labels including class histograms and box statistics.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>@TryExcept()  # known issue https://github.com/ultralytics/yolov5/issues/5395\n@plt_settings()\ndef plot_labels(boxes, cls, names=(), save_dir=Path(''), on_plot=None):\n\"\"\"Plot training labels including class histograms and box statistics.\"\"\"\nimport pandas as pd\nimport seaborn as sn\n# Filter matplotlib&gt;=3.7.2 warning\nwarnings.filterwarnings('ignore', category=UserWarning, message='The figure layout has changed to tight')\n# Plot dataset labels\nLOGGER.info(f\"Plotting labels to {save_dir / 'labels.jpg'}... \")\nnc = int(cls.max() + 1)  # number of classes\nboxes = boxes[:1000000]  # limit to 1M boxes\nx = pd.DataFrame(boxes, columns=['x', 'y', 'width', 'height'])\n# Seaborn correlogram\nsn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\nplt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\nplt.close()\n# Matplotlib labels\nax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)[1].ravel()\ny = ax[0].hist(cls, bins=np.linspace(0, nc, nc + 1) - 0.5, rwidth=0.8)\nwith contextlib.suppress(Exception):  # color histogram bars by class\n[y[2].patches[i].set_color([x / 255 for x in colors(i)]) for i in range(nc)]  # known issue #3195\nax[0].set_ylabel('instances')\nif 0 &lt; len(names) &lt; 30:\nax[0].set_xticks(range(len(names)))\nax[0].set_xticklabels(list(names.values()), rotation=90, fontsize=10)\nelse:\nax[0].set_xlabel('classes')\nsn.histplot(x, x='x', y='y', ax=ax[2], bins=50, pmax=0.9)\nsn.histplot(x, x='width', y='height', ax=ax[3], bins=50, pmax=0.9)\n# Rectangles\nboxes[:, 0:2] = 0.5  # center\nboxes = xywh2xyxy(boxes) * 1000\nimg = Image.fromarray(np.ones((1000, 1000, 3), dtype=np.uint8) * 255)\nfor cls, box in zip(cls[:500], boxes[:500]):\nImageDraw.Draw(img).rectangle(box, width=1, outline=colors(cls))  # plot\nax[1].imshow(img)\nax[1].axis('off')\nfor a in [0, 1, 2, 3]:\nfor s in ['top', 'right', 'left', 'bottom']:\nax[a].spines[s].set_visible(False)\nfname = save_dir / 'labels.jpg'\nplt.savefig(fname, dpi=200)\nplt.close()\nif on_plot:\non_plot(fname)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.save_one_box","title":"<code>ultralytics.utils.plotting.save_one_box(xyxy, im, file=Path('im.jpg'), gain=1.02, pad=10, square=False, BGR=False, save=True)</code>","text":"<p>Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop.</p> <p>This function takes a bounding box and an image, and then saves a cropped portion of the image according to the bounding box. Optionally, the crop can be squared, and the function allows for gain and padding adjustments to the bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>Tensor or list</code> <p>A tensor or list representing the bounding box in xyxy format.</p> required <code>im</code> <code>ndarray</code> <p>The input image.</p> required <code>file</code> <code>Path</code> <p>The path where the cropped image will be saved. Defaults to 'im.jpg'.</p> <code>Path('im.jpg')</code> <code>gain</code> <code>float</code> <p>A multiplicative factor to increase the size of the bounding box. Defaults to 1.02.</p> <code>1.02</code> <code>pad</code> <code>int</code> <p>The number of pixels to add to the width and height of the bounding box. Defaults to 10.</p> <code>10</code> <code>square</code> <code>bool</code> <p>If True, the bounding box will be transformed into a square. Defaults to False.</p> <code>False</code> <code>BGR</code> <code>bool</code> <p>If True, the image will be saved in BGR format, otherwise in RGB. Defaults to False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>If True, the cropped image will be saved to disk. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The cropped image.</p> Example <pre><code>from ultralytics.utils.plotting import save_one_box\nxyxy = [50, 50, 150, 150]\nim = cv2.imread('image.jpg')\ncropped_im = save_one_box(xyxy, im, file='cropped.jpg', square=True)\n</code></pre> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def save_one_box(xyxy, im, file=Path('im.jpg'), gain=1.02, pad=10, square=False, BGR=False, save=True):\n\"\"\"Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop.\n    This function takes a bounding box and an image, and then saves a cropped portion of the image according\n    to the bounding box. Optionally, the crop can be squared, and the function allows for gain and padding\n    adjustments to the bounding box.\n    Args:\n        xyxy (torch.Tensor or list): A tensor or list representing the bounding box in xyxy format.\n        im (numpy.ndarray): The input image.\n        file (Path, optional): The path where the cropped image will be saved. Defaults to 'im.jpg'.\n        gain (float, optional): A multiplicative factor to increase the size of the bounding box. Defaults to 1.02.\n        pad (int, optional): The number of pixels to add to the width and height of the bounding box. Defaults to 10.\n        square (bool, optional): If True, the bounding box will be transformed into a square. Defaults to False.\n        BGR (bool, optional): If True, the image will be saved in BGR format, otherwise in RGB. Defaults to False.\n        save (bool, optional): If True, the cropped image will be saved to disk. Defaults to True.\n    Returns:\n        (numpy.ndarray): The cropped image.\n    Example:\n        ```python\n        from ultralytics.utils.plotting import save_one_box\n        xyxy = [50, 50, 150, 150]\n        im = cv2.imread('image.jpg')\n        cropped_im = save_one_box(xyxy, im, file='cropped.jpg', square=True)\n        ```\n    \"\"\"\nif not isinstance(xyxy, torch.Tensor):  # may be list\nxyxy = torch.stack(xyxy)\nb = xyxy2xywh(xyxy.view(-1, 4))  # boxes\nif square:\nb[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\nb[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\nxyxy = xywh2xyxy(b).long()\nclip_boxes(xyxy, im.shape)\ncrop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\nif save:\nfile.parent.mkdir(parents=True, exist_ok=True)  # make directory\nf = str(increment_path(file).with_suffix('.jpg'))\n# cv2.imwrite(f, crop)  # save BGR, https://github.com/ultralytics/yolov5/issues/7007 chroma subsampling issue\nImage.fromarray(crop[..., ::-1]).save(f, quality=95, subsampling=0)  # save RGB\nreturn crop\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.plot_images","title":"<code>ultralytics.utils.plotting.plot_images(images, batch_idx, cls, bboxes=np.zeros(0, dtype=np.float32), masks=np.zeros(0, dtype=np.uint8), kpts=np.zeros((0, 51), dtype=np.float32), paths=None, fname='images.jpg', names=None, on_plot=None)</code>","text":"<p>Plot image grid with labels.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>@threaded\ndef plot_images(images,\nbatch_idx,\ncls,\nbboxes=np.zeros(0, dtype=np.float32),\nmasks=np.zeros(0, dtype=np.uint8),\nkpts=np.zeros((0, 51), dtype=np.float32),\npaths=None,\nfname='images.jpg',\nnames=None,\non_plot=None):\n\"\"\"Plot image grid with labels.\"\"\"\nif isinstance(images, torch.Tensor):\nimages = images.cpu().float().numpy()\nif isinstance(cls, torch.Tensor):\ncls = cls.cpu().numpy()\nif isinstance(bboxes, torch.Tensor):\nbboxes = bboxes.cpu().numpy()\nif isinstance(masks, torch.Tensor):\nmasks = masks.cpu().numpy().astype(int)\nif isinstance(kpts, torch.Tensor):\nkpts = kpts.cpu().numpy()\nif isinstance(batch_idx, torch.Tensor):\nbatch_idx = batch_idx.cpu().numpy()\nmax_size = 1920  # max image size\nmax_subplots = 16  # max image subplots, i.e. 4x4\nbs, _, h, w = images.shape  # batch size, _, height, width\nbs = min(bs, max_subplots)  # limit plot images\nns = np.ceil(bs ** 0.5)  # number of subplots (square)\nif np.max(images[0]) &lt;= 1:\nimages *= 255  # de-normalise (optional)\n# Build Image\nmosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\nfor i, im in enumerate(images):\nif i == max_subplots:  # if last batch has fewer images than we expect\nbreak\nx, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\nim = im.transpose(1, 2, 0)\nmosaic[y:y + h, x:x + w, :] = im\n# Resize (optional)\nscale = max_size / ns / max(h, w)\nif scale &lt; 1:\nh = math.ceil(scale * h)\nw = math.ceil(scale * w)\nmosaic = cv2.resize(mosaic, tuple(int(x * ns) for x in (w, h)))\n# Annotate\nfs = int((h + w) * ns * 0.01)  # font size\nannotator = Annotator(mosaic, line_width=round(fs / 10), font_size=fs, pil=True, example=names)\nfor i in range(i + 1):\nx, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\nannotator.rectangle([x, y, x + w, y + h], None, (255, 255, 255), width=2)  # borders\nif paths:\nannotator.text((x + 5, y + 5), text=Path(paths[i]).name[:40], txt_color=(220, 220, 220))  # filenames\nif len(cls) &gt; 0:\nidx = batch_idx == i\nclasses = cls[idx].astype('int')\nif len(bboxes):\nboxes = xywh2xyxy(bboxes[idx, :4]).T\nlabels = bboxes.shape[1] == 4  # labels if no conf column\nconf = None if labels else bboxes[idx, 4]  # check for confidence presence (label vs pred)\nif boxes.shape[1]:\nif boxes.max() &lt;= 1.01:  # if normalized with tolerance 0.01\nboxes[[0, 2]] *= w  # scale to pixels\nboxes[[1, 3]] *= h\nelif scale &lt; 1:  # absolute coords need scale if image scales\nboxes *= scale\nboxes[[0, 2]] += x\nboxes[[1, 3]] += y\nfor j, box in enumerate(boxes.T.tolist()):\nc = classes[j]\ncolor = colors(c)\nc = names.get(c, c) if names else c\nif labels or conf[j] &gt; 0.25:  # 0.25 conf thresh\nlabel = f'{c}' if labels else f'{c} {conf[j]:.1f}'\nannotator.box_label(box, label, color=color)\nelif len(classes):\nfor c in classes:\ncolor = colors(c)\nc = names.get(c, c) if names else c\nannotator.text((x, y), f'{c}', txt_color=color, box_style=True)\n# Plot keypoints\nif len(kpts):\nkpts_ = kpts[idx].copy()\nif len(kpts_):\nif kpts_[..., 0].max() &lt;= 1.01 or kpts_[..., 1].max() &lt;= 1.01:  # if normalized with tolerance .01\nkpts_[..., 0] *= w  # scale to pixels\nkpts_[..., 1] *= h\nelif scale &lt; 1:  # absolute coords need scale if image scales\nkpts_ *= scale\nkpts_[..., 0] += x\nkpts_[..., 1] += y\nfor j in range(len(kpts_)):\nif labels or conf[j] &gt; 0.25:  # 0.25 conf thresh\nannotator.kpts(kpts_[j])\n# Plot masks\nif len(masks):\nif idx.shape[0] == masks.shape[0]:  # overlap_masks=False\nimage_masks = masks[idx]\nelse:  # overlap_masks=True\nimage_masks = masks[[i]]  # (1, 640, 640)\nnl = idx.sum()\nindex = np.arange(nl).reshape((nl, 1, 1)) + 1\nimage_masks = np.repeat(image_masks, nl, axis=0)\nimage_masks = np.where(image_masks == index, 1.0, 0.0)\nim = np.asarray(annotator.im).copy()\nfor j, box in enumerate(boxes.T.tolist()):\nif labels or conf[j] &gt; 0.25:  # 0.25 conf thresh\ncolor = colors(classes[j])\nmh, mw = image_masks[j].shape\nif mh != h or mw != w:\nmask = image_masks[j].astype(np.uint8)\nmask = cv2.resize(mask, (w, h))\nmask = mask.astype(bool)\nelse:\nmask = image_masks[j].astype(bool)\nwith contextlib.suppress(Exception):\nim[y:y + h, x:x + w, :][mask] = im[y:y + h, x:x + w, :][mask] * 0.4 + np.array(color) * 0.6\nannotator.fromarray(im)\nannotator.im.save(fname)  # save\nif on_plot:\non_plot(fname)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.plot_results","title":"<code>ultralytics.utils.plotting.plot_results(file='path/to/results.csv', dir='', segment=False, pose=False, classify=False, on_plot=None)</code>","text":"<p>Plot training results from results CSV file.</p> Example <pre><code>from ultralytics.utils.plotting import plot_results\nplot_results('path/to/results.csv')\n</code></pre> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>@plt_settings()\ndef plot_results(file='path/to/results.csv', dir='', segment=False, pose=False, classify=False, on_plot=None):\n\"\"\"\n    Plot training results from results CSV file.\n    Example:\n        ```python\n        from ultralytics.utils.plotting import plot_results\n        plot_results('path/to/results.csv')\n        ```\n    \"\"\"\nimport pandas as pd\nfrom scipy.ndimage import gaussian_filter1d\nsave_dir = Path(file).parent if file else Path(dir)\nif classify:\nfig, ax = plt.subplots(2, 2, figsize=(6, 6), tight_layout=True)\nindex = [1, 4, 2, 3]\nelif segment:\nfig, ax = plt.subplots(2, 8, figsize=(18, 6), tight_layout=True)\nindex = [1, 2, 3, 4, 5, 6, 9, 10, 13, 14, 15, 16, 7, 8, 11, 12]\nelif pose:\nfig, ax = plt.subplots(2, 9, figsize=(21, 6), tight_layout=True)\nindex = [1, 2, 3, 4, 5, 6, 7, 10, 11, 14, 15, 16, 17, 18, 8, 9, 12, 13]\nelse:\nfig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\nindex = [1, 2, 3, 4, 5, 8, 9, 10, 6, 7]\nax = ax.ravel()\nfiles = list(save_dir.glob('results*.csv'))\nassert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\nfor f in files:\ntry:\ndata = pd.read_csv(f)\ns = [x.strip() for x in data.columns]\nx = data.values[:, 0]\nfor i, j in enumerate(index):\ny = data.values[:, j].astype('float')\n# y[y == 0] = np.nan  # don't show zero values\nax[i].plot(x, y, marker='.', label=f.stem, linewidth=2, markersize=8)  # actual results\nax[i].plot(x, gaussian_filter1d(y, sigma=3), ':', label='smooth', linewidth=2)  # smoothing line\nax[i].set_title(s[j], fontsize=12)\n# if j in [8, 9, 10]:  # share train and val loss y axes\n#     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\nexcept Exception as e:\nLOGGER.warning(f'WARNING: Plotting error for {f}: {e}')\nax[1].legend()\nfname = save_dir / 'results.png'\nfig.savefig(fname, dpi=200)\nplt.close()\nif on_plot:\non_plot(fname)\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.output_to_target","title":"<code>ultralytics.utils.plotting.output_to_target(output, max_det=300)</code>","text":"<p>Convert model output to target format [batch_id, class_id, x, y, w, h, conf] for plotting.</p> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def output_to_target(output, max_det=300):\n\"\"\"Convert model output to target format [batch_id, class_id, x, y, w, h, conf] for plotting.\"\"\"\ntargets = []\nfor i, o in enumerate(output):\nbox, conf, cls = o[:max_det, :6].cpu().split((4, 1, 1), 1)\nj = torch.full((conf.shape[0], 1), i)\ntargets.append(torch.cat((j, cls, xyxy2xywh(box), conf), 1))\ntargets = torch.cat(targets, 0).numpy()\nreturn targets[:, 0], targets[:, 1], targets[:, 2:]\n</code></pre>"},{"location":"reference/utils/plotting/#ultralytics.utils.plotting.feature_visualization","title":"<code>ultralytics.utils.plotting.feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp'))</code>","text":"<p>Visualize feature maps of a given model module during inference.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Features to be visualized.</p> required <code>module_type</code> <code>str</code> <p>Module type.</p> required <code>stage</code> <code>int</code> <p>Module stage within the model.</p> required <code>n</code> <code>int</code> <p>Maximum number of feature maps to plot. Defaults to 32.</p> <code>32</code> <code>save_dir</code> <code>Path</code> <p>Directory to save results. Defaults to Path('runs/detect/exp').</p> <code>Path('runs/detect/exp')</code> Source code in <code>ultralytics/utils/plotting.py</code> <pre><code>def feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n\"\"\"\n    Visualize feature maps of a given model module during inference.\n    Args:\n        x (torch.Tensor): Features to be visualized.\n        module_type (str): Module type.\n        stage (int): Module stage within the model.\n        n (int, optional): Maximum number of feature maps to plot. Defaults to 32.\n        save_dir (Path, optional): Directory to save results. Defaults to Path('runs/detect/exp').\n    \"\"\"\nfor m in ['Detect', 'Pose', 'Segment']:\nif m in module_type:\nreturn\nbatch, channels, height, width = x.shape  # batch, channels, height, width\nif height &gt; 1 and width &gt; 1:\nf = save_dir / f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename\nblocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels\nn = min(n, channels)  # number of plots\nfig, ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)  # 8 rows x n/8 cols\nax = ax.ravel()\nplt.subplots_adjust(wspace=0.05, hspace=0.05)\nfor i in range(n):\nax[i].imshow(blocks[i].squeeze())  # cmap='gray'\nax[i].axis('off')\nLOGGER.info(f'Saving {f}... ({n}/{channels})')\nplt.savefig(f, dpi=300, bbox_inches='tight')\nplt.close()\nnp.save(str(f.with_suffix('.npy')), x[0].cpu().numpy())  # npy save\n</code></pre>"},{"location":"reference/utils/tal/","title":"Reference for <code>ultralytics/utils/tal.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/tal.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner","title":"<code>ultralytics.utils.tal.TaskAlignedAssigner</code>","text":"<p>             Bases: <code>Module</code></p> <p>A task-aligned assigner for object detection.</p> <p>This class assigns ground-truth (gt) objects to anchors based on the task-aligned metric, which combines both classification and localization information.</p> <p>Attributes:</p> Name Type Description <code>topk</code> <code>int</code> <p>The number of top candidates to consider.</p> <code>num_classes</code> <code>int</code> <p>The number of object classes.</p> <code>alpha</code> <code>float</code> <p>The alpha parameter for the classification component of the task-aligned metric.</p> <code>beta</code> <code>float</code> <p>The beta parameter for the localization component of the task-aligned metric.</p> <code>eps</code> <code>float</code> <p>A small value to prevent division by zero.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>class TaskAlignedAssigner(nn.Module):\n\"\"\"\n    A task-aligned assigner for object detection.\n    This class assigns ground-truth (gt) objects to anchors based on the task-aligned metric,\n    which combines both classification and localization information.\n    Attributes:\n        topk (int): The number of top candidates to consider.\n        num_classes (int): The number of object classes.\n        alpha (float): The alpha parameter for the classification component of the task-aligned metric.\n        beta (float): The beta parameter for the localization component of the task-aligned metric.\n        eps (float): A small value to prevent division by zero.\n    \"\"\"\ndef __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9):\n\"\"\"Initialize a TaskAlignedAssigner object with customizable hyperparameters.\"\"\"\nsuper().__init__()\nself.topk = topk\nself.num_classes = num_classes\nself.bg_idx = num_classes\nself.alpha = alpha\nself.beta = beta\nself.eps = eps\n@torch.no_grad()\ndef forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\"\"\"\n        Compute the task-aligned assignment.\n        Reference https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py\n        Args:\n            pd_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n            pd_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n            anc_points (Tensor): shape(num_total_anchors, 2)\n            gt_labels (Tensor): shape(bs, n_max_boxes, 1)\n            gt_bboxes (Tensor): shape(bs, n_max_boxes, 4)\n            mask_gt (Tensor): shape(bs, n_max_boxes, 1)\n        Returns:\n            target_labels (Tensor): shape(bs, num_total_anchors)\n            target_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n            target_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n            fg_mask (Tensor): shape(bs, num_total_anchors)\n            target_gt_idx (Tensor): shape(bs, num_total_anchors)\n        \"\"\"\nself.bs = pd_scores.size(0)\nself.n_max_boxes = gt_bboxes.size(1)\nif self.n_max_boxes == 0:\ndevice = gt_bboxes.device\nreturn (torch.full_like(pd_scores[..., 0], self.bg_idx).to(device), torch.zeros_like(pd_bboxes).to(device),\ntorch.zeros_like(pd_scores).to(device), torch.zeros_like(pd_scores[..., 0]).to(device),\ntorch.zeros_like(pd_scores[..., 0]).to(device))\nmask_pos, align_metric, overlaps = self.get_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points,\nmask_gt)\ntarget_gt_idx, fg_mask, mask_pos = select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n# Assigned target\ntarget_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n# Normalize\nalign_metric *= mask_pos\npos_align_metrics = align_metric.amax(axis=-1, keepdim=True)  # b, max_num_obj\npos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)  # b, max_num_obj\nnorm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\ntarget_scores = target_scores * norm_align_metric\nreturn target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\ndef get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\"\"\"Get in_gts mask, (b, max_num_obj, h*w).\"\"\"\nmask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes)\n# Get anchor_align metric, (b, max_num_obj, h*w)\nalign_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n# Get topk_metric mask, (b, max_num_obj, h*w)\nmask_topk = self.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-1, -1, self.topk).bool())\n# Merge all mask to a final mask, (b, max_num_obj, h*w)\nmask_pos = mask_topk * mask_in_gts * mask_gt\nreturn mask_pos, align_metric, overlaps\ndef get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt):\n\"\"\"Compute alignment metric given predicted and ground truth bounding boxes.\"\"\"\nna = pd_bboxes.shape[-2]\nmask_gt = mask_gt.bool()  # b, max_num_obj, h*w\noverlaps = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\nbbox_scores = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\nind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\nind[0] = torch.arange(end=self.bs).view(-1, 1).expand(-1, self.n_max_boxes)  # b, max_num_obj\nind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n# Get the scores of each grid for each gt cls\nbbox_scores[mask_gt] = pd_scores[ind[0], :, ind[1]][mask_gt]  # b, max_num_obj, h*w\n# (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\npd_boxes = pd_bboxes.unsqueeze(1).expand(-1, self.n_max_boxes, -1, -1)[mask_gt]\ngt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[mask_gt]\noverlaps[mask_gt] = bbox_iou(gt_boxes, pd_boxes, xywh=False, CIoU=True).squeeze(-1).clamp_(0)\nalign_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\nreturn align_metric, overlaps\ndef select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n\"\"\"\n        Select the top-k candidates based on the given metrics.\n        Args:\n            metrics (Tensor): A tensor of shape (b, max_num_obj, h*w), where b is the batch size,\n                              max_num_obj is the maximum number of objects, and h*w represents the\n                              total number of anchor points.\n            largest (bool): If True, select the largest values; otherwise, select the smallest values.\n            topk_mask (Tensor): An optional boolean tensor of shape (b, max_num_obj, topk), where\n                                topk is the number of top candidates to consider. If not provided,\n                                the top-k values are automatically computed based on the given metrics.\n        Returns:\n            (Tensor): A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.\n        \"\"\"\n# (b, max_num_obj, topk)\ntopk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\nif topk_mask is None:\ntopk_mask = (topk_metrics.max(-1, keepdim=True)[0] &gt; self.eps).expand_as(topk_idxs)\n# (b, max_num_obj, topk)\ntopk_idxs.masked_fill_(~topk_mask, 0)\n# (b, max_num_obj, topk, h*w) -&gt; (b, max_num_obj, h*w)\ncount_tensor = torch.zeros(metrics.shape, dtype=torch.int8, device=topk_idxs.device)\nones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\nfor k in range(self.topk):\n# Expand topk_idxs for each value of k and add 1 at the specified positions\ncount_tensor.scatter_add_(-1, topk_idxs[:, :, k:k + 1], ones)\n# count_tensor.scatter_add_(-1, topk_idxs, torch.ones_like(topk_idxs, dtype=torch.int8, device=topk_idxs.device))\n# filter invalid bboxes\ncount_tensor.masked_fill_(count_tensor &gt; 1, 0)\nreturn count_tensor.to(metrics.dtype)\ndef get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n\"\"\"\n        Compute target labels, target bounding boxes, and target scores for the positive anchor points.\n        Args:\n            gt_labels (Tensor): Ground truth labels of shape (b, max_num_obj, 1), where b is the\n                                batch size and max_num_obj is the maximum number of objects.\n            gt_bboxes (Tensor): Ground truth bounding boxes of shape (b, max_num_obj, 4).\n            target_gt_idx (Tensor): Indices of the assigned ground truth objects for positive\n                                    anchor points, with shape (b, h*w), where h*w is the total\n                                    number of anchor points.\n            fg_mask (Tensor): A boolean tensor of shape (b, h*w) indicating the positive\n                              (foreground) anchor points.\n        Returns:\n            (Tuple[Tensor, Tensor, Tensor]): A tuple containing the following tensors:\n                - target_labels (Tensor): Shape (b, h*w), containing the target labels for\n                                          positive anchor points.\n                - target_bboxes (Tensor): Shape (b, h*w, 4), containing the target bounding boxes\n                                          for positive anchor points.\n                - target_scores (Tensor): Shape (b, h*w, num_classes), containing the target scores\n                                          for positive anchor points, where num_classes is the number\n                                          of object classes.\n        \"\"\"\n# Assigned target labels, (b, 1)\nbatch_ind = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\ntarget_gt_idx = target_gt_idx + batch_ind * self.n_max_boxes  # (b, h*w)\ntarget_labels = gt_labels.long().flatten()[target_gt_idx]  # (b, h*w)\n# Assigned target boxes, (b, max_num_obj, 4) -&gt; (b, h*w)\ntarget_bboxes = gt_bboxes.view(-1, 4)[target_gt_idx]\n# Assigned target scores\ntarget_labels.clamp_(0)\n# 10x faster than F.one_hot()\ntarget_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.num_classes),\ndtype=torch.int64,\ndevice=target_labels.device)  # (b, h*w, 80)\ntarget_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\nfg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\ntarget_scores = torch.where(fg_scores_mask &gt; 0, target_scores, 0)\nreturn target_labels, target_bboxes, target_scores\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.__init__","title":"<code>__init__(topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-09)</code>","text":"<p>Initialize a TaskAlignedAssigner object with customizable hyperparameters.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9):\n\"\"\"Initialize a TaskAlignedAssigner object with customizable hyperparameters.\"\"\"\nsuper().__init__()\nself.topk = topk\nself.num_classes = num_classes\nself.bg_idx = num_classes\nself.alpha = alpha\nself.beta = beta\nself.eps = eps\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.forward","title":"<code>forward(pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)</code>","text":"<p>Compute the task-aligned assignment. Reference https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py</p> <p>Parameters:</p> Name Type Description Default <code>pd_scores</code> <code>Tensor</code> <p>shape(bs, num_total_anchors, num_classes)</p> required <code>pd_bboxes</code> <code>Tensor</code> <p>shape(bs, num_total_anchors, 4)</p> required <code>anc_points</code> <code>Tensor</code> <p>shape(num_total_anchors, 2)</p> required <code>gt_labels</code> <code>Tensor</code> <p>shape(bs, n_max_boxes, 1)</p> required <code>gt_bboxes</code> <code>Tensor</code> <p>shape(bs, n_max_boxes, 4)</p> required <code>mask_gt</code> <code>Tensor</code> <p>shape(bs, n_max_boxes, 1)</p> required <p>Returns:</p> Name Type Description <code>target_labels</code> <code>Tensor</code> <p>shape(bs, num_total_anchors)</p> <code>target_bboxes</code> <code>Tensor</code> <p>shape(bs, num_total_anchors, 4)</p> <code>target_scores</code> <code>Tensor</code> <p>shape(bs, num_total_anchors, num_classes)</p> <code>fg_mask</code> <code>Tensor</code> <p>shape(bs, num_total_anchors)</p> <code>target_gt_idx</code> <code>Tensor</code> <p>shape(bs, num_total_anchors)</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>@torch.no_grad()\ndef forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\"\"\"\n    Compute the task-aligned assignment.\n    Reference https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py\n    Args:\n        pd_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n        pd_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n        anc_points (Tensor): shape(num_total_anchors, 2)\n        gt_labels (Tensor): shape(bs, n_max_boxes, 1)\n        gt_bboxes (Tensor): shape(bs, n_max_boxes, 4)\n        mask_gt (Tensor): shape(bs, n_max_boxes, 1)\n    Returns:\n        target_labels (Tensor): shape(bs, num_total_anchors)\n        target_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n        target_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n        fg_mask (Tensor): shape(bs, num_total_anchors)\n        target_gt_idx (Tensor): shape(bs, num_total_anchors)\n    \"\"\"\nself.bs = pd_scores.size(0)\nself.n_max_boxes = gt_bboxes.size(1)\nif self.n_max_boxes == 0:\ndevice = gt_bboxes.device\nreturn (torch.full_like(pd_scores[..., 0], self.bg_idx).to(device), torch.zeros_like(pd_bboxes).to(device),\ntorch.zeros_like(pd_scores).to(device), torch.zeros_like(pd_scores[..., 0]).to(device),\ntorch.zeros_like(pd_scores[..., 0]).to(device))\nmask_pos, align_metric, overlaps = self.get_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points,\nmask_gt)\ntarget_gt_idx, fg_mask, mask_pos = select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n# Assigned target\ntarget_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n# Normalize\nalign_metric *= mask_pos\npos_align_metrics = align_metric.amax(axis=-1, keepdim=True)  # b, max_num_obj\npos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)  # b, max_num_obj\nnorm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\ntarget_scores = target_scores * norm_align_metric\nreturn target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.get_box_metrics","title":"<code>get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt)</code>","text":"<p>Compute alignment metric given predicted and ground truth bounding boxes.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt):\n\"\"\"Compute alignment metric given predicted and ground truth bounding boxes.\"\"\"\nna = pd_bboxes.shape[-2]\nmask_gt = mask_gt.bool()  # b, max_num_obj, h*w\noverlaps = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\nbbox_scores = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\nind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\nind[0] = torch.arange(end=self.bs).view(-1, 1).expand(-1, self.n_max_boxes)  # b, max_num_obj\nind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n# Get the scores of each grid for each gt cls\nbbox_scores[mask_gt] = pd_scores[ind[0], :, ind[1]][mask_gt]  # b, max_num_obj, h*w\n# (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\npd_boxes = pd_bboxes.unsqueeze(1).expand(-1, self.n_max_boxes, -1, -1)[mask_gt]\ngt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[mask_gt]\noverlaps[mask_gt] = bbox_iou(gt_boxes, pd_boxes, xywh=False, CIoU=True).squeeze(-1).clamp_(0)\nalign_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\nreturn align_metric, overlaps\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.get_pos_mask","title":"<code>get_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)</code>","text":"<p>Get in_gts mask, (b, max_num_obj, h*w).</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\"\"\"Get in_gts mask, (b, max_num_obj, h*w).\"\"\"\nmask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes)\n# Get anchor_align metric, (b, max_num_obj, h*w)\nalign_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n# Get topk_metric mask, (b, max_num_obj, h*w)\nmask_topk = self.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-1, -1, self.topk).bool())\n# Merge all mask to a final mask, (b, max_num_obj, h*w)\nmask_pos = mask_topk * mask_in_gts * mask_gt\nreturn mask_pos, align_metric, overlaps\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.get_targets","title":"<code>get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)</code>","text":"<p>Compute target labels, target bounding boxes, and target scores for the positive anchor points.</p> <p>Parameters:</p> Name Type Description Default <code>gt_labels</code> <code>Tensor</code> <p>Ground truth labels of shape (b, max_num_obj, 1), where b is the                 batch size and max_num_obj is the maximum number of objects.</p> required <code>gt_bboxes</code> <code>Tensor</code> <p>Ground truth bounding boxes of shape (b, max_num_obj, 4).</p> required <code>target_gt_idx</code> <code>Tensor</code> <p>Indices of the assigned ground truth objects for positive                     anchor points, with shape (b, hw), where hw is the total                     number of anchor points.</p> required <code>fg_mask</code> <code>Tensor</code> <p>A boolean tensor of shape (b, h*w) indicating the positive               (foreground) anchor points.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing the following tensors: - target_labels (Tensor): Shape (b, hw), containing the target labels for                           positive anchor points. - target_bboxes (Tensor): Shape (b, hw, 4), containing the target bounding boxes                           for positive anchor points. - target_scores (Tensor): Shape (b, h*w, num_classes), containing the target scores                           for positive anchor points, where num_classes is the number                           of object classes.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n\"\"\"\n    Compute target labels, target bounding boxes, and target scores for the positive anchor points.\n    Args:\n        gt_labels (Tensor): Ground truth labels of shape (b, max_num_obj, 1), where b is the\n                            batch size and max_num_obj is the maximum number of objects.\n        gt_bboxes (Tensor): Ground truth bounding boxes of shape (b, max_num_obj, 4).\n        target_gt_idx (Tensor): Indices of the assigned ground truth objects for positive\n                                anchor points, with shape (b, h*w), where h*w is the total\n                                number of anchor points.\n        fg_mask (Tensor): A boolean tensor of shape (b, h*w) indicating the positive\n                          (foreground) anchor points.\n    Returns:\n        (Tuple[Tensor, Tensor, Tensor]): A tuple containing the following tensors:\n            - target_labels (Tensor): Shape (b, h*w), containing the target labels for\n                                      positive anchor points.\n            - target_bboxes (Tensor): Shape (b, h*w, 4), containing the target bounding boxes\n                                      for positive anchor points.\n            - target_scores (Tensor): Shape (b, h*w, num_classes), containing the target scores\n                                      for positive anchor points, where num_classes is the number\n                                      of object classes.\n    \"\"\"\n# Assigned target labels, (b, 1)\nbatch_ind = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\ntarget_gt_idx = target_gt_idx + batch_ind * self.n_max_boxes  # (b, h*w)\ntarget_labels = gt_labels.long().flatten()[target_gt_idx]  # (b, h*w)\n# Assigned target boxes, (b, max_num_obj, 4) -&gt; (b, h*w)\ntarget_bboxes = gt_bboxes.view(-1, 4)[target_gt_idx]\n# Assigned target scores\ntarget_labels.clamp_(0)\n# 10x faster than F.one_hot()\ntarget_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.num_classes),\ndtype=torch.int64,\ndevice=target_labels.device)  # (b, h*w, 80)\ntarget_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\nfg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\ntarget_scores = torch.where(fg_scores_mask &gt; 0, target_scores, 0)\nreturn target_labels, target_bboxes, target_scores\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.TaskAlignedAssigner.select_topk_candidates","title":"<code>select_topk_candidates(metrics, largest=True, topk_mask=None)</code>","text":"<p>Select the top-k candidates based on the given metrics.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Tensor</code> <p>A tensor of shape (b, max_num_obj, hw), where b is the batch size,               max_num_obj is the maximum number of objects, and hw represents the               total number of anchor points.</p> required <code>largest</code> <code>bool</code> <p>If True, select the largest values; otherwise, select the smallest values.</p> <code>True</code> <code>topk_mask</code> <code>Tensor</code> <p>An optional boolean tensor of shape (b, max_num_obj, topk), where                 topk is the number of top candidates to consider. If not provided,                 the top-k values are automatically computed based on the given metrics.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n\"\"\"\n    Select the top-k candidates based on the given metrics.\n    Args:\n        metrics (Tensor): A tensor of shape (b, max_num_obj, h*w), where b is the batch size,\n                          max_num_obj is the maximum number of objects, and h*w represents the\n                          total number of anchor points.\n        largest (bool): If True, select the largest values; otherwise, select the smallest values.\n        topk_mask (Tensor): An optional boolean tensor of shape (b, max_num_obj, topk), where\n                            topk is the number of top candidates to consider. If not provided,\n                            the top-k values are automatically computed based on the given metrics.\n    Returns:\n        (Tensor): A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.\n    \"\"\"\n# (b, max_num_obj, topk)\ntopk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\nif topk_mask is None:\ntopk_mask = (topk_metrics.max(-1, keepdim=True)[0] &gt; self.eps).expand_as(topk_idxs)\n# (b, max_num_obj, topk)\ntopk_idxs.masked_fill_(~topk_mask, 0)\n# (b, max_num_obj, topk, h*w) -&gt; (b, max_num_obj, h*w)\ncount_tensor = torch.zeros(metrics.shape, dtype=torch.int8, device=topk_idxs.device)\nones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\nfor k in range(self.topk):\n# Expand topk_idxs for each value of k and add 1 at the specified positions\ncount_tensor.scatter_add_(-1, topk_idxs[:, :, k:k + 1], ones)\n# count_tensor.scatter_add_(-1, topk_idxs, torch.ones_like(topk_idxs, dtype=torch.int8, device=topk_idxs.device))\n# filter invalid bboxes\ncount_tensor.masked_fill_(count_tensor &gt; 1, 0)\nreturn count_tensor.to(metrics.dtype)\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.select_candidates_in_gts","title":"<code>ultralytics.utils.tal.select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-09)</code>","text":"<p>Select the positive anchor center in gt.</p> <p>Parameters:</p> Name Type Description Default <code>xy_centers</code> <code>Tensor</code> <p>shape(h*w, 4)</p> required <code>gt_bboxes</code> <code>Tensor</code> <p>shape(b, n_boxes, 4)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>shape(b, n_boxes, h*w)</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9):\n\"\"\"\n    Select the positive anchor center in gt.\n    Args:\n        xy_centers (Tensor): shape(h*w, 4)\n        gt_bboxes (Tensor): shape(b, n_boxes, 4)\n    Returns:\n        (Tensor): shape(b, n_boxes, h*w)\n    \"\"\"\nn_anchors = xy_centers.shape[0]\nbs, n_boxes, _ = gt_bboxes.shape\nlt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\nbbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1)\n# return (bbox_deltas.min(3)[0] &gt; eps).to(gt_bboxes.dtype)\nreturn bbox_deltas.amin(3).gt_(eps)\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.select_highest_overlaps","title":"<code>ultralytics.utils.tal.select_highest_overlaps(mask_pos, overlaps, n_max_boxes)</code>","text":"<p>If an anchor box is assigned to multiple gts, the one with the highest IoI will be selected.</p> <p>Parameters:</p> Name Type Description Default <code>mask_pos</code> <code>Tensor</code> <p>shape(b, n_max_boxes, h*w)</p> required <code>overlaps</code> <code>Tensor</code> <p>shape(b, n_max_boxes, h*w)</p> required <p>Returns:</p> Name Type Description <code>target_gt_idx</code> <code>Tensor</code> <p>shape(b, h*w)</p> <code>fg_mask</code> <code>Tensor</code> <p>shape(b, h*w)</p> <code>mask_pos</code> <code>Tensor</code> <p>shape(b, n_max_boxes, h*w)</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def select_highest_overlaps(mask_pos, overlaps, n_max_boxes):\n\"\"\"\n    If an anchor box is assigned to multiple gts, the one with the highest IoI will be selected.\n    Args:\n        mask_pos (Tensor): shape(b, n_max_boxes, h*w)\n        overlaps (Tensor): shape(b, n_max_boxes, h*w)\n    Returns:\n        target_gt_idx (Tensor): shape(b, h*w)\n        fg_mask (Tensor): shape(b, h*w)\n        mask_pos (Tensor): shape(b, n_max_boxes, h*w)\n    \"\"\"\n# (b, n_max_boxes, h*w) -&gt; (b, h*w)\nfg_mask = mask_pos.sum(-2)\nif fg_mask.max() &gt; 1:  # one anchor is assigned to multiple gt_bboxes\nmask_multi_gts = (fg_mask.unsqueeze(1) &gt; 1).expand(-1, n_max_boxes, -1)  # (b, n_max_boxes, h*w)\nmax_overlaps_idx = overlaps.argmax(1)  # (b, h*w)\nis_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\nis_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\nmask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()  # (b, n_max_boxes, h*w)\nfg_mask = mask_pos.sum(-2)\n# Find each grid serve which gt(index)\ntarget_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\nreturn target_gt_idx, fg_mask, mask_pos\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.make_anchors","title":"<code>ultralytics.utils.tal.make_anchors(feats, strides, grid_cell_offset=0.5)</code>","text":"<p>Generate anchors from features.</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def make_anchors(feats, strides, grid_cell_offset=0.5):\n\"\"\"Generate anchors from features.\"\"\"\nanchor_points, stride_tensor = [], []\nassert feats is not None\ndtype, device = feats[0].dtype, feats[0].device\nfor i, stride in enumerate(strides):\n_, _, h, w = feats[i].shape\nsx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset  # shift x\nsy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset  # shift y\nsy, sx = torch.meshgrid(sy, sx, indexing='ij') if TORCH_1_10 else torch.meshgrid(sy, sx)\nanchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\nstride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\nreturn torch.cat(anchor_points), torch.cat(stride_tensor)\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.dist2bbox","title":"<code>ultralytics.utils.tal.dist2bbox(distance, anchor_points, xywh=True, dim=-1)</code>","text":"<p>Transform distance(ltrb) to box(xywh or xyxy).</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def dist2bbox(distance, anchor_points, xywh=True, dim=-1):\n\"\"\"Transform distance(ltrb) to box(xywh or xyxy).\"\"\"\nlt, rb = distance.chunk(2, dim)\nx1y1 = anchor_points - lt\nx2y2 = anchor_points + rb\nif xywh:\nc_xy = (x1y1 + x2y2) / 2\nwh = x2y2 - x1y1\nreturn torch.cat((c_xy, wh), dim)  # xywh bbox\nreturn torch.cat((x1y1, x2y2), dim)  # xyxy bbox\n</code></pre>"},{"location":"reference/utils/tal/#ultralytics.utils.tal.bbox2dist","title":"<code>ultralytics.utils.tal.bbox2dist(anchor_points, bbox, reg_max)</code>","text":"<p>Transform bbox(xyxy) to dist(ltrb).</p> Source code in <code>ultralytics/utils/tal.py</code> <pre><code>def bbox2dist(anchor_points, bbox, reg_max):\n\"\"\"Transform bbox(xyxy) to dist(ltrb).\"\"\"\nx1y1, x2y2 = bbox.chunk(2, -1)\nreturn torch.cat((anchor_points - x1y1, x2y2 - anchor_points), -1).clamp_(0, reg_max - 0.01)  # dist (lt, rb)\n</code></pre>"},{"location":"reference/utils/torch_utils/","title":"Reference for <code>ultralytics/utils/torch_utils.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/torch_utils.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.ModelEMA","title":"<code>ultralytics.utils.torch_utils.ModelEMA</code>","text":"<p>Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models Keeps a moving average of everything in the model state_dict (parameters and buffers) For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage To disable EMA set the <code>enabled</code> attribute to <code>False</code>.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>class ModelEMA:\n\"\"\"Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    To disable EMA set the `enabled` attribute to `False`.\n    \"\"\"\ndef __init__(self, model, decay=0.9999, tau=2000, updates=0):\n\"\"\"Create EMA.\"\"\"\nself.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA\nself.updates = updates  # number of EMA updates\nself.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)\nfor p in self.ema.parameters():\np.requires_grad_(False)\nself.enabled = True\ndef update(self, model):\n\"\"\"Update EMA parameters.\"\"\"\nif self.enabled:\nself.updates += 1\nd = self.decay(self.updates)\nmsd = de_parallel(model).state_dict()  # model state_dict\nfor k, v in self.ema.state_dict().items():\nif v.dtype.is_floating_point:  # true for FP16 and FP32\nv *= d\nv += (1 - d) * msd[k].detach()\n# assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype},  model {msd[k].dtype}'\ndef update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n\"\"\"Updates attributes and saves stripped model with optimizer removed.\"\"\"\nif self.enabled:\ncopy_attr(self.ema, model, include, exclude)\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.ModelEMA.__init__","title":"<code>__init__(model, decay=0.9999, tau=2000, updates=0)</code>","text":"<p>Create EMA.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n\"\"\"Create EMA.\"\"\"\nself.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA\nself.updates = updates  # number of EMA updates\nself.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)\nfor p in self.ema.parameters():\np.requires_grad_(False)\nself.enabled = True\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.ModelEMA.update","title":"<code>update(model)</code>","text":"<p>Update EMA parameters.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def update(self, model):\n\"\"\"Update EMA parameters.\"\"\"\nif self.enabled:\nself.updates += 1\nd = self.decay(self.updates)\nmsd = de_parallel(model).state_dict()  # model state_dict\nfor k, v in self.ema.state_dict().items():\nif v.dtype.is_floating_point:  # true for FP16 and FP32\nv *= d\nv += (1 - d) * msd[k].detach()\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.ModelEMA.update_attr","title":"<code>update_attr(model, include=(), exclude=('process_group', 'reducer'))</code>","text":"<p>Updates attributes and saves stripped model with optimizer removed.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n\"\"\"Updates attributes and saves stripped model with optimizer removed.\"\"\"\nif self.enabled:\ncopy_attr(self.ema, model, include, exclude)\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.EarlyStopping","title":"<code>ultralytics.utils.torch_utils.EarlyStopping</code>","text":"<p>Early stopping class that stops training when a specified number of epochs have passed without improvement.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>class EarlyStopping:\n\"\"\"\n    Early stopping class that stops training when a specified number of epochs have passed without improvement.\n    \"\"\"\ndef __init__(self, patience=50):\n\"\"\"\n        Initialize early stopping object\n        Args:\n            patience (int, optional): Number of epochs to wait after fitness stops improving before stopping.\n        \"\"\"\nself.best_fitness = 0.0  # i.e. mAP\nself.best_epoch = 0\nself.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\nself.possible_stop = False  # possible stop may occur next epoch\ndef __call__(self, epoch, fitness):\n\"\"\"\n        Check whether to stop training\n        Args:\n            epoch (int): Current epoch of training\n            fitness (float): Fitness value of current epoch\n        Returns:\n            (bool): True if training should stop, False otherwise\n        \"\"\"\nif fitness is None:  # check if fitness=None (happens when val=False)\nreturn False\nif fitness &gt;= self.best_fitness:  # &gt;= 0 to allow for early zero-fitness stage of training\nself.best_epoch = epoch\nself.best_fitness = fitness\ndelta = epoch - self.best_epoch  # epochs without improvement\nself.possible_stop = delta &gt;= (self.patience - 1)  # possible stop may occur next epoch\nstop = delta &gt;= self.patience  # stop training if patience exceeded\nif stop:\nLOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '\nf'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\\n'\nf'To update EarlyStopping(patience={self.patience}) pass a new patience value, '\nf'i.e. `patience=300` or use `patience=0` to disable EarlyStopping.')\nreturn stop\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.EarlyStopping.__call__","title":"<code>__call__(epoch, fitness)</code>","text":"<p>Check whether to stop training</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch of training</p> required <code>fitness</code> <code>float</code> <p>Fitness value of current epoch</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if training should stop, False otherwise</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def __call__(self, epoch, fitness):\n\"\"\"\n    Check whether to stop training\n    Args:\n        epoch (int): Current epoch of training\n        fitness (float): Fitness value of current epoch\n    Returns:\n        (bool): True if training should stop, False otherwise\n    \"\"\"\nif fitness is None:  # check if fitness=None (happens when val=False)\nreturn False\nif fitness &gt;= self.best_fitness:  # &gt;= 0 to allow for early zero-fitness stage of training\nself.best_epoch = epoch\nself.best_fitness = fitness\ndelta = epoch - self.best_epoch  # epochs without improvement\nself.possible_stop = delta &gt;= (self.patience - 1)  # possible stop may occur next epoch\nstop = delta &gt;= self.patience  # stop training if patience exceeded\nif stop:\nLOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '\nf'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\\n'\nf'To update EarlyStopping(patience={self.patience}) pass a new patience value, '\nf'i.e. `patience=300` or use `patience=0` to disable EarlyStopping.')\nreturn stop\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.EarlyStopping.__init__","title":"<code>__init__(patience=50)</code>","text":"<p>Initialize early stopping object</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs to wait after fitness stops improving before stopping.</p> <code>50</code> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def __init__(self, patience=50):\n\"\"\"\n    Initialize early stopping object\n    Args:\n        patience (int, optional): Number of epochs to wait after fitness stops improving before stopping.\n    \"\"\"\nself.best_fitness = 0.0  # i.e. mAP\nself.best_epoch = 0\nself.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\nself.possible_stop = False  # possible stop may occur next epoch\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.torch_distributed_zero_first","title":"<code>ultralytics.utils.torch_utils.torch_distributed_zero_first(local_rank)</code>","text":"<p>Decorator to make all processes in distributed training wait for each local_master to do something.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>@contextmanager\ndef torch_distributed_zero_first(local_rank: int):\n\"\"\"Decorator to make all processes in distributed training wait for each local_master to do something.\"\"\"\ninitialized = torch.distributed.is_available() and torch.distributed.is_initialized()\nif initialized and local_rank not in (-1, 0):\ndist.barrier(device_ids=[local_rank])\nyield\nif initialized and local_rank == 0:\ndist.barrier(device_ids=[0])\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.smart_inference_mode","title":"<code>ultralytics.utils.torch_utils.smart_inference_mode()</code>","text":"<p>Applies torch.inference_mode() decorator if torch&gt;=1.9.0 else torch.no_grad() decorator.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def smart_inference_mode():\n\"\"\"Applies torch.inference_mode() decorator if torch&gt;=1.9.0 else torch.no_grad() decorator.\"\"\"\ndef decorate(fn):\n\"\"\"Applies appropriate torch decorator for inference mode based on torch version.\"\"\"\nreturn (torch.inference_mode if TORCH_1_9 else torch.no_grad)()(fn)\nreturn decorate\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_cpu_info","title":"<code>ultralytics.utils.torch_utils.get_cpu_info()</code>","text":"<p>Return a string with system CPU information, i.e. 'Apple M2'.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_cpu_info():\n\"\"\"Return a string with system CPU information, i.e. 'Apple M2'.\"\"\"\nimport cpuinfo  # pip install py-cpuinfo\nk = 'brand_raw', 'hardware_raw', 'arch_string_raw'  # info keys sorted by preference (not all keys always available)\ninfo = cpuinfo.get_cpu_info()  # info dict\nstring = info.get(k[0] if k[0] in info else k[1] if k[1] in info else k[2], 'unknown')\nreturn string.replace('(R)', '').replace('CPU ', '').replace('@ ', '')\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.select_device","title":"<code>ultralytics.utils.torch_utils.select_device(device='', batch=0, newline=False, verbose=True)</code>","text":"<p>Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def select_device(device='', batch=0, newline=False, verbose=True):\n\"\"\"Selects PyTorch Device. Options are device = None or 'cpu' or 0 or '0' or '0,1,2,3'.\"\"\"\ns = f'Ultralytics YOLOv{__version__} \ud83d\ude80 Python-{platform.python_version()} torch-{torch.__version__} '\ndevice = str(device).lower()\nfor remove in 'cuda:', 'none', '(', ')', '[', ']', \"'\", ' ':\ndevice = device.replace(remove, '')  # to string, 'cuda:0' -&gt; '0' and '(0, 1)' -&gt; '0,1'\ncpu = device == 'cpu'\nmps = device == 'mps'  # Apple Metal Performance Shaders (MPS)\nif cpu or mps:\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\nelif device:  # non-cpu device requested\nif device == 'cuda':\ndevice = '0'\nvisible = os.environ.get('CUDA_VISIBLE_DEVICES', None)\nos.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()\nif not (torch.cuda.is_available() and torch.cuda.device_count() &gt;= len(device.replace(',', ''))):\nLOGGER.info(s)\ninstall = 'See https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no ' \\\n                      'CUDA devices are seen by torch.\\n' if torch.cuda.device_count() == 0 else ''\nraise ValueError(f\"Invalid CUDA 'device={device}' requested.\"\nf\" Use 'device=cpu' or pass valid CUDA device(s) if available,\"\nf\" i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\\n\"\nf'\\ntorch.cuda.is_available(): {torch.cuda.is_available()}'\nf'\\ntorch.cuda.device_count(): {torch.cuda.device_count()}'\nf\"\\nos.environ['CUDA_VISIBLE_DEVICES']: {visible}\\n\"\nf'{install}')\nif not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available\ndevices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7\nn = len(devices)  # device count\nif n &gt; 1 and batch &gt; 0 and batch % n != 0:  # check batch_size is divisible by device_count\nraise ValueError(f\"'batch={batch}' must be a multiple of GPU count {n}. Try 'batch={batch // n * n}' or \"\nf\"'batch={batch // n * n + n}', the nearest batch sizes evenly divisible by {n}.\")\nspace = ' ' * (len(s) + 1)\nfor i, d in enumerate(devices):\np = torch.cuda.get_device_properties(i)\ns += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 &lt;&lt; 20):.0f}MiB)\\n\"  # bytes to MB\narg = 'cuda:0'\nelif mps and getattr(torch, 'has_mps', False) and torch.backends.mps.is_available() and TORCH_2_0:\n# Prefer MPS if available\ns += f'MPS ({get_cpu_info()})\\n'\narg = 'mps'\nelse:  # revert to CPU\ns += f'CPU ({get_cpu_info()})\\n'\narg = 'cpu'\nif verbose and RANK == -1:\nLOGGER.info(s if newline else s.rstrip())\nreturn torch.device(arg)\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.time_sync","title":"<code>ultralytics.utils.torch_utils.time_sync()</code>","text":"<p>PyTorch-accurate time.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def time_sync():\n\"\"\"PyTorch-accurate time.\"\"\"\nif torch.cuda.is_available():\ntorch.cuda.synchronize()\nreturn time.time()\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.fuse_conv_and_bn","title":"<code>ultralytics.utils.torch_utils.fuse_conv_and_bn(conv, bn)</code>","text":"<p>Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def fuse_conv_and_bn(conv, bn):\n\"\"\"Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/.\"\"\"\nfusedconv = nn.Conv2d(conv.in_channels,\nconv.out_channels,\nkernel_size=conv.kernel_size,\nstride=conv.stride,\npadding=conv.padding,\ndilation=conv.dilation,\ngroups=conv.groups,\nbias=True).requires_grad_(False).to(conv.weight.device)\n# Prepare filters\nw_conv = conv.weight.clone().view(conv.out_channels, -1)\nw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\nfusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\n# Prepare spatial bias\nb_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\nb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\nfusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\nreturn fusedconv\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.fuse_deconv_and_bn","title":"<code>ultralytics.utils.torch_utils.fuse_deconv_and_bn(deconv, bn)</code>","text":"<p>Fuse ConvTranspose2d() and BatchNorm2d() layers.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def fuse_deconv_and_bn(deconv, bn):\n\"\"\"Fuse ConvTranspose2d() and BatchNorm2d() layers.\"\"\"\nfuseddconv = nn.ConvTranspose2d(deconv.in_channels,\ndeconv.out_channels,\nkernel_size=deconv.kernel_size,\nstride=deconv.stride,\npadding=deconv.padding,\noutput_padding=deconv.output_padding,\ndilation=deconv.dilation,\ngroups=deconv.groups,\nbias=True).requires_grad_(False).to(deconv.weight.device)\n# Prepare filters\nw_deconv = deconv.weight.clone().view(deconv.out_channels, -1)\nw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\nfuseddconv.weight.copy_(torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))\n# Prepare spatial bias\nb_conv = torch.zeros(deconv.weight.size(1), device=deconv.weight.device) if deconv.bias is None else deconv.bias\nb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\nfuseddconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\nreturn fuseddconv\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.model_info","title":"<code>ultralytics.utils.torch_utils.model_info(model, detailed=False, verbose=True, imgsz=640)</code>","text":"<p>Model information. imgsz may be int or list, i.e. imgsz=640 or imgsz=[640, 320].</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def model_info(model, detailed=False, verbose=True, imgsz=640):\n\"\"\"Model information. imgsz may be int or list, i.e. imgsz=640 or imgsz=[640, 320].\"\"\"\nif not verbose:\nreturn\nn_p = get_num_params(model)  # number of parameters\nn_g = get_num_gradients(model)  # number of gradients\nn_l = len(list(model.modules()))  # number of layers\nif detailed:\nLOGGER.info(\nf\"{'layer':&gt;5} {'name':&gt;40} {'gradient':&gt;9} {'parameters':&gt;12} {'shape':&gt;20} {'mu':&gt;10} {'sigma':&gt;10}\")\nfor i, (name, p) in enumerate(model.named_parameters()):\nname = name.replace('module_list.', '')\nLOGGER.info('%5g %40s %9s %12g %20s %10.3g %10.3g %10s' %\n(i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std(), p.dtype))\nflops = get_flops(model, imgsz)\nfused = ' (fused)' if getattr(model, 'is_fused', lambda: False)() else ''\nfs = f', {flops:.1f} GFLOPs' if flops else ''\nyaml_file = getattr(model, 'yaml_file', '') or getattr(model, 'yaml', {}).get('yaml_file', '')\nmodel_name = Path(yaml_file).stem.replace('yolo', 'YOLO') or 'Model'\nLOGGER.info(f'{model_name} summary{fused}: {n_l} layers, {n_p} parameters, {n_g} gradients{fs}')\nreturn n_l, n_p, n_g, flops\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_num_params","title":"<code>ultralytics.utils.torch_utils.get_num_params(model)</code>","text":"<p>Return the total number of parameters in a YOLO model.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_num_params(model):\n\"\"\"Return the total number of parameters in a YOLO model.\"\"\"\nreturn sum(x.numel() for x in model.parameters())\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_num_gradients","title":"<code>ultralytics.utils.torch_utils.get_num_gradients(model)</code>","text":"<p>Return the total number of parameters with gradients in a YOLO model.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_num_gradients(model):\n\"\"\"Return the total number of parameters with gradients in a YOLO model.\"\"\"\nreturn sum(x.numel() for x in model.parameters() if x.requires_grad)\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.model_info_for_loggers","title":"<code>ultralytics.utils.torch_utils.model_info_for_loggers(trainer)</code>","text":"<p>Return model info dict with useful model information.</p> Example for YOLOv8n <p>{'model/parameters': 3151904,  'model/GFLOPs': 8.746,  'model/speed_ONNX(ms)': 41.244,  'model/speed_TensorRT(ms)': 3.211,  'model/speed_PyTorch(ms)': 18.755}</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def model_info_for_loggers(trainer):\n\"\"\"\n    Return model info dict with useful model information.\n    Example for YOLOv8n:\n        {'model/parameters': 3151904,\n         'model/GFLOPs': 8.746,\n         'model/speed_ONNX(ms)': 41.244,\n         'model/speed_TensorRT(ms)': 3.211,\n         'model/speed_PyTorch(ms)': 18.755}\n    \"\"\"\nif trainer.args.profile:  # profile ONNX and TensorRT times\nfrom ultralytics.utils.benchmarks import ProfileModels\nresults = ProfileModels([trainer.last], device=trainer.device).profile()[0]\nresults.pop('model/name')\nelse:  # only return PyTorch times from most recent validation\nresults = {\n'model/parameters': get_num_params(trainer.model),\n'model/GFLOPs': round(get_flops(trainer.model), 3)}\nresults['model/speed_PyTorch(ms)'] = round(trainer.validator.speed['inference'], 3)\nreturn results\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_flops","title":"<code>ultralytics.utils.torch_utils.get_flops(model, imgsz=640)</code>","text":"<p>Return a YOLO model's FLOPs.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_flops(model, imgsz=640):\n\"\"\"Return a YOLO model's FLOPs.\"\"\"\ntry:\nmodel = de_parallel(model)\np = next(model.parameters())\nstride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32  # max stride\nim = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # input image in BCHW format\nflops = thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1E9 * 2 if thop else 0  # stride GFLOPs\nimgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # expand if int/float\nreturn flops * imgsz[0] / stride * imgsz[1] / stride  # 640x640 GFLOPs\nexcept Exception:\nreturn 0\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_flops_with_torch_profiler","title":"<code>ultralytics.utils.torch_utils.get_flops_with_torch_profiler(model, imgsz=640)</code>","text":"<p>Compute model FLOPs (thop alternative).</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_flops_with_torch_profiler(model, imgsz=640):\n\"\"\"Compute model FLOPs (thop alternative).\"\"\"\nif TORCH_2_0:\nmodel = de_parallel(model)\np = next(model.parameters())\nstride = (max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32) * 2  # max stride\nim = torch.zeros((1, p.shape[1], stride, stride), device=p.device)  # input image in BCHW format\nwith torch.profiler.profile(with_flops=True) as prof:\nmodel(im)\nflops = sum(x.flops for x in prof.key_averages()) / 1E9\nimgsz = imgsz if isinstance(imgsz, list) else [imgsz, imgsz]  # expand if int/float\nflops = flops * imgsz[0] / stride * imgsz[1] / stride  # 640x640 GFLOPs\nreturn flops\nreturn 0\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.initialize_weights","title":"<code>ultralytics.utils.torch_utils.initialize_weights(model)</code>","text":"<p>Initialize model weights to random values.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def initialize_weights(model):\n\"\"\"Initialize model weights to random values.\"\"\"\nfor m in model.modules():\nt = type(m)\nif t is nn.Conv2d:\npass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\nelif t is nn.BatchNorm2d:\nm.eps = 1e-3\nm.momentum = 0.03\nelif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\nm.inplace = True\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.scale_img","title":"<code>ultralytics.utils.torch_utils.scale_img(img, ratio=1.0, same_shape=False, gs=32)</code>","text":"Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n# Scales img(bs,3,y,x) by ratio constrained to gs-multiple\nif ratio == 1.0:\nreturn img\nh, w = img.shape[2:]\ns = (int(h * ratio), int(w * ratio))  # new size\nimg = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\nif not same_shape:  # pad/crop img\nh, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))\nreturn F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.make_divisible","title":"<code>ultralytics.utils.torch_utils.make_divisible(x, divisor)</code>","text":"<p>Returns nearest x divisible by divisor.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def make_divisible(x, divisor):\n\"\"\"Returns nearest x divisible by divisor.\"\"\"\nif isinstance(divisor, torch.Tensor):\ndivisor = int(divisor.max())  # to int\nreturn math.ceil(x / divisor) * divisor\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.copy_attr","title":"<code>ultralytics.utils.torch_utils.copy_attr(a, b, include=(), exclude=())</code>","text":"<p>Copies attributes from object 'b' to object 'a', with options to include/exclude certain attributes.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def copy_attr(a, b, include=(), exclude=()):\n\"\"\"Copies attributes from object 'b' to object 'a', with options to include/exclude certain attributes.\"\"\"\nfor k, v in b.__dict__.items():\nif (len(include) and k not in include) or k.startswith('_') or k in exclude:\ncontinue\nelse:\nsetattr(a, k, v)\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.get_latest_opset","title":"<code>ultralytics.utils.torch_utils.get_latest_opset()</code>","text":"<p>Return second-most (for maturity) recently supported ONNX opset by this version of torch.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def get_latest_opset():\n\"\"\"Return second-most (for maturity) recently supported ONNX opset by this version of torch.\"\"\"\nreturn max(int(k[14:]) for k in vars(torch.onnx) if 'symbolic_opset' in k) - 1  # opset\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.intersect_dicts","title":"<code>ultralytics.utils.torch_utils.intersect_dicts(da, db, exclude=())</code>","text":"<p>Returns a dictionary of intersecting keys with matching shapes, excluding 'exclude' keys, using da values.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def intersect_dicts(da, db, exclude=()):\n\"\"\"Returns a dictionary of intersecting keys with matching shapes, excluding 'exclude' keys, using da values.\"\"\"\nreturn {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.is_parallel","title":"<code>ultralytics.utils.torch_utils.is_parallel(model)</code>","text":"<p>Returns True if model is of type DP or DDP.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def is_parallel(model):\n\"\"\"Returns True if model is of type DP or DDP.\"\"\"\nreturn isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.de_parallel","title":"<code>ultralytics.utils.torch_utils.de_parallel(model)</code>","text":"<p>De-parallelize a model: returns single-GPU model if model is of type DP or DDP.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def de_parallel(model):\n\"\"\"De-parallelize a model: returns single-GPU model if model is of type DP or DDP.\"\"\"\nreturn model.module if is_parallel(model) else model\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.one_cycle","title":"<code>ultralytics.utils.torch_utils.one_cycle(y1=0.0, y2=1.0, steps=100)</code>","text":"<p>Returns a lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def one_cycle(y1=0.0, y2=1.0, steps=100):\n\"\"\"Returns a lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf.\"\"\"\nreturn lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.init_seeds","title":"<code>ultralytics.utils.torch_utils.init_seeds(seed=0, deterministic=False)</code>","text":"<p>Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html.</p> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def init_seeds(seed=0, deterministic=False):\n\"\"\"Initialize random number generator (RNG) seeds https://pytorch.org/docs/stable/notes/randomness.html.\"\"\"\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe\n# torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287\nif deterministic:\nif TORCH_2_0:\ntorch.use_deterministic_algorithms(True, warn_only=True)  # warn if deterministic is not possible\ntorch.backends.cudnn.deterministic = True\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\nos.environ['PYTHONHASHSEED'] = str(seed)\nelse:\nLOGGER.warning('WARNING \u26a0\ufe0f Upgrade to torch&gt;=2.0.0 for deterministic training.')\nelse:\ntorch.use_deterministic_algorithms(False)\ntorch.backends.cudnn.deterministic = False\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.strip_optimizer","title":"<code>ultralytics.utils.torch_utils.strip_optimizer(f='best.pt', s='')</code>","text":"<p>Strip optimizer from 'f' to finalize training, optionally save as 's'.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>file path to model to strip the optimizer from. Default is 'best.pt'.</p> <code>'best.pt'</code> <code>s</code> <code>str</code> <p>file path to save the model with stripped optimizer to. If not provided, 'f' will be overwritten.</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <pre><code>from pathlib import Path\nfrom ultralytics.utils.torch_utils import strip_optimizer\nfor f in Path('path/to/weights').rglob('*.pt'):\nstrip_optimizer(f)\n</code></pre> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def strip_optimizer(f: Union[str, Path] = 'best.pt', s: str = '') -&gt; None:\n\"\"\"\n    Strip optimizer from 'f' to finalize training, optionally save as 's'.\n    Args:\n        f (str): file path to model to strip the optimizer from. Default is 'best.pt'.\n        s (str): file path to save the model with stripped optimizer to. If not provided, 'f' will be overwritten.\n    Returns:\n        None\n    Example:\n        ```python\n        from pathlib import Path\n        from ultralytics.utils.torch_utils import strip_optimizer\n        for f in Path('path/to/weights').rglob('*.pt'):\n            strip_optimizer(f)\n        ```\n    \"\"\"\n# Use dill (if exists) to serialize the lambda functions where pickle does not do this\ntry:\nimport dill as pickle\nexcept ImportError:\nimport pickle\nx = torch.load(f, map_location=torch.device('cpu'))\nif 'model' not in x:\nLOGGER.info(f'Skipping {f}, not a valid Ultralytics model.')\nreturn\nif hasattr(x['model'], 'args'):\nx['model'].args = dict(x['model'].args)  # convert from IterableSimpleNamespace to dict\nargs = {**DEFAULT_CFG_DICT, **x['train_args']} if 'train_args' in x else None  # combine args\nif x.get('ema'):\nx['model'] = x['ema']  # replace model with ema\nfor k in 'optimizer', 'best_fitness', 'ema', 'updates':  # keys\nx[k] = None\nx['epoch'] = -1\nx['model'].half()  # to FP16\nfor p in x['model'].parameters():\np.requires_grad = False\nx['train_args'] = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # strip non-default keys\n# x['model'].args = x['train_args']\ntorch.save(x, s or f, pickle_module=pickle)\nmb = os.path.getsize(s or f) / 1E6  # filesize\nLOGGER.info(f\"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB\")\n</code></pre>"},{"location":"reference/utils/torch_utils/#ultralytics.utils.torch_utils.profile","title":"<code>ultralytics.utils.torch_utils.profile(input, ops, n=10, device=None)</code>","text":"<p>Ultralytics speed, memory and FLOPs profiler.</p> Example <pre><code>from ultralytics.utils.torch_utils import profile\ninput = torch.randn(16, 3, 640, 640)\nm1 = lambda x: x * torch.sigmoid(x)\nm2 = nn.SiLU()\nprofile(input, [m1, m2], n=100)  # profile over 100 iterations\n</code></pre> Source code in <code>ultralytics/utils/torch_utils.py</code> <pre><code>def profile(input, ops, n=10, device=None):\n\"\"\"\n    Ultralytics speed, memory and FLOPs profiler.\n    Example:\n        ```python\n        from ultralytics.utils.torch_utils import profile\n        input = torch.randn(16, 3, 640, 640)\n        m1 = lambda x: x * torch.sigmoid(x)\n        m2 = nn.SiLU()\n        profile(input, [m1, m2], n=100)  # profile over 100 iterations\n        ```\n    \"\"\"\nresults = []\nif not isinstance(device, torch.device):\ndevice = select_device(device)\nLOGGER.info(f\"{'Params':&gt;12s}{'GFLOPs':&gt;12s}{'GPU_mem (GB)':&gt;14s}{'forward (ms)':&gt;14s}{'backward (ms)':&gt;14s}\"\nf\"{'input':&gt;24s}{'output':&gt;24s}\")\nfor x in input if isinstance(input, list) else [input]:\nx = x.to(device)\nx.requires_grad = True\nfor m in ops if isinstance(ops, list) else [ops]:\nm = m.to(device) if hasattr(m, 'to') else m  # device\nm = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m\ntf, tb, t = 0, 0, [0, 0, 0]  # dt forward, backward\ntry:\nflops = thop.profile(m, inputs=[x], verbose=False)[0] / 1E9 * 2 if thop else 0  # GFLOPs\nexcept Exception:\nflops = 0\ntry:\nfor _ in range(n):\nt[0] = time_sync()\ny = m(x)\nt[1] = time_sync()\ntry:\n(sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()\nt[2] = time_sync()\nexcept Exception:  # no backward method\n# print(e)  # for debug\nt[2] = float('nan')\ntf += (t[1] - t[0]) * 1000 / n  # ms per op forward\ntb += (t[2] - t[1]) * 1000 / n  # ms per op backward\nmem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)\ns_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else 'list' for x in (x, y))  # shapes\np = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0  # parameters\nLOGGER.info(f'{p:12}{flops:12.4g}{mem:&gt;14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):&gt;24s}{str(s_out):&gt;24s}')\nresults.append([p, flops, mem, tf, tb, s_in, s_out])\nexcept Exception as e:\nLOGGER.info(e)\nresults.append(None)\ntorch.cuda.empty_cache()\nreturn results\n</code></pre>"},{"location":"reference/utils/tuner/","title":"Reference for <code>ultralytics/utils/tuner.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/tuner.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/utils/tuner/#ultralytics.utils.tuner.run_ray_tune","title":"<code>ultralytics.utils.tuner.run_ray_tune(model, space=None, grace_period=10, gpu_per_trial=None, max_samples=10, **train_args)</code>","text":"<p>Runs hyperparameter tuning using Ray Tune.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>YOLO</code> <p>Model to run the tuner on.</p> required <code>space</code> <code>dict</code> <p>The hyperparameter search space. Defaults to None.</p> <code>None</code> <code>grace_period</code> <code>int</code> <p>The grace period in epochs of the ASHA scheduler. Defaults to 10.</p> <code>10</code> <code>gpu_per_trial</code> <code>int</code> <p>The number of GPUs to allocate per trial. Defaults to None.</p> <code>None</code> <code>max_samples</code> <code>int</code> <p>The maximum number of trials to run. Defaults to 10.</p> <code>10</code> <code>train_args</code> <code>dict</code> <p>Additional arguments to pass to the <code>train()</code> method. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the hyperparameter search.</p> <p>Raises:</p> Type Description <code>ModuleNotFoundError</code> <p>If Ray Tune is not installed.</p> Source code in <code>ultralytics/utils/tuner.py</code> <pre><code>def run_ray_tune(model,\nspace: dict = None,\ngrace_period: int = 10,\ngpu_per_trial: int = None,\nmax_samples: int = 10,\n**train_args):\n\"\"\"\n    Runs hyperparameter tuning using Ray Tune.\n    Args:\n        model (YOLO): Model to run the tuner on.\n        space (dict, optional): The hyperparameter search space. Defaults to None.\n        grace_period (int, optional): The grace period in epochs of the ASHA scheduler. Defaults to 10.\n        gpu_per_trial (int, optional): The number of GPUs to allocate per trial. Defaults to None.\n        max_samples (int, optional): The maximum number of trials to run. Defaults to 10.\n        train_args (dict, optional): Additional arguments to pass to the `train()` method. Defaults to {}.\n    Returns:\n        (dict): A dictionary containing the results of the hyperparameter search.\n    Raises:\n        ModuleNotFoundError: If Ray Tune is not installed.\n    \"\"\"\nif train_args is None:\ntrain_args = {}\ntry:\nfrom ray import tune\nfrom ray.air import RunConfig\nfrom ray.air.integrations.wandb import WandbLoggerCallback\nfrom ray.tune.schedulers import ASHAScheduler\nexcept ImportError:\nraise ModuleNotFoundError('Tuning hyperparameters requires Ray Tune. Install with: pip install \"ray[tune]\"')\ntry:\nimport wandb\nassert hasattr(wandb, '__version__')\nexcept (ImportError, AssertionError):\nwandb = False\ndefault_space = {\n# 'optimizer': tune.choice(['SGD', 'Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSProp']),\n'lr0': tune.uniform(1e-5, 1e-1),\n'lrf': tune.uniform(0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n'momentum': tune.uniform(0.6, 0.98),  # SGD momentum/Adam beta1\n'weight_decay': tune.uniform(0.0, 0.001),  # optimizer weight decay 5e-4\n'warmup_epochs': tune.uniform(0.0, 5.0),  # warmup epochs (fractions ok)\n'warmup_momentum': tune.uniform(0.0, 0.95),  # warmup initial momentum\n'box': tune.uniform(0.02, 0.2),  # box loss gain\n'cls': tune.uniform(0.2, 4.0),  # cls loss gain (scale with pixels)\n'hsv_h': tune.uniform(0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n'hsv_s': tune.uniform(0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n'hsv_v': tune.uniform(0.0, 0.9),  # image HSV-Value augmentation (fraction)\n'degrees': tune.uniform(0.0, 45.0),  # image rotation (+/- deg)\n'translate': tune.uniform(0.0, 0.9),  # image translation (+/- fraction)\n'scale': tune.uniform(0.0, 0.9),  # image scale (+/- gain)\n'shear': tune.uniform(0.0, 10.0),  # image shear (+/- deg)\n'perspective': tune.uniform(0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\n'flipud': tune.uniform(0.0, 1.0),  # image flip up-down (probability)\n'fliplr': tune.uniform(0.0, 1.0),  # image flip left-right (probability)\n'mosaic': tune.uniform(0.0, 1.0),  # image mixup (probability)\n'mixup': tune.uniform(0.0, 1.0),  # image mixup (probability)\n'copy_paste': tune.uniform(0.0, 1.0)}  # segment copy-paste (probability)\ndef _tune(config):\n\"\"\"\n        Trains the YOLO model with the specified hyperparameters and additional arguments.\n        Args:\n            config (dict): A dictionary of hyperparameters to use for training.\n        Returns:\n            None.\n        \"\"\"\nmodel._reset_callbacks()\nconfig.update(train_args)\nmodel.train(**config)\n# Get search space\nif not space:\nspace = default_space\nLOGGER.warning('WARNING \u26a0\ufe0f search space not provided, using default search space.')\n# Get dataset\ndata = train_args.get('data', TASK2DATA[model.task])\nspace['data'] = data\nif 'data' not in train_args:\nLOGGER.warning(f'WARNING \u26a0\ufe0f data not provided, using default \"data={data}\".')\n# Define the trainable function with allocated resources\ntrainable_with_resources = tune.with_resources(_tune, {'cpu': NUM_THREADS, 'gpu': gpu_per_trial or 0})\n# Define the ASHA scheduler for hyperparameter search\nasha_scheduler = ASHAScheduler(time_attr='epoch',\nmetric=TASK2METRIC[model.task],\nmode='max',\nmax_t=train_args.get('epochs') or DEFAULT_CFG_DICT['epochs'] or 100,\ngrace_period=grace_period,\nreduction_factor=3)\n# Define the callbacks for the hyperparameter search\ntuner_callbacks = [WandbLoggerCallback(project='YOLOv8-tune')] if wandb else []\n# Create the Ray Tune hyperparameter search tuner\ntuner = tune.Tuner(trainable_with_resources,\nparam_space=space,\ntune_config=tune.TuneConfig(scheduler=asha_scheduler, num_samples=max_samples),\nrun_config=RunConfig(callbacks=tuner_callbacks, storage_path='./runs/tune'))\n# Run the hyperparameter search\ntuner.fit()\n# Return the results of the hyperparameter search\nreturn tuner.get_results()\n</code></pre>"},{"location":"reference/utils/callbacks/base/","title":"Reference for <code>ultralytics/utils/callbacks/base.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/base.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.base.on_pretrain_routine_start(trainer)</code>","text":"<p>Called before the pretraining routine starts.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Called before the pretraining routine starts.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_pretrain_routine_end","title":"<code>ultralytics.utils.callbacks.base.on_pretrain_routine_end(trainer)</code>","text":"<p>Called after the pretraining routine ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_pretrain_routine_end(trainer):\n\"\"\"Called after the pretraining routine ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_start","title":"<code>ultralytics.utils.callbacks.base.on_train_start(trainer)</code>","text":"<p>Called when the training starts.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_start(trainer):\n\"\"\"Called when the training starts.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_epoch_start","title":"<code>ultralytics.utils.callbacks.base.on_train_epoch_start(trainer)</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_epoch_start(trainer):\n\"\"\"Called at the start of each training epoch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_batch_start","title":"<code>ultralytics.utils.callbacks.base.on_train_batch_start(trainer)</code>","text":"<p>Called at the start of each training batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_batch_start(trainer):\n\"\"\"Called at the start of each training batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.optimizer_step","title":"<code>ultralytics.utils.callbacks.base.optimizer_step(trainer)</code>","text":"<p>Called when the optimizer takes a step.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def optimizer_step(trainer):\n\"\"\"Called when the optimizer takes a step.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_before_zero_grad","title":"<code>ultralytics.utils.callbacks.base.on_before_zero_grad(trainer)</code>","text":"<p>Called before the gradients are set to zero.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_before_zero_grad(trainer):\n\"\"\"Called before the gradients are set to zero.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_batch_end","title":"<code>ultralytics.utils.callbacks.base.on_train_batch_end(trainer)</code>","text":"<p>Called at the end of each training batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_batch_end(trainer):\n\"\"\"Called at the end of each training batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_epoch_end","title":"<code>ultralytics.utils.callbacks.base.on_train_epoch_end(trainer)</code>","text":"<p>Called at the end of each training epoch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_epoch_end(trainer):\n\"\"\"Called at the end of each training epoch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.base.on_fit_epoch_end(trainer)</code>","text":"<p>Called at the end of each fit epoch (train + val).</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Called at the end of each fit epoch (train + val).\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_model_save","title":"<code>ultralytics.utils.callbacks.base.on_model_save(trainer)</code>","text":"<p>Called when the model is saved.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_model_save(trainer):\n\"\"\"Called when the model is saved.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_train_end","title":"<code>ultralytics.utils.callbacks.base.on_train_end(trainer)</code>","text":"<p>Called when the training ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Called when the training ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_params_update","title":"<code>ultralytics.utils.callbacks.base.on_params_update(trainer)</code>","text":"<p>Called when the model parameters are updated.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_params_update(trainer):\n\"\"\"Called when the model parameters are updated.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.teardown","title":"<code>ultralytics.utils.callbacks.base.teardown(trainer)</code>","text":"<p>Called during the teardown of the training process.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def teardown(trainer):\n\"\"\"Called during the teardown of the training process.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_val_start","title":"<code>ultralytics.utils.callbacks.base.on_val_start(validator)</code>","text":"<p>Called when the validation starts.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_val_start(validator):\n\"\"\"Called when the validation starts.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_val_batch_start","title":"<code>ultralytics.utils.callbacks.base.on_val_batch_start(validator)</code>","text":"<p>Called at the start of each validation batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_val_batch_start(validator):\n\"\"\"Called at the start of each validation batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_val_batch_end","title":"<code>ultralytics.utils.callbacks.base.on_val_batch_end(validator)</code>","text":"<p>Called at the end of each validation batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_val_batch_end(validator):\n\"\"\"Called at the end of each validation batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_val_end","title":"<code>ultralytics.utils.callbacks.base.on_val_end(validator)</code>","text":"<p>Called when the validation ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_val_end(validator):\n\"\"\"Called when the validation ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_predict_start","title":"<code>ultralytics.utils.callbacks.base.on_predict_start(predictor)</code>","text":"<p>Called when the prediction starts.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_predict_start(predictor):\n\"\"\"Called when the prediction starts.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_predict_batch_start","title":"<code>ultralytics.utils.callbacks.base.on_predict_batch_start(predictor)</code>","text":"<p>Called at the start of each prediction batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_predict_batch_start(predictor):\n\"\"\"Called at the start of each prediction batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_predict_batch_end","title":"<code>ultralytics.utils.callbacks.base.on_predict_batch_end(predictor)</code>","text":"<p>Called at the end of each prediction batch.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_predict_batch_end(predictor):\n\"\"\"Called at the end of each prediction batch.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_predict_postprocess_end","title":"<code>ultralytics.utils.callbacks.base.on_predict_postprocess_end(predictor)</code>","text":"<p>Called after the post-processing of the prediction ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_predict_postprocess_end(predictor):\n\"\"\"Called after the post-processing of the prediction ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_predict_end","title":"<code>ultralytics.utils.callbacks.base.on_predict_end(predictor)</code>","text":"<p>Called when the prediction ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_predict_end(predictor):\n\"\"\"Called when the prediction ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_export_start","title":"<code>ultralytics.utils.callbacks.base.on_export_start(exporter)</code>","text":"<p>Called when the model export starts.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_export_start(exporter):\n\"\"\"Called when the model export starts.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.on_export_end","title":"<code>ultralytics.utils.callbacks.base.on_export_end(exporter)</code>","text":"<p>Called when the model export ends.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def on_export_end(exporter):\n\"\"\"Called when the model export ends.\"\"\"\npass\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.get_default_callbacks","title":"<code>ultralytics.utils.callbacks.base.get_default_callbacks()</code>","text":"<p>Return a copy of the default_callbacks dictionary with lists as default values.</p> <p>Returns:</p> Type Description <code>defaultdict</code> <p>A defaultdict with keys from default_callbacks and empty lists as default values.</p> Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def get_default_callbacks():\n\"\"\"\n    Return a copy of the default_callbacks dictionary with lists as default values.\n    Returns:\n        (defaultdict): A defaultdict with keys from default_callbacks and empty lists as default values.\n    \"\"\"\nreturn defaultdict(list, deepcopy(default_callbacks))\n</code></pre>"},{"location":"reference/utils/callbacks/base/#ultralytics.utils.callbacks.base.add_integration_callbacks","title":"<code>ultralytics.utils.callbacks.base.add_integration_callbacks(instance)</code>","text":"<p>Add integration callbacks from various sources to the instance's callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>(Trainer, Predictor, Validator, Exporter)</code> <p>An object with a 'callbacks' attribute that is a dictionary of callback lists.</p> required Source code in <code>ultralytics/utils/callbacks/base.py</code> <pre><code>def add_integration_callbacks(instance):\n\"\"\"\n    Add integration callbacks from various sources to the instance's callbacks.\n    Args:\n        instance (Trainer, Predictor, Validator, Exporter): An object with a 'callbacks' attribute that is a dictionary\n            of callback lists.\n    \"\"\"\nfrom .clearml import callbacks as clearml_cb\nfrom .comet import callbacks as comet_cb\nfrom .dvc import callbacks as dvc_cb\nfrom .hub import callbacks as hub_cb\nfrom .mlflow import callbacks as mlflow_cb\nfrom .neptune import callbacks as neptune_cb\nfrom .raytune import callbacks as tune_cb\nfrom .tensorboard import callbacks as tensorboard_cb\nfrom .wb import callbacks as wb_cb\nfor x in clearml_cb, comet_cb, hub_cb, mlflow_cb, neptune_cb, tune_cb, tensorboard_cb, wb_cb, dvc_cb:\nfor k, v in x.items():\nif v not in instance.callbacks[k]:  # prevent duplicate callbacks addition\ninstance.callbacks[k].append(v)  # callback[name].append(func)\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/","title":"Reference for <code>ultralytics/utils/callbacks/clearml.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/clearml.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml._log_debug_samples","title":"<code>ultralytics.utils.callbacks.clearml._log_debug_samples(files, title='Debug Samples')</code>","text":"<p>Log files (images) as debug samples in the ClearML task.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list</code> <p>A list of file paths in PosixPath format.</p> required <code>title</code> <code>str</code> <p>A title that groups together images with the same values.</p> <code>'Debug Samples'</code> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def _log_debug_samples(files, title='Debug Samples') -&gt; None:\n\"\"\"\n    Log files (images) as debug samples in the ClearML task.\n    Args:\n        files (list): A list of file paths in PosixPath format.\n        title (str): A title that groups together images with the same values.\n    \"\"\"\nif task := Task.current_task():\nfor f in files:\nif f.exists():\nit = re.search(r'_batch(\\d+)', f.name)\niteration = int(it.groups()[0]) if it else 0\ntask.get_logger().report_image(title=title,\nseries=f.name.replace(it.group(), ''),\nlocal_path=str(f),\niteration=iteration)\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml._log_plot","title":"<code>ultralytics.utils.callbacks.clearml._log_plot(title, plot_path)</code>","text":"<p>Log an image as a plot in the plot section of ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the plot.</p> required <code>plot_path</code> <code>str</code> <p>The path to the saved image file.</p> required Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def _log_plot(title, plot_path) -&gt; None:\n\"\"\"\n    Log an image as a plot in the plot section of ClearML.\n    Args:\n        title (str): The title of the plot.\n        plot_path (str): The path to the saved image file.\n    \"\"\"\nimg = mpimg.imread(plot_path)\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect='auto', xticks=[], yticks=[])  # no ticks\nax.imshow(img)\nTask.current_task().get_logger().report_matplotlib_figure(title=title,\nseries='',\nfigure=fig,\nreport_interactive=False)\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.clearml.on_pretrain_routine_start(trainer)</code>","text":"<p>Runs at start of pretraining routine; initializes and connects/ logs task to ClearML.</p> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Runs at start of pretraining routine; initializes and connects/ logs task to ClearML.\"\"\"\ntry:\nif task := Task.current_task():\n# Make sure the automatic pytorch and matplotlib bindings are disabled!\n# We are logging these plots and model files manually in the integration\nPatchPyTorchModelIO.update_current_task(None)\nPatchedMatplotlib.update_current_task(None)\nelse:\ntask = Task.init(project_name=trainer.args.project or 'YOLOv8',\ntask_name=trainer.args.name,\ntags=['YOLOv8'],\noutput_uri=True,\nreuse_last_task_id=False,\nauto_connect_frameworks={\n'pytorch': False,\n'matplotlib': False})\nLOGGER.warning('ClearML Initialized a new task. If you want to run remotely, '\n'please add clearml-init and connect your arguments before initializing YOLO.')\ntask.connect(vars(trainer.args), name='General')\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f ClearML installed but not initialized correctly, not logging this run. {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml.on_train_epoch_end","title":"<code>ultralytics.utils.callbacks.clearml.on_train_epoch_end(trainer)</code>","text":"<p>Logs debug samples for the first epoch of YOLO training and report current training progress.</p> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def on_train_epoch_end(trainer):\n\"\"\"Logs debug samples for the first epoch of YOLO training and report current training progress.\"\"\"\nif task := Task.current_task():\n# Log debug samples\nif trainer.epoch == 1:\n_log_debug_samples(sorted(trainer.save_dir.glob('train_batch*.jpg')), 'Mosaic')\n# Report the current training progress\nfor k, v in trainer.validator.metrics.results_dict.items():\ntask.get_logger().report_scalar('train', k, v, iteration=trainer.epoch)\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.clearml.on_fit_epoch_end(trainer)</code>","text":"<p>Reports model information to logger at the end of an epoch.</p> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Reports model information to logger at the end of an epoch.\"\"\"\nif task := Task.current_task():\n# You should have access to the validation bboxes under jdict\ntask.get_logger().report_scalar(title='Epoch Time',\nseries='Epoch Time',\nvalue=trainer.epoch_time,\niteration=trainer.epoch)\nif trainer.epoch == 0:\nfor k, v in model_info_for_loggers(trainer).items():\ntask.get_logger().report_single_value(k, v)\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml.on_val_end","title":"<code>ultralytics.utils.callbacks.clearml.on_val_end(validator)</code>","text":"<p>Logs validation results including labels and predictions.</p> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def on_val_end(validator):\n\"\"\"Logs validation results including labels and predictions.\"\"\"\nif Task.current_task():\n# Log val_labels and val_pred\n_log_debug_samples(sorted(validator.save_dir.glob('val*.jpg')), 'Validation')\n</code></pre>"},{"location":"reference/utils/callbacks/clearml/#ultralytics.utils.callbacks.clearml.on_train_end","title":"<code>ultralytics.utils.callbacks.clearml.on_train_end(trainer)</code>","text":"<p>Logs final model and its name on training completion.</p> Source code in <code>ultralytics/utils/callbacks/clearml.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Logs final model and its name on training completion.\"\"\"\nif task := Task.current_task():\n# Log final results, CM matrix + PR plots\nfiles = [\n'results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png',\n*(f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R'))]\nfiles = [(trainer.save_dir / f) for f in files if (trainer.save_dir / f).exists()]  # filter\nfor f in files:\n_log_plot(title=f.stem, plot_path=f)\n# Report final metrics\nfor k, v in trainer.validator.metrics.results_dict.items():\ntask.get_logger().report_single_value(k, v)\n# Log the final model\ntask.update_output_model(model_path=str(trainer.best), model_name=trainer.args.name, auto_delete_file=False)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/","title":"Reference for <code>ultralytics/utils/callbacks/comet.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/comet.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._get_comet_mode","title":"<code>ultralytics.utils.callbacks.comet._get_comet_mode()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _get_comet_mode():\nreturn os.getenv('COMET_MODE', 'online')\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._get_comet_model_name","title":"<code>ultralytics.utils.callbacks.comet._get_comet_model_name()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _get_comet_model_name():\nreturn os.getenv('COMET_MODEL_NAME', 'YOLOv8')\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._get_eval_batch_logging_interval","title":"<code>ultralytics.utils.callbacks.comet._get_eval_batch_logging_interval()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _get_eval_batch_logging_interval():\nreturn int(os.getenv('COMET_EVAL_BATCH_LOGGING_INTERVAL', 1))\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._get_max_image_predictions_to_log","title":"<code>ultralytics.utils.callbacks.comet._get_max_image_predictions_to_log()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _get_max_image_predictions_to_log():\nreturn int(os.getenv('COMET_MAX_IMAGE_PREDICTIONS', 100))\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._scale_confidence_score","title":"<code>ultralytics.utils.callbacks.comet._scale_confidence_score(score)</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _scale_confidence_score(score):\nscale = float(os.getenv('COMET_MAX_CONFIDENCE_SCORE', 100.0))\nreturn score * scale\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._should_log_confusion_matrix","title":"<code>ultralytics.utils.callbacks.comet._should_log_confusion_matrix()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _should_log_confusion_matrix():\nreturn os.getenv('COMET_EVAL_LOG_CONFUSION_MATRIX', 'false').lower() == 'true'\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._should_log_image_predictions","title":"<code>ultralytics.utils.callbacks.comet._should_log_image_predictions()</code>","text":"Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _should_log_image_predictions():\nreturn os.getenv('COMET_EVAL_LOG_IMAGE_PREDICTIONS', 'true').lower() == 'true'\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._get_experiment_type","title":"<code>ultralytics.utils.callbacks.comet._get_experiment_type(mode, project_name)</code>","text":"<p>Return an experiment based on mode and project name.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _get_experiment_type(mode, project_name):\n\"\"\"Return an experiment based on mode and project name.\"\"\"\nif mode == 'offline':\nreturn comet_ml.OfflineExperiment(project_name=project_name)\nreturn comet_ml.Experiment(project_name=project_name)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._create_experiment","title":"<code>ultralytics.utils.callbacks.comet._create_experiment(args)</code>","text":"<p>Ensures that the experiment object is only created in a single process during distributed training.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _create_experiment(args):\n\"\"\"Ensures that the experiment object is only created in a single process during distributed training.\"\"\"\nif RANK not in (-1, 0):\nreturn\ntry:\ncomet_mode = _get_comet_mode()\n_project_name = os.getenv('COMET_PROJECT_NAME', args.project)\nexperiment = _get_experiment_type(comet_mode, _project_name)\nexperiment.log_parameters(vars(args))\nexperiment.log_others({\n'eval_batch_logging_interval': _get_eval_batch_logging_interval(),\n'log_confusion_matrix_on_eval': _should_log_confusion_matrix(),\n'log_image_predictions': _should_log_image_predictions(),\n'max_image_predictions': _get_max_image_predictions_to_log(), })\nexperiment.log_other('Created from', 'yolov8')\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f Comet installed but not initialized correctly, not logging this run. {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._fetch_trainer_metadata","title":"<code>ultralytics.utils.callbacks.comet._fetch_trainer_metadata(trainer)</code>","text":"<p>Returns metadata for YOLO training including epoch and asset saving status.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _fetch_trainer_metadata(trainer):\n\"\"\"Returns metadata for YOLO training including epoch and asset saving status.\"\"\"\ncurr_epoch = trainer.epoch + 1\ntrain_num_steps_per_epoch = len(trainer.train_loader.dataset) // trainer.batch_size\ncurr_step = curr_epoch * train_num_steps_per_epoch\nfinal_epoch = curr_epoch == trainer.epochs\nsave = trainer.args.save\nsave_period = trainer.args.save_period\nsave_interval = curr_epoch % save_period == 0\nsave_assets = save and save_period &gt; 0 and save_interval and not final_epoch\nreturn dict(\ncurr_epoch=curr_epoch,\ncurr_step=curr_step,\nsave_assets=save_assets,\nfinal_epoch=final_epoch,\n)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._scale_bounding_box_to_original_image_shape","title":"<code>ultralytics.utils.callbacks.comet._scale_bounding_box_to_original_image_shape(box, resized_image_shape, original_image_shape, ratio_pad)</code>","text":"<p>YOLOv8 resizes images during training and the label values are normalized based on this resized shape. This function rescales the bounding box labels to the original image shape.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _scale_bounding_box_to_original_image_shape(box, resized_image_shape, original_image_shape, ratio_pad):\n\"\"\"YOLOv8 resizes images during training and the label values\n    are normalized based on this resized shape. This function rescales the\n    bounding box labels to the original image shape.\n    \"\"\"\nresized_image_height, resized_image_width = resized_image_shape\n# Convert normalized xywh format predictions to xyxy in resized scale format\nbox = ops.xywhn2xyxy(box, h=resized_image_height, w=resized_image_width)\n# Scale box predictions from resized image scale back to original image scale\nbox = ops.scale_boxes(resized_image_shape, box, original_image_shape, ratio_pad)\n# Convert bounding box format from xyxy to xywh for Comet logging\nbox = ops.xyxy2xywh(box)\n# Adjust xy center to correspond top-left corner\nbox[:2] -= box[2:] / 2\nbox = box.tolist()\nreturn box\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._format_ground_truth_annotations_for_detection","title":"<code>ultralytics.utils.callbacks.comet._format_ground_truth_annotations_for_detection(img_idx, image_path, batch, class_name_map=None)</code>","text":"<p>Format ground truth annotations for detection.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _format_ground_truth_annotations_for_detection(img_idx, image_path, batch, class_name_map=None):\n\"\"\"Format ground truth annotations for detection.\"\"\"\nindices = batch['batch_idx'] == img_idx\nbboxes = batch['bboxes'][indices]\nif len(bboxes) == 0:\nLOGGER.debug(f'COMET WARNING: Image: {image_path} has no bounding boxes labels')\nreturn None\ncls_labels = batch['cls'][indices].squeeze(1).tolist()\nif class_name_map:\ncls_labels = [str(class_name_map[label]) for label in cls_labels]\noriginal_image_shape = batch['ori_shape'][img_idx]\nresized_image_shape = batch['resized_shape'][img_idx]\nratio_pad = batch['ratio_pad'][img_idx]\ndata = []\nfor box, label in zip(bboxes, cls_labels):\nbox = _scale_bounding_box_to_original_image_shape(box, resized_image_shape, original_image_shape, ratio_pad)\ndata.append({\n'boxes': [box],\n'label': f'gt_{label}',\n'score': _scale_confidence_score(1.0), })\nreturn {'name': 'ground_truth', 'data': data}\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._format_prediction_annotations_for_detection","title":"<code>ultralytics.utils.callbacks.comet._format_prediction_annotations_for_detection(image_path, metadata, class_label_map=None)</code>","text":"<p>Format YOLO predictions for object detection visualization.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _format_prediction_annotations_for_detection(image_path, metadata, class_label_map=None):\n\"\"\"Format YOLO predictions for object detection visualization.\"\"\"\nstem = image_path.stem\nimage_id = int(stem) if stem.isnumeric() else stem\npredictions = metadata.get(image_id)\nif not predictions:\nLOGGER.debug(f'COMET WARNING: Image: {image_path} has no bounding boxes predictions')\nreturn None\ndata = []\nfor prediction in predictions:\nboxes = prediction['bbox']\nscore = _scale_confidence_score(prediction['score'])\ncls_label = prediction['category_id']\nif class_label_map:\ncls_label = str(class_label_map[cls_label])\ndata.append({'boxes': [boxes], 'label': cls_label, 'score': score})\nreturn {'name': 'prediction', 'data': data}\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._fetch_annotations","title":"<code>ultralytics.utils.callbacks.comet._fetch_annotations(img_idx, image_path, batch, prediction_metadata_map, class_label_map)</code>","text":"<p>Join the ground truth and prediction annotations if they exist.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _fetch_annotations(img_idx, image_path, batch, prediction_metadata_map, class_label_map):\n\"\"\"Join the ground truth and prediction annotations if they exist.\"\"\"\nground_truth_annotations = _format_ground_truth_annotations_for_detection(img_idx, image_path, batch,\nclass_label_map)\nprediction_annotations = _format_prediction_annotations_for_detection(image_path, prediction_metadata_map,\nclass_label_map)\nannotations = [\nannotation for annotation in [ground_truth_annotations, prediction_annotations] if annotation is not None]\nreturn [annotations] if annotations else None\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._create_prediction_metadata_map","title":"<code>ultralytics.utils.callbacks.comet._create_prediction_metadata_map(model_predictions)</code>","text":"<p>Create metadata map for model predictions by groupings them based on image ID.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _create_prediction_metadata_map(model_predictions):\n\"\"\"Create metadata map for model predictions by groupings them based on image ID.\"\"\"\npred_metadata_map = {}\nfor prediction in model_predictions:\npred_metadata_map.setdefault(prediction['image_id'], [])\npred_metadata_map[prediction['image_id']].append(prediction)\nreturn pred_metadata_map\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._log_confusion_matrix","title":"<code>ultralytics.utils.callbacks.comet._log_confusion_matrix(experiment, trainer, curr_step, curr_epoch)</code>","text":"<p>Log the confusion matrix to Comet experiment.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _log_confusion_matrix(experiment, trainer, curr_step, curr_epoch):\n\"\"\"Log the confusion matrix to Comet experiment.\"\"\"\nconf_mat = trainer.validator.confusion_matrix.matrix\nnames = list(trainer.data['names'].values()) + ['background']\nexperiment.log_confusion_matrix(\nmatrix=conf_mat,\nlabels=names,\nmax_categories=len(names),\nepoch=curr_epoch,\nstep=curr_step,\n)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._log_images","title":"<code>ultralytics.utils.callbacks.comet._log_images(experiment, image_paths, curr_step, annotations=None)</code>","text":"<p>Logs images to the experiment with optional annotations.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _log_images(experiment, image_paths, curr_step, annotations=None):\n\"\"\"Logs images to the experiment with optional annotations.\"\"\"\nif annotations:\nfor image_path, annotation in zip(image_paths, annotations):\nexperiment.log_image(image_path, name=image_path.stem, step=curr_step, annotations=annotation)\nelse:\nfor image_path in image_paths:\nexperiment.log_image(image_path, name=image_path.stem, step=curr_step)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._log_image_predictions","title":"<code>ultralytics.utils.callbacks.comet._log_image_predictions(experiment, validator, curr_step)</code>","text":"<p>Logs predicted boxes for a single image during training.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _log_image_predictions(experiment, validator, curr_step):\n\"\"\"Logs predicted boxes for a single image during training.\"\"\"\nglobal _comet_image_prediction_count\ntask = validator.args.task\nif task not in COMET_SUPPORTED_TASKS:\nreturn\njdict = validator.jdict\nif not jdict:\nreturn\npredictions_metadata_map = _create_prediction_metadata_map(jdict)\ndataloader = validator.dataloader\nclass_label_map = validator.names\nbatch_logging_interval = _get_eval_batch_logging_interval()\nmax_image_predictions = _get_max_image_predictions_to_log()\nfor batch_idx, batch in enumerate(dataloader):\nif (batch_idx + 1) % batch_logging_interval != 0:\ncontinue\nimage_paths = batch['im_file']\nfor img_idx, image_path in enumerate(image_paths):\nif _comet_image_prediction_count &gt;= max_image_predictions:\nreturn\nimage_path = Path(image_path)\nannotations = _fetch_annotations(\nimg_idx,\nimage_path,\nbatch,\npredictions_metadata_map,\nclass_label_map,\n)\n_log_images(\nexperiment,\n[image_path],\ncurr_step,\nannotations=annotations,\n)\n_comet_image_prediction_count += 1\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._log_plots","title":"<code>ultralytics.utils.callbacks.comet._log_plots(experiment, trainer)</code>","text":"<p>Logs evaluation plots and label plots for the experiment.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _log_plots(experiment, trainer):\n\"\"\"Logs evaluation plots and label plots for the experiment.\"\"\"\nplot_filenames = [trainer.save_dir / f'{plots}.png' for plots in EVALUATION_PLOT_NAMES]\n_log_images(experiment, plot_filenames, None)\nlabel_plot_filenames = [trainer.save_dir / f'{labels}.jpg' for labels in LABEL_PLOT_NAMES]\n_log_images(experiment, label_plot_filenames, None)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet._log_model","title":"<code>ultralytics.utils.callbacks.comet._log_model(experiment, trainer)</code>","text":"<p>Log the best-trained model to Comet.ml.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def _log_model(experiment, trainer):\n\"\"\"Log the best-trained model to Comet.ml.\"\"\"\nmodel_name = _get_comet_model_name()\nexperiment.log_model(\nmodel_name,\nfile_or_folder=str(trainer.best),\nfile_name='best.pt',\noverwrite=True,\n)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.comet.on_pretrain_routine_start(trainer)</code>","text":"<p>Creates or resumes a CometML experiment at the start of a YOLO pre-training routine.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Creates or resumes a CometML experiment at the start of a YOLO pre-training routine.\"\"\"\nexperiment = comet_ml.get_global_experiment()\nis_alive = getattr(experiment, 'alive', False)\nif not experiment or not is_alive:\n_create_experiment(trainer.args)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet.on_train_epoch_end","title":"<code>ultralytics.utils.callbacks.comet.on_train_epoch_end(trainer)</code>","text":"<p>Log metrics and save batch images at the end of training epochs.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def on_train_epoch_end(trainer):\n\"\"\"Log metrics and save batch images at the end of training epochs.\"\"\"\nexperiment = comet_ml.get_global_experiment()\nif not experiment:\nreturn\nmetadata = _fetch_trainer_metadata(trainer)\ncurr_epoch = metadata['curr_epoch']\ncurr_step = metadata['curr_step']\nexperiment.log_metrics(\ntrainer.label_loss_items(trainer.tloss, prefix='train'),\nstep=curr_step,\nepoch=curr_epoch,\n)\nif curr_epoch == 1:\n_log_images(experiment, trainer.save_dir.glob('train_batch*.jpg'), curr_step)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.comet.on_fit_epoch_end(trainer)</code>","text":"<p>Logs model assets at the end of each epoch.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Logs model assets at the end of each epoch.\"\"\"\nexperiment = comet_ml.get_global_experiment()\nif not experiment:\nreturn\nmetadata = _fetch_trainer_metadata(trainer)\ncurr_epoch = metadata['curr_epoch']\ncurr_step = metadata['curr_step']\nsave_assets = metadata['save_assets']\nexperiment.log_metrics(trainer.metrics, step=curr_step, epoch=curr_epoch)\nexperiment.log_metrics(trainer.lr, step=curr_step, epoch=curr_epoch)\nif curr_epoch == 1:\nexperiment.log_metrics(model_info_for_loggers(trainer), step=curr_step, epoch=curr_epoch)\nif not save_assets:\nreturn\n_log_model(experiment, trainer)\nif _should_log_confusion_matrix():\n_log_confusion_matrix(experiment, trainer, curr_step, curr_epoch)\nif _should_log_image_predictions():\n_log_image_predictions(experiment, trainer.validator, curr_step)\n</code></pre>"},{"location":"reference/utils/callbacks/comet/#ultralytics.utils.callbacks.comet.on_train_end","title":"<code>ultralytics.utils.callbacks.comet.on_train_end(trainer)</code>","text":"<p>Perform operations at the end of training.</p> Source code in <code>ultralytics/utils/callbacks/comet.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Perform operations at the end of training.\"\"\"\nexperiment = comet_ml.get_global_experiment()\nif not experiment:\nreturn\nmetadata = _fetch_trainer_metadata(trainer)\ncurr_epoch = metadata['curr_epoch']\ncurr_step = metadata['curr_step']\nplots = trainer.args.plots\n_log_model(experiment, trainer)\nif plots:\n_log_plots(experiment, trainer)\n_log_confusion_matrix(experiment, trainer, curr_step, curr_epoch)\n_log_image_predictions(experiment, trainer.validator, curr_step)\nexperiment.end()\nglobal _comet_image_prediction_count\n_comet_image_prediction_count = 0\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/","title":"Reference for <code>ultralytics/utils/callbacks/dvc.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/dvc.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc._log_images","title":"<code>ultralytics.utils.callbacks.dvc._log_images(path, prefix='')</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def _log_images(path, prefix=''):\nif live:\nname = path.name\n# Group images by batch to enable sliders in UI\nif m := re.search(r'_batch(\\d+)', name):\nni = m[1]\nnew_stem = re.sub(r'_batch(\\d+)', '_batch', path.stem)\nname = (Path(new_stem) / ni).with_suffix(path.suffix)\nlive.log_image(os.path.join(prefix, name), path)\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc._log_plots","title":"<code>ultralytics.utils.callbacks.dvc._log_plots(plots, prefix='')</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def _log_plots(plots, prefix=''):\nfor name, params in plots.items():\ntimestamp = params['timestamp']\nif _processed_plots.get(name) != timestamp:\n_log_images(name, prefix)\n_processed_plots[name] = timestamp\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc._log_confusion_matrix","title":"<code>ultralytics.utils.callbacks.dvc._log_confusion_matrix(validator)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def _log_confusion_matrix(validator):\ntargets = []\npreds = []\nmatrix = validator.confusion_matrix.matrix\nnames = list(validator.names.values())\nif validator.confusion_matrix.task == 'detect':\nnames += ['background']\nfor ti, pred in enumerate(matrix.T.astype(int)):\nfor pi, num in enumerate(pred):\ntargets.extend([names[ti]] * num)\npreds.extend([names[pi]] * num)\nlive.log_sklearn_plot('confusion_matrix', targets, preds, name='cf.json', normalized=True)\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.dvc.on_pretrain_routine_start(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_pretrain_routine_start(trainer):\ntry:\nglobal live\nlive = dvclive.Live(save_dvc_exp=True, cache_images=True)\nLOGGER.info(\nf'DVCLive is detected and auto logging is enabled (can be disabled in the {SETTINGS.file} with `dvc: false`).'\n)\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f DVCLive installed but not initialized correctly, not logging this run. {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_pretrain_routine_end","title":"<code>ultralytics.utils.callbacks.dvc.on_pretrain_routine_end(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_pretrain_routine_end(trainer):\n_log_plots(trainer.plots, 'train')\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_train_start","title":"<code>ultralytics.utils.callbacks.dvc.on_train_start(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_train_start(trainer):\nif live:\nlive.log_params(trainer.args)\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_train_epoch_start","title":"<code>ultralytics.utils.callbacks.dvc.on_train_epoch_start(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_train_epoch_start(trainer):\nglobal _training_epoch\n_training_epoch = True\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.dvc.on_fit_epoch_end(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_fit_epoch_end(trainer):\nglobal _training_epoch\nif live and _training_epoch:\nall_metrics = {**trainer.label_loss_items(trainer.tloss, prefix='train'), **trainer.metrics, **trainer.lr}\nfor metric, value in all_metrics.items():\nlive.log_metric(metric, value)\nif trainer.epoch == 0:\nfor metric, value in model_info_for_loggers(trainer).items():\nlive.log_metric(metric, value, plot=False)\n_log_plots(trainer.plots, 'train')\n_log_plots(trainer.validator.plots, 'val')\nlive.next_step()\n_training_epoch = False\n</code></pre>"},{"location":"reference/utils/callbacks/dvc/#ultralytics.utils.callbacks.dvc.on_train_end","title":"<code>ultralytics.utils.callbacks.dvc.on_train_end(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/dvc.py</code> <pre><code>def on_train_end(trainer):\nif live:\n# At the end log the best metrics. It runs validator on the best model internally.\nall_metrics = {**trainer.label_loss_items(trainer.tloss, prefix='train'), **trainer.metrics, **trainer.lr}\nfor metric, value in all_metrics.items():\nlive.log_metric(metric, value, plot=False)\n_log_plots(trainer.plots, 'val')\n_log_plots(trainer.validator.plots, 'val')\n_log_confusion_matrix(trainer.validator)\nif trainer.best.exists():\nlive.log_artifact(trainer.best, copy=True, type='model')\nlive.end()\n</code></pre>"},{"location":"reference/utils/callbacks/hub/","title":"Reference for <code>ultralytics/utils/callbacks/hub.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/hub.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_pretrain_routine_end","title":"<code>ultralytics.utils.callbacks.hub.on_pretrain_routine_end(trainer)</code>","text":"<p>Logs info before starting timer for upload rate limit.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_pretrain_routine_end(trainer):\n\"\"\"Logs info before starting timer for upload rate limit.\"\"\"\nsession = getattr(trainer, 'hub_session', None)\nif session:\n# Start timer for upload rate limit\nLOGGER.info(f'{PREFIX}View model at {HUB_WEB_ROOT}/models/{session.model_id} \ud83d\ude80')\nsession.timers = {'metrics': time(), 'ckpt': time()}  # start timer on session.rate_limit\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.hub.on_fit_epoch_end(trainer)</code>","text":"<p>Uploads training progress metrics at the end of each epoch.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Uploads training progress metrics at the end of each epoch.\"\"\"\nsession = getattr(trainer, 'hub_session', None)\nif session:\n# Upload metrics after val end\nall_plots = {**trainer.label_loss_items(trainer.tloss, prefix='train'), **trainer.metrics}\nif trainer.epoch == 0:\nall_plots = {**all_plots, **model_info_for_loggers(trainer)}\nsession.metrics_queue[trainer.epoch] = json.dumps(all_plots)\nif time() - session.timers['metrics'] &gt; session.rate_limits['metrics']:\nsession.upload_metrics()\nsession.timers['metrics'] = time()  # reset timer\nsession.metrics_queue = {}  # reset queue\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_model_save","title":"<code>ultralytics.utils.callbacks.hub.on_model_save(trainer)</code>","text":"<p>Saves checkpoints to Ultralytics HUB with rate limiting.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_model_save(trainer):\n\"\"\"Saves checkpoints to Ultralytics HUB with rate limiting.\"\"\"\nsession = getattr(trainer, 'hub_session', None)\nif session:\n# Upload checkpoints with rate limiting\nis_best = trainer.best_fitness == trainer.fitness\nif time() - session.timers['ckpt'] &gt; session.rate_limits['ckpt']:\nLOGGER.info(f'{PREFIX}Uploading checkpoint {HUB_WEB_ROOT}/models/{session.model_id}')\nsession.upload_model(trainer.epoch, trainer.last, is_best)\nsession.timers['ckpt'] = time()  # reset timer\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_train_end","title":"<code>ultralytics.utils.callbacks.hub.on_train_end(trainer)</code>","text":"<p>Upload final model and metrics to Ultralytics HUB at the end of training.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Upload final model and metrics to Ultralytics HUB at the end of training.\"\"\"\nsession = getattr(trainer, 'hub_session', None)\nif session:\n# Upload final model and metrics with exponential standoff\nLOGGER.info(f'{PREFIX}Syncing final model...')\nsession.upload_model(trainer.epoch, trainer.best, map=trainer.metrics.get('metrics/mAP50-95(B)', 0), final=True)\nsession.alive = False  # stop heartbeats\nLOGGER.info(f'{PREFIX}Done \u2705\\n'\nf'{PREFIX}View model at {HUB_WEB_ROOT}/models/{session.model_id} \ud83d\ude80')\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_train_start","title":"<code>ultralytics.utils.callbacks.hub.on_train_start(trainer)</code>","text":"<p>Run events on train start.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_train_start(trainer):\n\"\"\"Run events on train start.\"\"\"\nevents(trainer.args)\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_val_start","title":"<code>ultralytics.utils.callbacks.hub.on_val_start(validator)</code>","text":"<p>Runs events on validation start.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_val_start(validator):\n\"\"\"Runs events on validation start.\"\"\"\nevents(validator.args)\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_predict_start","title":"<code>ultralytics.utils.callbacks.hub.on_predict_start(predictor)</code>","text":"<p>Run events on predict start.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_predict_start(predictor):\n\"\"\"Run events on predict start.\"\"\"\nevents(predictor.args)\n</code></pre>"},{"location":"reference/utils/callbacks/hub/#ultralytics.utils.callbacks.hub.on_export_start","title":"<code>ultralytics.utils.callbacks.hub.on_export_start(exporter)</code>","text":"<p>Run events on export start.</p> Source code in <code>ultralytics/utils/callbacks/hub.py</code> <pre><code>def on_export_start(exporter):\n\"\"\"Run events on export start.\"\"\"\nevents(exporter.args)\n</code></pre>"},{"location":"reference/utils/callbacks/mlflow/","title":"Reference for <code>ultralytics/utils/callbacks/mlflow.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/mlflow.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/mlflow/#ultralytics.utils.callbacks.mlflow.on_pretrain_routine_end","title":"<code>ultralytics.utils.callbacks.mlflow.on_pretrain_routine_end(trainer)</code>","text":"<p>Logs training parameters to MLflow.</p> Source code in <code>ultralytics/utils/callbacks/mlflow.py</code> <pre><code>def on_pretrain_routine_end(trainer):\n\"\"\"Logs training parameters to MLflow.\"\"\"\nglobal mlflow, run, experiment_name\nif os.environ.get('MLFLOW_TRACKING_URI') is None:\nmlflow = None\nif mlflow:\nmlflow_location = os.environ['MLFLOW_TRACKING_URI']  # \"http://192.168.xxx.xxx:5000\"\nmlflow.set_tracking_uri(mlflow_location)\nexperiment_name = os.environ.get('MLFLOW_EXPERIMENT_NAME') or trainer.args.project or '/Shared/YOLOv8'\nrun_name = os.environ.get('MLFLOW_RUN') or trainer.args.name\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nif experiment is None:\nmlflow.create_experiment(experiment_name)\nmlflow.set_experiment(experiment_name)\nprefix = colorstr('MLFlow: ')\ntry:\nrun, active_run = mlflow, mlflow.active_run()\nif not active_run:\nactive_run = mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name)\nLOGGER.info(f'{prefix}Using run_id({active_run.info.run_id}) at {mlflow_location}')\nrun.log_params(vars(trainer.model.args))\nexcept Exception as err:\nLOGGER.error(f'{prefix}Failing init - {repr(err)}')\nLOGGER.warning(f'{prefix}Continuing without Mlflow')\n</code></pre>"},{"location":"reference/utils/callbacks/mlflow/#ultralytics.utils.callbacks.mlflow.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.mlflow.on_fit_epoch_end(trainer)</code>","text":"<p>Logs training metrics to Mlflow.</p> Source code in <code>ultralytics/utils/callbacks/mlflow.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Logs training metrics to Mlflow.\"\"\"\nif mlflow:\nmetrics_dict = {f\"{re.sub('[()]', '', k)}\": float(v) for k, v in trainer.metrics.items()}\nrun.log_metrics(metrics=metrics_dict, step=trainer.epoch)\n</code></pre>"},{"location":"reference/utils/callbacks/mlflow/#ultralytics.utils.callbacks.mlflow.on_train_end","title":"<code>ultralytics.utils.callbacks.mlflow.on_train_end(trainer)</code>","text":"<p>Called at end of train loop to log model artifact info.</p> Source code in <code>ultralytics/utils/callbacks/mlflow.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Called at end of train loop to log model artifact info.\"\"\"\nif mlflow:\nroot_dir = Path(__file__).resolve().parents[3]\nrun.log_artifact(trainer.last)\nrun.log_artifact(trainer.best)\nrun.pyfunc.log_model(artifact_path=experiment_name,\ncode_path=[str(root_dir)],\nartifacts={'model_path': str(trainer.save_dir)},\npython_model=run.pyfunc.PythonModel())\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/","title":"Reference for <code>ultralytics/utils/callbacks/neptune.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/neptune.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune._log_scalars","title":"<code>ultralytics.utils.callbacks.neptune._log_scalars(scalars, step=0)</code>","text":"<p>Log scalars to the NeptuneAI experiment logger.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def _log_scalars(scalars, step=0):\n\"\"\"Log scalars to the NeptuneAI experiment logger.\"\"\"\nif run:\nfor k, v in scalars.items():\nrun[k].append(value=v, step=step)\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune._log_images","title":"<code>ultralytics.utils.callbacks.neptune._log_images(imgs_dict, group='')</code>","text":"<p>Log scalars to the NeptuneAI experiment logger.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def _log_images(imgs_dict, group=''):\n\"\"\"Log scalars to the NeptuneAI experiment logger.\"\"\"\nif run:\nfor k, v in imgs_dict.items():\nrun[f'{group}/{k}'].upload(File(v))\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune._log_plot","title":"<code>ultralytics.utils.callbacks.neptune._log_plot(title, plot_path)</code>","text":"<p>Log plots to the NeptuneAI experiment logger.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def _log_plot(title, plot_path):\n\"\"\"Log plots to the NeptuneAI experiment logger.\"\"\"\n\"\"\"\n        Log image as plot in the plot section of NeptuneAI\n        arguments:\n        title (str) Title of the plot\n        plot_path (PosixPath or str) Path to the saved image file\n        \"\"\"\nimg = mpimg.imread(plot_path)\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect='auto', xticks=[], yticks=[])  # no ticks\nax.imshow(img)\nrun[f'Plots/{title}'].upload(fig)\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.neptune.on_pretrain_routine_start(trainer)</code>","text":"<p>Callback function called before the training routine starts.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Callback function called before the training routine starts.\"\"\"\ntry:\nglobal run\nrun = neptune.init_run(project=trainer.args.project or 'YOLOv8', name=trainer.args.name, tags=['YOLOv8'])\nrun['Configuration/Hyperparameters'] = {k: '' if v is None else v for k, v in vars(trainer.args).items()}\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f NeptuneAI installed but not initialized correctly, not logging this run. {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune.on_train_epoch_end","title":"<code>ultralytics.utils.callbacks.neptune.on_train_epoch_end(trainer)</code>","text":"<p>Callback function called at end of each training epoch.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def on_train_epoch_end(trainer):\n\"\"\"Callback function called at end of each training epoch.\"\"\"\n_log_scalars(trainer.label_loss_items(trainer.tloss, prefix='train'), trainer.epoch + 1)\n_log_scalars(trainer.lr, trainer.epoch + 1)\nif trainer.epoch == 1:\n_log_images({f.stem: str(f) for f in trainer.save_dir.glob('train_batch*.jpg')}, 'Mosaic')\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.neptune.on_fit_epoch_end(trainer)</code>","text":"<p>Callback function called at end of each fit (train+val) epoch.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Callback function called at end of each fit (train+val) epoch.\"\"\"\nif run and trainer.epoch == 0:\nrun['Configuration/Model'] = model_info_for_loggers(trainer)\n_log_scalars(trainer.metrics, trainer.epoch + 1)\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune.on_val_end","title":"<code>ultralytics.utils.callbacks.neptune.on_val_end(validator)</code>","text":"<p>Callback function called at end of each validation.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def on_val_end(validator):\n\"\"\"Callback function called at end of each validation.\"\"\"\nif run:\n# Log val_labels and val_pred\n_log_images({f.stem: str(f) for f in validator.save_dir.glob('val*.jpg')}, 'Validation')\n</code></pre>"},{"location":"reference/utils/callbacks/neptune/#ultralytics.utils.callbacks.neptune.on_train_end","title":"<code>ultralytics.utils.callbacks.neptune.on_train_end(trainer)</code>","text":"<p>Callback function called at end of training.</p> Source code in <code>ultralytics/utils/callbacks/neptune.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Callback function called at end of training.\"\"\"\nif run:\n# Log final results, CM matrix + PR plots\nfiles = [\n'results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png',\n*(f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R'))]\nfiles = [(trainer.save_dir / f) for f in files if (trainer.save_dir / f).exists()]  # filter\nfor f in files:\n_log_plot(title=f.stem, plot_path=f)\n# Log the final model\nrun[f'weights/{trainer.args.name or trainer.args.task}/{str(trainer.best.name)}'].upload(File(str(\ntrainer.best)))\n</code></pre>"},{"location":"reference/utils/callbacks/raytune/","title":"Reference for <code>ultralytics/utils/callbacks/raytune.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/raytune.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p>"},{"location":"reference/utils/callbacks/raytune/#ultralytics.utils.callbacks.raytune.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.raytune.on_fit_epoch_end(trainer)</code>","text":"<p>Sends training metrics to Ray Tune at end of each epoch.</p> Source code in <code>ultralytics/utils/callbacks/raytune.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Sends training metrics to Ray Tune at end of each epoch.\"\"\"\nif ray.tune.is_session_enabled():\nmetrics = trainer.metrics\nmetrics['epoch'] = trainer.epoch\nsession.report(metrics)\n</code></pre>"},{"location":"reference/utils/callbacks/tensorboard/","title":"Reference for <code>ultralytics/utils/callbacks/tensorboard.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/tensorboard.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/tensorboard/#ultralytics.utils.callbacks.tensorboard._log_scalars","title":"<code>ultralytics.utils.callbacks.tensorboard._log_scalars(scalars, step=0)</code>","text":"<p>Logs scalar values to TensorBoard.</p> Source code in <code>ultralytics/utils/callbacks/tensorboard.py</code> <pre><code>def _log_scalars(scalars, step=0):\n\"\"\"Logs scalar values to TensorBoard.\"\"\"\nif WRITER:\nfor k, v in scalars.items():\nWRITER.add_scalar(k, v, step)\n</code></pre>"},{"location":"reference/utils/callbacks/tensorboard/#ultralytics.utils.callbacks.tensorboard._log_tensorboard_graph","title":"<code>ultralytics.utils.callbacks.tensorboard._log_tensorboard_graph(trainer)</code>","text":"Source code in <code>ultralytics/utils/callbacks/tensorboard.py</code> <pre><code>def _log_tensorboard_graph(trainer):\n# Log model graph to TensorBoard\ntry:\nimport warnings\nfrom ultralytics.utils.torch_utils import de_parallel, torch\nimgsz = trainer.args.imgsz\nimgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz\np = next(trainer.model.parameters())  # for device, type\nim = torch.zeros((1, 3, *imgsz), device=p.device, dtype=p.dtype)  # input image (must be zeros, not empty)\nwith warnings.catch_warnings():\nwarnings.simplefilter('ignore', category=UserWarning)  # suppress jit trace warning\nWRITER.add_graph(torch.jit.trace(de_parallel(trainer.model), im, strict=False), [])\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f TensorBoard graph visualization failure {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/tensorboard/#ultralytics.utils.callbacks.tensorboard.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.tensorboard.on_pretrain_routine_start(trainer)</code>","text":"<p>Initialize TensorBoard logging with SummaryWriter.</p> Source code in <code>ultralytics/utils/callbacks/tensorboard.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Initialize TensorBoard logging with SummaryWriter.\"\"\"\nif SummaryWriter:\ntry:\nglobal WRITER\nWRITER = SummaryWriter(str(trainer.save_dir))\nprefix = colorstr('TensorBoard: ')\nLOGGER.info(f\"{prefix}Start with 'tensorboard --logdir {trainer.save_dir}', view at http://localhost:6006/\")\n_log_tensorboard_graph(trainer)\nexcept Exception as e:\nLOGGER.warning(f'WARNING \u26a0\ufe0f TensorBoard not initialized correctly, not logging this run. {e}')\n</code></pre>"},{"location":"reference/utils/callbacks/tensorboard/#ultralytics.utils.callbacks.tensorboard.on_batch_end","title":"<code>ultralytics.utils.callbacks.tensorboard.on_batch_end(trainer)</code>","text":"<p>Logs scalar statistics at the end of a training batch.</p> Source code in <code>ultralytics/utils/callbacks/tensorboard.py</code> <pre><code>def on_batch_end(trainer):\n\"\"\"Logs scalar statistics at the end of a training batch.\"\"\"\n_log_scalars(trainer.label_loss_items(trainer.tloss, prefix='train'), trainer.epoch + 1)\n</code></pre>"},{"location":"reference/utils/callbacks/tensorboard/#ultralytics.utils.callbacks.tensorboard.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.tensorboard.on_fit_epoch_end(trainer)</code>","text":"<p>Logs epoch metrics at end of training epoch.</p> Source code in <code>ultralytics/utils/callbacks/tensorboard.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Logs epoch metrics at end of training epoch.\"\"\"\n_log_scalars(trainer.metrics, trainer.epoch + 1)\n</code></pre>"},{"location":"reference/utils/callbacks/wb/","title":"Reference for <code>ultralytics/utils/callbacks/wb.py</code>","text":"<p>Note</p> <p>Full source code for this file is available at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/callbacks/wb.py. Help us fix any issues you see by submitting a Pull Request \ud83d\udee0\ufe0f. Thank you \ud83d\ude4f!</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"reference/utils/callbacks/wb/#ultralytics.utils.callbacks.wb._log_plots","title":"<code>ultralytics.utils.callbacks.wb._log_plots(plots, step)</code>","text":"Source code in <code>ultralytics/utils/callbacks/wb.py</code> <pre><code>def _log_plots(plots, step):\nfor name, params in plots.items():\ntimestamp = params['timestamp']\nif _processed_plots.get(name) != timestamp:\nwb.run.log({name.stem: wb.Image(str(name))}, step=step)\n_processed_plots[name] = timestamp\n</code></pre>"},{"location":"reference/utils/callbacks/wb/#ultralytics.utils.callbacks.wb.on_pretrain_routine_start","title":"<code>ultralytics.utils.callbacks.wb.on_pretrain_routine_start(trainer)</code>","text":"<p>Initiate and start project if module is present.</p> Source code in <code>ultralytics/utils/callbacks/wb.py</code> <pre><code>def on_pretrain_routine_start(trainer):\n\"\"\"Initiate and start project if module is present.\"\"\"\nwb.run or wb.init(project=trainer.args.project or 'YOLOv8', name=trainer.args.name, config=vars(trainer.args))\n</code></pre>"},{"location":"reference/utils/callbacks/wb/#ultralytics.utils.callbacks.wb.on_fit_epoch_end","title":"<code>ultralytics.utils.callbacks.wb.on_fit_epoch_end(trainer)</code>","text":"<p>Logs training metrics and model information at the end of an epoch.</p> Source code in <code>ultralytics/utils/callbacks/wb.py</code> <pre><code>def on_fit_epoch_end(trainer):\n\"\"\"Logs training metrics and model information at the end of an epoch.\"\"\"\nwb.run.log(trainer.metrics, step=trainer.epoch + 1)\n_log_plots(trainer.plots, step=trainer.epoch + 1)\n_log_plots(trainer.validator.plots, step=trainer.epoch + 1)\nif trainer.epoch == 0:\nwb.run.log(model_info_for_loggers(trainer), step=trainer.epoch + 1)\n</code></pre>"},{"location":"reference/utils/callbacks/wb/#ultralytics.utils.callbacks.wb.on_train_epoch_end","title":"<code>ultralytics.utils.callbacks.wb.on_train_epoch_end(trainer)</code>","text":"<p>Log metrics and save images at the end of each training epoch.</p> Source code in <code>ultralytics/utils/callbacks/wb.py</code> <pre><code>def on_train_epoch_end(trainer):\n\"\"\"Log metrics and save images at the end of each training epoch.\"\"\"\nwb.run.log(trainer.label_loss_items(trainer.tloss, prefix='train'), step=trainer.epoch + 1)\nwb.run.log(trainer.lr, step=trainer.epoch + 1)\nif trainer.epoch == 1:\n_log_plots(trainer.plots, step=trainer.epoch + 1)\n</code></pre>"},{"location":"reference/utils/callbacks/wb/#ultralytics.utils.callbacks.wb.on_train_end","title":"<code>ultralytics.utils.callbacks.wb.on_train_end(trainer)</code>","text":"<p>Save the best model as an artifact at end of training.</p> Source code in <code>ultralytics/utils/callbacks/wb.py</code> <pre><code>def on_train_end(trainer):\n\"\"\"Save the best model as an artifact at end of training.\"\"\"\n_log_plots(trainer.validator.plots, step=trainer.epoch + 1)\n_log_plots(trainer.plots, step=trainer.epoch + 1)\nart = wb.Artifact(type='model', name=f'run_{wb.run.id}_model')\nif trainer.best.exists():\nart.add_file(trainer.best)\nwb.run.log_artifact(art, aliases=['best'])\n</code></pre>"},{"location":"tasks/","title":"Ultralytics YOLOv8 Tasks","text":"<p>YOLOv8 is an AI framework that supports multiple computer vision tasks. The framework can be used to perform detection, segmentation, classification, and pose estimation. Each of these tasks has a different objective and use case.</p> <p> </p>"},{"location":"tasks/#detection","title":"Detection","text":"<p>Detection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing bounding boxes around them. The detected objects are classified into different categories based on their features. YOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed.</p> <p>Detection Examples</p>"},{"location":"tasks/#segmentation","title":"Segmentation","text":"<p>Segmentation is a task that involves segmenting an image into different regions based on the content of the image. Each region is assigned a label based on its content. This task is useful in applications such as image segmentation and medical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation.</p> <p>Segmentation Examples</p>"},{"location":"tasks/#classification","title":"Classification","text":"<p>Classification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify images based on their content. It uses a variant of the EfficientNet architecture to perform classification.</p> <p>Classification Examples</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>Pose/keypoint detection is a task that involves detecting specific points in an image or video frame. These points are referred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect keypoints in an image or video frame with high accuracy and speed.</p> <p>Pose Examples</p>"},{"location":"tasks/#conclusion","title":"Conclusion","text":"<p>YOLOv8 supports multiple tasks, including detection, segmentation, classification, and keypoints detection. Each of these tasks has different objectives and use cases. By understanding the differences between these tasks, you can choose the appropriate task for your computer vision application.</p>"},{"location":"tasks/classify/","title":"Classify","text":"<p>Image classification is the simplest of the three tasks and involves classifying an entire image into one of a set of predefined classes.</p> <p></p> <p>The output of an image classifier is a single class label and a confidence score. Image classification is useful when you need to know only what class an image belongs to and don't need to know where objects of that class are located or what their exact shape is.</p> <p>Tip</p> <p>YOLOv8 Classify models use the <code>-cls</code> suffix, i.e. <code>yolov8n-cls.pt</code> and are pretrained on ImageNet.</p>"},{"location":"tasks/classify/#models","title":"Models","text":"<p>YOLOv8 pretrained Classify models are shown here. Detect, Segment and Pose models are pretrained on the COCO dataset, while Classify models are pretrained on the ImageNet dataset.</p> <p>Models download automatically from the latest Ultralytics release on first use.</p> Model size<sup>(pixels) acc<sup>top1 acc<sup>top5 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) at 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>acc values are model accuracies on the ImageNet dataset validation set.   Reproduce by <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Speed averaged over ImageNet val images using an Amazon EC2 P4d   instance.   Reproduce by <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#train","title":"Train","text":"<p>Train YOLOv8n-cls on the MNIST160 dataset for 100 epochs at image size 64. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # build from YAML and transfer weights\n# Train the model\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Build a new model from YAML and start training from scratch\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n# Start training from a pretrained *.pt model\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#dataset-format","title":"Dataset format","text":"<p>YOLO classification dataset format can be found in detail in the Dataset Guide.</p>"},{"location":"tasks/classify/#val","title":"Val","text":"<p>Validate trained YOLOv8n-cls model accuracy on the MNIST160 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.top1   # top1 accuracy\nmetrics.top5   # top5 accuracy\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # val official model\nyolo classify val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/classify/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n-cls model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Predict with the model\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predict with custom model\n</code></pre> <p>See full <code>predict</code> mode details in the Predict page.</p>"},{"location":"tasks/classify/#export","title":"Export","text":"<p>Export a YOLOv8n-cls model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-cls.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom trained\n# Export the model\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8-cls export formats are in the table below. You can predict or validate directly on exported models, i.e. <code>yolo predict model=yolov8n-cls.onnx</code>. Usage examples are shown for your model after export completes.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>See full <code>export</code> details in the Export page.</p>"},{"location":"tasks/detect/","title":"Detect","text":"<p>Object detection is a task that involves identifying the location and class of objects in an image or video stream.</p> <p></p> <p>The output of an object detector is a set of bounding boxes that enclose the objects in the image, along with class labels and confidence scores for each box. Object detection is a good choice when you need to identify objects of interest in a scene, but don't need to know exactly where the object is or its exact shape.</p> <p>Tip</p> <p>YOLOv8 Detect models are the default YOLOv8 models, i.e. <code>yolov8n.pt</code> and are pretrained on COCO.</p>"},{"location":"tasks/detect/#models","title":"Models","text":"<p>YOLOv8 pretrained Detect models are shown here. Detect, Segment and Pose models are pretrained on the COCO dataset, while Classify models are pretrained on the ImageNet dataset.</p> <p>Models download automatically from the latest Ultralytics release on first use.</p> Model size<sup>(pixels) mAP<sup>val50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>mAP<sup>val</sup> values are for single-model single-scale on COCO val2017 dataset.   Reproduce by <code>yolo val detect data=coco.yaml device=0</code></li> <li>Speed averaged over COCO val images using an Amazon EC2 P4d   instance.   Reproduce by <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#train","title":"Train","text":"<p>Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n# Train the model\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Build a new model from YAML and start training from scratch\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n# Start training from a pretrained *.pt model\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#dataset-format","title":"Dataset format","text":"<p>YOLO detection dataset format can be found in detail in the Dataset Guide. To convert your existing dataset from other formats( like COCO etc.) to YOLO format, please use json2yolo tool by Ultralytics.</p>"},{"location":"tasks/detect/#val","title":"Val","text":"<p>Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # a list contains map50-95 of each category\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val official model\nyolo detect val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/detect/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Predict with the model\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predict with custom model\n</code></pre> <p>See full <code>predict</code> mode details in the Predict page.</p>"},{"location":"tasks/detect/#export","title":"Export","text":"<p>Export a YOLOv8n model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom trained\n# Export the model\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8 export formats are in the table below. You can predict or validate directly on exported models, i.e. <code>yolo predict model=yolov8n.onnx</code>. Usage examples are shown for your model after export completes.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>See full <code>export</code> details in the Export page.</p>"},{"location":"tasks/pose/","title":"Pose","text":"<p>Pose estimation is a task that involves identifying the location of specific points in an image, usually referred to as keypoints. The keypoints can represent various parts of the object such as joints, landmarks, or other distinctive features. The locations of the keypoints are usually represented as a set of 2D <code>[x, y]</code> or 3D <code>[x, y, visible]</code> coordinates.</p> <p></p> <p>The output of a pose estimation model is a set of points that represent the keypoints on an object in the image, usually along with the confidence scores for each point. Pose estimation is a good choice when you need to identify specific parts of an object in a scene, and their location in relation to each other.</p> <p>Tip</p> <p>YOLOv8 pose models use the <code>-pose</code> suffix, i.e. <code>yolov8n-pose.pt</code>. These models are trained on the COCO keypoints dataset and are suitable for a variety of pose estimation tasks.</p>"},{"location":"tasks/pose/#models","title":"Models","text":"<p>YOLOv8 pretrained Pose models are shown here. Detect, Segment and Pose models are pretrained on the COCO dataset, while Classify models are pretrained on the ImageNet dataset.</p> <p>Models download automatically from the latest Ultralytics release on first use.</p> Model size<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>mAP<sup>val</sup> values are for single-model single-scale on COCO Keypoints val2017   dataset.   Reproduce by <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>Speed averaged over COCO val images using an Amazon EC2 P4d   instance.   Reproduce by <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#train","title":"Train","text":"<p>Train a YOLOv8-pose model on the COCO128-pose dataset.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8n-pose.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # build from YAML and transfer weights\n# Train the model\nresults = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Build a new model from YAML and start training from scratch\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n# Start training from a pretrained *.pt model\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#dataset-format","title":"Dataset format","text":"<p>YOLO pose dataset format can be found in detail in the Dataset Guide. To convert your existing dataset from other formats( like COCO etc.) to YOLO format, please use json2yolo tool by Ultralytics.</p>"},{"location":"tasks/pose/#val","title":"Val","text":"<p>Validate trained YOLOv8n-pose model accuracy on the COCO128-pose dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # a list contains map50-95 of each category\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # val official model\nyolo pose val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/pose/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n-pose model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Predict with the model\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo pose predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predict with custom model\n</code></pre> <p>See full <code>predict</code> mode details in the Predict page.</p>"},{"location":"tasks/pose/#export","title":"Export","text":"<p>Export a YOLOv8n Pose model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-pose.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom trained\n# Export the model\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8-pose export formats are in the table below. You can predict or validate directly on exported models, i.e. <code>yolo predict model=yolov8n-pose.onnx</code>. Usage examples are shown for your model after export completes.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>See full <code>export</code> details in the Export page.</p>"},{"location":"tasks/segment/","title":"Segment","text":"<p>Instance segmentation goes a step further than object detection and involves identifying individual objects in an image and segmenting them from the rest of the image.</p> <p></p> <p>The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object. Instance segmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is.</p> <p>Tip</p> <p>YOLOv8 Segment models use the <code>-seg</code> suffix, i.e. <code>yolov8n-seg.pt</code> and are pretrained on COCO.</p>"},{"location":"tasks/segment/#models","title":"Models","text":"<p>YOLOv8 pretrained Segment models are shown here. Detect, Segment and Pose models are pretrained on the COCO dataset, while Classify models are pretrained on the ImageNet dataset.</p> <p>Models download automatically from the latest Ultralytics release on first use.</p> Model size<sup>(pixels) mAP<sup>box50-95 mAP<sup>mask50-95 Speed<sup>CPU ONNX(ms) Speed<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>mAP<sup>val</sup> values are for single-model single-scale on COCO val2017 dataset.   Reproduce by <code>yolo val segment data=coco.yaml device=0</code></li> <li>Speed averaged over COCO val images using an Amazon EC2 P4d   instance.   Reproduce by <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#train","title":"Train","text":"<p>Train YOLOv8n-seg on the COCO128-seg dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8n-seg.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n# Train the model\nresults = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Build a new model from YAML and start training from scratch\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n# Start training from a pretrained *.pt model\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n# Build a new model from YAML, transfer pretrained weights to it and start training\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#dataset-format","title":"Dataset format","text":"<p>YOLO segmentation dataset format can be found in detail in the Dataset Guide. To convert your existing dataset from other formats( like COCO etc.) to YOLO format, please use json2yolo tool by Ultralytics.</p>"},{"location":"tasks/segment/#val","title":"Val","text":"<p>Validate trained YOLOv8n-seg model accuracy on the COCO128-seg dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Validate the model\nmetrics = model.val()  # no arguments needed, dataset and settings remembered\nmetrics.box.map    # map50-95(B)\nmetrics.box.map50  # map50(B)\nmetrics.box.map75  # map75(B)\nmetrics.box.maps   # a list contains map50-95(B) of each category\nmetrics.seg.map    # map50-95(M)\nmetrics.seg.map50  # map50(M)\nmetrics.seg.map75  # map75(M)\nmetrics.seg.maps   # a list contains map50-95(M) of each category\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # val official model\nyolo segment val model=path/to/best.pt  # val custom model\n</code></pre>"},{"location":"tasks/segment/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n-seg model to run predictions on images.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Predict with the model\nresults = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model\nyolo segment predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # predict with custom model\n</code></pre> <p>See full <code>predict</code> mode details in the Predict page.</p>"},{"location":"tasks/segment/#export","title":"Export","text":"<p>Export a YOLOv8n-seg model to a different format like ONNX, CoreML, etc.</p> PythonCLI <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n-seg.pt')  # load an official model\nmodel = YOLO('path/to/best.pt')  # load a custom trained\n# Export the model\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # export official model\nyolo export model=path/to/best.pt format=onnx  # export custom trained model\n</code></pre> <p>Available YOLOv8-seg export formats are in the table below. You can predict or validate directly on exported models, i.e. <code>yolo predict model=yolov8n-seg.onnx</code>. Usage examples are shown for your model after export completes.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>See full <code>export</code> details in the Export page.</p>"},{"location":"usage/callbacks/","title":"Callbacks","text":""},{"location":"usage/callbacks/#callbacks","title":"Callbacks","text":"<p>Ultralytics framework supports callbacks as entry points in strategic stages of train, val, export, and predict modes. Each callback accepts a <code>Trainer</code>, <code>Validator</code>, or <code>Predictor</code> object depending on the operation type. All properties of these objects can be found in Reference section of the docs.</p>"},{"location":"usage/callbacks/#examples","title":"Examples","text":""},{"location":"usage/callbacks/#returning-additional-information-with-prediction","title":"Returning additional information with Prediction","text":"<p>In this example, we want to return the original frame with each result object. Here's how we can do that</p> <pre><code>def on_predict_batch_end(predictor):\n# Retrieve the batch data\n_, im0s, _, _ = predictor.batch\n# Ensure that im0s is a list\nim0s = im0s if isinstance(im0s, list) else [im0s]\n# Combine the prediction results with the corresponding frames\npredictor.results = zip(predictor.results, im0s)\n# Create a YOLO model instance\nmodel = YOLO(f'yolov8n.pt')\n# Add the custom callback to the model\nmodel.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\n# Iterate through the results and frames\nfor (result, frame) in model.track/predict():\npass\n</code></pre>"},{"location":"usage/callbacks/#all-callbacks","title":"All callbacks","text":"<p>Here are all supported callbacks. See callbacks source code for additional details.</p>"},{"location":"usage/callbacks/#trainer-callbacks","title":"Trainer Callbacks","text":"Callback Description <code>on_pretrain_routine_start</code> Triggered at the beginning of pre-training routine <code>on_pretrain_routine_end</code> Triggered at the end of pre-training routine <code>on_train_start</code> Triggered when the training starts <code>on_train_epoch_start</code> Triggered at the start of each training epoch <code>on_train_batch_start</code> Triggered at the start of each training batch <code>optimizer_step</code> Triggered during the optimizer step <code>on_before_zero_grad</code> Triggered before gradients are zeroed <code>on_train_batch_end</code> Triggered at the end of each training batch <code>on_train_epoch_end</code> Triggered at the end of each training epoch <code>on_fit_epoch_end</code> Triggered at the end of each fit epoch <code>on_model_save</code> Triggered when the model is saved <code>on_train_end</code> Triggered when the training process ends <code>on_params_update</code> Triggered when model parameters are updated <code>teardown</code> Triggered when the training process is being cleaned up"},{"location":"usage/callbacks/#validator-callbacks","title":"Validator Callbacks","text":"Callback Description <code>on_val_start</code> Triggered when the validation starts <code>on_val_batch_start</code> Triggered at the start of each validation batch <code>on_val_batch_end</code> Triggered at the end of each validation batch <code>on_val_end</code> Triggered when the validation ends"},{"location":"usage/callbacks/#predictor-callbacks","title":"Predictor Callbacks","text":"Callback Description <code>on_predict_start</code> Triggered when the prediction process starts <code>on_predict_batch_start</code> Triggered at the start of each prediction batch <code>on_predict_postprocess_end</code> Triggered at the end of prediction postprocessing <code>on_predict_batch_end</code> Triggered at the end of each prediction batch <code>on_predict_end</code> Triggered when the prediction process ends"},{"location":"usage/callbacks/#exporter-callbacks","title":"Exporter Callbacks","text":"Callback Description <code>on_export_start</code> Triggered when the export process starts <code>on_export_end</code> Triggered when the export process ends"},{"location":"usage/cfg/","title":"Configuration","text":"<p>YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.</p> <p>YOLOv8 'yolo' CLI commands use the following syntax:</p> CLIPython <pre><code>yolo TASK MODE ARGS\n</code></pre> <pre><code>from ultralytics import YOLO\n# Load a YOLOv8 model from a pre-trained weights file\nmodel = YOLO('yolov8n.pt')\n# Run MODE mode using the custom arguments ARGS (guess TASK)\nmodel.MODE(ARGS)\n</code></pre> <p>Where:</p> <ul> <li><code>TASK</code> (optional) is one of <code>[detect, segment, classify, pose]</code>. If it is not passed explicitly YOLOv8 will try to   guess   the <code>TASK</code> from the model type.</li> <li><code>MODE</code> (required) is one of <code>[train, val, predict, export, track, benchmark]</code></li> <li><code>ARGS</code> (optional) are any number of custom <code>arg=value</code> pairs like <code>imgsz=320</code> that override defaults.   For a full list of available <code>ARGS</code> see the Configuration page and <code>defaults.yaml</code>   GitHub source.</li> </ul>"},{"location":"usage/cfg/#tasks","title":"Tasks","text":"<p>YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks differ in the type of output they produce and the specific problem they are designed to solve.</p> <p>Detect: For identifying and localizing objects or regions of interest in an image or video. Segment: For dividing an image or video into regions or pixels that correspond to different objects or classes. Classify: For predicting the class label of an input image. Pose: For identifying objects and estimating their keypoints in an image or video.</p> Key Value Description <code>task</code> <code>'detect'</code> YOLO task, i.e. detect, segment, classify, pose <p>Tasks Guide</p>"},{"location":"usage/cfg/#modes","title":"Modes","text":"<p>YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include:</p> <p>Train: For training a YOLOv8 model on a custom dataset. Val: For validating a YOLOv8 model after it has been trained. Predict: For making predictions using a trained YOLOv8 model on new images or videos. Export: For exporting a YOLOv8 model to a format that can be used for deployment. Track: For tracking objects in real-time using a YOLOv8 model. Benchmark: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.</p> Key Value Description <code>mode</code> <code>'train'</code> YOLO mode, i.e. train, val, predict, export, track, benchmark <p>Modes Guide</p>"},{"location":"usage/cfg/#train","title":"Train","text":"<p>The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.</p> Key Value Description <code>model</code> <code>None</code> path to model file, i.e. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> path to data file, i.e. coco128.yaml <code>epochs</code> <code>100</code> number of epochs to train for <code>patience</code> <code>50</code> epochs to wait for no observable improvement for early stopping of training <code>batch</code> <code>16</code> number of images per batch (-1 for AutoBatch) <code>imgsz</code> <code>640</code> size of input images as integer or w,h <code>save</code> <code>True</code> save train checkpoints and predict results <code>save_period</code> <code>-1</code> Save checkpoint every x epochs (disabled if &lt; 1) <code>cache</code> <code>False</code> True/ram, disk or False. Use cache for data loading <code>device</code> <code>None</code> device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu <code>workers</code> <code>8</code> number of worker threads for data loading (per RANK if DDP) <code>project</code> <code>None</code> project name <code>name</code> <code>None</code> experiment name <code>exist_ok</code> <code>False</code> whether to overwrite existing experiment <code>pretrained</code> <code>False</code> whether to use a pretrained model <code>optimizer</code> <code>'auto'</code> optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto] <code>verbose</code> <code>False</code> whether to print verbose output <code>seed</code> <code>0</code> random seed for reproducibility <code>deterministic</code> <code>True</code> whether to enable deterministic mode <code>single_cls</code> <code>False</code> train multi-class data as single-class <code>rect</code> <code>False</code> rectangular training with each batch collated for minimum padding <code>cos_lr</code> <code>False</code> use cosine learning rate scheduler <code>close_mosaic</code> <code>10</code> (int) disable mosaic augmentation for final epochs (0 to disable) <code>resume</code> <code>False</code> resume training from last checkpoint <code>amp</code> <code>True</code> Automatic Mixed Precision (AMP) training, choices=[True, False] <code>fraction</code> <code>1.0</code> dataset fraction to train on (default is 1.0, all images in train set) <code>profile</code> <code>False</code> profile ONNX and TensorRT speeds during training for loggers <code>freeze</code> <code>None</code> (int or list, optional) freeze first n layers, or freeze list of layer indices during training <code>lr0</code> <code>0.01</code> initial learning rate (i.e. SGD=1E-2, Adam=1E-3) <code>lrf</code> <code>0.01</code> final learning rate (lr0 * lrf) <code>momentum</code> <code>0.937</code> SGD momentum/Adam beta1 <code>weight_decay</code> <code>0.0005</code> optimizer weight decay 5e-4 <code>warmup_epochs</code> <code>3.0</code> warmup epochs (fractions ok) <code>warmup_momentum</code> <code>0.8</code> warmup initial momentum <code>warmup_bias_lr</code> <code>0.1</code> warmup initial bias lr <code>box</code> <code>7.5</code> box loss gain <code>cls</code> <code>0.5</code> cls loss gain (scale with pixels) <code>dfl</code> <code>1.5</code> dfl loss gain <code>pose</code> <code>12.0</code> pose loss gain (pose-only) <code>kobj</code> <code>2.0</code> keypoint obj loss gain (pose-only) <code>label_smoothing</code> <code>0.0</code> label smoothing (fraction) <code>nbs</code> <code>64</code> nominal batch size <code>overlap_mask</code> <code>True</code> masks should overlap during training (segment train only) <code>mask_ratio</code> <code>4</code> mask downsample ratio (segment train only) <code>dropout</code> <code>0.0</code> use dropout regularization (classify train only) <code>val</code> <code>True</code> validate/test during training <p>Train Guide</p>"},{"location":"usage/cfg/#predict","title":"Predict","text":"<p>The prediction settings for YOLO models encompass a range of hyperparameters and configurations that influence the model's performance, speed, and accuracy during inference on new data. Careful tuning and experimentation with these settings are essential to achieve optimal performance for a specific task. Key settings include the confidence threshold, Non-Maximum Suppression (NMS) threshold, and the number of classes considered. Additional factors affecting the prediction process are input data size and format, the presence of supplementary features such as masks or multiple labels per box, and the particular task the model is employed for.</p> Key Value Description <code>source</code> <code>'ultralytics/assets'</code> source directory for images or videos <code>conf</code> <code>0.25</code> object confidence threshold for detection <code>iou</code> <code>0.7</code> intersection over union (IoU) threshold for NMS <code>half</code> <code>False</code> use half precision (FP16) <code>device</code> <code>None</code> device to run on, i.e. cuda device=0/1/2/3 or device=cpu <code>show</code> <code>False</code> show results if possible <code>save</code> <code>False</code> save images with results <code>save_txt</code> <code>False</code> save results as .txt file <code>save_conf</code> <code>False</code> save results with confidence scores <code>save_crop</code> <code>False</code> save cropped images with results <code>show_labels</code> <code>True</code> show object labels in plots <code>show_conf</code> <code>True</code> show object confidence scores in plots <code>max_det</code> <code>300</code> maximum number of detections per image <code>vid_stride</code> <code>False</code> video frame-rate stride <code>line_width</code> <code>None</code> The line width of the bounding boxes. If None, it is scaled to the image size. <code>visualize</code> <code>False</code> visualize model features <code>augment</code> <code>False</code> apply image augmentation to prediction sources <code>agnostic_nms</code> <code>False</code> class-agnostic NMS <code>retina_masks</code> <code>False</code> use high-resolution segmentation masks <code>classes</code> <code>None</code> filter results by class, i.e. classes=0, or classes=[0,2,3] <code>boxes</code> <code>True</code> Show boxes in segmentation predictions <p>Predict Guide</p>"},{"location":"usage/cfg/#val","title":"Val","text":"<p>The val (validation) settings for YOLO models involve various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings influence the model's performance, speed, and accuracy. Common YOLO validation settings include batch size, validation frequency during training, and performance evaluation metrics. Other factors affecting the validation process include the validation dataset's size and composition, as well as the specific task the model is employed for. Careful tuning and experimentation with these settings are crucial to ensure optimal performance on the validation dataset and detect and prevent overfitting.</p> Key Value Description <code>save_json</code> <code>False</code> save results to JSON file <code>save_hybrid</code> <code>False</code> save hybrid version of labels (labels + additional predictions) <code>conf</code> <code>0.001</code> object confidence threshold for detection <code>iou</code> <code>0.6</code> intersection over union (IoU) threshold for NMS <code>max_det</code> <code>300</code> maximum number of detections per image <code>half</code> <code>True</code> use half precision (FP16) <code>device</code> <code>None</code> device to run on, i.e. cuda device=0/1/2/3 or device=cpu <code>dnn</code> <code>False</code> use OpenCV DNN for ONNX inference <code>plots</code> <code>False</code> show plots during training <code>rect</code> <code>False</code> rectangular val with each batch collated for minimum padding <code>split</code> <code>val</code> dataset split to use for validation, i.e. 'val', 'test' or 'train' <p>Val Guide</p>"},{"location":"usage/cfg/#export","title":"Export","text":"<p>Export settings for YOLO models encompass configurations and options related to saving or exporting the model for use in different environments or platforms. These settings can impact the model's performance, size, and compatibility with various systems. Key export settings include the exported model file format (e.g., ONNX, TensorFlow SavedModel), the target device (e.g., CPU, GPU), and additional features such as masks or multiple labels per box. The export process may also be affected by the model's specific task and the requirements or constraints of the destination environment or platform. It is crucial to thoughtfully configure these settings to ensure the exported model is optimized for the intended use case and functions effectively in the target environment.</p> Key Value Description <code>format</code> <code>'torchscript'</code> format to export to <code>imgsz</code> <code>640</code> image size as scalar or (h, w) list, i.e. (640, 480) <code>keras</code> <code>False</code> use Keras for TF SavedModel export <code>optimize</code> <code>False</code> TorchScript: optimize for mobile <code>half</code> <code>False</code> FP16 quantization <code>int8</code> <code>False</code> INT8 quantization <code>dynamic</code> <code>False</code> ONNX/TF/TensorRT: dynamic axes <code>simplify</code> <code>False</code> ONNX: simplify model <code>opset</code> <code>None</code> ONNX: opset version (optional, defaults to latest) <code>workspace</code> <code>4</code> TensorRT: workspace size (GB) <code>nms</code> <code>False</code> CoreML: add NMS <p>Export Guide</p>"},{"location":"usage/cfg/#augmentation","title":"Augmentation","text":"<p>Augmentation settings for YOLO models refer to the various transformations and modifications applied to the training data to increase the diversity and size of the dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the transformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each transformation is applied, and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the augmentation process include the size and composition of the original dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the augmented dataset is diverse and representative enough to train a high-performing model.</p> Key Value Description <code>hsv_h</code> 0.015 image HSV-Hue augmentation (fraction) <code>hsv_s</code> 0.7 image HSV-Saturation augmentation (fraction) <code>hsv_v</code> 0.4 image HSV-Value augmentation (fraction) <code>degrees</code> 0.0 image rotation (+/- deg) <code>translate</code> 0.1 image translation (+/- fraction) <code>scale</code> 0.5 image scale (+/- gain) <code>shear</code> 0.0 image shear (+/- deg) <code>perspective</code> 0.0 image perspective (+/- fraction), range 0-0.001 <code>flipud</code> 0.0 image flip up-down (probability) <code>fliplr</code> 0.5 image flip left-right (probability) <code>mosaic</code> 1.0 image mosaic (probability) <code>mixup</code> 0.0 image mixup (probability) <code>copy_paste</code> 0.0 segment copy-paste (probability)"},{"location":"usage/cfg/#logging-checkpoints-plotting-and-file-management","title":"Logging, checkpoints, plotting and file management","text":"<p>Logging, checkpoints, plotting, and file management are important considerations when training a YOLO model.</p> <ul> <li>Logging: It is often helpful to log various metrics and statistics during training to track the model's progress and   diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log   messages to a file.</li> <li>Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows   you to resume training from a previous point if the training process is interrupted or if you want to experiment with   different training configurations.</li> <li>Plotting: Visualizing the model's performance and training progress can be helpful for understanding how the model is   behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by   generating plots using a logging library such as TensorBoard.</li> <li>File management: Managing the various files generated during the training process, such as model checkpoints, log   files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of   these files and make it easy to access and analyze them as needed.</li> </ul> <p>Effective logging, checkpointing, plotting, and file management can help you keep track of the model's progress and make it easier to debug and optimize the training process.</p> Key Value Description <code>project</code> <code>'runs'</code> project name <code>name</code> <code>'exp'</code> experiment name. <code>exp</code> gets automatically incremented if not specified, i.e, <code>exp</code>, <code>exp2</code> ... <code>exist_ok</code> <code>False</code> whether to overwrite existing experiment <code>plots</code> <code>False</code> save plots during train/val <code>save</code> <code>False</code> save train checkpoints and predict results"},{"location":"usage/cli/","title":"Command Line Interface Usage","text":"<p>The YOLO command line interface (CLI) allows for simple single-line commands without the need for a Python environment. CLI requires no customization or Python code. You can simply run all tasks from the terminal with the <code>yolo</code> command.</p> <p>Example</p> SyntaxTrainPredictValExportSpecial <p>Ultralytics <code>yolo</code> commands use the following syntax: <pre><code>yolo TASK MODE ARGS\n\nWhere   TASK (optional) is one of [detect, segment, classify]\nMODE (required) is one of [train, val, predict, export, track]\nARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n</code></pre> See all ARGS in the full Configuration Guide or with <code>yolo cfg</code></p> <p>Train a detection model for 10 epochs with an initial learning_rate of 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predict a YouTube video using a pretrained segmentation model at image size 320: <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n</code></pre></p> <p>Val a pretrained detection model at batch-size 1 and image size 640: <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Run special commands to see version, view settings, run checks and more: <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Where:</p> <ul> <li><code>TASK</code> (optional) is one of <code>[detect, segment, classify]</code>. If it is not passed explicitly YOLOv8 will try to guess   the <code>TASK</code> from the model type.</li> <li><code>MODE</code> (required) is one of <code>[train, val, predict, export, track]</code></li> <li><code>ARGS</code> (optional) are any number of custom <code>arg=value</code> pairs like <code>imgsz=320</code> that override defaults.   For a full list of available <code>ARGS</code> see the Configuration page and <code>defaults.yaml</code>   GitHub source.</li> </ul> <p>Warning</p> <p>Arguments must be passed as <code>arg=val</code> pairs, split by an equals <code>=</code> sign and delimited by spaces <code></code> between pairs. Do not use <code>--</code> argument prefixes or commas <code>,</code> between arguments.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul>"},{"location":"usage/cli/#train","title":"Train","text":"<p>Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see the Configuration page.</p> <p>Example</p> TrainResume <p>Start training YOLOv8n on COCO128 for 100 epochs at image-size 640. <pre><code>yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n</code></pre></p> <p>Resume an interrupted training. <pre><code>yolo detect train resume model=last.pt\n</code></pre></p>"},{"location":"usage/cli/#val","title":"Val","text":"<p>Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the <code>model</code> retains it's training <code>data</code> and arguments as model attributes.</p> <p>Example</p> OfficialCustom <p>Validate an official YOLOv8n model. <pre><code>yolo detect val model=yolov8n.pt\n</code></pre></p> <p>Validate a custom-trained model. <pre><code>yolo detect val model=path/to/best.pt\n</code></pre></p>"},{"location":"usage/cli/#predict","title":"Predict","text":"<p>Use a trained YOLOv8n model to run predictions on images.</p> <p>Example</p> OfficialCustom <p>Predict with an official YOLOv8n model. <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n</code></pre></p> <p>Predict with a custom model. <pre><code>yolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'\n</code></pre></p>"},{"location":"usage/cli/#export","title":"Export","text":"<p>Export a YOLOv8n model to a different format like ONNX, CoreML, etc.</p> <p>Example</p> OfficialCustom <p>Export an official YOLOv8n model to ONNX format. <pre><code>yolo export model=yolov8n.pt format=onnx\n</code></pre></p> <p>Export a custom-trained model to ONNX format. <pre><code>yolo export model=path/to/best.pt format=onnx\n</code></pre></p> <p>Available YOLOv8 export formats are in the table below. You can export to any format using the <code>format</code> argument, i.e. <code>format='onnx'</code> or <code>format='engine'</code>.</p> Format <code>format</code> Argument Model Metadata Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"usage/cli/#overriding-default-arguments","title":"Overriding default arguments","text":"<p>Default arguments can be overridden by simply passing them as arguments in the CLI in <code>arg=value</code> pairs.</p> TrainPredictVal <p>Train a detection model for <code>10 epochs</code> with <code>learning_rate</code> of <code>0.01</code> <pre><code>yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Predict a YouTube video using a pretrained segmentation model at image size 320: <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n</code></pre></p> <p>Validate a pretrained detection model at batch-size 1 and image size 640: <pre><code>yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p>"},{"location":"usage/cli/#overriding-default-config-file","title":"Overriding default config file","text":"<p>You can override the <code>default.yaml</code> config file entirely by passing a new file with the <code>cfg</code> arguments, i.e. <code>cfg=custom.yaml</code>.</p> <p>To do this first create a copy of <code>default.yaml</code> in your current working dir with the <code>yolo copy-cfg</code> command.</p> <p>This will create <code>default_copy.yaml</code>, which you can then pass as <code>cfg=default_copy.yaml</code> along with any additional args, like <code>imgsz=320</code> in this example:</p> CLI <pre><code>yolo copy-cfg\nyolo cfg=default_copy.yaml imgsz=320\n</code></pre>"},{"location":"usage/engine/","title":"Advanced Customization","text":"<p>Both the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine executors. Let's take a look at the Trainer engine.</p>"},{"location":"usage/engine/#basetrainer","title":"BaseTrainer","text":"<p>BaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overriding the required functions or operations as long the as correct formats are followed. For example, you can support your own custom model and dataloader by just overriding these functions:</p> <ul> <li><code>get_model(cfg, weights)</code> - The function that builds the model to be trained</li> <li><code>get_dataloader()</code> - The function that builds the dataloader   More details and source code can be found in <code>BaseTrainer</code> Reference</li> </ul>"},{"location":"usage/engine/#detectiontrainer","title":"DetectionTrainer","text":"<p>Here's how you can use the YOLOv8 <code>DetectionTrainer</code> and customize it.</p> <pre><code>from ultralytics.models.yolo.detect import DetectionTrainer\ntrainer = DetectionTrainer(overrides={...})\ntrainer.train()\ntrained_model = trainer.best  # get best model\n</code></pre>"},{"location":"usage/engine/#customizing-the-detectiontrainer","title":"Customizing the DetectionTrainer","text":"<p>Let's customize the trainer to train a custom detection model that is not supported directly. You can do this by simply overloading the existing the <code>get_model</code> functionality:</p> <pre><code>from ultralytics.models.yolo.detect import DetectionTrainer\nclass CustomTrainer(DetectionTrainer):\ndef get_model(self, cfg, weights):\n...\ntrainer = CustomTrainer(overrides={...})\ntrainer.train()\n</code></pre> <p>You now realize that you need to customize the trainer further to:</p> <ul> <li>Customize the <code>loss function</code>.</li> <li>Add <code>callback</code> that uploads model to your Google Drive after every 10 <code>epochs</code>   Here's how you can do it:</li> </ul> <pre><code>from ultralytics.models.yolo.detect import DetectionTrainer\nfrom ultralytics.nn.tasks import DetectionModel\nclass MyCustomModel(DetectionModel):\ndef init_criterion(self):\n...\nclass CustomTrainer(DetectionTrainer):\ndef get_model(self, cfg, weights):\nreturn MyCustomModel(...)\n# callback to upload model weights\ndef log_model(trainer):\nlast_weight_path = trainer.last\n...\ntrainer = CustomTrainer(overrides={...})\ntrainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callback\ntrainer.train()\n</code></pre> <p>To know more about Callback triggering events and entry point, checkout our Callbacks Guide</p>"},{"location":"usage/engine/#other-engine-components","title":"Other engine components","text":"<p>There are other components that can be customized similarly like <code>Validators</code> and <code>Predictors</code> See Reference section for more information on these.</p>"},{"location":"usage/python/","title":"Python Usage","text":"<p>Welcome to the YOLOv8 Python Usage documentation! This guide is designed to help you seamlessly integrate YOLOv8 into your Python projects for object detection, segmentation, and classification. Here, you'll learn how to load and use pretrained models, train new models, and perform predictions on images. The easy-to-use Python interface is a valuable resource for anyone looking to incorporate YOLOv8 into their Python projects, allowing you to quickly implement advanced object detection capabilities. Let's get started!</p> <p>For example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX format with just a few lines of code.</p> <p>Python</p> <pre><code>from ultralytics import YOLO\n# Create a new YOLO model from scratch\nmodel = YOLO('yolov8n.yaml')\n# Load a pretrained YOLO model (recommended for training)\nmodel = YOLO('yolov8n.pt')\n# Train the model using the 'coco128.yaml' dataset for 3 epochs\nresults = model.train(data='coco128.yaml', epochs=3)\n# Evaluate the model's performance on the validation set\nresults = model.val()\n# Perform object detection on an image using the model\nresults = model('https://ultralytics.com/images/bus.jpg')\n# Export the model to ONNX format\nsuccess = model.export(format='onnx')\n</code></pre>"},{"location":"usage/python/#train","title":"Train","text":"<p>Train mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can accurately predict the classes and locations of objects in an image.</p> <p>Train</p> From pretrained(recommended)From scratchResume <pre><code>from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt') # pass any model type\nresults = model.train(epochs=5)\n</code></pre> <pre><code>from ultralytics import YOLO\nmodel = YOLO('yolov8n.yaml')\nresults = model.train(data='coco128.yaml', epochs=5)\n</code></pre> <pre><code>model = YOLO(\"last.pt\")\nresults = model.train(resume=True)\n</code></pre> <p>Train Examples</p>"},{"location":"usage/python/#val","title":"Val","text":"<p>Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters of the model to improve its performance.</p> <p>Val</p> Val after trainingVal independently <pre><code>  from ultralytics import YOLO\nmodel = YOLO('yolov8n.yaml')\nmodel.train(data='coco128.yaml', epochs=5)\nmodel.val()  # It'll automatically evaluate the data you trained.\n</code></pre> <pre><code>  from ultralytics import YOLO\nmodel = YOLO(\"model.pt\")\n# It'll use the data YAML file in model.pt if you don't set data.\nmodel.val()\n# or you can set the data you want to val\nmodel.val(data='coco128.yaml')\n</code></pre> <p>Val Examples</p>"},{"location":"usage/python/#predict","title":"Predict","text":"<p>Predict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the model is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model predicts the classes and locations of objects in the input images or videos.</p> <p>Predict</p> From sourceResults usage <pre><code>from ultralytics import YOLO\nfrom PIL import Image\nimport cv2\nmodel = YOLO(\"model.pt\")\n# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\nresults = model.predict(source=\"0\")\nresults = model.predict(source=\"folder\", show=True) # Display preds. Accepts all YOLO predict arguments\n# from PIL\nim1 = Image.open(\"bus.jpg\")\nresults = model.predict(source=im1, save=True)  # save plotted images\n# from ndarray\nim2 = cv2.imread(\"bus.jpg\")\nresults = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n# from list of PIL/ndarray\nresults = model.predict(source=[im1, im2])\n</code></pre> <pre><code># results would be a list of Results object including all the predictions by default\n# but be careful as it could occupy a lot memory when there're many images,\n# especially the task is segmentation.\n# 1. return as a list\nresults = model.predict(source=\"folder\")\n# results would be a generator which is more friendly to memory by setting stream=True\n# 2. return as a generator\nresults = model.predict(source=0, stream=True)\nfor result in results:\n# Detection\nresult.boxes.xyxy   # box with xyxy format, (N, 4)\nresult.boxes.xywh   # box with xywh format, (N, 4)\nresult.boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\nresult.boxes.xywhn  # box with xywh format but normalized, (N, 4)\nresult.boxes.conf   # confidence score, (N, 1)\nresult.boxes.cls    # cls, (N, 1)\n# Segmentation\nresult.masks.data      # masks, (N, H, W)\nresult.masks.xy        # x,y segments (pixels), List[segment] * N\nresult.masks.xyn       # x,y segments (normalized), List[segment] * N\n# Classification\nresult.probs     # cls prob, (num_class, )\n# Each result is composed of torch.Tensor by default,\n# in which you can easily use following functionality:\nresult = result.cuda()\nresult = result.cpu()\nresult = result.to(\"cpu\")\nresult = result.numpy()\n</code></pre> <p>Predict Examples</p>"},{"location":"usage/python/#export","title":"Export","text":"<p>Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is converted to a format that can be used by other software applications or hardware devices. This mode is useful when deploying the model to production environments.</p> <p>Export</p> Export to ONNXExport to TensorRT <p>Export an official YOLOv8n model to ONNX with dynamic batch-size and image-size. <pre><code>  from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\nmodel.export(format='onnx', dynamic=True)\n</code></pre></p> <p>Export an official YOLOv8n model to TensorRT on <code>device=0</code> for acceleration on CUDA devices. <pre><code>  from ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\nmodel.export(format='onnx', device=0)\n</code></pre></p> <p>Export Examples</p>"},{"location":"usage/python/#track","title":"Track","text":"<p>Track mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a checkpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful for applications such as surveillance systems or self-driving cars.</p> <p>Track</p> Python <pre><code>from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.pt')  # load an official detection model\nmodel = YOLO('yolov8n-seg.pt')  # load an official segmentation model\nmodel = YOLO('path/to/best.pt')  # load a custom model\n# Track with the model\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True)\nresults = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\")\n</code></pre> <p>Track Examples</p>"},{"location":"usage/python/#benchmark","title":"Benchmark","text":"<p>Benchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide information on the size of the exported format, its <code>mAP50-95</code> metrics (for object detection and segmentation) or <code>accuracy_top5</code> metrics (for classification), and the inference time in milliseconds per image across various export formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for their specific use case based on their requirements for speed and accuracy.</p> <p>Benchmark</p> Python <p>Benchmark an official YOLOv8n model across all export formats. <pre><code>from ultralytics.utils.benchmarks import benchmark\n# Benchmark\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre></p> <p>Benchmark Examples</p>"},{"location":"usage/python/#using-trainers","title":"Using Trainers","text":"<p><code>YOLO</code> model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits from <code>BaseTrainer</code>.</p> <p>Detection Trainer Example<pre><code>from ultralytics.models.yolo import DetectionTrainer, DetectionValidator, DetectionPredictor\n# trainer\ntrainer = DetectionTrainer(overrides={})\ntrainer.train()\ntrained_model = trainer.best\n# Validator\nval = DetectionValidator(args=...)\nval(model=trained_model)\n# predictor\npred = DetectionPredictor(overrides={})\npred(source=SOURCE, model=trained_model)\n# resume from last weight\noverrides[\"resume\"] = trainer.last\ntrainer = detect.DetectionTrainer(overrides=overrides)\n</code></pre> </p> <p>You can easily customize Trainers to support custom tasks or explore R&amp;D ideas. Learn more about Customizing <code>Trainers</code>, <code>Validators</code> and <code>Predictors</code> to suit your project needs in the Customization Section.</p> <p>Customization tutorials</p>"},{"location":"yolov5/","title":"Comprehensive Guide to Ultralytics YOLOv5","text":"Welcome to the Ultralytics' YOLOv5 \ud83d\ude80 Documentation! YOLOv5, the fifth iteration of the revolutionary \"You Only Look Once\" object detection model, is designed to deliver high-speed, high-accuracy results in real-time.  Built on PyTorch, this powerful deep learning framework has garnered immense popularity for its versatility, ease of use, and high performance. Our documentation guides you through the installation process, explains the architectural nuances of the model, showcases various use-cases, and provides a series of detailed tutorials. These resources will help you harness the full potential of YOLOv5 for your computer vision projects. Let's get started!"},{"location":"yolov5/#tutorials","title":"Tutorials","text":"<p>Here's a compilation of comprehensive tutorials that will guide you through different aspects of YOLOv5.</p> <ul> <li>Train Custom Data \ud83d\ude80 RECOMMENDED: Learn how to train the YOLOv5 model on your custom dataset.</li> <li>Tips for Best Training Results \u2618\ufe0f: Uncover practical tips to optimize your model training process.</li> <li>Multi-GPU Training: Understand how to leverage multiple GPUs to expedite your training.</li> <li>PyTorch Hub \ud83c\udf1f NEW: Learn to load pre-trained models via PyTorch Hub.</li> <li>TFLite, ONNX, CoreML, TensorRT Export \ud83d\ude80: Understand how to export your model to different formats.</li> <li>NVIDIA Jetson platform Deployment \ud83c\udf1f NEW: Learn how to deploy your YOLOv5 model on NVIDIA Jetson platform.</li> <li>Test-Time Augmentation (TTA): Explore how to use TTA to improve your model's prediction accuracy.</li> <li>Model Ensembling: Learn the strategy of combining multiple models for improved performance.</li> <li>Model Pruning/Sparsity: Understand pruning and sparsity concepts, and how to create a more efficient model.</li> <li>Hyperparameter Evolution: Discover the process of automated hyperparameter tuning for better model performance.</li> <li>Transfer Learning with Frozen Layers: Learn how to implement transfer learning by freezing layers in YOLOv5.</li> <li>Architecture Summary \ud83c\udf1f Delve into the structural details of the YOLOv5 model.</li> <li>Roboflow for Datasets: Understand how to utilize Roboflow for dataset management, labeling, and active learning.</li> <li>ClearML Logging \ud83c\udf1f Learn how to integrate ClearML for efficient logging during your model training.</li> <li>YOLOv5 with Neural Magic Discover how to use Neural Magic's Deepsparse to prune and quantize your YOLOv5 model.</li> <li>Comet Logging \ud83c\udf1f NEW: Explore how to utilize Comet for improved model training logging.</li> </ul>"},{"location":"yolov5/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date, verified environments, with all dependencies (including CUDA/CUDNN, Python, and PyTorch) pre-installed:</p> <ul> <li>Notebooks with free   GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/#status","title":"Status","text":"<p>This badge signifies that all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify the correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and with every new commit.</p> <p></p>"},{"location":"yolov5/quickstart_tutorial/","title":"YOLOv5 Quickstart","text":"<p>See below for quickstart examples.</p>"},{"location":"yolov5/quickstart_tutorial/#install","title":"Install","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/quickstart_tutorial/#inference","title":"Inference","text":"<p>YOLOv5 PyTorch Hub inference. Models download automatically from the latest YOLOv5 release.</p> <pre><code>import torch\n# Model\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # or yolov5n - yolov5x6, custom\n# Images\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # or file, Path, PIL, OpenCV, numpy, list\n# Inference\nresults = model(img)\n# Results\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n</code></pre>"},{"location":"yolov5/quickstart_tutorial/#inference-with-detectpy","title":"Inference with detect.py","text":"<p><code>detect.py</code> runs inference on a variety of sources, downloading models automatically from the latest YOLOv5 release and saving results to <code>runs/detect</code>.</p> <pre><code>python detect.py --weights yolov5s.pt --source 0                               # webcam\nimg.jpg                         # image\nvid.mp4                         # video\nscreen                          # screenshot\npath/                           # directory\nlist.txt                        # list of images\nlist.streams                    # list of streams\n'path/*.jpg'                    # glob\n'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n</code></pre>"},{"location":"yolov5/quickstart_tutorial/#training","title":"Training","text":"<p>The commands below reproduce YOLOv5 COCO results. Models and datasets download automatically from the latest YOLOv5 release. Training times for YOLOv5n/s/m/l/x are 1/2/4/6/8 days on a V100 GPU (Multi-GPU times faster). Use the largest <code>--batch-size</code> possible, or pass <code>--batch-size -1</code> for YOLOv5 AutoBatch. Batch sizes shown for V100-16GB.</p> <pre><code>python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml  --batch-size 128\nyolov5s                    64\nyolov5m                    40\nyolov5l                    24\nyolov5x                    16\n</code></pre> <p></p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/","title":"YOLOv5 \ud83d\ude80 on AWS Deep Learning Instance: A Comprehensive Guide","text":"<p>This guide will help new users run YOLOv5 on an Amazon Web Services (AWS) Deep Learning instance. AWS offers a Free Tier and a credit program for a quick and affordable start.</p> <p>Other quickstart options for YOLOv5 include our Colab Notebook , GCP Deep Learning VM, and our Docker image at Docker Hub . Updated: 21 April 2023.</p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#1-aws-console-sign-in","title":"1. AWS Console Sign-in","text":"<p>Create an account or sign in to the AWS console at https://aws.amazon.com/console/ and select the EC2 service.</p> <p></p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#2-launch-instance","title":"2. Launch Instance","text":"<p>In the EC2 section of the AWS console, click the Launch instance button.</p> <p></p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#choose-an-amazon-machine-image-ami","title":"Choose an Amazon Machine Image (AMI)","text":"<p>Enter 'Deep Learning' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or an alternative Deep Learning AMI. For more information on selecting an AMI, see Choosing Your DLAMI.</p> <p></p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#select-an-instance-type","title":"Select an Instance Type","text":"<p>A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. Multi-GPU instances or distributed training across multiple instances with GPUs can offer sub-linear scaling. To set up distributed training, see Distributed Training.</p> <p>Note: The size of your model should be a factor in selecting an instance. If your model exceeds an instance's available RAM, select a different instance type with enough memory for your application.</p> <p>Refer to EC2 Instance Types and choose Accelerated Computing to see the different GPU instance options.</p> <p></p> <p>For more information on GPU monitoring and optimization, see GPU Monitoring and Optimization. For pricing, see On-Demand Pricing and Spot Pricing.</p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#configure-instance-details","title":"Configure Instance Details","text":"<p>Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances, leave these settings at their default values.</p> <p></p> <p>Complete Steps 4-7 to finalize your instance hardware and security settings, and then launch the instance.</p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#3-connect-to-instance","title":"3. Connect to Instance","text":"<p>Select the checkbox next to your running instance, and then click Connect. Copy and paste the SSH terminal command into a terminal of your choice to connect to your instance.</p> <p></p>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#4-run-yolov5","title":"4. Run YOLOv5","text":"<p>Once you have logged in to your instance, clone the repository and install the dependencies in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre> <p>Then, start training, testing, detecting, and exporting YOLOv5 models:</p> <pre><code>python train.py  # train a model\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\n</code></pre>"},{"location":"yolov5/environments/aws_quickstart_tutorial/#optional-extras","title":"Optional Extras","text":"<p>Add 64GB of swap memory (to <code>--cache</code> large datasets):</p> <pre><code>sudo fallocate -l 64G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nfree -h  # check memory\n</code></pre> <p>Now you have successfully set up and run YOLOv5 on an AWS Deep Learning instance. Enjoy training, testing, and deploying your object detection models!</p>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/","title":"Get Started with YOLOv5 \ud83d\ude80 in Docker","text":"<p>This tutorial will guide you through the process of setting up and running YOLOv5 in a Docker container.</p> <p>You can also explore other quickstart options for YOLOv5, such as our Colab Notebook , GCP Deep Learning VM, and Amazon AWS. Updated: 21 April 2023.</p>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#prerequisites","title":"Prerequisites","text":"<ol> <li>Nvidia Driver: Version 455.23 or higher. Download from Nvidia's website.</li> <li>Nvidia-Docker: Allows Docker to interact with your local GPU. Installation instructions are available on the Nvidia-Docker GitHub repository.</li> <li>Docker Engine - CE: Version 19.03 or higher. Download and installation instructions can be found on the Docker website.</li> </ol>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#step-1-pull-the-yolov5-docker-image","title":"Step 1: Pull the YOLOv5 Docker Image","text":"<p>The Ultralytics YOLOv5 DockerHub repository is available at https://hub.docker.com/r/ultralytics/yolov5. Docker Autobuild ensures that the <code>ultralytics/yolov5:latest</code> image is always in sync with the most recent repository commit. To pull the latest image, run the following command:</p> <pre><code>sudo docker pull ultralytics/yolov5:latest\n</code></pre>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#step-2-run-the-docker-container","title":"Step 2: Run the Docker Container","text":""},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#basic-container","title":"Basic container:","text":"<p>Run an interactive instance of the YOLOv5 Docker image (called a \"container\") using the <code>-it</code> flag:</p> <pre><code>sudo docker run --ipc=host -it ultralytics/yolov5:latest\n</code></pre>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#container-with-local-file-access","title":"Container with local file access:","text":"<p>To run a container with access to local files (e.g., COCO training data in <code>/datasets</code>), use the <code>-v</code> flag:</p> <pre><code>sudo docker run --ipc=host -it -v \"$(pwd)\"/datasets:/usr/src/datasets ultralytics/yolov5:latest\n</code></pre>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#container-with-gpu-access","title":"Container with GPU access:","text":"<p>To run a container with GPU access, use the <code>--gpus all</code> flag:</p> <pre><code>sudo docker run --ipc=host -it --gpus all ultralytics/yolov5:latest\n</code></pre>"},{"location":"yolov5/environments/docker_image_quickstart_tutorial/#step-3-use-yolov5-within-the-docker-container","title":"Step 3: Use YOLOv5 \ud83d\ude80 within the Docker Container","text":"<p>Now you can train, test, detect, and export YOLOv5 models within the running Docker container:</p> <pre><code>python train.py  # train a model\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\n</code></pre> <p></p>"},{"location":"yolov5/environments/google_cloud_quickstart_tutorial/","title":"Run YOLOv5 \ud83d\ude80 on Google Cloud Platform (GCP) Deep Learning Virtual Machine (VM) \u2b50","text":"<p>This tutorial will guide you through the process of setting up and running YOLOv5 on a GCP Deep Learning VM. New GCP users are eligible for a $300 free credit offer.</p> <p>You can also explore other quickstart options for YOLOv5, such as our Colab Notebook , Amazon AWS and our Docker image at Docker Hub . Updated: 21 April 2023.</p> <p>Last Updated: 6 May 2022</p>"},{"location":"yolov5/environments/google_cloud_quickstart_tutorial/#step-1-create-a-deep-learning-vm","title":"Step 1: Create a Deep Learning VM","text":"<ol> <li>Go to the GCP marketplace and select a Deep Learning VM.</li> <li>Choose an n1-standard-8 instance (with 8 vCPUs and 30 GB memory).</li> <li>Add a GPU of your choice.</li> <li>Check 'Install NVIDIA GPU driver automatically on first startup?'</li> <li>Select a 300 GB SSD Persistent Disk for sufficient I/O speed.</li> <li>Click 'Deploy'.</li> </ol> <p>The preinstalled Anaconda Python environment includes all dependencies.</p> <p></p>"},{"location":"yolov5/environments/google_cloud_quickstart_tutorial/#step-2-set-up-the-vm","title":"Step 2: Set Up the VM","text":"<p>Clone the YOLOv5 repository and install the requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets will be downloaded automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/environments/google_cloud_quickstart_tutorial/#step-3-run-yolov5-on-the-vm","title":"Step 3: Run YOLOv5 \ud83d\ude80 on the VM","text":"<p>You can now train, test, detect, and export YOLOv5 models on your VM:</p> <pre><code>python train.py  # train a model\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/architecture_description/","title":"Ultralytics YOLOv5 Architecture","text":"<p>YOLOv5 (v6.0/6.1) is a powerful object detection algorithm developed by Ultralytics. This article dives deep into the YOLOv5 architecture, data augmentation strategies, training methodologies, and loss computation techniques. This comprehensive understanding will help improve your practical application of object detection in various fields, including surveillance, autonomous vehicles, and image recognition.</p>"},{"location":"yolov5/tutorials/architecture_description/#1-model-structure","title":"1. Model Structure","text":"<p>YOLOv5's architecture consists of three main parts:</p> <ul> <li>Backbone: This is the main body of the network. For YOLOv5, the backbone is designed using the <code>New CSP-Darknet53</code> structure, a modification of the Darknet architecture used in previous versions.</li> <li>Neck: This part connects the backbone and the head. In YOLOv5, <code>SPPF</code> and <code>New CSP-PAN</code> structures are utilized.</li> <li>Head: This part is responsible for generating the final output. YOLOv5 uses the <code>YOLOv3 Head</code> for this purpose.</li> </ul> <p>The structure of the model is depicted in the image below. The model structure details can be found in <code>yolov5l.yaml</code>.</p> <p></p> <p>YOLOv5 introduces some minor changes compared to its predecessors:</p> <ol> <li>The <code>Focus</code> structure, found in earlier versions, is replaced with a <code>6x6 Conv2d</code> structure. This change boosts efficiency #4825.</li> <li>The <code>SPP</code> structure is replaced with <code>SPPF</code>. This alteration more than doubles the speed of processing.</li> </ol> <p>To test the speed of <code>SPP</code> and <code>SPPF</code>, the following code can be used:</p> SPP vs SPPF speed profiling example (click to open) <pre><code>import time\nimport torch\nimport torch.nn as nn\nclass SPP(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.maxpool1 = nn.MaxPool2d(5, 1, padding=2)\nself.maxpool2 = nn.MaxPool2d(9, 1, padding=4)\nself.maxpool3 = nn.MaxPool2d(13, 1, padding=6)\ndef forward(self, x):\no1 = self.maxpool1(x)\no2 = self.maxpool2(x)\no3 = self.maxpool3(x)\nreturn torch.cat([x, o1, o2, o3], dim=1)\nclass SPPF(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.maxpool = nn.MaxPool2d(5, 1, padding=2)\ndef forward(self, x):\no1 = self.maxpool(x)\no2 = self.maxpool(o1)\no3 = self.maxpool(o2)\nreturn torch.cat([x, o1, o2, o3], dim=1)\ndef main():\ninput_tensor = torch.rand(8, 32, 16, 16)\nspp = SPP()\nsppf = SPPF()\noutput1 = spp(input_tensor)\noutput2 = sppf(input_tensor)\nprint(torch.equal(output1, output2))\nt_start = time.time()\nfor _ in range(100):\nspp(input_tensor)\nprint(f\"SPP time: {time.time() - t_start}\")\nt_start = time.time()\nfor _ in range(100):\nsppf(input_tensor)\nprint(f\"SPPF time: {time.time() - t_start}\")\nif __name__ == '__main__':\nmain()\n</code></pre> <p>result:</p> <pre><code>True\nSPP time: 0.5373051166534424\nSPPF time: 0.20780706405639648\n</code></pre>"},{"location":"yolov5/tutorials/architecture_description/#2-data-augmentation-techniques","title":"2. Data Augmentation Techniques","text":"<p>YOLOv5 employs various data augmentation techniques to improve the model's ability to generalize and reduce overfitting. These techniques include:</p> <ul> <li>Mosaic Augmentation: An image processing technique that combines four training images into one in ways that encourage object detection models to better handle various object scales and translations.</li> </ul> <p></p> <ul> <li>Copy-Paste Augmentation: An innovative data augmentation method that copies random patches from an image and pastes them onto another randomly chosen image, effectively generating a new training sample.</li> </ul> <p></p> <ul> <li>Random Affine Transformations: This includes random rotation, scaling, translation, and shearing of the images.</li> </ul> <p></p> <ul> <li>MixUp Augmentation: A method that creates composite images by taking a linear combination of two images and their associated labels.</li> </ul> <p></p> <ul> <li>Albumentations: A powerful library for image augmenting that supports a wide variety of augmentation techniques.</li> <li>HSV Augmentation: Random changes to the Hue, Saturation, and Value of the images.</li> </ul> <p></p> <ul> <li>Random Horizontal Flip: An augmentation method that randomly flips images horizontally.</li> </ul> <p></p>"},{"location":"yolov5/tutorials/architecture_description/#3-training-strategies","title":"3. Training Strategies","text":"<p>YOLOv5 applies several sophisticated training strategies to enhance the model's performance. They include:</p> <ul> <li>Multiscale Training: The input images are randomly rescaled within a range of 0.5 to 1.5 times their original size during the training process.</li> <li>AutoAnchor: This strategy optimizes the prior anchor boxes to match the statistical characteristics of the ground truth boxes in your custom data.</li> <li>Warmup and Cosine LR Scheduler: A method to adjust the learning rate to enhance model performance.</li> <li>Exponential Moving Average (EMA): A strategy that uses the average of parameters over past steps to stabilize the training process and reduce generalization error.</li> <li>Mixed Precision Training: A method to perform operations in half-precision format, reducing memory usage and enhancing computational speed.</li> <li>Hyperparameter Evolution: A strategy to automatically tune hyperparameters to achieve optimal performance.</li> </ul>"},{"location":"yolov5/tutorials/architecture_description/#4-additional-features","title":"4. Additional Features","text":""},{"location":"yolov5/tutorials/architecture_description/#41-compute-losses","title":"4.1 Compute Losses","text":"<p>The loss in YOLOv5 is computed as a combination of three individual loss components:</p> <ul> <li>Classes Loss (BCE Loss): Binary Cross-Entropy loss, measures the error for the classification task.</li> <li>Objectness Loss (BCE Loss): Another Binary Cross-Entropy loss, calculates the error in detecting whether an object is present in a particular grid cell or not.</li> <li>Location Loss (CIoU Loss): Complete IoU loss, measures the error in localizing the object within the grid cell.</li> </ul> <p>The overall loss function is depicted by:</p> <p></p>"},{"location":"yolov5/tutorials/architecture_description/#42-balance-losses","title":"4.2 Balance Losses","text":"<p>The objectness losses of the three prediction layers (<code>P3</code>, <code>P4</code>, <code>P5</code>) are weighted differently. The balance weights are <code>[4.0, 1.0, 0.4]</code> respectively. This approach ensures that the predictions at different scales contribute appropriately to the total loss.</p> <p></p>"},{"location":"yolov5/tutorials/architecture_description/#43-eliminate-grid-sensitivity","title":"4.3 Eliminate Grid Sensitivity","text":"<p>The YOLOv5 architecture makes some important changes to the box prediction strategy compared to earlier versions of YOLO. In YOLOv2 and YOLOv3, the box coordinates were directly predicted using the activation of the last layer.</p> <p> </p> <p></p> <p>However, in YOLOv5, the formula for predicting the box coordinates has been updated to reduce grid sensitivity and prevent the model from predicting unbounded box dimensions.</p> <p>The revised formulas for calculating the predicted bounding box are as follows:</p> <p> </p> <p>Compare the center point offset before and after scaling. The center point offset range is adjusted from (0, 1) to (-0.5, 1.5). Therefore, offset can easily get 0 or 1.</p> <p></p> <p>Compare the height and width scaling ratio(relative to anchor) before and after adjustment. The original yolo/darknet box equations have a serious flaw. Width and Height are completely unbounded as they are simply out=exp(in), which is dangerous, as it can lead to runaway gradients, instabilities, NaN losses and ultimately a complete loss of training. refer this issue</p> <p></p>"},{"location":"yolov5/tutorials/architecture_description/#44-build-targets","title":"4.4 Build Targets","text":"<p>The build target process in YOLOv5 is critical for training efficiency and model accuracy. It involves assigning ground truth boxes to the appropriate grid cells in the output map and matching them with the appropriate anchor boxes.</p> <p>This process follows these steps:</p> <ul> <li>Calculate the ratio of the ground truth box dimensions and the dimensions of each anchor template.</li> </ul> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <ul> <li>If the calculated ratio is within the threshold, match the ground truth box with the corresponding anchor.</li> </ul> <p></p> <ul> <li>Assign the matched anchor to the appropriate cells, keeping in mind that due to the revised center point offset, a ground truth box can be assigned to more than one anchor. Because the center point offset range is adjusted from (0, 1) to (-0.5, 1.5). GT Box can be assigned to more anchors.</li> </ul> <p></p> <p>This way, the build targets process ensures that each ground truth object is properly assigned and matched during the training process, allowing YOLOv5 to learn the task of object detection more effectively.</p>"},{"location":"yolov5/tutorials/architecture_description/#conclusion","title":"Conclusion","text":"<p>In conclusion, YOLOv5 represents a significant step forward in the development of real-time object detection models. By incorporating various new features, enhancements, and training strategies, it surpasses previous versions of the YOLO family in performance and efficiency.</p> <p>The primary enhancements in YOLOv5 include the use of a dynamic architecture, an extensive range of data augmentation techniques, innovative training strategies, as well as important adjustments in computing losses and the process of building targets. All these innovations significantly improve the accuracy and efficiency of object detection while retaining a high degree of speed, which is the trademark of YOLO models.</p>"},{"location":"yolov5/tutorials/clearml_logging_integration/","title":"ClearML Integration","text":""},{"location":"yolov5/tutorials/clearml_logging_integration/#about-clearml","title":"About ClearML","text":"<p>ClearML is an open-source toolbox designed to save you time \u23f1\ufe0f.</p> <p>\ud83d\udd28 Track every YOLOv5 training run in the experiment manager</p> <p>\ud83d\udd27 Version and easily access your custom training data with the integrated ClearML Data Versioning Tool</p> <p>\ud83d\udd26 Remotely train and monitor your YOLOv5 training runs using ClearML Agent</p> <p>\ud83d\udd2c Get the very best mAP using ClearML Hyperparameter Optimization</p> <p>\ud83d\udd2d Turn your newly trained YOLOv5 model into an API with just a few commands using ClearML Serving</p> <p> And so much more. It's up to you how many of these tools you want to use, you can stick to the experiment manager, or chain them all together into an impressive pipeline!  </p> <p></p> <p> </p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#setting-things-up","title":"\ud83e\uddbe Setting Things Up","text":"<p>To keep track of your experiments and/or data, ClearML needs to communicate to a server. You have 2 options to get one:</p> <p>Either sign up for free to the ClearML Hosted Service or you can set up your own server, see here. Even the server is open-source, so even if you're dealing with sensitive data, you should be good to go!</p> <ol> <li>Install the <code>clearml</code> python package:</li> </ol> <pre><code>pip install clearml\n</code></pre> <ol> <li>Connect the ClearML SDK to the server by creating credentials (go right top to Settings -&gt; Workspace -&gt; Create new credentials), then execute the command below and follow the instructions:</li> </ol> <pre><code>clearml-init\n</code></pre> <p>That's it! You're done \ud83d\ude0e</p> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#training-yolov5-with-clearml","title":"\ud83d\ude80 Training YOLOv5 With ClearML","text":"<p>To enable ClearML experiment tracking, simply install the ClearML pip package.</p> <pre><code>pip install clearml&gt;=1.2.0\n</code></pre> <p>This will enable integration with the YOLOv5 training script. Every training run from now on, will be captured and stored by the ClearML experiment manager.</p> <p>If you want to change the <code>project_name</code> or <code>task_name</code>, use the <code>--project</code> and <code>--name</code> arguments of the <code>train.py</code> script, by default the project will be called <code>YOLOv5</code> and the task <code>Training</code>. PLEASE NOTE: ClearML uses <code>/</code> as a delimiter for subprojects, so be careful when using <code>/</code> in your project name!</p> <pre><code>python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache\n</code></pre> <p>or with custom project and task name:</p> <pre><code>python train.py --project my_project --name my_training --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache\n</code></pre> <p>This will capture:</p> <ul> <li>Source code + uncommitted changes</li> <li>Installed packages</li> <li>(Hyper)parameters</li> <li>Model files (use <code>--save-period n</code> to save a checkpoint every n epochs)</li> <li>Console output</li> <li>Scalars (mAP_0.5, mAP_0.5:0.95, precision, recall, losses, learning rates, ...)</li> <li>General info such as machine details, runtime, creation date etc.</li> <li>All produced plots such as label correlogram and confusion matrix</li> <li>Images with bounding boxes per epoch</li> <li>Mosaic per epoch</li> <li>Validation images per epoch</li> <li>...</li> </ul> <p>That's a lot right? \ud83e\udd2f Now, we can visualize all of this information in the ClearML UI to get an overview of our training progress. Add custom columns to the table view (such as e.g. mAP_0.5) so you can easily sort on the best performing model. Or select multiple experiments and directly compare them!</p> <p>There even more we can do with all of this information, like hyperparameter optimization and remote execution, so keep reading if you want to see how that works!</p> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#dataset-version-management","title":"\ud83d\udd17 Dataset Version Management","text":"<p>Versioning your data separately from your code is generally a good idea and makes it easy to acquire the latest version too. This repository supports supplying a dataset version ID, and it will make sure to get the data if it's not there yet. Next to that, this workflow also saves the used dataset ID as part of the task parameters, so you will always know for sure which data was used in which experiment!</p> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#prepare-your-dataset","title":"Prepare Your Dataset","text":"<p>The YOLOv5 repository supports a number of different datasets by using YAML files containing their information. By default datasets are downloaded to the <code>../datasets</code> folder in relation to the repository root folder. So if you downloaded the <code>coco128</code> dataset using the link in the YAML or with the scripts provided by yolov5, you get this folder structure:</p> <pre><code>..\n|_ yolov5\n|_ datasets\n    |_ coco128\n        |_ images\n        |_ labels\n        |_ LICENSE\n        |_ README.txt\n</code></pre> <p>But this can be any dataset you wish. Feel free to use your own, as long as you keep to this folder structure.</p> <p>Next, \u26a0\ufe0fcopy the corresponding YAML file to the root of the dataset folder\u26a0\ufe0f. This YAML files contains the information ClearML will need to properly use the dataset. You can make this yourself too, of course, just follow the structure of the example YAMLs.</p> <p>Basically we need the following keys: <code>path</code>, <code>train</code>, <code>test</code>, <code>val</code>, <code>nc</code>, <code>names</code>.</p> <pre><code>..\n|_ yolov5\n|_ datasets\n    |_ coco128\n        |_ images\n        |_ labels\n        |_ coco128.yaml  # &lt;---- HERE!\n        |_ LICENSE\n        |_ README.txt\n</code></pre>"},{"location":"yolov5/tutorials/clearml_logging_integration/#upload-your-dataset","title":"Upload Your Dataset","text":"<p>To get this dataset into ClearML as a versioned dataset, go to the dataset root folder and run the following command:</p> <pre><code>cd coco128\nclearml-data sync --project YOLOv5 --name coco128 --folder .\n</code></pre> <p>The command <code>clearml-data sync</code> is actually a shorthand command. You could also run these commands one after the other:</p> <pre><code># Optionally add --parent &lt;parent_dataset_id&gt; if you want to base\n# this version on another dataset version, so no duplicate files are uploaded!\nclearml-data create --name coco128 --project YOLOv5\nclearml-data add --files .\nclearml-data close\n</code></pre>"},{"location":"yolov5/tutorials/clearml_logging_integration/#run-training-using-a-clearml-dataset","title":"Run Training Using A ClearML Dataset","text":"<p>Now that you have a ClearML dataset, you can very simply use it to train custom YOLOv5 \ud83d\ude80 models!</p> <pre><code>python train.py --img 640 --batch 16 --epochs 3 --data clearml://&lt;your_dataset_id&gt; --weights yolov5s.pt --cache\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#hyperparameter-optimization","title":"\ud83d\udc40 Hyperparameter Optimization","text":"<p>Now that we have our experiments and data versioned, it's time to take a look at what we can build on top!</p> <p>Using the code information, installed packages and environment details, the experiment itself is now completely reproducible. In fact, ClearML allows you to clone an experiment and even change its parameters. We can then just rerun it with these new parameters automatically, this is basically what HPO does!</p> <p>To run hyperparameter optimization locally, we've included a pre-made script for you. Just make sure a training task has been run at least once, so it is in the ClearML experiment manager, we will essentially clone it and change its hyperparameters.</p> <p>You'll need to fill in the ID of this <code>template task</code> in the script found at <code>utils/loggers/clearml/hpo.py</code> and then just run it :) You can change <code>task.execute_locally()</code> to <code>task.execute()</code> to put it in a ClearML queue and have a remote agent work on it instead.</p> <pre><code># To use optuna, install it first, otherwise you can change the optimizer to just be RandomSearch\npip install optuna\npython utils/loggers/clearml/hpo.py\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#remote-execution-advanced","title":"\ud83e\udd2f Remote Execution (advanced)","text":"<p>Running HPO locally is really handy, but what if we want to run our experiments on a remote machine instead? Maybe you have access to a very powerful GPU machine on-site, or you have some budget to use cloud GPUs. This is where the ClearML Agent comes into play. Check out what the agent can do here:</p> <ul> <li>YouTube video</li> <li>Documentation</li> </ul> <p>In short: every experiment tracked by the experiment manager contains enough information to reproduce it on a different machine (installed packages, uncommitted changes etc.). So a ClearML agent does just that: it listens to a queue for incoming tasks and when it finds one, it recreates the environment and runs it while still reporting scalars, plots etc. to the experiment manager.</p> <p>You can turn any machine (a cloud VM, a local GPU machine, your own laptop ... ) into a ClearML agent by simply running:</p> <pre><code>clearml-agent daemon --queue &lt;queues_to_listen_to&gt; [--docker]\n</code></pre>"},{"location":"yolov5/tutorials/clearml_logging_integration/#cloning-editing-and-enqueuing","title":"Cloning, Editing And Enqueuing","text":"<p>With our agent running, we can give it some work. Remember from the HPO section that we can clone a task and edit the hyperparameters? We can do that from the interface too!</p> <p>\ud83e\ude84 Clone the experiment by right-clicking it</p> <p>\ud83c\udfaf Edit the hyperparameters to what you wish them to be</p> <p>\u23f3 Enqueue the task to any of the queues by right-clicking it</p> <p></p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#executing-a-task-remotely","title":"Executing A Task Remotely","text":"<p>Now you can clone a task like we explained above, or simply mark your current script by adding <code>task.execute_remotely()</code> and on execution it will be put into a queue, for the agent to start working on!</p> <p>To run the YOLOv5 training script remotely, all you have to do is add this line to the training.py script after the clearml logger has been instantiated:</p> <pre><code># ...\n# Loggers\ndata_dict = None\nif RANK in {-1, 0}:\nloggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\nif loggers.clearml:\nloggers.clearml.task.execute_remotely(queue=\"my_queue\")  # &lt;------ ADD THIS LINE\n# Data_dict is either None is user did not choose for ClearML dataset or is filled in by ClearML\ndata_dict = loggers.clearml.data_dict\n# ...\n</code></pre> <p>When running the training script after this change, python will run the script up until that line, after which it will package the code and send it to the queue instead!</p>"},{"location":"yolov5/tutorials/clearml_logging_integration/#autoscaling-workers","title":"Autoscaling workers","text":"<p>ClearML comes with autoscalers too! This tool will automatically spin up new remote machines in the cloud of your choice (AWS, GCP, Azure) and turn them into ClearML agents for you whenever there are experiments detected in the queue. Once the tasks are processed, the autoscaler will automatically shut down the remote machines, and you stop paying!</p> <p>Check out the autoscalers getting started video below.</p> <p></p>"},{"location":"yolov5/tutorials/comet_logging_integration/","title":"Comet Logging","text":""},{"location":"yolov5/tutorials/comet_logging_integration/#yolov5-with-comet","title":"YOLOv5 with Comet","text":"<p>This guide will cover how to use YOLOv5 with Comet</p>"},{"location":"yolov5/tutorials/comet_logging_integration/#about-comet","title":"About Comet","text":"<p>Comet builds tools that help data scientists, engineers, and team leaders accelerate and optimize machine learning and deep learning models.</p> <p>Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with Comet Custom Panels! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!</p>"},{"location":"yolov5/tutorials/comet_logging_integration/#getting-started","title":"Getting Started","text":""},{"location":"yolov5/tutorials/comet_logging_integration/#install-comet","title":"Install Comet","text":"<pre><code>pip install comet_ml\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#configure-comet-credentials","title":"Configure Comet Credentials","text":"<p>There are two ways to configure Comet with YOLOv5.</p> <p>You can either set your credentials through environment variables</p> <p>Environment Variables</p> <pre><code>export COMET_API_KEY=&lt;Your Comet API Key&gt;\nexport COMET_PROJECT_NAME=&lt;Your Comet Project Name&gt; # This will default to 'yolov5'\n</code></pre> <p>Or create a <code>.comet.config</code> file in your working directory and set your credentials there.</p> <p>Comet Configuration File</p> <pre><code>[comet]\napi_key=&lt;Your Comet API Key&gt;\nproject_name=&lt;Your Comet Project Name&gt; # This will default to 'yolov5'\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#run-the-training-script","title":"Run the Training Script","text":"<pre><code># Train YOLOv5s on COCO128 for 5 epochs\npython train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt\n</code></pre> <p>That's it! Comet will automatically log your hyperparameters, command line arguments, training and validation metrics. You can visualize and analyze your runs in the Comet UI</p> <p></p>"},{"location":"yolov5/tutorials/comet_logging_integration/#try-out-an-example","title":"Try out an Example!","text":"<p>Check out an example of a completed run here</p> <p>Or better yet, try it out yourself in this Colab Notebook</p> <p></p>"},{"location":"yolov5/tutorials/comet_logging_integration/#log-automatically","title":"Log automatically","text":"<p>By default, Comet will log the following items</p>"},{"location":"yolov5/tutorials/comet_logging_integration/#metrics","title":"Metrics","text":"<ul> <li>Box Loss, Object Loss, Classification Loss for the training and validation data</li> <li>mAP_0.5, mAP_0.5:0.95 metrics for the validation data.</li> <li>Precision and Recall for the validation data</li> </ul>"},{"location":"yolov5/tutorials/comet_logging_integration/#parameters","title":"Parameters","text":"<ul> <li>Model Hyperparameters</li> <li>All parameters passed through the command line options</li> </ul>"},{"location":"yolov5/tutorials/comet_logging_integration/#visualizations","title":"Visualizations","text":"<ul> <li>Confusion Matrix of the model predictions on the validation data</li> <li>Plots for the PR and F1 curves across all classes</li> <li>Correlogram of the Class Labels</li> </ul>"},{"location":"yolov5/tutorials/comet_logging_integration/#configure-comet-logging","title":"Configure Comet Logging","text":"<p>Comet can be configured to log additional data either through command line flags passed to the training script or through environment variables.</p> <pre><code>export COMET_MODE=online # Set whether to run Comet in 'online' or 'offline' mode. Defaults to online\nexport COMET_MODEL_NAME=&lt;your model name&gt; #Set the name for the saved model. Defaults to yolov5\nexport COMET_LOG_CONFUSION_MATRIX=false # Set to disable logging a Comet Confusion Matrix. Defaults to true\nexport COMET_MAX_IMAGE_UPLOADS=&lt;number of allowed images to upload to Comet&gt; # Controls how many total image predictions to log to Comet. Defaults to 100.\nexport COMET_LOG_PER_CLASS_METRICS=true # Set to log evaluation metrics for each detected class at the end of training. Defaults to false\nexport COMET_DEFAULT_CHECKPOINT_FILENAME=&lt;your checkpoint filename&gt; # Set this if you would like to resume training from a different checkpoint. Defaults to 'last.pt'\nexport COMET_LOG_BATCH_LEVEL_METRICS=true # Set this if you would like to log training metrics at the batch level. Defaults to false.\nexport COMET_LOG_PREDICTIONS=true # Set this to false to disable logging model predictions\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#logging-checkpoints-with-comet","title":"Logging Checkpoints with Comet","text":"<p>Logging Models to Comet is disabled by default. To enable it, pass the <code>save-period</code> argument to the training script. This will save the logged checkpoints to Comet based on the interval value provided by <code>save-period</code></p> <pre><code>python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data coco128.yaml \\\n--weights yolov5s.pt \\\n--save-period 1\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#logging-model-predictions","title":"Logging Model Predictions","text":"<p>By default, model predictions (images, ground truth labels and bounding boxes) will be logged to Comet.</p> <p>You can control the frequency of logged predictions and the associated images by passing the <code>bbox_interval</code> command line argument. Predictions can be visualized using Comet's Object Detection Custom Panel. This frequency corresponds to every Nth batch of data per epoch. In the example below, we are logging every 2nd batch of data for each epoch.</p> <p>Note: The YOLOv5 validation dataloader will default to a batch size of 32, so you will have to set the logging frequency accordingly.</p> <p>Here is an example project using the Panel</p> <pre><code>python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data coco128.yaml \\\n--weights yolov5s.pt \\\n--bbox_interval 2\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#controlling-the-number-of-prediction-images-logged-to-comet","title":"Controlling the number of Prediction Images logged to Comet","text":"<p>When logging predictions from YOLOv5, Comet will log the images associated with each set of predictions. By default a maximum of 100 validation images are logged. You can increase or decrease this number using the <code>COMET_MAX_IMAGE_UPLOADS</code> environment variable.</p> <pre><code>env COMET_MAX_IMAGE_UPLOADS=200 python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data coco128.yaml \\\n--weights yolov5s.pt \\\n--bbox_interval 1\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#logging-class-level-metrics","title":"Logging Class Level Metrics","text":"<p>Use the <code>COMET_LOG_PER_CLASS_METRICS</code> environment variable to log mAP, precision, recall, f1 for each class.</p> <pre><code>env COMET_LOG_PER_CLASS_METRICS=true python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data coco128.yaml \\\n--weights yolov5s.pt\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#uploading-a-dataset-to-comet-artifacts","title":"Uploading a Dataset to Comet Artifacts","text":"<p>If you would like to store your data using Comet Artifacts, you can do so using the <code>upload_dataset</code> flag.</p> <p>The dataset be organized in the way described in the YOLOv5 documentation. The dataset config <code>yaml</code> file must follow the same format as that of the <code>coco128.yaml</code> file.</p> <pre><code>python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data coco128.yaml \\\n--weights yolov5s.pt \\\n--upload_dataset\n</code></pre> <p>You can find the uploaded dataset in the Artifacts tab in your Comet Workspace </p> <p>You can preview the data directly in the Comet UI. </p> <p>Artifacts are versioned and also support adding metadata about the dataset. Comet will automatically log the metadata from your dataset <code>yaml</code> file </p>"},{"location":"yolov5/tutorials/comet_logging_integration/#using-a-saved-artifact","title":"Using a saved Artifact","text":"<p>If you would like to use a dataset from Comet Artifacts, set the <code>path</code> variable in your dataset <code>yaml</code> file to point to the following Artifact resource URL.</p> <pre><code># contents of artifact.yaml file\npath: \"comet://&lt;workspace name&gt;/&lt;artifact name&gt;:&lt;artifact version or alias&gt;\"\n</code></pre> <p>Then pass this file to your training script in the following way</p> <pre><code>python train.py \\\n--img 640 \\\n--batch 16 \\\n--epochs 5 \\\n--data artifact.yaml \\\n--weights yolov5s.pt\n</code></pre> <p>Artifacts also allow you to track the lineage of data as it flows through your Experimentation workflow. Here you can see a graph that shows you all the experiments that have used your uploaded dataset. </p>"},{"location":"yolov5/tutorials/comet_logging_integration/#resuming-a-training-run","title":"Resuming a Training Run","text":"<p>If your training run is interrupted for any reason, e.g. disrupted internet connection, you can resume the run using the <code>resume</code> flag and the Comet Run Path.</p> <p>The Run Path has the following format <code>comet://&lt;your workspace name&gt;/&lt;your project name&gt;/&lt;experiment id&gt;</code>.</p> <p>This will restore the run to its state before the interruption, which includes restoring the model from a checkpoint, restoring all hyperparameters and training arguments and downloading Comet dataset Artifacts if they were used in the original run. The resumed run will continue logging to the existing Experiment in the Comet UI</p> <pre><code>python train.py \\\n--resume \"comet://&lt;your run path&gt;\"\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#hyperparameter-search-with-the-comet-optimizer","title":"Hyperparameter Search with the Comet Optimizer","text":"<p>YOLOv5 is also integrated with Comet's Optimizer, making is simple to visualize hyperparameter sweeps in the Comet UI.</p>"},{"location":"yolov5/tutorials/comet_logging_integration/#configuring-an-optimizer-sweep","title":"Configuring an Optimizer Sweep","text":"<p>To configure the Comet Optimizer, you will have to create a JSON file with the information about the sweep. An example file has been provided in <code>utils/loggers/comet/optimizer_config.json</code></p> <pre><code>python utils/loggers/comet/hpo.py \\\n--comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\"\n</code></pre> <p>The <code>hpo.py</code> script accepts the same arguments as <code>train.py</code>. If you wish to pass additional arguments to your sweep simply add them after the script.</p> <pre><code>python utils/loggers/comet/hpo.py \\\n--comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\" \\\n--save-period 1 \\\n--bbox_interval 1\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#running-a-sweep-in-parallel","title":"Running a Sweep in Parallel","text":"<pre><code>comet optimizer -j &lt;set number of workers&gt; utils/loggers/comet/hpo.py \\\nutils/loggers/comet/optimizer_config.json\"\n</code></pre>"},{"location":"yolov5/tutorials/comet_logging_integration/#visualizing-results","title":"Visualizing Results","text":"<p>Comet provides a number of ways to visualize the results of your sweep. Take a look at a project with a completed sweep here</p> <p></p>"},{"location":"yolov5/tutorials/hyperparameter_evolution/","title":"Hyperparameter evolution","text":"<p>\ud83d\udcda This guide explains hyperparameter evolution for YOLOv5 \ud83d\ude80. Hyperparameter evolution is a method of Hyperparameter Optimization using a Genetic Algorithm (GA) for optimization. UPDATED 25 September 2022.</p> <p>Hyperparameters in ML control various aspects of training, and finding optimal values for them can be a challenge. Traditional methods like grid searches can quickly become intractable due to 1) the high dimensional search space 2) unknown correlations among the dimensions, and 3) expensive nature of evaluating the fitness at each point, making GA a suitable candidate for hyperparameter searches.</p>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#1-initialize-hyperparameters","title":"1. Initialize Hyperparameters","text":"<p>YOLOv5 has about 30 hyperparameters used for various training settings. These are defined in <code>*.yaml</code> files in the <code>/data/hyps</code> directory. Better initial guesses will produce better final results, so it is important to initialize these values properly before evolving. If in doubt, simply use the default values, which are optimized for YOLOv5 COCO training from scratch.</p> <pre><code># YOLOv5 \ud83d\ude80 by Ultralytics, AGPL-3.0 license\n# Hyperparameters for low-augmentation COCO training from scratch\n# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.1  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)\n</code></pre>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#2-define-fitness","title":"2. Define Fitness","text":"<p>Fitness is the value we seek to maximize. In YOLOv5 we define a default fitness function as a weighted combination of metrics: <code>mAP@0.5</code> contributes 10% of the weight and <code>mAP@0.5:0.95</code> contributes the remaining 90%, with Precision <code>P</code> and Recall <code>R</code> absent. You may adjust these as you see fit or use the default fitness definition in utils/metrics.py (recommended).</p> <pre><code>def fitness(x):\n# Model fitness as a weighted combination of metrics\nw = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\nreturn (x[:, :4] * w).sum(1)\n</code></pre>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#3-evolve","title":"3. Evolve","text":"<p>Evolution is performed about a base scenario which we seek to improve upon. The base scenario in this example is finetuning COCO128 for 10 epochs using pretrained YOLOv5s. The base scenario training command is:</p> <pre><code>python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache\n</code></pre> <p>To evolve hyperparameters specific to this scenario, starting from our initial values defined in Section 1., and maximizing the fitness defined in Section 2., append <code>--evolve</code>:</p> <pre><code># Single-GPU\npython train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve\n\n# Multi-GPU\nfor i in 0 1 2 3 4 5 6 7; do\nsleep $(expr 30 \\* $i) &amp;&amp;  # 30-second delay (optional)\necho 'Starting GPU '$i'...' &amp;&amp;\nnohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve &gt; evolve_gpu_$i.log &amp;\ndone\n# Multi-GPU bash-while (not recommended)\nfor i in 0 1 2 3 4 5 6 7; do\nsleep $(expr 30 \\* $i) &amp;&amp;  # 30-second delay (optional)\necho 'Starting GPU '$i'...' &amp;&amp;\n\"$(while true; do nohup python train.py... --device $i --evolve 1 &gt; evolve_gpu_$i.log; done)\" &amp;\ndone\n</code></pre> <p>The default evolution settings will run the base scenario 300 times, i.e. for 300 generations. You can modify generations via the <code>--evolve</code> argument, i.e. <code>python train.py --evolve 1000</code>. https://github.com/ultralytics/yolov5/blob/6a3ee7cf03efb17fbffde0e68b1a854e80fe3213/train.py#L608</p> <p>The main genetic operators are crossover and mutation. In this work mutation is used, with an 80% probability and a 0.04 variance to create new offspring based on a combination of the best parents from all previous generations. Results are logged to <code>runs/evolve/exp/evolve.csv</code>, and the highest fitness offspring is saved every generation as <code>runs/evolve/hyp_evolved.yaml</code>:</p> <pre><code># YOLOv5 Hyperparameter Evolution Results\n# Best generation: 287\n# Last generation: 300\n#    metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss\n#              0.54634,              0.55625,              0.58201,              0.33665,             0.056451,             0.042892,             0.013441\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.1  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)\n</code></pre> <p>We recommend a minimum of 300 generations of evolution for best results. Note that evolution is generally expensive and time-consuming, as the base scenario is trained hundreds of times, possibly requiring hundreds or thousands of GPU hours.</p>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#4-visualize","title":"4. Visualize","text":"<p><code>evolve.csv</code> is plotted as <code>evolve.png</code> by <code>utils.plots.plot_evolve()</code> after evolution finishes with one subplot per hyperparameter showing fitness (y-axis) vs hyperparameter values (x-axis). Yellow indicates higher concentrations. Vertical distributions indicate that a parameter has been disabled and does not mutate. This is user selectable in the <code>meta</code> dictionary in train.py, and is useful for fixing parameters and preventing them from evolving.</p> <p></p>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/hyperparameter_evolution/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/model_ensembling/","title":"Model Ensembling","text":"<p>\ud83d\udcda This guide explains how to use YOLOv5 \ud83d\ude80 model ensembling during testing and inference for improved mAP and Recall. UPDATED 25 September 2022.</p> <p>From https://en.wikipedia.org/wiki/Ensemble_learning:</p> <p>Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.</p>"},{"location":"yolov5/tutorials/model_ensembling/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/model_ensembling/#test-normally","title":"Test Normally","text":"<p>Before ensembling we want to establish the baseline performance of a single model. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. <code>yolov5x.pt</code> is the largest and most accurate model available. Other options are <code>yolov5s.pt</code>, <code>yolov5m.pt</code> and <code>yolov5l.pt</code>, or you own checkpoint from training a custom dataset <code>./weights/best.pt</code>. For details on all available models please see our README table.</p> <pre><code>python val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n</code></pre> <p>Output:</p> <pre><code>val: data=./data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nFusing layers...\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\n\nval: Scanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01&lt;00:00, 2846.03it/s]\nval: New cache created: ../datasets/coco/val2017.cache\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [02:30&lt;00:00,  1.05it/s]\nall       5000      36335      0.746      0.626       0.68       0.49\nSpeed: 0.1ms pre-process, 22.4ms inference, 1.4ms NMS per image at shape (32, 3, 640, 640)  # &lt;--- baseline speed\nEvaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504  # &lt;--- baseline mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681  # &lt;--- baseline mAR\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\n</code></pre>"},{"location":"yolov5/tutorials/model_ensembling/#ensemble-test","title":"Ensemble Test","text":"<p>Multiple pretrained models may be ensembled together at test and inference time by simply appending extra models to the <code>--weights</code> argument in any existing val.py or detect.py command. This example tests an ensemble of 2 models together:</p> <ul> <li>YOLOv5x</li> <li>YOLOv5l6</li> </ul> <pre><code>python val.py --weights yolov5x.pt yolov5l6.pt --data coco.yaml --img 640 --half\n</code></pre> <p>Output:</p> <pre><code>val: data=./data/coco.yaml, weights=['yolov5x.pt', 'yolov5l6.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nFusing layers...\nModel Summary: 476 layers, 87730285 parameters, 0 gradients  # Model 1\nFusing layers...\nModel Summary: 501 layers, 77218620 parameters, 0 gradients  # Model 2\nEnsemble created with ['yolov5x.pt', 'yolov5l6.pt']  # Ensemble notice\nval: Scanning '../datasets/coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00&lt;00:00, 49695545.02it/s]\nClass     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [03:58&lt;00:00,  1.52s/it]\nall       5000      36335      0.747      0.637      0.692      0.502\nSpeed: 0.1ms pre-process, 39.5ms inference, 2.0ms NMS per image at shape (32, 3, 640, 640)  # &lt;--- ensemble speed\nEvaluating pycocotools mAP... saving runs/val/exp3/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.515  # &lt;--- ensemble mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.557\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.356\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.563\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.668\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.387\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.689  # &lt;--- ensemble mAR\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.743\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\n</code></pre>"},{"location":"yolov5/tutorials/model_ensembling/#ensemble-inference","title":"Ensemble Inference","text":"<p>Append extra models to the <code>--weights</code> argument to run ensemble inference:</p> <pre><code>python detect.py --weights yolov5x.pt yolov5l6.pt --img 640 --source data/images\n</code></pre> <p>Output:</p> <pre><code>detect: weights=['yolov5x.pt', 'yolov5l6.pt'], source=data/images, imgsz=640, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_width=3, hide_labels=False, hide_conf=False, half=False\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nFusing layers...\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\nFusing layers...\nModel Summary: 501 layers, 77218620 parameters, 0 gradients\nEnsemble created with ['yolov5x.pt', 'yolov5l6.pt']\nimage 1/2 /content/yolov5/data/images/bus.jpg: 640x512 4 persons, 1 bus, 1 tie, Done. (0.063s)\nimage 2/2 /content/yolov5/data/images/zidane.jpg: 384x640 3 persons, 2 ties, Done. (0.056s)\nResults saved to runs/detect/exp2\nDone. (0.223s)\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/model_ensembling/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/model_ensembling/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/model_export/","title":"TFLite, ONNX, CoreML, TensorRT Export","text":"<p>\ud83d\udcda This guide explains how to export a trained YOLOv5 \ud83d\ude80 model from PyTorch to ONNX and TorchScript formats. UPDATED 8 December 2022.</p>"},{"location":"yolov5/tutorials/model_export/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre> <p>For TensorRT export example (requires GPU) see our Colab notebook appendix section. </p>"},{"location":"yolov5/tutorials/model_export/#formats","title":"Formats","text":"<p>YOLOv5 inference is officially supported in 11 formats:</p> <p>\ud83d\udca1 ProTip: Export to ONNX or OpenVINO for up to 3x CPU speedup. See CPU Benchmarks. \ud83d\udca1 ProTip: Export to TensorRT for up to 5x GPU speedup. See GPU Benchmarks.</p> Format <code>export.py --include</code> Model PyTorch - <code>yolov5s.pt</code> TorchScript <code>torchscript</code> <code>yolov5s.torchscript</code> ONNX <code>onnx</code> <code>yolov5s.onnx</code> OpenVINO <code>openvino</code> <code>yolov5s_openvino_model/</code> TensorRT <code>engine</code> <code>yolov5s.engine</code> CoreML <code>coreml</code> <code>yolov5s.mlmodel</code> TensorFlow SavedModel <code>saved_model</code> <code>yolov5s_saved_model/</code> TensorFlow GraphDef <code>pb</code> <code>yolov5s.pb</code> TensorFlow Lite <code>tflite</code> <code>yolov5s.tflite</code> TensorFlow Edge TPU <code>edgetpu</code> <code>yolov5s_edgetpu.tflite</code> TensorFlow.js <code>tfjs</code> <code>yolov5s_web_model/</code> PaddlePaddle <code>paddle</code> <code>yolov5s_paddle_model/</code>"},{"location":"yolov5/tutorials/model_export/#benchmarks","title":"Benchmarks","text":"<p>Benchmarks below run on a Colab Pro with the YOLOv5 tutorial notebook . To reproduce:</p> <pre><code>python benchmarks.py --weights yolov5s.pt --imgsz 640 --device 0\n</code></pre>"},{"location":"yolov5/tutorials/model_export/#colab-pro-v100-gpu","title":"Colab Pro V100 GPU","text":"<pre><code>benchmarks: weights=/content/yolov5/yolov5s.pt, imgsz=640, batch_size=1, data=/content/yolov5/data/coco128.yaml, device=0, half=False, test=False\nChecking setup...\nYOLOv5 \ud83d\ude80 v6.1-135-g7926afc torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\nSetup complete \u2705 (8 CPUs, 51.0 GB RAM, 46.7/166.8 GB disk)\n\nBenchmarks complete (458.07s)\n                   Format  mAP@0.5:0.95  Inference time (ms)\n0                 PyTorch        0.4623                10.19\n1             TorchScript        0.4623                 6.85\n2                    ONNX        0.4623                14.63\n3                OpenVINO           NaN                  NaN\n4                TensorRT        0.4617                 1.89\n5                  CoreML           NaN                  NaN\n6   TensorFlow SavedModel        0.4623                21.28\n7     TensorFlow GraphDef        0.4623                21.22\n8         TensorFlow Lite           NaN                  NaN\n9     TensorFlow Edge TPU           NaN                  NaN\n10          TensorFlow.js           NaN                  NaN\n</code></pre>"},{"location":"yolov5/tutorials/model_export/#colab-pro-cpu","title":"Colab Pro CPU","text":"<pre><code>benchmarks: weights=/content/yolov5/yolov5s.pt, imgsz=640, batch_size=1, data=/content/yolov5/data/coco128.yaml, device=cpu, half=False, test=False\nChecking setup...\nYOLOv5 \ud83d\ude80 v6.1-135-g7926afc torch 1.10.0+cu111 CPU\nSetup complete \u2705 (8 CPUs, 51.0 GB RAM, 41.5/166.8 GB disk)\n\nBenchmarks complete (241.20s)\n                   Format  mAP@0.5:0.95  Inference time (ms)\n0                 PyTorch        0.4623               127.61\n1             TorchScript        0.4623               131.23\n2                    ONNX        0.4623                69.34\n3                OpenVINO        0.4623                66.52\n4                TensorRT           NaN                  NaN\n5                  CoreML           NaN                  NaN\n6   TensorFlow SavedModel        0.4623               123.79\n7     TensorFlow GraphDef        0.4623               121.57\n8         TensorFlow Lite        0.4623               316.61\n9     TensorFlow Edge TPU           NaN                  NaN\n10          TensorFlow.js           NaN                  NaN\n</code></pre>"},{"location":"yolov5/tutorials/model_export/#export-a-trained-yolov5-model","title":"Export a Trained YOLOv5 Model","text":"<p>This command exports a pretrained YOLOv5s model to TorchScript and ONNX formats. <code>yolov5s.pt</code> is the 'small' model, the second-smallest model available. Other options are <code>yolov5n.pt</code>, <code>yolov5m.pt</code>, <code>yolov5l.pt</code> and <code>yolov5x.pt</code>, along with their P6 counterparts i.e. <code>yolov5s6.pt</code> or you own custom training checkpoint i.e. <code>runs/exp/weights/best.pt</code>. For details on all available models please see our README table.</p> <pre><code>python export.py --weights yolov5s.pt --include torchscript onnx\n</code></pre> <p>\ud83d\udca1 ProTip: Add <code>--half</code> to export models at FP16 half precision for smaller file sizes</p> <p>Output:</p> <pre><code>export: data=data/coco128.yaml, weights=['yolov5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['torchscript', 'onnx']\nYOLOv5 \ud83d\ude80 v6.2-104-ge3e5122 Python-3.8.0 torch-1.12.1+cu113 CPU\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n100% 14.1M/14.1M [00:00&lt;00:00, 274MB/s]\nFusing layers...\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n\nPyTorch: starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\nTorchScript: starting export with torch 1.12.1+cu113...\nTorchScript: export success \u2705 1.7s, saved as yolov5s.torchscript (28.1 MB)\nONNX: starting export with onnx 1.12.0...\nONNX: export success \u2705 2.3s, saved as yolov5s.onnx (28.0 MB)\nExport complete (5.5s)\nResults saved to /content/yolov5\nDetect:          python detect.py --weights yolov5s.onnx\nValidate:        python val.py --weights yolov5s.onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.onnx')\nVisualize:       https://netron.app/\n</code></pre> <p>The 3 exported models will be saved alongside the original PyTorch model:</p> <p></p> <p>Netron Viewer is recommended for visualizing exported models:</p> <p></p>"},{"location":"yolov5/tutorials/model_export/#exported-model-usage-examples","title":"Exported Model Usage Examples","text":"<p><code>detect.py</code> runs inference on exported models:</p> <pre><code>python detect.py --weights yolov5s.pt                 # PyTorch\nyolov5s.torchscript        # TorchScript\nyolov5s.onnx               # ONNX Runtime or OpenCV DNN with dnn=True\nyolov5s_openvino_model     # OpenVINO\nyolov5s.engine             # TensorRT\nyolov5s.mlmodel            # CoreML (macOS only)\nyolov5s_saved_model        # TensorFlow SavedModel\nyolov5s.pb                 # TensorFlow GraphDef\nyolov5s.tflite             # TensorFlow Lite\nyolov5s_edgetpu.tflite     # TensorFlow Edge TPU\nyolov5s_paddle_model       # PaddlePaddle\n</code></pre> <p><code>val.py</code> runs validation on exported models:</p> <pre><code>python val.py --weights yolov5s.pt                 # PyTorch\nyolov5s.torchscript        # TorchScript\nyolov5s.onnx               # ONNX Runtime or OpenCV DNN with dnn=True\nyolov5s_openvino_model     # OpenVINO\nyolov5s.engine             # TensorRT\nyolov5s.mlmodel            # CoreML (macOS Only)\nyolov5s_saved_model        # TensorFlow SavedModel\nyolov5s.pb                 # TensorFlow GraphDef\nyolov5s.tflite             # TensorFlow Lite\nyolov5s_edgetpu.tflite     # TensorFlow Edge TPU\nyolov5s_paddle_model       # PaddlePaddle\n</code></pre> <p>Use PyTorch Hub with exported YOLOv5 models:</p> <pre><code>import torch\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')\n'yolov5s.torchscript ')       # TorchScript\n'yolov5s.onnx')               # ONNX Runtime\n'yolov5s_openvino_model')     # OpenVINO\n'yolov5s.engine')             # TensorRT\n'yolov5s.mlmodel')            # CoreML (macOS Only)\n'yolov5s_saved_model')        # TensorFlow SavedModel\n'yolov5s.pb')                 # TensorFlow GraphDef\n'yolov5s.tflite')             # TensorFlow Lite\n'yolov5s_edgetpu.tflite')     # TensorFlow Edge TPU\n'yolov5s_paddle_model')       # PaddlePaddle\n# Images\nimg = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n# Inference\nresults = model(img)\n# Results\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n</code></pre>"},{"location":"yolov5/tutorials/model_export/#opencv-dnn-inference","title":"OpenCV DNN inference","text":"<p>OpenCV inference with ONNX models:</p> <pre><code>python export.py --weights yolov5s.pt --include onnx\n\npython detect.py --weights yolov5s.onnx --dnn  # detect\npython val.py --weights yolov5s.onnx --dnn  # validate\n</code></pre>"},{"location":"yolov5/tutorials/model_export/#c-inference","title":"C++ Inference","text":"<p>YOLOv5 OpenCV DNN C++ inference on exported ONNX model examples:</p> <ul> <li>https://github.com/Hexmagic/ONNX-yolov5/blob/master/src/test.cpp</li> <li>https://github.com/doleron/yolov5-opencv-cpp-python</li> </ul> <p>YOLOv5 OpenVINO C++ inference examples:</p> <ul> <li>https://github.com/dacquaviva/yolov5-openvino-cpp-python</li> <li>https://github.com/UNeedCryDear/yolov5-seg-opencv-dnn-cpp</li> </ul>"},{"location":"yolov5/tutorials/model_export/#tensorflowjs-web-browser-inference","title":"TensorFlow.js Web Browser Inference","text":"<ul> <li>https://aukerul-shuvo.github.io/YOLOv5_TensorFlow-JS/</li> </ul>"},{"location":"yolov5/tutorials/model_export/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/model_export/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/","title":"Pruning/Sparsity Tutorial","text":"<p>\ud83d\udcda This guide explains how to apply pruning to YOLOv5 \ud83d\ude80 models. UPDATED 25 September 2022.</p>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/#test-normally","title":"Test Normally","text":"<p>Before pruning we want to establish a baseline performance to compare to. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. <code>yolov5x.pt</code> is the largest and most accurate model available. Other options are <code>yolov5s.pt</code>, <code>yolov5m.pt</code> and <code>yolov5l.pt</code>, or you own checkpoint from training a custom dataset <code>./weights/best.pt</code>. For details on all available models please see our README table.</p> <pre><code>python val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n</code></pre> <p>Output:</p> <pre><code>val: data=/content/yolov5/data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\nYOLOv5 \ud83d\ude80 v6.0-224-g4c40933 torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\nFusing layers...\nModel Summary: 444 layers, 86705005 parameters, 0 gradients\nval: Scanning '/content/datasets/coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00&lt;?, ?it/s]\nClass     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:12&lt;00:00,  2.16it/s]\nall       5000      36335      0.732      0.628      0.683      0.496\nSpeed: 0.1ms pre-process, 5.2ms inference, 1.7ms NMS per image at shape (32, 3, 640, 640)  # &lt;--- base speed\nEvaluating pycocotools mAP... saving runs/val/exp2/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507  # &lt;--- base mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.731\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829\nResults saved to runs/val/exp\n</code></pre>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/#test-yolov5x-on-coco-030-sparsity","title":"Test YOLOv5x on COCO (0.30 sparsity)","text":"<p>We repeat the above test with a pruned model by using the <code>torch_utils.prune()</code> command. We update <code>val.py</code> to prune YOLOv5x to 0.3 sparsity:</p> <p></p> <p>30% pruned output:</p> <pre><code>val: data=/content/yolov5/data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\nYOLOv5 \ud83d\ude80 v6.0-224-g4c40933 torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\nFusing layers...\nModel Summary: 444 layers, 86705005 parameters, 0 gradients\nPruning model...  0.3 global sparsity\nval: Scanning '/content/datasets/coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00&lt;?, ?it/s]\nClass     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:11&lt;00:00,  2.19it/s]\nall       5000      36335      0.724      0.614      0.671      0.478\nSpeed: 0.1ms pre-process, 5.2ms inference, 1.7ms NMS per image at shape (32, 3, 640, 640)  # &lt;--- prune mAP\nEvaluating pycocotools mAP... saving runs/val/exp3/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489  # &lt;--- prune mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.677\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.537\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.334\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.542\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.635\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.370\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.612\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.496\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.803\nResults saved to runs/val/exp3\n</code></pre> <p>In the results we can observe that we have achieved a sparsity of 30% in our model after pruning, which means that 30% of the model's weight parameters in <code>nn.Conv2d</code> layers are equal to 0. Inference time is essentially unchanged, while the model's AP and AR scores a slightly reduced.</p>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/model_pruning_and_sparsity/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/","title":"Multi-GPU Training","text":"<p>\ud83d\udcda This guide explains how to properly use multiple GPUs to train a dataset with YOLOv5 \ud83d\ude80 on single or multiple machine(s). UPDATED 25 December 2022.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre> <p>\ud83d\udca1 ProTip! Docker Image is recommended for all Multi-GPU trainings. See Docker Quickstart Guide </p> <p>\ud83d\udca1 ProTip! <code>torch.distributed.run</code> replaces <code>torch.distributed.launch</code> in PyTorch&gt;=1.9. See docs for details.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#training","title":"Training","text":"<p>Select a pretrained model to start training from. Here we select YOLOv5s, the smallest and fastest model available. See our README table for a full comparison of all models. We will train this model with Multi-GPU on the COCO dataset.</p> <p></p>"},{"location":"yolov5/tutorials/multi_gpu_training/#single-gpu","title":"Single GPU","text":"<pre><code>python train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0\n</code></pre>"},{"location":"yolov5/tutorials/multi_gpu_training/#multi-gpu-dataparallel-mode-not-recommended","title":"Multi-GPU DataParallel Mode (\u26a0\ufe0f not recommended)","text":"<p>You can increase the <code>device</code> to use Multiple GPUs in DataParallel mode.</p> <pre><code>python train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\n</code></pre> <p>This method is slow and barely speeds up training compared to using just 1 GPU.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#multi-gpu-distributeddataparallel-mode-recommended","title":"Multi-GPU DistributedDataParallel Mode (\u2705 recommended)","text":"<p>You will have to pass <code>python -m torch.distributed.run --nproc_per_node</code>, followed by the usual arguments.</p> <pre><code>python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\n</code></pre> <p><code>--nproc_per_node</code> specifies how many GPUs you would like to use. In the example above, it is 2. <code>--batch</code> is the total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/2=32 per GPU.</p> <p>The code above will use GPUs <code>0... (N-1)</code>.</p> Use specific GPUs (click to expand) <p>You can do so by simply passing <code>--device</code> followed by your specific GPUs. For example, in the code below, we will use GPUs <code>2,3</code>.</p> <pre><code>python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 2,3\n</code></pre> Use SyncBatchNorm (click to expand) <p>SyncBatchNorm could increase accuracy for multiple gpu training, however, it will slow down training by a significant factor. It is only available for Multiple GPU DistributedDataParallel training.</p> <p>It is best used when the batch-size on each GPU is small (&lt;= 8).</p> <p>To use SyncBatchNorm, simple pass <code>--sync-bn</code> to the command like below,</p> <pre><code>python -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --sync-bn\n</code></pre> Use Multiple machines (click to expand) <p>This is only available for Multiple GPU DistributedDataParallel training.</p> <p>Before we continue, make sure the files on all machines are the same, dataset, codebase, etc. Afterwards, make sure the machines can communicate to each other.</p> <p>You will have to choose a master machine(the machine that the others will talk to). Note down its address(<code>master_addr</code>) and choose a port(<code>master_port</code>). I will use <code>master_addr = 192.168.1.1</code> and <code>master_port = 1234</code> for the example below.</p> <p>To use it, you can do as the following,</p> <pre><code># On master machine 0\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank 0 --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''\n</code></pre> <pre><code># On machine R\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank R --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights ''\n</code></pre> <p>where <code>G</code> is number of GPU per machine, <code>N</code> is the number of machines, and <code>R</code> is the machine number from <code>0...(N-1)</code>. Let's say I have two machines with two GPUs each, it would be <code>G = 2</code> , <code>N = 2</code>, and <code>R = 1</code> for the above.</p> <p>Training will not start until all  <code>N</code> machines are connected. Output will only be shown on master machine!</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#notes","title":"Notes","text":"<ul> <li>Windows support is untested, Linux is recommended.</li> <li><code>--batch</code> must be a multiple of the number of GPUs.</li> <li>GPU 0 will take slightly more memory than the other GPUs as it maintains EMA and is responsible for checkpointing etc.</li> <li>If you get <code>RuntimeError: Address already in use</code>, it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding <code>--master_port</code> like below,</li> </ul> <pre><code>python -m torch.distributed.run --master_port 1234 --nproc_per_node 2 ...\n</code></pre>"},{"location":"yolov5/tutorials/multi_gpu_training/#results","title":"Results","text":"<p>DDP profiling results on an AWS EC2 P4d instance with 8x A100 SXM4-40GB for YOLOv5l for 1 COCO epoch.</p> Profiling code <pre><code># prepare\nt=ultralytics/yolov5:latest &amp;&amp; sudo docker pull $t &amp;&amp; sudo docker run -it --ipc=host --gpus all -v \"$(pwd)\"/coco:/usr/src/coco $t\npip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\ncd .. &amp;&amp; rm -rf app &amp;&amp; git clone https://github.com/ultralytics/yolov5 -b master app &amp;&amp; cd app\ncp data/coco.yaml data/coco_profile.yaml\n\n# profile\npython train.py --batch-size 16 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch-size 32 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1\npython -m torch.distributed.run --nproc_per_node 4 train.py --batch-size 64 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3\npython -m torch.distributed.run --nproc_per_node 8 train.py --batch-size 128 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3,4,5,6,7\n</code></pre> GPUsA100 batch-size CUDA_mem<sup>device0 (G) COCO<sup>train COCO<sup>val 1x 16 26GB 20:39 0:55 2x 32 26GB 11:43 0:57 4x 64 26GB 5:57 0:55 8x 128 26GB 3:09 0:57"},{"location":"yolov5/tutorials/multi_gpu_training/#faq","title":"FAQ","text":"<p>If an error occurs, please read the checklist below first! (It could save your time)</p> Checklist (click to expand)  <ul> <li>Have you properly read this post?  </li> <li>Have you tried to reclone the codebase? The code changes daily.</li> <li>Have you tried to search for your error? Someone may have already encountered it in this repo or in another and have the solution. </li> <li>Have you installed all the requirements listed on top (including the correct Python and Pytorch versions)? </li> <li>Have you tried in other environments listed in the \"Environments\" section below? </li> <li>Have you tried with another dataset like coco128 or coco2017? It will make it easier to find the root cause. </li> </ul> <p>If you went through all the above, feel free to raise an Issue by giving as much detail as possible following the template.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/multi_gpu_training/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/multi_gpu_training/#credits","title":"Credits","text":"<p>I would like to thank @MagicFrogSJTU, who did all the heavy lifting, and @glenn-jocher for guiding us along the way.</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/","title":"Neural Magic's DeepSparse","text":"<p>Welcome to software-delivered AI.</p> <p>This guide explains how to deploy YOLOv5 with Neural Magic's DeepSparse.</p> <p>DeepSparse is an inference runtime with exceptional performance on CPUs. For instance, compared to the ONNX Runtime baseline, DeepSparse offers a 5.8x speed-up for YOLOv5s, running on the same machine!</p> <p> </p> <p>For the first time, your deep learning workloads can meet the performance demands of production without the complexity and costs of hardware accelerators. Put simply, DeepSparse gives you the performance of GPUs and the simplicity of software:</p> <ul> <li>Flexible Deployments: Run consistently across cloud, data center, and edge with any hardware provider from Intel to AMD to ARM</li> <li>Infinite Scalability: Scale vertically to 100s of cores, out with standard Kubernetes, or fully-abstracted with Serverless</li> <li>Easy Integration: Clean APIs for integrating your model into an application and monitoring it in production</li> </ul>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#how-does-deepsparse-achieve-gpu-class-performance","title":"How Does DeepSparse Achieve GPU-Class Performance?","text":"<p>DeepSparse takes advantage of model sparsity to gain its performance speedup.</p> <p>Sparsification through pruning and quantization is a broadly studied technique, allowing order-of-magnitude reductions in the size and compute needed to execute a network, while maintaining high accuracy. DeepSparse is sparsity-aware, meaning it skips the zeroed out parameters, shrinking amount of compute in a forward pass. Since the sparse computation is now memory bound, DeepSparse executes the network depth-wise, breaking the problem into Tensor Columns, vertical stripes of computation that fit in cache.</p> <p> </p> <p>Sparse networks with compressed computation, executed depth-wise in cache, allows DeepSparse to deliver GPU-class performance on CPUs!</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#how-do-i-create-a-sparse-version-of-yolov5-trained-on-my-data","title":"How Do I Create A Sparse Version of YOLOv5 Trained on My Data?","text":"<p>Neural Magic's open-source model repository, SparseZoo, contains pre-sparsified checkpoints of each YOLOv5 model. Using SparseML, which is integrated with Ultralytics, you can fine-tune a sparse checkpoint onto your data with a single CLI command.</p> <p>Checkout Neural Magic's YOLOv5 documentation for more details.</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#deepsparse-usage","title":"DeepSparse Usage","text":"<p>We will walk through an example benchmarking and deploying a sparse version of YOLOv5s with DeepSparse.</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#install-deepsparse","title":"Install DeepSparse","text":"<p>Run the following to install DeepSparse. We recommend you use a virtual environment with Python.</p> <pre><code>pip install \"deepsparse[server,yolo,onnxruntime]\"\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#collect-an-onnx-file","title":"Collect an ONNX File","text":"<p>DeepSparse accepts a model in the ONNX format, passed either as:</p> <ul> <li>A SparseZoo stub which identifies an ONNX file in the SparseZoo</li> <li>A local path to an ONNX model in a filesystem</li> </ul> <p>The examples below use the standard dense and pruned-quantized YOLOv5s checkpoints, identified by the following SparseZoo stubs:</p> <pre><code>zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\nzoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#deploy-a-model","title":"Deploy a Model","text":"<p>DeepSparse offers convenient APIs for integrating your model into an application.</p> <p>To try the deployment examples below, pull down a sample image and save it as <code>basilica.jpg</code> with the following:</p> <pre><code>wget -O basilica.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#python-api","title":"Python API","text":"<p><code>Pipelines</code> wrap pre-processing and output post-processing around the runtime, providing a clean interface for adding DeepSparse to an application. The DeepSparse-Ultralytics integration includes an out-of-the-box <code>Pipeline</code> that accepts raw images and outputs the bounding boxes.</p> <p>Create a <code>Pipeline</code> and run inference:</p> <pre><code>from deepsparse import Pipeline\n# list of images in local filesystem\nimages = [\"basilica.jpg\"]\n# create Pipeline\nmodel_stub = \"zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\"\nyolo_pipeline = Pipeline.create(\ntask=\"yolo\",\nmodel_path=model_stub,\n)\n# run inference on images, receive bounding boxes + classes\npipeline_outputs = yolo_pipeline(images=images, iou_thres=0.6, conf_thres=0.001)\nprint(pipeline_outputs)\n</code></pre> <p>If you are running in the cloud, you may get an error that open-cv cannot find <code>libGL.so.1</code>. Running the following on Ubuntu installs it:</p> <pre><code>apt-get install libgl1-mesa-glx\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#http-server","title":"HTTP Server","text":"<p>DeepSparse Server runs on top of the popular FastAPI web framework and Uvicorn web server. With just a single CLI command, you can easily setup a model service endpoint with DeepSparse. The Server supports any Pipeline from DeepSparse, including object detection with YOLOv5, enabling you to send raw images to the endpoint and receive the bounding boxes.</p> <p>Spin up the Server with the pruned-quantized YOLOv5s:</p> <pre><code>deepsparse.server \\\n--task yolo \\\n--model_path zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n</code></pre> <p>An example request, using Python's <code>requests</code> package:</p> <pre><code>import requests, json\n# list of images for inference (local files on client side)\npath = ['basilica.jpg']\nfiles = [('request', open(img, 'rb')) for img in path]\n# send request over HTTP to /predict/from_files endpoint\nurl = 'http://0.0.0.0:5543/predict/from_files'\nresp = requests.post(url=url, files=files)\n# response is returned in JSON\nannotations = json.loads(resp.text) # dictionary of annotation results\nbounding_boxes = annotations[\"boxes\"]\nlabels = annotations[\"labels\"]\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#annotate-cli","title":"Annotate CLI","text":"<p>You can also use the annotate command to have the engine save an annotated photo on disk. Try --source 0 to annotate your live webcam feed!</p> <pre><code>deepsparse.object_detection.annotate --model_filepath zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none --source basilica.jpg\n</code></pre> <p>Running the above command will create an <code>annotation-results</code> folder and save the annotated image inside.</p> <p> </p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#benchmarking-performance","title":"Benchmarking Performance","text":"<p>We will compare DeepSparse's throughput to ONNX Runtime's throughput on YOLOv5s, using DeepSparse's benchmarking script.</p> <p>The benchmarks were run on an AWS <code>c6i.8xlarge</code> instance (16 cores).</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#batch-32-performance-comparison","title":"Batch 32 Performance Comparison","text":""},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#onnx-runtime-baseline","title":"ONNX Runtime Baseline","text":"<p>At batch 32, ONNX Runtime achieves 42 images/sec with the standard dense YOLOv5s:</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 32 -nstreams 1 -e onnxruntime\n\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\n&gt; Batch Size: 32\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 41.9025\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#deepsparse-dense-performance","title":"DeepSparse Dense Performance","text":"<p>While DeepSparse offers its best performance with optimized sparse models, it also performs well with the standard dense YOLOv5s.</p> <p>At batch 32, DeepSparse achieves 70 images/sec with the standard dense YOLOv5s, a 1.7x performance improvement over ORT!</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 32 -nstreams 1\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\n&gt; Batch Size: 32\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 69.5546\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#deepsparse-sparse-performance","title":"DeepSparse Sparse Performance","text":"<p>When sparsity is applied to the model, DeepSparse's performance gains over ONNX Runtime is even stronger.</p> <p>At batch 32, DeepSparse achieves 241 images/sec with the pruned-quantized YOLOv5s, a 5.8x performance improvement over ORT!</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none -s sync -b 32 -nstreams 1\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n&gt; Batch Size: 32\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 241.2452\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#batch-1-performance-comparison","title":"Batch 1 Performance Comparison","text":"<p>DeepSparse is also able to gain a speed-up over ONNX Runtime for the latency-sensitive, batch 1 scenario.</p>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#onnx-runtime-baseline_1","title":"ONNX Runtime Baseline","text":"<p>At batch 1, ONNX Runtime achieves 48 images/sec with the standard, dense YOLOv5s.</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 1 -nstreams 1 -e onnxruntime\n\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\n&gt; Batch Size: 1\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 48.0921\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#deepsparse-sparse-performance_1","title":"DeepSparse Sparse Performance","text":"<p>At batch 1, DeepSparse achieves 135 items/sec with a pruned-quantized YOLOv5s, a 2.8x performance gain over ONNX Runtime!</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none -s sync -b 1 -nstreams 1\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\n&gt; Batch Size: 1\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 134.9468\n</code></pre> <p>Since <code>c6i.8xlarge</code> instances have VNNI instructions, DeepSparse's throughput can be pushed further if weights are pruned in blocks of 4.</p> <p>At batch 1, DeepSparse achieves 180 items/sec with a 4-block pruned-quantized YOLOv5s, a 3.7x performance gain over ONNX Runtime!</p> <pre><code>deepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni -s sync -b 1 -nstreams 1\n&gt; Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni\n&gt; Batch Size: 1\n&gt; Scenario: sync\n&gt; Throughput (items/sec): 179.7375\n</code></pre>"},{"location":"yolov5/tutorials/neural_magic_pruning_quantization/#get-started-with-deepsparse","title":"Get Started With DeepSparse","text":"<p>Research or Testing? DeepSparse Community is free for research and testing. Get started with our Documentation.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/","title":"PyTorch Hub","text":"<p>\ud83d\udcda This guide explains how to load YOLOv5 \ud83d\ude80 from PyTorch Hub at https://pytorch.org/hub/ultralytics_yolov5. UPDATED 26 March 2023.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#before-you-start","title":"Before You Start","text":"<p>Install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n</code></pre> <p>\ud83d\udca1 ProTip: Cloning https://github.com/ultralytics/yolov5 is not required \ud83d\ude03</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#load-yolov5-with-pytorch-hub","title":"Load YOLOv5 with PyTorch Hub","text":""},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#simple-example","title":"Simple Example","text":"<p>This example loads a pretrained YOLOv5s model from PyTorch Hub as <code>model</code> and passes an image for inference. <code>'yolov5s'</code> is the lightest and fastest YOLOv5 model. For details on all available models please see the README.</p> <pre><code>import torch\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n# Image\nim = 'https://ultralytics.com/images/zidane.jpg'\n# Inference\nresults = model(im)\nresults.pandas().xyxy[0]\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#detailed-example","title":"Detailed Example","text":"<p>This example shows batched inference with PIL and OpenCV image sources. <code>results</code> can be printed to console, saved to <code>runs/hub</code>, showed to screen on supported environments, and returned as tensors or pandas dataframes.</p> <pre><code>import cv2\nimport torch\nfrom PIL import Image\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n# Images\nfor f in 'zidane.jpg', 'bus.jpg':\ntorch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\nim1 = Image.open('zidane.jpg')  # PIL image\nim2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\n# Inference\nresults = model([im1, im2], size=640) # batch of images\n# Results\nresults.print()\nresults.save()  # or .show()\nresults.xyxy[0]  # im1 predictions (tensor)\nresults.pandas().xyxy[0]  # im1 predictions (pandas)\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n</code></pre> <p> </p> <p>For all inference options see YOLOv5 <code>AutoShape()</code> forward method.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#inference-settings","title":"Inference Settings","text":"<p>YOLOv5 models contain various inference attributes such as confidence threshold, IoU threshold, etc. which can be set by:</p> <pre><code>model.conf = 0.25  # NMS confidence threshold\niou = 0.45  # NMS IoU threshold\nagnostic = False  # NMS class-agnostic\nmulti_label = False  # NMS multiple labels per box\nclasses = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\nmax_det = 1000  # maximum number of detections per image\namp = False  # Automatic Mixed Precision (AMP) inference\nresults = model(im, size=320)  # custom inference size\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#device","title":"Device","text":"<p>Models can be transferred to any device after creation:</p> <pre><code>model.cpu()  # CPU\nmodel.cuda()  # GPU\nmodel.to(device)  # i.e. device=torch.device(0)\n</code></pre> <p>Models can also be created directly on any <code>device</code>:</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', device='cpu')  # load on CPU\n</code></pre> <p>\ud83d\udca1 ProTip: Input images are automatically transferred to the correct model device before inference.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#silence-outputs","title":"Silence Outputs","text":"<p>Models can be loaded silently with <code>_verbose=False</code>:</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', _verbose=False)  # load silently\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#input-channels","title":"Input Channels","text":"<p>To load a pretrained YOLOv5s model with 4 input channels rather than the default 3:</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4)\n</code></pre> <p>In this case the model will be composed of pretrained weights except for the very first input layer, which is no longer the same shape as the pretrained input layer. The input layer will remain initialized by random weights.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#number-of-classes","title":"Number of Classes","text":"<p>To load a pretrained YOLOv5s model with 10 output classes rather than the default 80:</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=10)\n</code></pre> <p>In this case the model will be composed of pretrained weights except for the output layers, which are no longer the same shape as the pretrained output layers. The output layers will remain initialized by random weights.</p>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#force-reload","title":"Force Reload","text":"<p>If you run into problems with the above steps, setting <code>force_reload=True</code> may help by discarding the existing cache and force a fresh download of the latest YOLOv5 version from PyTorch Hub.</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)  # force reload\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#screenshot-inference","title":"Screenshot Inference","text":"<p>To run inference on your desktop screen:</p> <pre><code>import torch\nfrom PIL import ImageGrab\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n# Image\nim = ImageGrab.grab()  # take a screenshot\n# Inference\nresults = model(im)\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#multi-gpu-inference","title":"Multi-GPU Inference","text":"<p>YOLOv5 models can be loaded to multiple GPUs in parallel with threaded inference:</p> <pre><code>import torch\nimport threading\ndef run(model, im):\nresults = model(im)\nresults.save()\n# Models\nmodel0 = torch.hub.load('ultralytics/yolov5', 'yolov5s', device=0)\nmodel1 = torch.hub.load('ultralytics/yolov5', 'yolov5s', device=1)\n# Inference\nthreading.Thread(target=run, args=[model0, 'https://ultralytics.com/images/zidane.jpg'], daemon=True).start()\nthreading.Thread(target=run, args=[model1, 'https://ultralytics.com/images/bus.jpg'], daemon=True).start()\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#training","title":"Training","text":"<p>To load a YOLOv5 model for training rather than inference, set <code>autoshape=False</code>. To load a model with randomly initialized weights (to train from scratch) use <code>pretrained=False</code>. You must provide your own training script in this case. Alternatively see our YOLOv5 Train Custom Data Tutorial for model training.</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)  # load pretrained\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False)  # load scratch\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#base64-results","title":"Base64 Results","text":"<p>For use with API services. See https://github.com/ultralytics/yolov5/pull/2291 and Flask REST API example for details.</p> <pre><code>results = model(im)  # inference\nresults.ims # array of original images (as np array) passed to model for inference\nresults.render()  # updates results.ims with boxes and labels\nfor im in results.ims:\nbuffered = BytesIO()\nim_base64 = Image.fromarray(im)\nim_base64.save(buffered, format=\"JPEG\")\nprint(base64.b64encode(buffered.getvalue()).decode('utf-8'))  # base64 encoded image with results\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#cropped-results","title":"Cropped Results","text":"<p>Results can be returned and saved as detection crops:</p> <pre><code>results = model(im)  # inference\ncrops = results.crop(save=True)  # cropped detections dictionary\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#pandas-results","title":"Pandas Results","text":"<p>Results can be returned as Pandas DataFrames:</p> <pre><code>results = model(im)  # inference\nresults.pandas().xyxy[0]  # Pandas DataFrame\n</code></pre> Pandas Output (click to expand) <pre><code>print(results.pandas().xyxy[0])\n#      xmin    ymin    xmax   ymax  confidence  class    name\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#sorted-results","title":"Sorted Results","text":"<p>Results can be sorted by column, i.e. to sort license plate digit detection left-to-right (x-axis):</p> <pre><code>results = model(im)  # inference\nresults.pandas().xyxy[0].sort_values('xmin')  # sorted left-right\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#box-cropped-results","title":"Box-Cropped Results","text":"<p>Results can be returned and saved as detection crops:</p> <pre><code>results = model(im)  # inference\ncrops = results.crop(save=True)  # cropped detections dictionary\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#json-results","title":"JSON Results","text":"<p>Results can be returned in JSON format once converted to <code>.pandas()</code> dataframes using the <code>.to_json()</code> method. The JSON format can be modified using the <code>orient</code> argument. See pandas <code>.to_json()</code> documentation for details.</p> <pre><code>results = model(ims)  # inference\nresults.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\n</code></pre> JSON Output (click to expand) <pre><code>[\n{\"xmin\":749.5,\"ymin\":43.5,\"xmax\":1148.0,\"ymax\":704.5,\"confidence\":0.8740234375,\"class\":0,\"name\":\"person\"},\n{\"xmin\":433.5,\"ymin\":433.5,\"xmax\":517.5,\"ymax\":714.5,\"confidence\":0.6879882812,\"class\":27,\"name\":\"tie\"},\n{\"xmin\":115.25,\"ymin\":195.75,\"xmax\":1096.0,\"ymax\":708.0,\"confidence\":0.6254882812,\"class\":0,\"name\":\"person\"},\n{\"xmin\":986.0,\"ymin\":304.0,\"xmax\":1028.0,\"ymax\":420.0,\"confidence\":0.2873535156,\"class\":27,\"name\":\"tie\"}\n]\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#custom-models","title":"Custom Models","text":"<p>This example loads a custom 20-class VOC-trained YOLOv5s model <code>'best.pt'</code> with PyTorch Hub.</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt')  # local model\nmodel = torch.hub.load('path/to/yolov5', 'custom', path='path/to/best.pt', source='local')  # local repo\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#tensorrt-onnx-and-openvino-models","title":"TensorRT, ONNX and OpenVINO Models","text":"<p>PyTorch Hub supports inference on most YOLOv5 export formats, including custom trained models. See TFLite, ONNX, CoreML, TensorRT Export tutorial for details on exporting models.</p> <p>\ud83d\udca1 ProTip: TensorRT may be up to 2-5X faster than PyTorch on GPU benchmarks \ud83d\udca1 ProTip: ONNX and OpenVINO may be up to 2-3X faster than PyTorch on CPU benchmarks</p> <pre><code>model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5s.pt')  # PyTorch\n'yolov5s.torchscript')  # TorchScript\n'yolov5s.onnx')  # ONNX\n'yolov5s_openvino_model/')  # OpenVINO\n'yolov5s.engine')  # TensorRT\n'yolov5s.mlmodel')  # CoreML (macOS-only)\n'yolov5s.tflite')  # TFLite\n'yolov5s_paddle_model/')  # PaddlePaddle\n</code></pre>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/pytorch_hub_model_loading/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/","title":"Roboflow Datasets","text":"<p>You can now use Roboflow to organize, label, prepare, version, and host your datasets for training YOLOv5 \ud83d\ude80 models. Roboflow is free to use with YOLOv5 if you make your workspace public. UPDATED 7 June 2023.</p> <p>Warning</p> <p>Roboflow users can use Ultralytics under the AGPL license or procure an Enterprise license directly from Ultralytics. Be aware that Roboflow does not provide Ultralytics licenses, and it is the responsibility of the user to ensure appropriate licensing.</p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#upload","title":"Upload","text":"<p>You can upload your data to Roboflow via web UI, rest API, or python.</p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#labeling","title":"Labeling","text":"<p>After uploading data to Roboflow, you can label your data and review previous labels.</p> <p></p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#versioning","title":"Versioning","text":"<p>You can make versions of your dataset with different preprocessing and offline augmentation options. YOLOv5 does online augmentations natively, so be intentional when layering Roboflow's offline augs on top.</p> <p></p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#exporting-data","title":"Exporting Data","text":"<p>You can download your data in YOLOv5 format to quickly begin training.</p> <pre><code>from roboflow import Roboflow\nrf = Roboflow(api_key=\"YOUR API KEY HERE\")\nproject = rf.workspace().project(\"YOUR PROJECT\")\ndataset = project.version(\"YOUR VERSION\").download(\"yolov5\")\n</code></pre>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#custom-training","title":"Custom Training","text":"<p>We have released a custom training tutorial demonstrating all of the above capabilities. You can access the code here:</p> <p></p>"},{"location":"yolov5/tutorials/roboflow_datasets_integration/#active-learning","title":"Active Learning","text":"<p>The real world is messy and your model will invariably encounter situations your dataset didn't anticipate. Using active learning is an important strategy to iteratively improve your dataset and model. With the Roboflow and YOLOv5 integration, you can quickly make improvements on your model deployments by using a battle tested machine learning pipeline.</p> <p></p>"},{"location":"yolov5/tutorials/running_on_jetson_nano/","title":"Deploy on NVIDIA Jetson using TensorRT and DeepStream SDK","text":"<p>\ud83d\udcda This guide explains how to deploy a trained model into NVIDIA Jetson Platform and perform inference using TensorRT and DeepStream SDK. Here we use TensorRT to maximize the inference performance on the Jetson platform. UPDATED 18 November 2022.</p>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#hardware-verification","title":"Hardware Verification","text":"<p>We have tested and verified this guide on the following Jetson devices</p> <ul> <li>Seeed reComputer J1010 built with Jetson Nano module</li> <li>Seeed reComputer J2021 built with Jetson Xavier NX module</li> </ul>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#before-you-start","title":"Before You Start","text":"<p>Make sure you have properly installed JetPack SDK with all the SDK Components and DeepStream SDK on the Jetson device as this includes CUDA, TensorRT and DeepStream SDK which are needed for this guide.</p> <p>JetPack SDK provides a full development environment for hardware-accelerated AI-at-the-edge development. All Jetson modules and developer kits are supported by JetPack SDK.</p> <p>There are two major installation methods including,</p> <ol> <li>SD Card Image Method</li> <li>NVIDIA SDK Manager Method</li> </ol> <p>You can find a very detailed installation guide from NVIDIA official website. You can also find guides corresponding to the above-mentioned reComputer J1010 and reComputer J2021.</p>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#install-necessary-packages","title":"Install Necessary Packages","text":"<ul> <li>Step 1. Access the terminal of Jetson device, install pip and upgrade it</li> </ul> <pre><code>sudo apt update\nsudo apt install -y python3-pip\npip3 install --upgrade pip\n</code></pre> <ul> <li>Step 2. Clone the following repo</li> </ul> <pre><code>git clone https://github.com/ultralytics/yolov5\n</code></pre> <ul> <li>Step 3. Open requirements.txt</li> </ul> <pre><code>cd yolov5\nvi requirements.txt\n</code></pre> <ul> <li>Step 5. Edit the following lines. Here you need to press i first to enter editing mode. Press ESC, then type :wq to save and quit</li> </ul> <pre><code># torch&gt;=1.8.0\n# torchvision&gt;=0.9.0\n</code></pre> <p>Note: torch and torchvision are excluded for now because they will be installed later.</p> <ul> <li>Step 6. install the below dependency</li> </ul> <pre><code>sudo apt install -y libfreetype6-dev\n</code></pre> <ul> <li>Step 7. Install the necessary packages</li> </ul> <pre><code>pip3 install -r requirements.txt\n</code></pre>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#install-pytorch-and-torchvision","title":"Install PyTorch and Torchvision","text":"<p>We cannot install PyTorch and Torchvision from pip because they are not compatible to run on Jetson platform which is based on ARM aarch64 architecture. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.</p> <p>Visit this page to access all the PyTorch and Torchvision links.</p> <p>Here are some of the versions supported by JetPack 4.6 and above.</p> <p>PyTorch v1.10.0</p> <p>Supported by JetPack 4.4 (L4T R32.4.3) / JetPack 4.4.1 (L4T R32.4.4) / JetPack 4.5 (L4T R32.5.0) / JetPack 4.5.1 (L4T R32.5.1) / JetPack 4.6 (L4T R32.6.1) with Python 3.6</p> <p>file_name: torch-1.10.0-cp36-cp36m-linux_aarch64.whl URL: https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl</p> <p>PyTorch v1.12.0</p> <p>Supported by JetPack 5.0 (L4T R34.1.0) / JetPack 5.0.1 (L4T R34.1.1) / JetPack 5.0.2 (L4T R35.1.0) with Python 3.8</p> <p>file_name: torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl URL: https://developer.download.nvidia.com/compute/redist/jp/v50/pytorch/torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl</p> <ul> <li>Step 1. Install torch according to your JetPack version in the following format</li> </ul> <pre><code>wget &lt;URL&gt; -O &lt;file_name&gt;\npip3 install &lt;file_name&gt;\n</code></pre> <p>For example, here we are running JP4.6.1, and therefore we choose PyTorch v1.10.0</p> <pre><code>cd ~\nsudo apt-get install -y libopenblas-base libopenmpi-dev\nwget https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl -O torch-1.10.0-cp36-cp36m-linux_aarch64.whl\npip3 install torch-1.10.0-cp36-cp36m-linux_aarch64.whl\n</code></pre> <ul> <li>Step 2. Install torchvision depending on the version of PyTorch that you have installed. For example, we chose PyTorch v1.10.0, which means, we need to choose Torchvision v0.11.1</li> </ul> <pre><code>sudo apt install -y libjpeg-dev zlib1g-dev\ngit clone --branch v0.11.1 https://github.com/pytorch/vision torchvision\ncd torchvision\nsudo python3 setup.py install\n</code></pre> <p>Here a list of the corresponding torchvision version that you need to install according to the PyTorch version:</p> <ul> <li>PyTorch v1.10 - torchvision v0.11.1</li> <li>PyTorch v1.12 - torchvision v0.13.0</li> </ul>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#deepstream-configuration-for-yolov5","title":"DeepStream Configuration for YOLOv5","text":"<ul> <li>Step 1. Clone the following repo</li> </ul> <pre><code>cd ~\ngit clone https://github.com/marcoslucianops/DeepStream-Yolo\n</code></pre> <ul> <li>Step 2. Copy gen_wts_yoloV5.py from DeepStream-Yolo/utils into yolov5 directory</li> </ul> <pre><code>cp DeepStream-Yolo/utils/gen_wts_yoloV5.py yolov5\n</code></pre> <ul> <li>Step 3. Inside the yolov5 repo, download pt file from YOLOv5 releases (example for YOLOv5s 6.1)</li> </ul> <pre><code>cd yolov5\nwget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt\n</code></pre> <ul> <li>Step 4. Generate the cfg and wts files</li> </ul> <pre><code>python3 gen_wts_yoloV5.py -w yolov5s.pt\n</code></pre> <p>Note: To change the inference size (default: 640)</p> <pre><code>-s SIZE\n--size SIZE\n-s HEIGHT WIDTH\n--size HEIGHT WIDTH\n\nExample for 1280:\n\n-s 1280\nor\n-s 1280 1280\n</code></pre> <ul> <li>Step 5. Copy the generated cfg and wts files into the DeepStream-Yolo folder</li> </ul> <pre><code>cp yolov5s.cfg ~/DeepStream-Yolo\ncp yolov5s.wts ~/DeepStream-Yolo\n</code></pre> <ul> <li>Step 6. Open the DeepStream-Yolo folder and compile the library</li> </ul> <pre><code>cd ~/DeepStream-Yolo\nCUDA_VER=11.4 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.1\nCUDA_VER=10.2 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.0.1 / 6.0\n</code></pre> <ul> <li>Step 7. Edit the config_infer_primary_yoloV5.txt file according to your model</li> </ul> <pre><code>[property]\n...\ncustom-network-config=yolov5s.cfg\nmodel-file=yolov5s.wts\n...\n</code></pre> <ul> <li>Step 8. Edit the deepstream_app_config file</li> </ul> <pre><code>...\n[primary-gie]\n...\nconfig-file=config_infer_primary_yoloV5.txt\n</code></pre> <ul> <li>Step 9. Change the video source in deepstream_app_config file. Here a default video file is loaded as you can see below</li> </ul> <pre><code>...\n[source0]\n...\nuri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4\n</code></pre>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#run-the-inference","title":"Run the Inference","text":"<pre><code>deepstream-app -c deepstream_app_config.txt\n</code></pre> <p>The above result is running on Jetson Xavier NX with FP32 and YOLOv5s 640x640. We can see that the FPS is around 30.</p>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#int8-calibration","title":"INT8 Calibration","text":"<p>If you want to use INT8 precision for inference, you need to follow the steps below</p> <ul> <li>Step 1. Install OpenCV</li> </ul> <pre><code>sudo apt-get install libopencv-dev\n</code></pre> <ul> <li>Step 2. Compile/recompile the nvdsinfer_custom_impl_Yolo library with OpenCV support</li> </ul> <pre><code>cd ~/DeepStream-Yolo\nCUDA_VER=11.4 OPENCV=1 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.1\nCUDA_VER=10.2 OPENCV=1 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.0.1 / 6.0\n</code></pre> <ul> <li> <p>Step 3. For COCO dataset, download the val2017, extract, and move to DeepStream-Yolo folder</p> </li> <li> <p>Step 4. Make a new directory for calibration images</p> </li> </ul> <pre><code>mkdir calibration\n</code></pre> <ul> <li>Step 5. Run the following to select 1000 random images from COCO dataset to run calibration</li> </ul> <pre><code>for jpg in $(ls -1 val2017/*.jpg | sort -R | head -1000); do \\\ncp ${jpg} calibration/; \\\ndone\n</code></pre> <p>Note: NVIDIA recommends at least 500 images to get a good accuracy. On this example, 1000 images are chosen to get better accuracy (more images = more accuracy). Higher INT8_CALIB_BATCH_SIZE values will result in more accuracy and faster calibration speed. Set it according to you GPU memory. You can set it from head -1000. For example, for 2000 images, head -2000. This process can take a long time.</p> <ul> <li>Step 6. Create the calibration.txt file with all selected images</li> </ul> <pre><code>realpath calibration/*jpg &gt; calibration.txt\n</code></pre> <ul> <li>Step 7. Set environment variables</li> </ul> <pre><code>export INT8_CALIB_IMG_PATH=calibration.txt\nexport INT8_CALIB_BATCH_SIZE=1\n</code></pre> <ul> <li>Step 8. Update the config_infer_primary_yoloV5.txt file</li> </ul> <p>From</p> <pre><code>...\nmodel-engine-file=model_b1_gpu0_fp32.engine\n#int8-calib-file=calib.table\n...\nnetwork-mode=0\n...\n</code></pre> <p>To</p> <pre><code>...\nmodel-engine-file=model_b1_gpu0_int8.engine\nint8-calib-file=calib.table\n...\nnetwork-mode=1\n...\n</code></pre> <ul> <li>Step 9. Run the inference</li> </ul> <pre><code>deepstream-app -c deepstream_app_config.txt\n</code></pre> <p>The above result is running on Jetson Xavier NX with INT8 and YOLOv5s 640x640. We can see that the FPS is around 60.</p>"},{"location":"yolov5/tutorials/running_on_jetson_nano/#benchmark-results","title":"Benchmark results","text":"<p>The following table summarizes how different models perform on Jetson Xavier NX.</p> Model Name Precision Inference Size Inference Time (ms) FPS YOLOv5s FP32 320x320 16.66 60 FP32 640x640 33.33 30 INT8 640x640 16.66 60 YOLOv5n FP32 640x640 16.66 60"},{"location":"yolov5/tutorials/running_on_jetson_nano/#additional","title":"Additional","text":"<p>This tutorial is written by our friends at seeed @lakshanthad and Elaine</p>"},{"location":"yolov5/tutorials/test_time_augmentation/","title":"Test-Time Augmentation (TTA)","text":"<p>\ud83d\udcda This guide explains how to use Test Time Augmentation (TTA) during testing and inference for improved mAP and Recall with YOLOv5 \ud83d\ude80. UPDATED 25 September 2022.</p>"},{"location":"yolov5/tutorials/test_time_augmentation/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/test_time_augmentation/#test-normally","title":"Test Normally","text":"<p>Before trying TTA we want to establish a baseline performance to compare to. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. <code>yolov5x.pt</code> is the largest and most accurate model available. Other options are <code>yolov5s.pt</code>, <code>yolov5m.pt</code> and <code>yolov5l.pt</code>, or you own checkpoint from training a custom dataset <code>./weights/best.pt</code>. For details on all available models please see our README table.</p> <pre><code>python val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\n</code></pre> <p>Output:</p> <pre><code>val: data=./data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nFusing layers...\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\n\nval: Scanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01&lt;00:00, 2846.03it/s]\nval: New cache created: ../datasets/coco/val2017.cache\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [02:30&lt;00:00,  1.05it/s]\nall       5000      36335      0.746      0.626       0.68       0.49\nSpeed: 0.1ms pre-process, 22.4ms inference, 1.4ms NMS per image at shape (32, 3, 640, 640)  # &lt;--- baseline speed\nEvaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504  # &lt;--- baseline mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681  # &lt;--- baseline mAR\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\n</code></pre>"},{"location":"yolov5/tutorials/test_time_augmentation/#test-with-tta","title":"Test with TTA","text":"<p>Append <code>--augment</code> to any existing <code>val.py</code> command to enable TTA, and increase the image size by about 30% for improved results. Note that inference with TTA enabled will typically take about 2-3X the time of normal inference as the images are being left-right flipped and processed at 3 different resolutions, with the outputs merged before NMS. Part of the speed decrease is simply due to larger image sizes (832 vs 640), while part is due to the actual TTA operations.</p> <pre><code>python val.py --weights yolov5x.pt --data coco.yaml --img 832 --augment --half\n</code></pre> <p>Output:</p> <pre><code>val: data=./data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=832, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nFusing layers...\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\nreturn torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\nval: Scanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01&lt;00:00, 2885.61it/s]\nval: New cache created: ../datasets/coco/val2017.cache\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [07:29&lt;00:00,  2.86s/it]\nall       5000      36335      0.718      0.656      0.695      0.503\nSpeed: 0.2ms pre-process, 80.6ms inference, 2.7ms NMS per image at shape (32, 3, 832, 832)  # &lt;--- TTA speed\nEvaluating pycocotools mAP... saving runs/val/exp2/yolov5x_predictions.json...\n...\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.516  # &lt;--- TTA mAP\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.562\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.564\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.656\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.388\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.640\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.696  # &lt;--- TTA mAR\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.553\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.744\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.833\n</code></pre>"},{"location":"yolov5/tutorials/test_time_augmentation/#inference-with-tta","title":"Inference with TTA","text":"<p><code>detect.py</code> TTA inference operates identically to <code>val.py</code> TTA: simply append <code>--augment</code> to any existing <code>detect.py</code> command:</p> <pre><code>python detect.py --weights yolov5s.pt --img 832 --source data/images --augment\n</code></pre> <p>Output:</p> <pre><code>detect: weights=['yolov5s.pt'], source=data/images, imgsz=832, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=True, update=False, project=runs/detect, name=exp, exist_ok=False, line_width=3, hide_labels=False, hide_conf=False, half=False\nYOLOv5 \ud83d\ude80 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n100% 14.1M/14.1M [00:00&lt;00:00, 81.9MB/s]\nFusing layers...\nModel Summary: 224 layers, 7266973 parameters, 0 gradients\nimage 1/2 /content/yolov5/data/images/bus.jpg: 832x640 4 persons, 1 bus, 1 fire hydrant, Done. (0.029s)\nimage 2/2 /content/yolov5/data/images/zidane.jpg: 480x832 3 persons, 3 ties, Done. (0.024s)\nResults saved to runs/detect/exp\nDone. (0.156s)\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/test_time_augmentation/#pytorch-hub-tta","title":"PyTorch Hub TTA","text":"<p>TTA is automatically integrated into all YOLOv5 PyTorch Hub models, and can be accessed by passing <code>augment=True</code> at inference time.</p> <pre><code>import torch\n# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5x, custom\n# Images\nimg = 'https://ultralytics.com/images/zidane.jpg'  # or file, PIL, OpenCV, numpy, multiple\n# Inference\nresults = model(img, augment=True)  # &lt;--- TTA inference\n# Results\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n</code></pre>"},{"location":"yolov5/tutorials/test_time_augmentation/#customize","title":"Customize","text":"<p>You can customize the TTA ops applied in the YOLOv5 <code>forward_augment()</code> method here.</p>"},{"location":"yolov5/tutorials/test_time_augmentation/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/test_time_augmentation/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/tips_for_best_training_results/","title":"Tips for Best Training Results","text":"<p>\ud83d\udcda This guide explains how to produce the best mAP and training results with YOLOv5 \ud83d\ude80. UPDATED 25 May 2022.</p> <p>Most of the time good results can be obtained with no changes to the models or training settings, provided your dataset is sufficiently large and well labelled. If at first you don't get good results, there are steps you might be able to take to improve, but we always recommend users first train with all default settings before considering any changes. This helps establish a performance baseline and spot areas for improvement.</p> <p>If you have questions about your training results we recommend you provide the maximum amount of information possible if you expect a helpful response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics images such as labels.png. All of these are located in your <code>project/name</code> directory, typically <code>yolov5/runs/train/exp</code>.</p> <p>We've put together a full guide for users looking to get the best results on their YOLOv5 trainings below.</p>"},{"location":"yolov5/tutorials/tips_for_best_training_results/#dataset","title":"Dataset","text":"<ul> <li>Images per class. \u2265 1500 images per class recommended</li> <li>Instances per class. \u2265 10000 instances (labeled objects) per class recommended</li> <li>Image variety. Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc.</li> <li>Label consistency. All instances of all classes in all images must be labelled. Partial labelling will not work.</li> <li>Label accuracy. Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label.</li> <li>Label verification. View <code>train_batch*.jpg</code> on train start to verify your labels appear correct, i.e. see example mosaic.</li> <li>Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). No labels are required for background images.</li> </ul>"},{"location":"yolov5/tutorials/tips_for_best_training_results/#model-selection","title":"Model Selection","text":"<p>Larger models like YOLOv5x and YOLOv5x6 will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For mobile deployments we recommend YOLOv5s/m, for cloud deployments we recommend YOLOv5l/x. See our README table for a full comparison of all models.</p> <p></p> <ul> <li>Start from Pretrained weights. Recommended for small to medium-sized datasets (i.e. VOC, VisDrone, GlobalWheat). Pass the name of the model to the <code>--weights</code> argument. Models download automatically from the latest YOLOv5 release.</li> </ul> <pre><code>python train.py --data custom.yaml --weights yolov5s.pt\n                                             yolov5m.pt\n                                             yolov5l.pt\n                                             yolov5x.pt\n                                             custom_pretrained.pt\n</code></pre> <ul> <li>Start from Scratch. Recommended for large datasets (i.e. COCO, Objects365, OIv6). Pass the model architecture YAML you are interested in, along with an empty <code>--weights ''</code> argument:</li> </ul> <pre><code>python train.py --data custom.yaml --weights '' --cfg yolov5s.yaml\n                                                      yolov5m.yaml\n                                                      yolov5l.yaml\n                                                      yolov5x.yaml\n</code></pre>"},{"location":"yolov5/tutorials/tips_for_best_training_results/#training-settings","title":"Training Settings","text":"<p>Before modifying anything, first train with default settings to establish a performance baseline. A full list of train.py settings can be found in the train.py argparser.</p> <ul> <li>Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs.</li> <li>Image size. COCO trains at native resolution of <code>--img 640</code>, though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as <code>--img 1280</code>. If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same <code>--img</code> as the training was run at, i.e. if you train at <code>--img 1280</code> you should also test and detect at <code>--img 1280</code>.</li> <li>Batch size. Use the largest <code>--batch-size</code> that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided.</li> <li>Hyperparameters. Default hyperparameters are in hyp.scratch-low.yaml. We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like <code>hyp['obj']</code> will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our Hyperparameter Evolution Tutorial.</li> </ul>"},{"location":"yolov5/tutorials/tips_for_best_training_results/#further-reading","title":"Further Reading","text":"<p>If you'd like to know more, a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across all ML domains: http://karpathy.github.io/2019/04/25/recipe/</p> <p>Good luck \ud83c\udf40 and let us know if you have any other questions!</p>"},{"location":"yolov5/tutorials/train_custom_data/","title":"Train Custom Data","text":"<p>\ud83d\udcda This guide explains how to train your own custom dataset with YOLOv5 \ud83d\ude80. UPDATED 7 June 2023.</p>"},{"location":"yolov5/tutorials/train_custom_data/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/train_custom_data/#train-on-custom-data","title":"Train On Custom Data","text":"<p>Creating a custom model to detect your objects is an iterative process of collecting and organizing images, labeling your objects of interest, training a model, deploying it into the wild to make predictions, and then using that deployed model to collect examples of edge cases to repeat and improve.</p>"},{"location":"yolov5/tutorials/train_custom_data/#1-create-dataset","title":"1. Create Dataset","text":"<p>YOLOv5 models must be trained on labelled data in order to learn classes of objects in that data. There are two options for creating your dataset before you start training:</p> Use Roboflow to create your dataset in YOLO format \ud83c\udf1f <p>Note</p> <p>Roboflow users can use Ultralytics under the AGPL license or can request an Enterprise license directly from Ultralytics. Be aware that Roboflow does not provide Ultralytics licenses, and it is the responsibility of the user to ensure appropriate licensing.</p> Or manually prepare your dataset"},{"location":"yolov5/tutorials/train_custom_data/#11-collect-images","title":"1.1 Collect Images","text":"<p>Your model will learn by example. Training on images similar to the ones it will see in the wild is of the utmost importance. Ideally, you will collect a wide variety of images from the same configuration (camera, angle, lighting, etc.) as you will ultimately deploy your project.</p> <p>If this is not possible, you can start from a public dataset to train your initial model and then sample images from the wild during inference to improve your dataset and model iteratively.</p>"},{"location":"yolov5/tutorials/train_custom_data/#12-create-labels","title":"1.2 Create Labels","text":"<p>Once you have collected images, you will need to annotate the objects of interest to create a ground truth for your model to learn from.</p> <p></p> <p>Roboflow Annotate is a simple web-based tool for managing and labeling your images with your team and exporting them in YOLOv5's annotation format.</p>"},{"location":"yolov5/tutorials/train_custom_data/#13-prepare-dataset-for-yolov5","title":"1.3 Prepare Dataset for YOLOv5","text":"<p>Whether you label your images with Roboflow or not, you can use it to convert your dataset into YOLO format, create a YOLOv5 YAML configuration file, and host it for importing into your training script.</p> <p>Create a free Roboflow account and upload your dataset to a <code>Public</code> workspace, label any unannotated images, then generate and export a version of your dataset in <code>YOLOv5 Pytorch</code> format.</p> <p>Note: YOLOv5 does online augmentation during training, so we do not recommend applying any augmentation steps in Roboflow for training with YOLOv5. But we recommend applying the following preprocessing steps:</p> <p></p> <ul> <li>Auto-Orient - to strip EXIF orientation from your images.</li> <li>Resize (Stretch) - to the square input size of your model (640x640 is the YOLOv5 default).</li> </ul> <p>Generating a version will give you a point in time snapshot of your dataset so you can always go back and compare your future model training runs against it, even if you add more images or change its configuration later.</p> <p></p> <p>Export in <code>YOLOv5 Pytorch</code> format, then copy the snippet into your training script or notebook to download your dataset.</p> <p></p> <p>Now continue with <code>2. Select a Model</code>.</p>"},{"location":"yolov5/tutorials/train_custom_data/#11-create-datasetyaml","title":"1.1 Create dataset.yaml","text":"<p>COCO128 is an example small tutorial dataset composed of the first 128 images in COCO train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.yaml, shown below, is the dataset config file that defines 1) the dataset root directory <code>path</code> and relative paths to <code>train</code> / <code>val</code> / <code>test</code> image directories (or *.txt files with image paths) and 2) a class <code>names</code> dictionary:</p> <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/coco128  # dataset root dir\ntrain: images/train2017  # train images (relative to 'path') 128 images\nval: images/train2017  # val images (relative to 'path') 128 images\ntest:  # test images (optional)\n# Classes (80 COCO classes)\nnames:\n0: person\n1: bicycle\n2: car\n...\n77: teddy bear\n78: hair drier\n79: toothbrush\n</code></pre>"},{"location":"yolov5/tutorials/train_custom_data/#12-create-labels_1","title":"1.2 Create Labels","text":"<p>After using an annotation tool to label your images, export your labels to YOLO format, with one <code>*.txt</code> file per image (if no objects in image, no <code>*.txt</code> file is required). The <code>*.txt</code> file specifications are:</p> <ul> <li>One row per object</li> <li>Each row is <code>class x_center y_center width height</code> format.</li> <li>Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide <code>x_center</code> and <code>width</code> by image width, and <code>y_center</code> and <code>height</code> by image height.</li> <li>Class numbers are zero-indexed (start from 0).</li> </ul> <p></p> <p>The label file corresponding to the above image contains 2 persons (class <code>0</code>) and a tie (class <code>27</code>):</p> <p></p>"},{"location":"yolov5/tutorials/train_custom_data/#13-organize-directories","title":"1.3 Organize Directories","text":"<p>Organize your train and val images and labels according to the example below. YOLOv5 assumes  <code>/coco128</code> is inside a <code>/datasets</code> directory next to the <code>/yolov5</code> directory. YOLOv5 locates labels automatically for each image by replacing the last instance of <code>/images/</code> in each image path with <code>/labels/</code>. For example:</p> <pre><code>../datasets/coco128/images/im0.jpg  # image\n../datasets/coco128/labels/im0.txt  # label\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/train_custom_data/#2-select-a-model","title":"2. Select a Model","text":"<p>Select a pretrained model to start training from. Here we select YOLOv5s, the second-smallest and fastest model available. See our README table for a full comparison of all models.</p> <p></p>"},{"location":"yolov5/tutorials/train_custom_data/#3-train","title":"3. Train","text":"<p>Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained <code>--weights yolov5s.pt</code> (recommended), or randomly initialized <code>--weights '' --cfg yolov5s.yaml</code> (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release.</p> <pre><code>python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt\n</code></pre> <p>Tip</p> <p>\ud83d\udca1 Add <code>--cache ram</code> or <code>--cache disk</code> to speed up training (requires significant RAM/disk resources).</p> <p>Tip</p> <p>\ud83d\udca1 Always train from a local dataset. Mounted or network drives like Google Drive will be very slow.</p> <p>All training results are saved to <code>runs/train/</code> with incrementing run directories, i.e. <code>runs/train/exp2</code>, <code>runs/train/exp3</code> etc. For more details see the Training section of our tutorial notebook.  </p>"},{"location":"yolov5/tutorials/train_custom_data/#4-visualize","title":"4. Visualize","text":""},{"location":"yolov5/tutorials/train_custom_data/#comet-logging-and-visualization-new","title":"Comet Logging and Visualization \ud83c\udf1f NEW","text":"<p>Comet is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with Comet Custom Panels! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!</p> <p>Getting started is easy:</p> <pre><code>pip install comet_ml  # 1. install\nexport COMET_API_KEY=&lt;Your API Key&gt;  # 2. paste API key\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n</code></pre> <p>To learn more about all the supported Comet features for this integration, check out the Comet Tutorial. If you'd like to learn more about Comet, head over to our documentation. Get started by trying out the Comet Colab Notebook: </p> <p></p>"},{"location":"yolov5/tutorials/train_custom_data/#clearml-logging-and-automation-new","title":"ClearML Logging and Automation \ud83c\udf1f NEW","text":"<p>ClearML is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML:</p> <ul> <li><code>pip install clearml</code></li> <li>run <code>clearml-init</code> to connect to a ClearML server (deploy your own open-source server here, or use our free hosted server here)</li> </ul> <p>You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).</p> <p>You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the ClearML Tutorial for details!</p> <p> </p>"},{"location":"yolov5/tutorials/train_custom_data/#local-logging","title":"Local Logging","text":"<p>Training results are automatically logged with Tensorboard and CSV loggers to <code>runs/train</code>, with a new experiment directory created for each new training as <code>runs/train/exp2</code>, <code>runs/train/exp3</code>, etc.</p> <p>This directory contains train and val statistics, mosaics, labels, predictions and augmented mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices.</p> <p></p> <p>Results file <code>results.csv</code> is updated after each epoch, and then plotted as <code>results.png</code> (below) after training completes. You can also plot any <code>results.csv</code> file manually:</p> <pre><code>from utils.plots import plot_results\nplot_results('path/to/results.csv')  # plot 'results.csv' as 'results.png'\n</code></pre> <p></p>"},{"location":"yolov5/tutorials/train_custom_data/#next-steps","title":"Next Steps","text":"<p>Once your model is trained you can use your best checkpoint <code>best.pt</code> to:</p> <ul> <li>Run CLI or Python inference on new images and videos</li> <li>Validate accuracy on train, val and test splits</li> <li>Export to TensorFlow, Keras, ONNX, TFlite, TF.js, CoreML and TensorRT formats</li> <li>Evolve hyperparameters to improve performance</li> <li>Improve your model by sampling real-world images and adding them to your dataset</li> </ul>"},{"location":"yolov5/tutorials/train_custom_data/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/train_custom_data/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/","title":"Transfer learning with frozen layers","text":"<p>\ud83d\udcda This guide explains how to freeze YOLOv5 \ud83d\ude80 layers when transfer learning. Transfer learning is a useful way to quickly retrain a model on new data without having to retrain the entire network. Instead, part of the initial weights are frozen in place, and the rest of the weights are used to compute loss and are updated by the optimizer. This requires less resources than normal training and allows for faster training times, though it may also result in reductions to final trained accuracy. UPDATED 25 September 2022.</p>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#before-you-start","title":"Before You Start","text":"<p>Clone repo and install requirements.txt in a Python&gt;=3.8.0 environment, including PyTorch&gt;=1.8. Models and datasets download automatically from the latest YOLOv5 release.</p> <pre><code>git clone https://github.com/ultralytics/yolov5  # clone\ncd yolov5\npip install -r requirements.txt  # install\n</code></pre>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#freeze-backbone","title":"Freeze Backbone","text":"<p>All layers that match the train.py <code>freeze</code> list in train.py will be frozen by setting their gradients to zero before training starts.</p> <pre><code> # Freeze\nfreeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\nfor k, v in model.named_parameters():\nv.requires_grad = True  # train all layers\nif any(x in k for x in freeze):\nprint(f'freezing {k}')\nv.requires_grad = False\n</code></pre> <p>To see a list of module names:</p> <pre><code>for k, v in model.named_parameters():\nprint(k)\n# Output\nmodel.0.conv.conv.weight\nmodel.0.conv.bn.weight\nmodel.0.conv.bn.bias\nmodel.1.conv.weight\nmodel.1.bn.weight\nmodel.1.bn.bias\nmodel.2.cv1.conv.weight\nmodel.2.cv1.bn.weight\n...\nmodel.23.m.0.cv2.bn.weight\nmodel.23.m.0.cv2.bn.bias\nmodel.24.m.0.weight\nmodel.24.m.0.bias\nmodel.24.m.1.weight\nmodel.24.m.1.bias\nmodel.24.m.2.weight\nmodel.24.m.2.bias\n</code></pre> <p>Looking at the model architecture we can see that the model backbone is layers 0-9:</p> <pre><code># YOLOv5 backbone\nbackbone:\n# [from, number, module, args]\n[[-1, 1, Focus, [64, 3]],  # 0-P1/2\n[-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n[-1, 3, BottleneckCSP, [128]],\n[-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n[-1, 9, BottleneckCSP, [256]],\n[-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n[-1, 9, BottleneckCSP, [512]],\n[-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n[-1, 1, SPP, [1024, [5, 9, 13]]],\n[-1, 3, BottleneckCSP, [1024, False]],  # 9\n]\n# YOLOv5 head\nhead:\n[[-1, 1, Conv, [512, 1, 1]],\n[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n[[-1, 6], 1, Concat, [1]],  # cat backbone P4\n[-1, 3, BottleneckCSP, [512, False]],  # 13\n[-1, 1, Conv, [256, 1, 1]],\n[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n[[-1, 4], 1, Concat, [1]],  # cat backbone P3\n[-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n[-1, 1, Conv, [256, 3, 2]],\n[[-1, 14], 1, Concat, [1]],  # cat head P4\n[-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n[-1, 1, Conv, [512, 3, 2]],\n[[-1, 10], 1, Concat, [1]],  # cat head P5\n[-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n[[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n]\n</code></pre> <p>so we can define the freeze list to contain all modules with 'model.0.' - 'model.9.' in their names:</p> <pre><code>python train.py --freeze 10\n</code></pre>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#freeze-all-layers","title":"Freeze All Layers","text":"<p>To freeze the full model except for the final output convolution layers in Detect(), we set freeze list to contain all modules with 'model.0.' - 'model.23.' in their names:</p> <pre><code>python train.py --freeze 24\n</code></pre>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#results","title":"Results","text":"<p>We train YOLOv5m on VOC on both of the above scenarios, along with a default model (no freezing), starting from the official COCO pretrained <code>--weights yolov5m.pt</code>:</p> <pre><code>train.py --batch 48 --weights yolov5m.pt --data voc.yaml --epochs 50 --cache --img 512 --hyp hyp.finetune.yaml\n</code></pre>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#accuracy-comparison","title":"Accuracy Comparison","text":"<p>The results show that freezing speeds up training, but reduces final accuracy slightly.</p> <p></p> <p></p> <p></p>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#gpu-utilization-comparison","title":"GPU Utilization Comparison","text":"<p>Interestingly, the more modules are frozen the less GPU memory is required to train, and the lower GPU utilization. This indicates that larger models, or models trained at larger --image-size may benefit from freezing in order to train faster.</p> <p></p> <p></p>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#environments","title":"Environments","text":"<p>YOLOv5 is designed to be run in the following up-to-date verified environments (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled):</p> <ul> <li>Notebooks with free GPU:  </li> <li>Google Cloud Deep Learning VM. See GCP Quickstart Guide</li> <li>Amazon Deep Learning AMI. See AWS Quickstart Guide</li> <li>Docker Image. See Docker Quickstart Guide </li> </ul>"},{"location":"yolov5/tutorials/transfer_learning_with_frozen_layers/#status","title":"Status","text":"<p>If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training, validation, inference, export and benchmarks on macOS, Windows, and Ubuntu every 24 hours and on every commit.</p>"}]}