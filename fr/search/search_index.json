{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Accueil","text":"<p>Pr\u00e9sentation d'Ultralytics YOLOv8, la derni\u00e8re version du mod\u00e8le r\u00e9put\u00e9 de d\u00e9tection d'objets en temps r\u00e9el et de segmentation d'images. YOLOv8 est construit sur des avanc\u00e9es de pointe en apprentissage profond et vision par ordinateur, offrant des performances in\u00e9gal\u00e9es en termes de vitesse et de pr\u00e9cision. Sa conception \u00e9pur\u00e9e le rend adapt\u00e9 \u00e0 diverses applications et facilement adaptable \u00e0 diff\u00e9rentes plateformes mat\u00e9rielles, des appareils de bord aux API cloud.</p> <p>Explorez les Docs YOLOv8, une ressource compl\u00e8te con\u00e7ue pour vous aider \u00e0 comprendre et \u00e0 utiliser ses fonctionnalit\u00e9s et capacit\u00e9s. Que vous soyez un praticien chevronn\u00e9 de l'apprentissage automatique ou nouveau dans le domaine, ce hub vise \u00e0 maximiser le potentiel de YOLOv8 dans vos projets.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"#par-ou-commencer","title":"Par o\u00f9 commencer","text":"<ul> <li>Installer <code>ultralytics</code> avec pip et d\u00e9marrer en quelques minutes \u00a0  Commencer</li> <li>Pr\u00e9dire de nouvelles images et vid\u00e9os avec YOLOv8 \u00a0  Pr\u00e9dire sur Images</li> <li>Entra\u00eener un nouveau mod\u00e8le YOLOv8 sur votre propre ensemble de donn\u00e9es customis\u00e9 \u00a0  Entra\u00eener un mod\u00e8le</li> <li>Explorer les t\u00e2ches YOLOv8 comme la segmentation, la classification, l'estimation de pose et le suivi \u00a0  Explorer les t\u00e2ches</li> </ul> <p> Regarder : Comment entra\u00eener un mod\u00e8le YOLOv8 sur votre ensemble de donn\u00e9es customis\u00e9 dans Google Colab. </p>"},{"location":"#yolo-un-bref-historique","title":"YOLO : Un bref historique","text":"<p>YOLO (You Only Look Once), un mod\u00e8le populaire de d\u00e9tection d'objets et de segmentation d'images, a \u00e9t\u00e9 d\u00e9velopp\u00e9 par Joseph Redmon et Ali Farhadi \u00e0 l'Universit\u00e9 de Washington. Lanc\u00e9 en 2015, YOLO a rapidement gagn\u00e9 en popularit\u00e9 pour sa vitesse et sa pr\u00e9cision \u00e9lev\u00e9es.</p> <ul> <li>YOLOv2, publi\u00e9 en 2016, a am\u00e9lior\u00e9 le mod\u00e8le original en int\u00e9grant la normalisation par lots, les bo\u00eetes d'ancrage et les clusters de dimensions.</li> <li>YOLOv3, lanc\u00e9 en 2018, a davantage am\u00e9lior\u00e9 la performance du mod\u00e8le en utilisant un r\u00e9seau dorsal plus efficace, des ancres multiples et un pool pyramidal spatial.</li> <li>YOLOv4 a \u00e9t\u00e9 publi\u00e9 en 2020, introduisant des innovations telles que l'augmentation de donn\u00e9es Mosaic, une nouvelle t\u00eate de d\u00e9tection sans ancre et une nouvelle fonction de perte.</li> <li>YOLOv5 a encore am\u00e9lior\u00e9 la performance du mod\u00e8le et a ajout\u00e9 des fonctionnalit\u00e9s nouvelles telles que l'optimisation des hyperparam\u00e8tres, le suivi int\u00e9gr\u00e9 des exp\u00e9riences et l'export automatique vers des formats d'exportation populaires.</li> <li>YOLOv6 a \u00e9t\u00e9 rendu open-source par Meituan en 2022 et est utilis\u00e9 dans de nombreux robots de livraison autonomes de l'entreprise.</li> <li>YOLOv7 a ajout\u00e9 des t\u00e2ches suppl\u00e9mentaires telles que l'estimation de pose sur le jeu de donn\u00e9es de points cl\u00e9s COCO.</li> <li>YOLOv8 est la derni\u00e8re version de YOLO par Ultralytics. En tant que mod\u00e8le de pointe et dernier cri (state-of-the-art, SOTA), YOLOv8 s'appuie sur le succ\u00e8s des versions pr\u00e9c\u00e9dentes, introduisant de nouvelles fonctionnalit\u00e9s et am\u00e9liorations pour des performances, une flexibilit\u00e9 et une efficacit\u00e9 renforc\u00e9es. YOLOv8 prend en charge une gamme compl\u00e8te de t\u00e2ches d'intelligence artificielle visuelle, y compris la d\u00e9tection, la segmentation, l'estimation de pose, le suivi et la classification. Cette polyvalence permet aux utilisateurs de tirer parti des capacit\u00e9s de YOLOv8 dans diverses applications et domaines.</li> </ul>"},{"location":"#licences-yolo-comment-est-licencie-ultralytics-yolo","title":"Licences YOLO : Comment est licenci\u00e9 Ultralytics YOLO ?","text":"<p>Ultralytics offre deux options de licence pour r\u00e9pondre aux diff\u00e9rents cas d'utilisation :</p> <ul> <li>Licence AGPL-3.0 : Cette licence open source approuv\u00e9e par OSI est id\u00e9ale pour les \u00e9tudiants et les passionn\u00e9s, favorisant la collaboration ouverte et le partage des connaissances. Voir le fichier LICENSE pour plus de d\u00e9tails.</li> <li>Licence Enterprise : Con\u00e7ue pour un usage commercial, cette licence permet l'int\u00e9gration transparente des logiciels et mod\u00e8les d'IA Ultralytics dans des biens et services commerciaux, en contournant les exigences open source de l'AGPL-3.0. Si votre sc\u00e9nario implique l'incorporation de nos solutions dans une offre commerciale, n'h\u00e9sitez pas \u00e0 contacter Ultralytics Licensing.</li> </ul> <p>Notre strat\u00e9gie de licence est con\u00e7ue pour garantir que toute am\u00e9lioration de nos projets open source soit restitu\u00e9e \u00e0 la communaut\u00e9. Nous tenons les principes de l'open source \u00e0 c\u0153ur \u2764\ufe0f, et notre mission est de garantir que nos contributions puissent \u00eatre utilis\u00e9es et d\u00e9velopp\u00e9es de mani\u00e8re b\u00e9n\u00e9fique pour tous.</p>"},{"location":"quickstart/","title":"D\u00e9marrage rapide","text":""},{"location":"quickstart/#installer-ultralytics","title":"Installer Ultralytics","text":"<p>Ultralytics propose diverses m\u00e9thodes d'installation, y compris pip, conda et Docker. Installez YOLOv8 via le package <code>ultralytics</code> avec pip pour obtenir la derni\u00e8re version stable ou en clonant le r\u00e9pertoire GitHub d'Ultralytics pour la version la plus r\u00e9cente. Docker peut \u00eatre utilis\u00e9 pour ex\u00e9cuter le package dans un conteneur isol\u00e9, \u00e9vitant l'installation locale.</p> <p>Installer</p> Installation avec Pip (recommand\u00e9)Installation avec CondaClone Git <p>Installez le package <code>ultralytics</code> en utilisant pip, ou mettez \u00e0 jour une installation existante en ex\u00e9cutant <code>pip install -U ultralytics</code>. Visitez l'Index des Packages Python (PyPI) pour plus de d\u00e9tails sur le package <code>ultralytics</code> : https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Installer le package ultralytics depuis PyPI\npip install ultralytics\n</code></pre> <p>Vous pouvez \u00e9galement installer le package <code>ultralytics</code> directement depuis le r\u00e9pertoire GitHub. Cela peut \u00eatre utile si vous voulez la version de d\u00e9veloppement la plus r\u00e9cente. Assurez-vous d'avoir l'outil en ligne de commande Git install\u00e9 sur votre syst\u00e8me. La commande <code>@main</code> installe la branche <code>main</code> et peut \u00eatre modifi\u00e9e pour une autre branche, p. ex. <code>@my-branch</code>, ou supprim\u00e9e enti\u00e8rement pour revenir par d\u00e9faut \u00e0 la branche <code>main</code>.</p> <pre><code># Installer le package ultralytics depuis GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda est un gestionnaire de packages alternatif \u00e0 pip qui peut \u00e9galement \u00eatre utilis\u00e9 pour l'installation. Visitez Anaconda pour plus de d\u00e9tails \u00e0 https://anaconda.org/conda-forge/ultralytics. Le r\u00e9pertoire feedstock d'Ultralytics pour la mise \u00e0 jour du package conda est sur https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Installer le package ultralytics en utilisant conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Note</p> <p>Si vous installez dans un environnement CUDA, la meilleure pratique est d'installer <code>ultralytics</code>, <code>pytorch</code> et <code>pytorch-cuda</code> dans la m\u00eame commande pour permettre au gestionnaire de package conda de r\u00e9soudre les conflits, ou bien d'installer <code>pytorch-cuda</code> en dernier pour lui permettre de remplacer le package <code>pytorch</code> sp\u00e9cifique aux CPU si n\u00e9cessaire. <pre><code># Installer tous les packages ensemble en utilisant conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clonez le r\u00e9pertoire <code>ultralytics</code> si vous \u00eates int\u00e9ress\u00e9 par la contribution au d\u00e9veloppement ou si vous souhaitez exp\u00e9rimenter avec le dernier code source. Apr\u00e8s le clonage, naviguez dans le r\u00e9pertoire et installez le package en mode \u00e9ditable <code>-e</code> en utilisant pip. <pre><code># Cloner le r\u00e9pertoire ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Naviguer vers le r\u00e9pertoire clon\u00e9\ncd ultralytics\n\n# Installer le package en mode \u00e9ditable pour le d\u00e9veloppement\npip install -e .\n</code></pre></p> <p>Voir le fichier requirements.txt d'<code>ultralytics</code> pour une liste des d\u00e9pendances. Notez que tous les exemples ci-dessus installent toutes les d\u00e9pendances requises.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Conseil</p> <p>Les pr\u00e9requis de PyTorch varient selon le syst\u00e8me d'exploitation et les exigences CUDA, donc il est recommand\u00e9 d'installer PyTorch en premier en suivant les instructions sur https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#image-docker-conda","title":"Image Docker Conda","text":"<p>Les images Docker Conda d'Ultralytics sont \u00e9galement disponibles sur DockerHub. Ces images sont bas\u00e9es sur Miniconda3 et constituent un moyen simple de commencer \u00e0 utiliser <code>ultralytics</code> dans un environnement Conda.</p> <pre><code># D\u00e9finir le nom de l'image comme variable\nt=ultralytics/ultralytics:latest-conda\n\n# T\u00e9l\u00e9charger la derni\u00e8re image ultralytics de Docker Hub\nsudo docker pull $t\n\n# Ex\u00e9cuter l'image ultralytics dans un conteneur avec support GPU\nsudo docker run -it --ipc=host --gpus all $t  # tous les GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # sp\u00e9cifier les GPUs\n</code></pre>"},{"location":"quickstart/#utiliser-ultralytics-avec-cli","title":"Utiliser Ultralytics avec CLI","text":"<p>L'interface en ligne de commande (CLI) d'Ultralytics permet l'utilisation de commandes simples en une seule ligne sans n\u00e9cessiter d'environnement Python. La CLI ne requiert pas de personnalisation ou de code Python. Vous pouvez simplement ex\u00e9cuter toutes les t\u00e2ches depuis le terminal avec la commande <code>yolo</code>. Consultez le Guide CLI pour en savoir plus sur l'utilisation de YOLOv8 depuis la ligne de commande.</p> <p>Exemple</p> SyntaxeEntra\u00eenementPr\u00e9dictionValidationExportationSp\u00e9cial <p>Les commandes <code>yolo</code> d'Ultralytics utilisent la syntaxe suivante : <pre><code>yolo T\u00c2CHE MODE ARGS\n\nO\u00f9   T\u00c2CHE (facultatif) est l'une de [detect, segment, classify]\n     MODE (obligatoire) est l'un de [train, val, predict, export, track]\n     ARGS (facultatif) sont n'importe quel nombre de paires personnalis\u00e9es 'arg=valeur' comme 'imgsz=320' qui remplacent les valeurs par d\u00e9faut.\n</code></pre> Voyez tous les ARGS dans le Guide de Configuration complet ou avec <code>yolo cfg</code></p> <p>Entra\u00eenez un mod\u00e8le de d\u00e9tection pour 10 epochs avec un learning_rate initial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Pr\u00e9disez une vid\u00e9o YouTube en utilisant un mod\u00e8le de segmentation pr\u00e9-entra\u00een\u00e9 \u00e0 une taille d'image de 320 : <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validez un mod\u00e8le de d\u00e9tection pr\u00e9-entra\u00een\u00e9 avec un batch-size de 1 et une taille d'image de 640 : <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportez un mod\u00e8le de classification YOLOv8n au format ONNX \u00e0 une taille d'image de 224 par 128 (pas de T\u00c2CHE requise) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Ex\u00e9cutez des commandes sp\u00e9ciales pour voir la version, afficher les param\u00e8tres, effectuer des v\u00e9rifications et plus encore : <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Avertissement</p> <p>Les arguments doivent \u00eatre pass\u00e9s sous forme de paires <code>arg=val</code>, s\u00e9par\u00e9s par un signe \u00e9gal <code>=</code> et d\u00e9limit\u00e9s par des espaces <code></code> entre les paires. N'utilisez pas de pr\u00e9fixes d'arguments <code>--</code> ou de virgules <code>,</code> entre les arguments.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Guide CLI</p>"},{"location":"quickstart/#utiliser-ultralytics-avec-python","title":"Utiliser Ultralytics avec Python","text":"<p>L'interface Python de YOLOv8 permet une int\u00e9gration transparente dans vos projets Python, facilitant le chargement, l'ex\u00e9cution et le traitement de la sortie du mod\u00e8le. Con\u00e7ue avec simplicit\u00e9 et facilit\u00e9 d'utilisation \u00e0 l'esprit, l'interface Python permet aux utilisateurs de mettre en \u0153uvre rapidement la d\u00e9tection d'objets, la segmentation et la classification dans leurs projets. Cela fait de l'interface Python de YOLOv8 un outil inestimable pour quiconque cherche \u00e0 int\u00e9grer ces fonctionnalit\u00e9s dans ses projets Python.</p> <p>Par exemple, les utilisateurs peuvent charger un mod\u00e8le, l'entra\u00eener, \u00e9valuer ses performances sur un set de validation, et m\u00eame l'exporter au format ONNX avec seulement quelques lignes de code. Consultez le Guide Python pour en savoir plus sur l'utilisation de YOLOv8 au sein de vos projets Python.</p> <p>Exemple</p> <pre><code>from ultralytics import YOLO\n\n# Cr\u00e9er un nouveau mod\u00e8le YOLO \u00e0 partir de z\u00e9ro\nmodel = YOLO('yolov8n.yaml')\n\n# Charger un mod\u00e8le YOLO pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.pt')\n\n# Entra\u00eener le mod\u00e8le en utilisant le jeu de donn\u00e9es 'coco128.yaml' pour 3 epochs\nr\u00e9sultats = model.train(data='coco128.yaml', epochs=3)\n\n# \u00c9valuer la performance du mod\u00e8le sur le set de validation\nr\u00e9sultats = model.val()\n\n# Effectuer la d\u00e9tection d'objets sur une image en utilisant le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')\n\n# Exporter le mod\u00e8le au format ONNX\nsucc\u00e8s = model.export(format='onnx')\n</code></pre> <p>Guide Python</p>"},{"location":"datasets/","title":"Aper\u00e7u des ensembles de donn\u00e9es","text":"<p>Ultralytics fournit un soutien pour divers ensembles de donn\u00e9es pour faciliter les t\u00e2ches de vision par ordinateur telles que la d\u00e9tection, la segmentation d'instance, l'estimation de la pose, la classification et le suivi multi-objets. Ci-dessous se trouve une liste des principaux ensembles de donn\u00e9es Ultralytics, suivie d'un r\u00e9sum\u00e9 de chaque t\u00e2che de vision par ordinateur et des ensembles de donn\u00e9es respectifs.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en cours de construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"datasets/#ensembles-de-donnees-de-detection","title":"Ensembles de donn\u00e9es de d\u00e9tection","text":"<p>La d\u00e9tection d'objets par bo\u00eete englobante est une technique de vision par ordinateur qui consiste \u00e0 d\u00e9tecter et localiser des objets dans une image en dessinant une bo\u00eete englobante autour de chaque objet.</p> <ul> <li>Argoverse : Un ensemble de donn\u00e9es contenant des donn\u00e9es de suivi 3D et de pr\u00e9vision de mouvement dans des environnements urbains avec des annotations d\u00e9taill\u00e9es.</li> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle con\u00e7u pour la d\u00e9tection d'objets, la segmentation et l'annotation avec plus de 200K images \u00e9tiquet\u00e9es.</li> <li>COCO8 : Contient les 4 premi\u00e8res images de COCO train et COCO val, adapt\u00e9es pour des tests rapides.</li> <li>Global Wheat 2020 : Un ensemble de donn\u00e9es d'images de t\u00eates de bl\u00e9 recueillies dans le monde entier pour les t\u00e2ches de d\u00e9tection et de localisation d'objets.</li> <li>Objects365 : Un ensemble de donn\u00e9es de grande qualit\u00e9 et \u00e0 grande \u00e9chelle pour la d\u00e9tection d'objets avec 365 cat\u00e9gories d'objets et plus de 600K images annot\u00e9es.</li> <li>OpenImagesV7 : Un ensemble de donn\u00e9es complet de Google avec 1.7M d'images d'entra\u00eenement et 42k images de validation.</li> <li>SKU-110K : Un ensemble de donn\u00e9es mettant en vedette la d\u00e9tection d'objets denses dans les environnements de vente au d\u00e9tail avec plus de 11K images et 1.7 million de bo\u00eetes englobantes.</li> <li>VisDrone : Un ensemble de donn\u00e9es contenant des donn\u00e9es de d\u00e9tection d'objets et de suivi multi-objets \u00e0 partir d'images captur\u00e9es par drone avec plus de 10K images et s\u00e9quences vid\u00e9o.</li> <li>VOC : L'ensemble de donn\u00e9es de classes d'objets visuels Pascal (VOC) pour la d\u00e9tection d'objets et la segmentation avec 20 classes d'objets et plus de 11K images.</li> <li>xView : Un ensemble de donn\u00e9es pour la d\u00e9tection d'objets dans l'imagerie a\u00e9rienne avec 60 cat\u00e9gories d'objets et plus d'un million d'objets annot\u00e9s.</li> </ul>"},{"location":"datasets/#ensembles-de-donnees-de-segmentation-dinstance","title":"Ensembles de donn\u00e9es de segmentation d'instance","text":"<p>La segmentation d'instance est une technique de vision par ordinateur qui consiste \u00e0 identifier et localiser des objets dans une image au niveau des pixels.</p> <ul> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle con\u00e7u pour la d\u00e9tection d'objets, la segmentation et les t\u00e2ches d'annotation avec plus de 200K images \u00e9tiquet\u00e9es.</li> <li>COCO8-seg : Un ensemble de donn\u00e9es plus petit pour les t\u00e2ches de segmentation d'instance, contenant un sous-ensemble de 8 images COCO avec des annotations de segmentation.</li> </ul>"},{"location":"datasets/#estimation-de-pose","title":"Estimation de pose","text":"<p>L'estimation de la pose est une technique utilis\u00e9e pour d\u00e9terminer la pose de l'objet par rapport \u00e0 la cam\u00e9ra ou au syst\u00e8me de coordonn\u00e9es mondial.</p> <ul> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle avec des annotations de poses humaines con\u00e7u pour les t\u00e2ches d'estimation de la pose.</li> <li>COCO8-pose : Un ensemble de donn\u00e9es plus petit pour les t\u00e2ches d'estimation de la pose, contenant un sous-ensemble de 8 images COCO avec des annotations de pose humaine.</li> <li>Tiger-pose : Un ensemble de donn\u00e9es compact compos\u00e9 de 263 images centr\u00e9es sur les tigres, annot\u00e9es avec 12 points par tigre pour les t\u00e2ches d'estimation de la pose.</li> </ul>"},{"location":"datasets/#classification","title":"Classification","text":"<p>La classification d'images est une t\u00e2che de vision par ordinateur qui implique de cat\u00e9goriser une image dans une ou plusieurs classes ou cat\u00e9gories pr\u00e9d\u00e9finies en fonction de son contenu visuel.</p> <ul> <li>Caltech 101 : Un ensemble de donn\u00e9es contenant des images de 101 cat\u00e9gories d'objets pour les t\u00e2ches de classification d'images.</li> <li>Caltech 256 : Une version \u00e9tendue de Caltech 101 avec 256 cat\u00e9gories d'objets et des images plus complexes.</li> <li>CIFAR-10 : Un ensemble de donn\u00e9es de 60K images couleur 32x32 r\u00e9parties en 10 classes, avec 6K images par classe.</li> <li>CIFAR-100 : Une version \u00e9tendue de CIFAR-10 avec 100 cat\u00e9gories d'objets et 600 images par classe.</li> <li>Fashion-MNIST : Un ensemble de donn\u00e9es compos\u00e9 de 70 000 images en niveaux de gris de 10 cat\u00e9gories de mode pour les t\u00e2ches de classification d'images.</li> <li>ImageNet : Un ensemble de donn\u00e9es \u00e0 grande \u00e9chelle pour la d\u00e9tection d'objets et la classification d'images avec plus de 14 millions d'images et 20 000 cat\u00e9gories.</li> <li>ImageNet-10 : Un sous-ensemble plus petit d'ImageNet avec 10 cat\u00e9gories pour des exp\u00e9riences et des tests plus rapides.</li> <li>Imagenette : Un sous-ensemble plus petit d'ImageNet qui contient 10 classes facilement distinctes pour un entra\u00eenement et des tests plus rapides.</li> <li>Imagewoof : Un sous-ensemble d'ImageNet plus difficile contenant 10 cat\u00e9gories de races de chiens pour les t\u00e2ches de classification d'images.</li> <li>MNIST : Un ensemble de donn\u00e9es de 70 000 images en niveaux de gris de chiffres manuscrits pour les t\u00e2ches de classification d'images.</li> </ul>"},{"location":"datasets/#boites-englobantes-orientees-obb","title":"Bo\u00eetes Englobantes Orient\u00e9es (OBB)","text":"<p>Les Bo\u00eetes Englobantes Orient\u00e9es (OBB) sont une m\u00e9thode en vision par ordinateur pour d\u00e9tecter des objets inclin\u00e9s dans les images en utilisant des bo\u00eetes englobantes rotatives, souvent appliqu\u00e9e \u00e0 l'imagerie a\u00e9rienne et satellite.</p> <ul> <li>DOTAv2 : Un ensemble de donn\u00e9es d'imagerie a\u00e9rienne populaire avec 1.7 million d'instances et 11 268 images.</li> </ul>"},{"location":"datasets/#suivi-multi-objets","title":"Suivi Multi-Objets","text":"<p>Le suivi multi-objets est une technique de vision par ordinateur qui consiste \u00e0 d\u00e9tecter et suivre plusieurs objets dans le temps dans une s\u00e9quence vid\u00e9o.</p> <ul> <li>Argoverse : Un ensemble de donn\u00e9es contenant des donn\u00e9es de suivi 3D et de pr\u00e9vision de mouvement dans des environnements urbains avec des annotations d\u00e9taill\u00e9es pour les t\u00e2ches de suivi multi-objets.</li> <li>VisDrone : Un ensemble de donn\u00e9es contenant des donn\u00e9es de d\u00e9tection d'objets et de suivi multi-objets \u00e0 partir d'images captur\u00e9es par drone avec plus de 10K images et s\u00e9quences vid\u00e9o.</li> </ul>"},{"location":"datasets/#contribuer-de-nouveaux-ensembles-de-donnees","title":"Contribuer de Nouveaux Ensembles de Donn\u00e9es","text":"<p>Contribuer un nouvel ensemble de donn\u00e9es implique plusieurs \u00e9tapes pour s'assurer qu'il s'aligne bien avec l'infrastructure existante. Voici les \u00e9tapes n\u00e9cessaires :</p>"},{"location":"datasets/#etapes-pour-contribuer-un-nouvel-ensemble-de-donnees","title":"\u00c9tapes pour Contribuer un Nouvel Ensemble de Donn\u00e9es","text":"<ol> <li> <p>Collecter des Images : Rassemblez les images qui appartiennent \u00e0 l'ensemble de donn\u00e9es. Celles-ci pourraient \u00eatre collect\u00e9es \u00e0 partir de diff\u00e9rentes sources, telles que des bases de donn\u00e9es publiques ou votre propre collection.</p> </li> <li> <p>Annoter des Images : Annotez ces images avec des bo\u00eetes englobantes, des segments ou des points cl\u00e9s, en fonction de la t\u00e2che.</p> </li> <li> <p>Exporter des Annotations : Convertissez ces annotations au format de fichier YOLO *.txt pris en charge par Ultralytics.</p> </li> <li> <p>Organiser l'Ensemble de Donn\u00e9es : Rangez votre ensemble de donn\u00e9es dans la bonne structure de dossiers. Vous devriez avoir des r\u00e9pertoires de niveau sup\u00e9rieur <code>train/</code> et <code>val/</code>, et \u00e0 l'int\u00e9rieur de chacun, un sous-r\u00e9pertoire <code>images/</code> et <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Cr\u00e9er un Fichier <code>data.yaml</code> : Dans le r\u00e9pertoire racine de votre ensemble de donn\u00e9es, cr\u00e9ez un fichier <code>data.yaml</code> qui d\u00e9crit l'ensemble de donn\u00e9es, les classes et les autres informations n\u00e9cessaires.</p> </li> <li> <p>Optimiser les Images (Optionnel) : Si vous souhaitez r\u00e9duire la taille de l'ensemble de donn\u00e9es pour un traitement plus efficace, vous pouvez optimiser les images en utilisant le code ci-dessous. Ceci n'est pas requis, mais recommand\u00e9 pour des tailles d'ensemble de donn\u00e9es plus petites et des vitesses de t\u00e9l\u00e9chargement plus rapides.</p> </li> <li> <p>Zipper l'Ensemble de Donn\u00e9es : Compressez le dossier complet de l'ensemble de donn\u00e9es dans un fichier zip.</p> </li> <li> <p>Documenter et PR : Cr\u00e9ez une page de documentation d\u00e9crivant votre ensemble de donn\u00e9es et comment il s'int\u00e8gre dans le cadre existant. Apr\u00e8s cela, soumettez une Pull Request (PR). R\u00e9f\u00e9rez-vous aux lignes directrices de contribution Ultralytics pour plus de d\u00e9tails sur la mani\u00e8re de soumettre une PR.</p> </li> </ol>"},{"location":"datasets/#exemple-de-code-pour-optimiser-et-zipper-un-ensemble-de-donnees","title":"Exemple de Code pour Optimiser et Zipper un Ensemble de Donn\u00e9es","text":"<p>Optimiser et Zipper un Ensemble de Donn\u00e9es</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# D\u00e9finir le r\u00e9pertoire de l'ensemble de donn\u00e9es\npath = Path('chemin/vers/ensemble-de-donn\u00e9es')\n\n# Optimiser les images dans l'ensemble de donn\u00e9es (optionnel)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Zipper l'ensemble de donn\u00e9es dans 'chemin/vers/ensemble-de-donn\u00e9es.zip'\nzip_directory(path)\n</code></pre> <p>En suivant ces \u00e9tapes, vous pouvez contribuer un nouvel ensemble de donn\u00e9es qui s'int\u00e8gre bien avec la structure existante d'Ultralytics.</p>"},{"location":"models/","title":"Mod\u00e8les pris en charge par Ultralytics","text":"<p>Bienvenue dans la documentation des mod\u00e8les d'Ultralytics ! Nous offrons un soutien pour une large gamme de mod\u00e8les, chacun \u00e9tant adapt\u00e9 \u00e0 des t\u00e2ches sp\u00e9cifiques comme la d\u00e9tection d'objets, la segmentation d'instance, la classification d'images, l'estimation de pose, et le suivi multi-objets. Si vous \u00eates int\u00e9ress\u00e9 \u00e0 contribuer avec votre architecture de mod\u00e8le \u00e0 Ultralytics, consultez notre Guide de Contribution.</p> <p>Remarque</p> <p>\ud83d\udea7 Notre documentation dans diff\u00e9rentes langues est actuellement en construction, et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"models/#modeles-en-vedette","title":"Mod\u00e8les en vedette","text":"<p>Voici quelques-uns des mod\u00e8les cl\u00e9s pris en charge :</p> <ol> <li>YOLOv3 : La troisi\u00e8me it\u00e9ration de la famille de mod\u00e8les YOLO, initialement par Joseph Redmon, connue pour ses capacit\u00e9s de d\u00e9tection d'objets en temps r\u00e9el efficaces.</li> <li>YOLOv4 : Une mise \u00e0 jour native darknet de YOLOv3, publi\u00e9e par Alexey Bochkovskiy en 2020.</li> <li>YOLOv5 : Une version am\u00e9lior\u00e9e de l'architecture YOLO par Ultralytics, offrant de meilleures performances et compromis de vitesse par rapport aux versions pr\u00e9c\u00e9dentes.</li> <li>YOLOv6 : Publi\u00e9 par Meituan en 2022, et utilis\u00e9 dans beaucoup de ses robots de livraison autonomes.</li> <li>YOLOv7 : Mod\u00e8les YOLO mis \u00e0 jour publi\u00e9s en 2022 par les auteurs de YOLOv4.</li> <li>YOLOv8 NOUVEAU \ud83d\ude80: La derni\u00e8re version de la famille YOLO, pr\u00e9sentant des capacit\u00e9s am\u00e9lior\u00e9es telles que la segmentation d'instance, l'estimation de pose/points cl\u00e9s et la classification.</li> <li>Segment Anything Model (SAM) : Le mod\u00e8le Segment Anything Model (SAM) de Meta.</li> <li>Mobile Segment Anything Model (MobileSAM) : MobileSAM pour applications mobiles, d\u00e9velopp\u00e9 par l'Universit\u00e9 de Kyung Hee.</li> <li>Fast Segment Anything Model (FastSAM) : FastSAM par le Image &amp; Video Analysis Group, Institute of Automation, Chinese Academy of Sciences.</li> <li>YOLO-NAS : Mod\u00e8les de Recherche d'Architecture Neuronale YOLO (NAS).</li> <li>Realtime Detection Transformers (RT-DETR) : Mod\u00e8les du Transformateur de D\u00e9tection en Temps R\u00e9el (RT-DETR) de PaddlePaddle de Baidu.</li> </ol> <p> Regardez : Ex\u00e9cutez les mod\u00e8les YOLO d'Ultralytics en seulement quelques lignes de code. </p>"},{"location":"models/#pour-commencer-exemples-dutilisation","title":"Pour Commencer : Exemples d'Utilisation","text":"<p>Cet exemple fournit des exemples simples d'entra\u00eenement et d'inf\u00e9rence YOLO. Pour une documentation compl\u00e8te sur ces modes et d'autres, consultez les pages de documentation Pr\u00e9dire, Entra\u00eener, Val et Exporter.</p> <p>Notez que l'exemple ci-dessous concerne les mod\u00e8les Detect YOLOv8 pour la d\u00e9tection d'objets. Pour des t\u00e2ches suppl\u00e9mentaires prises en charge, voir les documentations Segmenter, Classifier et Poser.</p> <p>Exemple</p> PythonCLI <p>Des mod\u00e8les pr\u00e9-entra\u00een\u00e9s PyTorch <code>*.pt</code> ainsi que des fichiers de configuration <code>*.yaml</code> peuvent \u00eatre pass\u00e9s aux classes <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> et <code>RTDETR()</code> pour cr\u00e9er une instance de mod\u00e8le en Python :</p> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO\nmodel = YOLO('yolov8n.pt')\n\n# Afficher les informations du mod\u00e8le (optionnel)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur le jeu de donn\u00e9es exemple COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ex\u00e9cuter l'inf\u00e9rence avec le mod\u00e8le YOLOv8n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO et l'entra\u00eener sur le jeu de donn\u00e9es exemple COCO8 pendant 100 \u00e9poques\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO et ex\u00e9cuter l'inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribution-de-nouveaux-modeles","title":"Contribution de Nouveaux Mod\u00e8les","text":"<p>Vous \u00eates int\u00e9ress\u00e9 \u00e0 contribuer votre mod\u00e8le \u00e0 Ultralytics ? G\u00e9nial ! Nous sommes toujours ouverts \u00e0 l'expansion de notre portefeuille de mod\u00e8les.</p> <ol> <li> <p>Forkez le R\u00e9f\u00e9rentiel : Commencez par forker le r\u00e9f\u00e9rentiel GitHub d'Ultralytics.</p> </li> <li> <p>Clonez Votre Fork : Clonez votre fork sur votre machine locale et cr\u00e9ez une nouvelle branche pour travailler dessus.</p> </li> <li> <p>Impl\u00e9mentez Votre Mod\u00e8le : Ajoutez votre mod\u00e8le en suivant les normes et directives de codage fournies dans notre Guide de Contribution.</p> </li> <li> <p>Testez Rigoureusement : Assurez-vous de tester votre mod\u00e8le de mani\u00e8re rigoureuse, \u00e0 la fois isol\u00e9ment et comme partie du pipeline.</p> </li> <li> <p>Cr\u00e9ez une Pull Request : Une fois que vous \u00eates satisfait de votre mod\u00e8le, cr\u00e9ez une pull request au r\u00e9pertoire principal pour examen.</p> </li> <li> <p>Revue de Code &amp; Fusion : Apr\u00e8s examen, si votre mod\u00e8le r\u00e9pond \u00e0 nos crit\u00e8res, il sera fusionn\u00e9 dans le r\u00e9pertoire principal.</p> </li> </ol> <p>Pour des \u00e9tapes d\u00e9taill\u00e9es, consultez notre Guide de Contribution.</p>"},{"location":"models/fast-sam/","title":"Fast Segment Anything Model (FastSAM)","text":"<p>Le Fast Segment Anything Model (FastSAM) est une solution bas\u00e9e sur les r\u00e9seaux de neurones \u00e0 convolution (CNN) en temps r\u00e9el pour la t\u00e2che Segment Anything. Cette t\u00e2che est con\u00e7ue pour segmenter n'importe quel objet dans une image en fonction de diff\u00e9rentes interactions utilisateur possibles. FastSAM r\u00e9duit consid\u00e9rablement les demandes computationnelles tout en maintenant des performances comp\u00e9titives, ce qui en fait un choix pratique pour diverses t\u00e2ches de vision.</p> <p></p>"},{"location":"models/fast-sam/#vue-densemble","title":"Vue d'ensemble","text":"<p>FastSAM est con\u00e7u pour rem\u00e9dier aux limitations du Segment Anything Model (SAM), un mod\u00e8le Transformer lourd n\u00e9cessitant des ressources computationnelles importantes. FastSAM d\u00e9coupe la t\u00e2che de segmentation en deux \u00e9tapes s\u00e9quentielles : la segmentation de toutes les instances et la s\u00e9lection guid\u00e9e par une invitation. La premi\u00e8re \u00e9tape utilise YOLOv8-seg pour produire les masques de segmentation de toutes les instances de l'image. Dans la deuxi\u00e8me \u00e9tape, il g\u00e9n\u00e8re la r\u00e9gion d'int\u00e9r\u00eat correspondant \u00e0 l'invitation.</p>"},{"location":"models/fast-sam/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<ol> <li> <p>Solution en temps r\u00e9el : En exploitant l'efficacit\u00e9 computationnelle des CNN, FastSAM fournit une solution en temps r\u00e9el pour la t\u00e2che Segment Anything, ce qui en fait une solution pr\u00e9cieuse pour les applications industrielles n\u00e9cessitant des r\u00e9sultats rapides.</p> </li> <li> <p>Efficacit\u00e9 et performances : FastSAM offre une r\u00e9duction significative des demandes computationnelles et des ressources sans compromettre la qualit\u00e9 des performances. Il atteint des performances comparables \u00e0 SAM, mais avec une r\u00e9duction drastique des ressources computationnelles, ce qui permet une application en temps r\u00e9el.</p> </li> <li> <p>Segmentation guid\u00e9e par une invitation : FastSAM peut segmenter n'importe quel objet dans une image, guid\u00e9 par diff\u00e9rentes invitations d'interaction utilisateur possibles, offrant ainsi flexibilit\u00e9 et adaptabilit\u00e9 dans diff\u00e9rents sc\u00e9narios.</p> </li> <li> <p>Bas\u00e9 sur YOLOv8-seg : FastSAM est bas\u00e9 sur YOLOv8-seg, un d\u00e9tecteur d'objets \u00e9quip\u00e9 d'une branche de segmentation d'instances. Cela lui permet de produire efficacement les masques de segmentation de toutes les instances dans une image.</p> </li> <li> <p>R\u00e9sultats concurrentiels sur les bancs d'essai : Dans la t\u00e2che de proposition d'objets sur MS COCO, FastSAM obtient des scores \u00e9lev\u00e9s \u00e0 une vitesse significativement plus rapide que SAM sur une seule NVIDIA RTX 3090, d\u00e9montrant ainsi son efficacit\u00e9 et sa capacit\u00e9.</p> </li> <li> <p>Applications pratiques : Cette approche propose une nouvelle solution pratique pour un grand nombre de t\u00e2ches de vision \u00e0 une vitesse tr\u00e8s \u00e9lev\u00e9e, des dizaines ou des centaines de fois plus rapide que les m\u00e9thodes actuelles.</p> </li> <li> <p>Faisabilit\u00e9 de la compression du mod\u00e8le : FastSAM d\u00e9montre la faisabilit\u00e9 d'une voie qui peut r\u00e9duire consid\u00e9rablement l'effort computationnel en introduisant une contrainte artificielle dans la structure, ouvrant ainsi de nouvelles possibilit\u00e9s pour l'architecture de mod\u00e8les de grande taille pour les t\u00e2ches de vision g\u00e9n\u00e9rales.</p> </li> </ol>"},{"location":"models/fast-sam/#modeles-disponibles-taches-prises-en-charge-et-modes-dexploitation","title":"Mod\u00e8les disponibles, t\u00e2ches prises en charge et modes d'exploitation","text":"<p>Ce tableau pr\u00e9sente les mod\u00e8les disponibles avec leurs poids pr\u00e9-entra\u00een\u00e9s sp\u00e9cifiques, les t\u00e2ches qu'ils prennent en charge et leur compatibilit\u00e9 avec diff\u00e9rents modes d'exploitation tels que Inf\u00e9rence, Validation, Entra\u00eenement et Exportation, indiqu\u00e9s par des emojis \u2705 pour les modes pris en charge et des emojis \u274c pour les modes non pris en charge.</p> Type de mod\u00e8le Poids pr\u00e9-entra\u00een\u00e9s T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Exportation FastSAM-s <code>FastSAM-s.pt</code> Segmentation d'instances \u2705 \u274c \u274c \u2705 FastSAM-x <code>FastSAM-x.pt</code> Segmentation d'instances \u2705 \u274c \u274c \u2705"},{"location":"models/fast-sam/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Les mod\u00e8les FastSAM sont faciles \u00e0 int\u00e9grer dans vos applications Python. Ultralytics propose une API Python conviviale et des commandes CLI pour simplifier le d\u00e9veloppement.</p>"},{"location":"models/fast-sam/#utilisation-de-la-prediction","title":"Utilisation de la pr\u00e9diction","text":"<p>Pour effectuer une d\u00e9tection d'objets sur une image, utilisez la m\u00e9thode <code>Predict</code> comme indiqu\u00e9 ci-dessous :</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import FastSAM\nfrom ultralytics.models.fastsam import FastSAMPrompt\n\n# D\u00e9finir une source d'inf\u00e9rence\nsource = 'chemin/vers/bus.jpg'\n\n# Cr\u00e9er un mod\u00e8le FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # ou FastSAM-x.pt\n\n# Effectuer une inf\u00e9rence sur une image\neverything_results = model(source, device='cpu', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n\n# Pr\u00e9parer un objet Processus Invitation\nprompt_process = FastSAMPrompt(source, everything_results, device='cpu')\n\n# Invitation Everything\nann = prompt_process.everything_prompt()\n\n# Bbox shape par d\u00e9faut [0,0,0,0] -&gt; [x1,y1,x2,y2]\nann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n\n# Invitation Text\nann = prompt_process.text_prompt(text='une photo d\\'un chien')\n\n# Invitation Point\n# points par d\u00e9faut [[0,0]] [[x1,y1],[x2,y2]]\n# point_label par d\u00e9faut [0] [1,0] 0:fond, 1:premier plan\nann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\nprompt_process.plot(annotations=ann, output='./')\n</code></pre> <pre><code># Charger un mod\u00e8le FastSAM et segmenter tout avec\nyolo segment predict model=FastSAM-s.pt source=chemin/vers/bus.jpg imgsz=640\n</code></pre> <p>Cet exemple d\u00e9montre la simplicit\u00e9 du chargement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 et de l'ex\u00e9cution d'une pr\u00e9diction sur une image.</p>"},{"location":"models/fast-sam/#utilisation-de-la-validation","title":"Utilisation de la validation","text":"<p>La validation du mod\u00e8le sur un ensemble de donn\u00e9es peut \u00eatre effectu\u00e9e de la mani\u00e8re suivante :</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import FastSAM\n\n# Cr\u00e9er un mod\u00e8le FastSAM\nmodel = FastSAM('FastSAM-s.pt')  # ou FastSAM-x.pt\n\n# Valider le mod\u00e8le\nresults = model.val(data='coco8-seg.yaml')\n</code></pre> <pre><code># Charger un mod\u00e8le FastSAM et le valider sur l'ensemble de donn\u00e9es d'exemple COCO8 avec une taille d'image de 640 pixels\nyolo segment val model=FastSAM-s.pt data=coco8.yaml imgsz=640\n</code></pre> <p>Veuillez noter que FastSAM ne prend en charge que la d\u00e9tection et la segmentation d'une seule classe d'objet. Cela signifie qu'il reconna\u00eetra et segmentera tous les objets comme \u00e9tant de la m\u00eame classe. Par cons\u00e9quent, lors de la pr\u00e9paration de l'ensemble de donn\u00e9es, vous devez convertir tous les identifiants de cat\u00e9gorie d'objet en 0.</p>"},{"location":"models/fast-sam/#utilisation-officielle-de-fastsam","title":"Utilisation officielle de FastSAM","text":"<p>FastSAM est \u00e9galement disponible directement \u00e0 partir du d\u00e9p\u00f4t https://github.com/CASIA-IVA-Lab/FastSAM. Voici un bref aper\u00e7u des \u00e9tapes typiques que vous pourriez suivre pour utiliser FastSAM :</p>"},{"location":"models/fast-sam/#installation","title":"Installation","text":"<ol> <li> <p>Clonez le d\u00e9p\u00f4t FastSAM :    <pre><code>git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n</code></pre></p> </li> <li> <p>Cr\u00e9ez et activez un environnement Conda avec Python 3.9 :    <pre><code>conda create -n FastSAM python=3.9\nconda activate FastSAM\n</code></pre></p> </li> <li> <p>Acc\u00e9dez au d\u00e9p\u00f4t clon\u00e9 et installez les packages requis :    <pre><code>cd FastSAM\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Installez le mod\u00e8le CLIP :    <pre><code>pip install git+https://github.com/openai/CLIP.git\n</code></pre></p> </li> </ol>"},{"location":"models/fast-sam/#exemple-dutilisation","title":"Exemple d'utilisation","text":"<ol> <li> <p>T\u00e9l\u00e9chargez un point de contr\u00f4le de mod\u00e8le.</p> </li> <li> <p>Utilisez FastSAM pour l'inf\u00e9rence. Exemples de commandes :</p> <ul> <li> <p>Segmentez tout dans une image :   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg\n</code></pre></p> </li> <li> <p>Segmentez des objets sp\u00e9cifiques \u00e0 l'aide de l'invitation de texte :   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --text_prompt \"le chien jaune\"\n</code></pre></p> </li> <li> <p>Segmentez des objets dans un rectangle englobant (fournir les coordonn\u00e9es du rectangle au format xywh) :   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt \"[570,200,230,400]\"\n</code></pre></p> </li> <li> <p>Segmentez des objets \u00e0 proximit\u00e9 de points sp\u00e9cifiques :   <pre><code>python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --point_prompt \"[[520,360],[620,300]]\" --point_label \"[1,0]\"\n</code></pre></p> </li> </ul> </li> </ol> <p>De plus, vous pouvez essayer FastSAM via une d\u00e9monstration Colab ou sur la d\u00e9monstration Web HuggingFace pour une exp\u00e9rience visuelle.</p>"},{"location":"models/fast-sam/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Nous tenons \u00e0 remercier les auteurs de FastSAM pour leurs contributions importantes dans le domaine de la segmentation d'instances en temps r\u00e9el :</p> BibTeX <pre><code>@misc{zhao2023fast,\n      title={Fast Segment Anything},\n      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},\n      year={2023},\n      eprint={2306.12156},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Le document original FastSAM peut \u00eatre consult\u00e9 sur arXiv. Les auteurs ont rendu leur travail accessible au public, et le code source peut \u00eatre consult\u00e9 sur GitHub. Nous appr\u00e9cions leurs efforts pour faire avancer le domaine et rendre leur travail accessible \u00e0 la communaut\u00e9 dans son ensemble.</p>"},{"location":"models/mobile-sam/","title":"MobileSAM (Mobile Segment Anything Model)","text":""},{"location":"models/mobile-sam/#segmenter-nimporte-quoi-sur-mobile-mobilesam","title":"Segmenter N'importe Quoi sur Mobile (MobileSAM)","text":"<p>Le document MobileSAM est maintenant disponible sur arXiv.</p> <p>Une d\u00e9monstration de MobileSAM ex\u00e9cut\u00e9e sur un processeur CPU est accessible via ce lien de d\u00e9monstration. Les performances sur un CPU Mac i5 prennent environ 3 secondes. Sur la d\u00e9mo de Hugging Face, l'interface ainsi que les CPU moins performants contribuent \u00e0 une r\u00e9ponse plus lente, mais cela continue de fonctionner efficacement.</p> <p>MobileSAM est impl\u00e9ment\u00e9 dans divers projets, notamment Grounding-SAM, AnyLabeling, et Segment Anything en 3D.</p> <p>MobileSAM est entra\u00een\u00e9 sur un seul GPU avec un ensemble de donn\u00e9es de 100 000 images (1% des images originales) en moins d'une journ\u00e9e. Le code de cet entra\u00eenement sera disponible \u00e0 l'avenir.</p>"},{"location":"models/mobile-sam/#modeles-disponibles-taches-prises-en-charge-et-modes-dutilisation","title":"Mod\u00e8les Disponibles, T\u00e2ches Prises en Charge et Modes d'Utilisation","text":"<p>Ce tableau pr\u00e9sente les mod\u00e8les disponibles avec leurs poids pr\u00e9-entra\u00een\u00e9s sp\u00e9cifiques, les t\u00e2ches qu'ils prennent en charge, et leur compatibilit\u00e9 avec les diff\u00e9rents modes d'utilisation tels que Inf\u00e9rence, Validation, Entra\u00eenement et Export, indiqu\u00e9s par les emojis \u2705 pour les modes pris en charge et \u274c pour les modes non pris en charge.</p> Type de Mod\u00e8le Poids Pr\u00e9-entra\u00een\u00e9s T\u00e2ches Prises en Charge Inf\u00e9rence Validation Entra\u00eenement Export MobileSAM <code>mobile_sam.pt</code> Segmentation d'Instances \u2705 \u274c \u274c \u2705"},{"location":"models/mobile-sam/#passage-de-sam-a-mobilesam","title":"Passage de SAM \u00e0 MobileSAM","text":"<p>\u00c9tant donn\u00e9 que MobileSAM conserve le m\u00eame pipeline que SAM d'origine, nous avons incorpor\u00e9 le pr\u00e9-traitement, le post-traitement et toutes les autres interfaces de l'original. Par cons\u00e9quent, ceux qui utilisent actuellement SAM d'origine peuvent passer \u00e0 MobileSAM avec un effort minimal.</p> <p>MobileSAM a des performances comparables \u00e0 celles de SAM d'origine et conserve le m\u00eame pipeline \u00e0 l'exception d'un changement dans l'encodeur d'image. Plus pr\u00e9cis\u00e9ment, nous rempla\u00e7ons l'encodeur d'image lourd original ViT-H (632M) par un encodeur Tiny-ViT plus petit (5M). Sur un seul GPU, MobileSAM fonctionne \u00e0 environ 12 ms par image : 8 ms sur l'encodeur d'image et 4 ms sur le d\u00e9codeur de masque.</p> <p>Le tableau suivant pr\u00e9sente une comparaison des encodeurs d'image bas\u00e9s sur ViT :</p> Encodeur d'Image SAM d'Origine MobileSAM Param\u00e8tres 611M 5M Vitesse 452 ms 8 ms <p>SAM d'origine et MobileSAM utilisent tous deux le m\u00eame d\u00e9codeur de masque bas\u00e9 sur une instruction :</p> D\u00e9codeur de Masque SAM d'Origine MobileSAM Param\u00e8tres 3.876M 3.876M Vitesse 4 ms 4 ms <p>Voici une comparaison du pipeline complet :</p> Pipeline Complet (Enc+Dec) SAM d'Origine MobileSAM Param\u00e8tres 615M 9.66M Vitesse 456 ms 12 ms <p>Les performances de MobileSAM et de SAM d'origine sont d\u00e9montr\u00e9es en utilisant \u00e0 la fois un point et une bo\u00eete comme instructions.</p> <p></p> <p></p> <p>Avec ses performances sup\u00e9rieures, MobileSAM est environ 5 fois plus petit et 7 fois plus rapide que FastSAM actuel. Plus de d\u00e9tails sont disponibles sur la page du projet MobileSAM.</p>"},{"location":"models/mobile-sam/#test-de-mobilesam-dans-ultralytics","title":"Test de MobileSAM dans Ultralytics","text":"<p>Tout comme SAM d'origine, nous proposons une m\u00e9thode de test simple dans Ultralytics, comprenant des modes pour les instructions Point et Bo\u00eete.</p>"},{"location":"models/mobile-sam/#telechargement-du-modele","title":"T\u00e9l\u00e9chargement du mod\u00e8le","text":"<p>Vous pouvez t\u00e9l\u00e9charger le mod\u00e8le ici.</p>"},{"location":"models/mobile-sam/#instruction-point","title":"Instruction Point","text":"<p>Exemple</p> Python <pre><code>from ultralytics import SAM\n\n# Chargement du mod\u00e8le\nmodel = SAM('mobile_sam.pt')\n\n# Pr\u00e9diction d'un segment \u00e0 partir d'une instruction Point\nmodel.predict('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre>"},{"location":"models/mobile-sam/#instruction-boite","title":"Instruction Bo\u00eete","text":"<p>Exemple</p> Python <pre><code>from ultralytics import SAM\n\n# Chargement du mod\u00e8le\nmodel = SAM('mobile_sam.pt')\n\n# Pr\u00e9diction d'un segment \u00e0 partir d'une instruction Bo\u00eete\nmodel.predict('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n</code></pre> <p>Nous avons mis en \u0153uvre <code>MobileSAM</code> et <code>SAM</code> en utilisant la m\u00eame API. Pour plus d'informations sur l'utilisation, veuillez consulter la page SAM.</p>"},{"location":"models/mobile-sam/#citations-et-remerciements","title":"Citations et Remerciements","text":"<p>Si vous trouvez MobileSAM utile dans vos travaux de recherche ou de d\u00e9veloppement, veuillez envisager de citer notre document :</p> BibTeX <p>```bibtex @article{mobile_sam,   title={Faster Segment Anything: Towards Lightweight SAM for Mobile Applications},   author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung Ho and Lee, Seungkyu and Hong, Choong Seon},   journal={arXiv preprint arXiv:2306.14289},   year={2023} }</p>"},{"location":"models/rtdetr/","title":"RT-DETR de Baidu : un d\u00e9tecteur d'objets en temps r\u00e9el bas\u00e9 sur les Vision Transformers","text":""},{"location":"models/rtdetr/#presentation","title":"Pr\u00e9sentation","text":"<p>Le Real-Time Detection Transformer (RT-DETR), d\u00e9velopp\u00e9 par Baidu, est un d\u00e9tecteur d'objets de pointe de bout en bout qui offre des performances en temps r\u00e9el tout en maintenant une grande pr\u00e9cision. Il exploite la puissance des Vision Transformers (ViT) pour traiter efficacement les caract\u00e9ristiques multiscalaires en dissociant l'interaction intra-\u00e9chelle et la fusion inter-\u00e9chelles. RT-DETR est hautement adaptable, permettant un ajustement flexible de la vitesse d'inf\u00e9rence en utilisant diff\u00e9rentes couches de d\u00e9codeur sans n\u00e9cessiter de nouvelle formation. Le mod\u00e8le est performant sur des infrastructures acc\u00e9l\u00e9r\u00e9es telles que CUDA avec TensorRT, surpassant de nombreux autres d\u00e9tecteurs d'objets en temps r\u00e9el.</p> <p> Vue d'ensemble du RT-DETR de Baidu. Le diagramme d'architecture du mod\u00e8le RT-DETR montre les trois derni\u00e8res \u00e9tapes du r\u00e9seau {S3, S4, S5} comme entr\u00e9e de l'encodeur. L'encodeur hybride efficace transforme les caract\u00e9ristiques multiscalaires en une s\u00e9quence de caract\u00e9ristiques d'image gr\u00e2ce \u00e0 l'interaction \u00e0 l'int\u00e9rieur de l'\u00e9chelle (AIFI - Adeptation of Intra-scale Feature Interaction) et au module de fusion inter-\u00e9chelles (CCFM - Cross-scale Context-aware Feature Fusion Module). La s\u00e9lection de requ\u00eates inform\u00e9e par IoU est utilis\u00e9e pour s\u00e9lectionner un nombre fixe de caract\u00e9ristiques d'image pour servir de requ\u00eates d'objets initiales pour le d\u00e9codeur. Enfin, le d\u00e9codeur avec des t\u00eates de pr\u00e9dictions auxiliaires optimise de mani\u00e8re it\u00e9rative les requ\u00eates d'objets pour g\u00e9n\u00e9rer des bo\u00eetes et des scores de confiance (source).</p>"},{"location":"models/rtdetr/#fonctionnalites-principales","title":"Fonctionnalit\u00e9s principales","text":"<ul> <li>Encodeur hybride efficace : RT-DETR de Baidu utilise un encodeur hybride efficace qui traite les caract\u00e9ristiques multiscalaires en dissociant l'interaction intra-\u00e9chelle et la fusion inter-\u00e9chelles. Cette conception unique bas\u00e9e sur les Vision Transformers r\u00e9duit les co\u00fbts de calcul et permet une d\u00e9tection d'objets en temps r\u00e9el.</li> <li>S\u00e9lection de requ\u00eates inform\u00e9e par IoU : RT-DETR de Baidu am\u00e9liore l'initialisation des requ\u00eates d'objets en utilisant une s\u00e9lection de requ\u00eates inform\u00e9e par IoU. Cela permet au mod\u00e8le de se concentrer sur les objets les plus pertinents de la sc\u00e8ne, am\u00e9liorant ainsi la pr\u00e9cision de la d\u00e9tection.</li> <li>Vitesse d'inf\u00e9rence adaptable : RT-DETR de Baidu prend en charge des ajustements flexibles de la vitesse d'inf\u00e9rence en utilisant diff\u00e9rentes couches de d\u00e9codeur sans n\u00e9cessiter de nouvelle formation. Cette adaptabilit\u00e9 facilite l'application pratique dans diff\u00e9rents sc\u00e9narios de d\u00e9tection d'objets en temps r\u00e9el.</li> </ul>"},{"location":"models/rtdetr/#modeles-pre-entraines","title":"Mod\u00e8les pr\u00e9-entra\u00een\u00e9s","text":"<p>L'API Python Ultralytics fournit des mod\u00e8les pr\u00e9-entra\u00een\u00e9s RT-DETR de PaddlePaddle avec diff\u00e9rentes \u00e9chelles :</p> <ul> <li>RT-DETR-L : 53,0 % de pr\u00e9cision moyenne (AP) sur COCO val2017, 114 images par seconde (FPS) sur GPU T4</li> <li>RT-DETR-X : 54,8 % de pr\u00e9cision moyenne (AP) sur COCO val2017, 74 images par seconde (FPS) sur GPU T4</li> </ul>"},{"location":"models/rtdetr/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Cet exemple pr\u00e9sente des exemples simples d'entra\u00eenement et d'inf\u00e9rence avec RT-DETRR. Pour une documentation compl\u00e8te sur ceux-ci et d'autres modes, consultez les pages de documentation Predict,  Train, Val et Export.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import RTDETR\n\n# Charger un mod\u00e8le RT-DETR-l pr\u00e9-entra\u00een\u00e9 sur COCO\nmodel = RTDETR('rtdetr-l.pt')\n\n# Afficher des informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Effectuer une inf\u00e9rence avec le mod\u00e8le RT-DETR-l sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <pre><code># Charger un mod\u00e8le RT-DETR-l pr\u00e9-entra\u00een\u00e9 sur COCO et l'entra\u00eener sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nyolo train model=rtdetr-l.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le RT-DETR-l pr\u00e9-entra\u00een\u00e9 sur COCO et effectuer une inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=rtdetr-l.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/rtdetr/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>Ce tableau pr\u00e9sente les types de mod\u00e8les, les poids pr\u00e9-entra\u00een\u00e9s sp\u00e9cifiques, les t\u00e2ches prises en charge par chaque mod\u00e8le et les diff\u00e9rents modes (Train, Val, Predict, Export) pris en charge, indiqu\u00e9s par des emojis \u2705.</p> Type de mod\u00e8le Poids pr\u00e9-entra\u00een\u00e9s T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Export RT-DETR Large <code>rtdetr-l.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 RT-DETR Extra-Large <code>rtdetr-x.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705"},{"location":"models/rtdetr/#citations-et-remerciements","title":"Citations et Remerciements","text":"<p>Si vous utilisez RT-DETR de Baidu dans votre travail de recherche ou de d\u00e9veloppement, veuillez citer l'article original :</p> BibTeX <pre><code>@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Wenyu Lv and Shangliang Xu and Yian Zhao and Guanzhong Wang and Jinman Wei and Cheng Cui and Yuning Du and Qingqing Dang and Yi Liu},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Nous tenons \u00e0 remercier Baidu et l'\u00e9quipe PaddlePaddle pour la cr\u00e9ation et la maintenance de cette pr\u00e9cieuse ressource pour la communaut\u00e9 de la vision par ordinateur. Leur contribution au domaine avec le d\u00e9veloppement du d\u00e9tecteur d'objets en temps r\u00e9el bas\u00e9 sur les Vision Transformers, RT-DETR, est grandement appr\u00e9ci\u00e9e.</p> <p>keywords: RT-DETR, Transformer, ViT, Vision Transformers, RT-DETR de Baidu, PaddlePaddle, Mod\u00e8les PaddlePaddle RT-DETR pr\u00e9-entra\u00een\u00e9s, utilisation de RT-DETR de Baidu, API Python Ultralytics, d\u00e9tection d'objets en temps r\u00e9el</p>"},{"location":"models/sam/","title":"Segment Anything Model (SAM)","text":"<p>Bienvenue \u00e0 la pointe de la segmentation d'image avec le mod\u00e8le Segment Anything, ou SAM. Ce mod\u00e8le r\u00e9volutionnaire a chang\u00e9 la donne en introduisant la segmentation d'image promptable avec des performances en temps r\u00e9el, \u00e9tablissant de nouvelles normes dans le domaine.</p>"},{"location":"models/sam/#introduction-a-sam-le-modele-segment-anything","title":"Introduction \u00e0 SAM : Le mod\u00e8le Segment Anything","text":"<p>Le mod\u00e8le Segment Anything, ou SAM, est un mod\u00e8le de segmentation d'image de pointe qui permet une segmentation promptable, offrant une polyvalence in\u00e9gal\u00e9e dans les t\u00e2ches d'analyse d'image. SAM forme le c\u0153ur de l'initiative Segment Anything, un projet innovant qui introduit un mod\u00e8le, une t\u00e2che et un jeu de donn\u00e9es novateurs pour la segmentation d'images.</p> <p>La conception avanc\u00e9e de SAM lui permet de s'adapter \u00e0 de nouvelles distributions et t\u00e2ches d'images sans connaissance pr\u00e9alable, une fonctionnalit\u00e9 connue sous le nom de transfert hors \u00e9chantillon. Entra\u00een\u00e9 sur le vaste ensemble de donn\u00e9es SA-1B, qui contient plus d'un milliard de masques r\u00e9partis sur 11 millions d'images soigneusement s\u00e9lectionn\u00e9es, SAM a affich\u00e9 des performances hors \u00e9chantillon impressionnantes, d\u00e9passant les r\u00e9sultats enti\u00e8rement supervis\u00e9s pr\u00e9c\u00e9dents dans de nombreux cas.</p> <p> Exemple d'images avec des masques superpos\u00e9s provenant de notre nouveau jeu de donn\u00e9es, SA-1B. SA-1B contient 11 millions d'images diverses, haute r\u00e9solution, autoris\u00e9es et prot\u00e9geant la vie priv\u00e9e, ainsi que 1,1 milliard de masques de segmentation de haute qualit\u00e9. Ces masques ont \u00e9t\u00e9 annot\u00e9s enti\u00e8rement automatiquement par SAM, et comme le confirment des \u00e9valuations humaines et de nombreux tests, leur qualit\u00e9 et leur diversit\u00e9 sont \u00e9lev\u00e9es. Les images sont regroup\u00e9es par nombre de masques par image pour la visualisation (il y a environ 100 masques par image en moyenne).</p>"},{"location":"models/sam/#caracteristiques-cles-du-modele-segment-anything-sam","title":"Caract\u00e9ristiques cl\u00e9s du mod\u00e8le Segment Anything (SAM)","text":"<ul> <li>T\u00e2che de segmentation promptable : SAM a \u00e9t\u00e9 con\u00e7u en gardant \u00e0 l'esprit une t\u00e2che de segmentation promptable, ce qui lui permet de g\u00e9n\u00e9rer des masques de segmentation valides \u00e0 partir de n'importe quelle indication donn\u00e9e, telle que des indices spatiaux ou des indices textuels identifiant un objet.</li> <li>Architecture avanc\u00e9e : Le mod\u00e8le Segment Anything utilise un puissant encodeur d'images, un encodeur de prompt et un d\u00e9codeur de masques l\u00e9ger. Cette architecture unique permet une invitation flexible, un calcul de masques en temps r\u00e9el et une prise en compte de l'ambigu\u00eft\u00e9 dans les t\u00e2ches de segmentation.</li> <li>Le jeu de donn\u00e9es SA-1B : Introduit par le projet Segment Anything, le jeu de donn\u00e9es SA-1B comprend plus d'un milliard de masques sur 11 millions d'images. En tant que plus grand jeu de donn\u00e9es de segmentation \u00e0 ce jour, il offre \u00e0 SAM une source de donn\u00e9es d'entra\u00eenement diversifi\u00e9e et \u00e0 grande \u00e9chelle.</li> <li>Performances hors \u00e9chantillon : SAM affiche des performances hors \u00e9chantillon exceptionnelles dans diverses t\u00e2ches de segmentation, ce qui en fait un outil pr\u00eat \u00e0 l'emploi pour des applications diverses n\u00e9cessitant un minimum d'ing\u00e9nierie de prompt.</li> </ul> <p>Pour une analyse approfondie du mod\u00e8le Segment Anything et du jeu de donn\u00e9es SA-1B, veuillez visiter le site web Segment Anything et consulter l'article de recherche Segment Anything.</p>"},{"location":"models/sam/#modeles-disponibles-taches-prises-en-charge-et-modes-dexploitation","title":"Mod\u00e8les disponibles, t\u00e2ches prises en charge et modes d'exploitation","text":"<p>Ce tableau pr\u00e9sente les mod\u00e8les disponibles avec leurs poids pr\u00e9-entra\u00een\u00e9s sp\u00e9cifiques, les t\u00e2ches qu'ils prennent en charge et leur compatibilit\u00e9 avec diff\u00e9rents modes d'exploitation tels que Inf\u00e9rence, Validation, Entra\u00eenement et Exportation, indiqu\u00e9s par des emojis \u2705 pour les modes pris en charge et des emojis \u274c pour les modes non pris en charge.</p> Type de mod\u00e8le Poids pr\u00e9-entra\u00een\u00e9s T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Exportation SAM de base <code>sam_b.pt</code> Segmentation d'instance \u2705 \u274c \u274c \u2705 SAM large <code>sam_l.pt</code> Segmentation d'instance \u2705 \u274c \u274c \u2705"},{"location":"models/sam/#comment-utiliser-sam-polyvalence-et-puissance-dans-la-segmentation-dimages","title":"Comment utiliser SAM : Polyvalence et puissance dans la segmentation d'images","text":"<p>Le mod\u00e8le Segment Anything peut \u00eatre utilis\u00e9 pour une multitude de t\u00e2ches secondaires qui vont au-del\u00e0 de ses donn\u00e9es d'entra\u00eenement. Cela comprend la d\u00e9tection des contours, la g\u00e9n\u00e9ration de propositions d'objets, la segmentation d'instances et la pr\u00e9diction pr\u00e9liminaire texte-\u00e0-masque. Gr\u00e2ce \u00e0 l'ing\u00e9nierie de prompts, SAM peut s'adapter rapidement \u00e0 de nouvelles t\u00e2ches et distributions de donn\u00e9es de mani\u00e8re sans apprentissage, ce qui en fait un outil polyvalent et puissant pour tous vos besoins en mati\u00e8re de segmentation d'images.</p>"},{"location":"models/sam/#exemple-de-prediction-sam","title":"Exemple de pr\u00e9diction SAM","text":"<p>Segmentation avec des prompts</p> <p>Segmenter l'image avec des prompts donn\u00e9s.</p> Python <pre><code>from ultralytics import SAM\n\n# Charger un mod\u00e8le\nmodel = SAM('sam_b.pt')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Ex\u00e9cuter l'inf\u00e9rence avec un prompt de zones de d\u00e9limitation\nmodel('ultralytics/assets/zidane.jpg', bboxes=[439, 437, 524, 709])\n\n# Ex\u00e9cuter l'inf\u00e9rence avec un prompt de points\nmodel('ultralytics/assets/zidane.jpg', points=[900, 370], labels=[1])\n</code></pre> <p>Segmenter tout</p> <p>Segmenter toute l'image.</p> PythonCLI <pre><code>from ultralytics import SAM\n\n# Charger un mod\u00e8le\nmodel = SAM('sam_b.pt')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Ex\u00e9cuter l'inf\u00e9rence\nmodel('path/to/image.jpg')\n</code></pre> <pre><code># Ex\u00e9cuter l'inf\u00e9rence avec un mod\u00e8le SAM\nyolo predict model=sam_b.pt source=path/to/image.jpg\n</code></pre> <ul> <li>La logique ici est de segmenter toute l'image si vous ne passez aucun prompt (bboxes/points/masks).</li> </ul> <p>Exemple SAMPredictor</p> <p>De cette mani\u00e8re, vous pouvez d\u00e9finir l'image une fois et ex\u00e9cuter l'inf\u00e9rence des prompts plusieurs fois sans ex\u00e9cuter l'encodeur d'image plusieurs fois.</p> Inf\u00e9rence avec des prompts <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Cr\u00e9er un SAMPredictor\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# D\u00e9finir l'image\npredictor.set_image(\"ultralytics/assets/zidane.jpg\")  # d\u00e9finir avec un fichier image\npredictor.set_image(cv2.imread(\"ultralytics/assets/zidane.jpg\"))  # d\u00e9finir avec np.ndarray\nresults = predictor(bboxes=[439, 437, 524, 709])\nresults = predictor(points=[900, 370], labels=[1])\n\n# R\u00e9initialiser l'image\npredictor.reset_image()\n</code></pre> <p>Segmenter toute l'image avec des arguments suppl\u00e9mentaires.</p> Segmenter tout <pre><code>from ultralytics.models.sam import Predictor as SAMPredictor\n\n# Cr\u00e9er un SAMPredictor\noverrides = dict(conf=0.25, task='segment', mode='predict', imgsz=1024, model=\"mobile_sam.pt\")\npredictor = SAMPredictor(overrides=overrides)\n\n# Segmenter avec des arguments suppl\u00e9mentaires\nresults = predictor(source=\"ultralytics/assets/zidane.jpg\", crop_n_layers=1, points_stride=64)\n</code></pre> <ul> <li>Plus d'arguments suppl\u00e9mentaires pour <code>Segmenter tout</code> voir la r\u00e9f\u00e9rence <code>Predictor/generate</code>.</li> </ul>"},{"location":"models/sam/#comparaison-de-sam-avec-yolov8","title":"Comparaison de SAM avec YOLOv8","text":"<p>Nous comparons ici le plus petit mod\u00e8le SAM de Meta, SAM-b, avec le plus petit mod\u00e8le de segmentation d'Ultralytics, YOLOv8n-seg :</p> Mod\u00e8le Taille Param\u00e8tres Vitesse (CPU) SAM-b - Meta's SAM-b 358 Mo 94,7 M 51096 ms/im MobileSAM 40,7 Mo 10,1 M 46122 ms/im FastSAM-s with YOLOv8 backbone 23,7 Mo 11,8 M 115 ms/im YOLOv8n-seg - Ultralytics YOLOv8n-seg 6,7 Mo (53,4 fois plus petit) 3,4 M (27,9 fois moins) 59 ms/im (866 fois plus rapide) <p>Cette comparaison montre les diff\u00e9rences d'ordre de grandeur dans les tailles et les vitesses des mod\u00e8les. Alors que SAM pr\u00e9sente des fonctionnalit\u00e9s uniques pour la segmentation automatique, il ne rivalise pas directement avec les mod\u00e8les de segmentation YOLOv8, qui sont plus petits, plus rapides et plus efficaces.</p> <p>Tests effectu\u00e9s sur un MacBook Apple M2 de 2023 avec 16 Go de RAM. Pour reproduire ce test :</p> <p>Exemple</p> Python <pre><code>from ultralytics import FastSAM, SAM, YOLO\n\n# Profiler SAM-b\nmod\u00e8le = SAM('sam_b.pt')\nmod\u00e8le.info()\nmod\u00e8le('ultralytics/assets')\n\n# Profiler MobileSAM\nmod\u00e8le = SAM('mobile_sam.pt')\nmod\u00e8le.info()\nmod\u00e8le('ultralytics/assets')\n\n# Profiler FastSAM-s\nmod\u00e8le = FastSAM('FastSAM-s.pt')\nmod\u00e8le.info()\nmod\u00e8le('ultralytics/assets')\n\n# Profiler YOLOv8n-seg\nmod\u00e8le = YOLO('yolov8n-seg.pt')\nmod\u00e8le.info()\nmod\u00e8le('ultralytics/assets')\n</code></pre>"},{"location":"models/sam/#annotation-automatique-un-moyen-rapide-dobtenir-des-jeux-de-donnees-de-segmentation","title":"Annotation automatique : Un moyen rapide d'obtenir des jeux de donn\u00e9es de segmentation","text":"<p>L'annotation automatique est une fonctionnalit\u00e9 cl\u00e9 de SAM, permettant aux utilisateurs de g\u00e9n\u00e9rer un jeu de donn\u00e9es de segmentation \u00e0 l'aide d'un mod\u00e8le de d\u00e9tection pr\u00e9-entra\u00een\u00e9. Cette fonctionnalit\u00e9 permet une annotation rapide et pr\u00e9cise d'un grand nombre d'images, en contournant la n\u00e9cessit\u00e9 d'une annotation manuelle chronophage.</p>"},{"location":"models/sam/#generez-votre-jeu-de-donnees-de-segmentation-a-laide-dun-modele-de-detection","title":"G\u00e9n\u00e9rez votre jeu de donn\u00e9es de segmentation \u00e0 l'aide d'un mod\u00e8le de d\u00e9tection","text":"<p>Pour annoter automatiquement votre jeu de donn\u00e9es avec le framework Ultralytics, utilisez la fonction <code>auto_annotate</code> comme indiqu\u00e9 ci-dessous :</p> <p>Exemple</p> Python <pre><code>from ultralytics.data.annotator import auto_annotate\n\nauto_annotate(data=\"path/to/images\", det_model=\"yolov8x.pt\", sam_model='sam_b.pt')\n</code></pre> Argument Type Description Default data str Chemin d'acc\u00e8s \u00e0 un dossier contenant les images \u00e0 annoter. det_model str, optionnel Mod\u00e8le de d\u00e9tection pr\u00e9-entra\u00een\u00e9 YOLO. Par d\u00e9faut, 'yolov8x.pt'. 'yolov8x.pt' sam_model str, optionnel Mod\u00e8le de segmentation pr\u00e9-entra\u00een\u00e9 SAM. Par d\u00e9faut, 'sam_b.pt'. 'sam_b.pt' device str, optionnel Appareil sur lequel ex\u00e9cuter les mod\u00e8les. Par d\u00e9faut, une cha\u00eene vide (CPU ou GPU, si disponible). output_dir str, None, optionnel R\u00e9pertoire pour enregistrer les r\u00e9sultats annot\u00e9s. Par d\u00e9faut, un dossier 'labels' dans le m\u00eame r\u00e9pertoire que 'data'. None <p>La fonction <code>auto_annotate</code> prend en compte le chemin de vos images, avec des arguments optionnels pour sp\u00e9cifier les mod\u00e8les de d\u00e9tection et de segmentation SAM pr\u00e9-entra\u00een\u00e9s, l'appareil sur lequel ex\u00e9cuter les mod\u00e8les et le r\u00e9pertoire de sortie pour enregistrer les r\u00e9sultats annot\u00e9s.</p> <p>L'annotation automatique avec des mod\u00e8les pr\u00e9-entra\u00een\u00e9s peut r\u00e9duire consid\u00e9rablement le temps et les efforts n\u00e9cessaires pour cr\u00e9er des jeux de donn\u00e9es de segmentation de haute qualit\u00e9. Cette fonctionnalit\u00e9 est particuli\u00e8rement b\u00e9n\u00e9fique pour les chercheurs et les d\u00e9veloppeurs travaillant avec de grandes collections d'images, car elle leur permet de se concentrer sur le d\u00e9veloppement et l'\u00e9valuation des mod\u00e8les plut\u00f4t que sur l'annotation manuelle.</p>"},{"location":"models/sam/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Si vous trouvez SAM utile dans vos travaux de recherche ou de d\u00e9veloppement, veuillez envisager de citer notre article :</p> BibTeX <pre><code>@misc{kirillov2023segment,\n      title={Segment Anything},\n      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Doll\u00e1r and Ross Girshick},\n      year={2023},\n      eprint={2304.02643},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Nous tenons \u00e0 exprimer notre gratitude \u00e0 Meta AI pour la cr\u00e9ation et la maintenance de cette ressource pr\u00e9cieuse pour la communaut\u00e9 de la vision par ordinateur.</p> <p>keywords: Segment Anything, Segment Anything Model, SAM, Meta SAM, segmentation d'image, segmentation promptable, performances hors \u00e9chantillon, jeu de donn\u00e9es SA-1B, architecture avanc\u00e9e, annotation automatique, Ultralytics, mod\u00e8les pr\u00e9-entra\u00een\u00e9s, SAM de base, SAM large, segmentation d'instance, vision par ordinateur, IA, intelligence artificielle, apprentissage automatique, annotation de donn\u00e9es, masques de segmentation, mod\u00e8le de d\u00e9tection, mod\u00e8le de d\u00e9tection YOLO, bibtex, Meta AI.</p>"},{"location":"models/yolo-nas/","title":"YOLO-NAS","text":""},{"location":"models/yolo-nas/#apercu","title":"Aper\u00e7u","text":"<p>D\u00e9velopp\u00e9 par Deci AI, YOLO-NAS est un mod\u00e8le de d\u00e9tection d'objets r\u00e9volutionnaire. Il est le fruit d'une technologie avanc\u00e9e de recherche architecturale neuronale, minutieusement con\u00e7u pour pallier les limitations des pr\u00e9c\u00e9dents mod\u00e8les YOLO. Avec des am\u00e9liorations significatives en mati\u00e8re de prise en charge de la quantification et de compromis entre pr\u00e9cision et latence, YOLO-NAS repr\u00e9sente une avanc\u00e9e majeure en mati\u00e8re de d\u00e9tection d'objets.</p> <p> Aper\u00e7u de YOLO-NAS. YOLO-NAS utilise des blocs adapt\u00e9s \u00e0 la quantification et une quantification s\u00e9lective pour des performances optimales. Le mod\u00e8le, une fois converti en version quantifi\u00e9e INT8, pr\u00e9sente une baisse de pr\u00e9cision minimale, ce qui constitue une am\u00e9lioration significative par rapport aux autres mod\u00e8les. Ces avanc\u00e9es aboutissent \u00e0 une architecture sup\u00e9rieure offrant des capacit\u00e9s de d\u00e9tection d'objets in\u00e9gal\u00e9es et des performances exceptionnelles.</p>"},{"location":"models/yolo-nas/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<ul> <li>Bloc de base compatible avec la quantification: YOLO-NAS introduit un nouveau bloc de base adapt\u00e9 \u00e0 la quantification, ce qui permet de pallier l'une des principales limitations des pr\u00e9c\u00e9dents mod\u00e8les YOLO.</li> <li>Entra\u00eenement sophistiqu\u00e9 et quantification: YOLO-NAS utilise des sch\u00e9mas d'entra\u00eenement avanc\u00e9s et une quantification apr\u00e8s l'entra\u00eenement pour am\u00e9liorer les performances.</li> <li>Optimisation AutoNAC et pr\u00e9-entra\u00eenement: YOLO-NAS utilise l'optimisation AutoNAC et est pr\u00e9-entra\u00een\u00e9 sur des ensembles de donn\u00e9es renomm\u00e9s tels que COCO, Objects365 et Roboflow 100. Ce pr\u00e9-entra\u00eenement le rend extr\u00eamement adapt\u00e9 aux t\u00e2ches de d\u00e9tection d'objets ult\u00e9rieures dans des environnements de production.</li> </ul>"},{"location":"models/yolo-nas/#modeles-pre-entraines","title":"Mod\u00e8les pr\u00e9-entra\u00een\u00e9s","text":"<p>D\u00e9couvrez la puissance de la d\u00e9tection d'objets de nouvelle g\u00e9n\u00e9ration avec les mod\u00e8les YOLO-NAS pr\u00e9-entra\u00een\u00e9s fournis par Ultralytics. Ces mod\u00e8les sont con\u00e7us pour offrir des performances exceptionnelles en termes de vitesse et de pr\u00e9cision. Choisissez parmi une vari\u00e9t\u00e9 d'options adapt\u00e9es \u00e0 vos besoins sp\u00e9cifiques :</p> Mod\u00e8le mAP Latence (ms) YOLO-NAS S 47.5 3.21 YOLO-NAS M 51.55 5.85 YOLO-NAS L 52.22 7.87 YOLO-NAS S INT-8 47.03 2.36 YOLO-NAS M INT-8 51.0 3.78 YOLO-NAS L INT-8 52.1 4.78 <p>Chaque variante de mod\u00e8le est con\u00e7ue pour offrir un \u00e9quilibre entre la pr\u00e9cision moyenne (mAP) et la latence, vous permettant ainsi d'optimiser vos t\u00e2ches de d\u00e9tection d'objets en termes de performance et de vitesse.</p>"},{"location":"models/yolo-nas/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Ultralytics a rendu les mod\u00e8les YOLO-NAS faciles \u00e0 int\u00e9grer dans vos applications Python gr\u00e2ce \u00e0 notre package Python <code>ultralytics</code>. Le package fournit une interface conviviale pour simplifier le processus.</p> <p>Les exemples suivants montrent comment utiliser les mod\u00e8les YOLO-NAS avec le package <code>ultralytics</code> pour l'inf\u00e9rence et la validation :</p>"},{"location":"models/yolo-nas/#exemples-dinference-et-de-validation","title":"Exemples d'inf\u00e9rence et de validation","text":"<p>Dans cet exemple, nous validons YOLO-NAS-s sur l'ensemble de donn\u00e9es COCO8.</p> <p>Exemple</p> <p>Cet exemple fournit un code simple pour l'inf\u00e9rence et la validation de YOLO-NAS. Pour g\u00e9rer les r\u00e9sultats de l'inf\u00e9rence, consultez le mode Predict. Pour utiliser YOLO-NAS avec des modes suppl\u00e9mentaires, consultez Val et Export. L'entra\u00eenement n'est pas pris en charge pour YOLO-NAS avec le package <code>ultralytics</code>.</p> PythonCLI <p>Il est possible de passer des mod\u00e8les pr\u00e9-entra\u00een\u00e9s <code>*.pt</code> de PyTorch \u00e0 la classe <code>NAS()</code> pour cr\u00e9er une instance de mod\u00e8le en Python :</p> <pre><code>from ultralytics import NAS\n\n# Charger un mod\u00e8le YOLO-NAS-s pr\u00e9-entra\u00een\u00e9 sur COCO\nmodel = NAS('yolo_nas_s.pt')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Valider le mod\u00e8le sur l'ensemble de donn\u00e9es COCO8\nresults = model.val(data='coco8.yaml')\n\n# Effectuer une inf\u00e9rence avec le mod\u00e8le YOLO-NAS-s sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLO-NAS-s pr\u00e9-entra\u00een\u00e9 sur COCO et valider ses performances sur l'ensemble de donn\u00e9es COCO8\nyolo val model=yolo_nas_s.pt data=coco8.yaml\n\n# Charger un mod\u00e8le YOLO-NAS-s pr\u00e9-entra\u00een\u00e9 sur COCO et effectuer une inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolo_nas_s.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolo-nas/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>Nous proposons trois variantes des mod\u00e8les YOLO-NAS : Small (s), Medium (m) et Large (l). Chaque variante est con\u00e7ue pour r\u00e9pondre \u00e0 des besoins computationnels et de performances diff\u00e9rents :</p> <ul> <li>YOLO-NAS-s : Optimis\u00e9 pour les environnements o\u00f9 les ressources computationnelles sont limit\u00e9es mais l'efficacit\u00e9 est primordiale.</li> <li>YOLO-NAS-m : Offre une approche \u00e9quilibr\u00e9e, adapt\u00e9e \u00e0 la d\u00e9tection d'objets polyvalente avec une pr\u00e9cision accrue.</li> <li>YOLO-NAS-l : Adapt\u00e9 aux sc\u00e9narios n\u00e9cessitant la plus haute pr\u00e9cision, o\u00f9 les ressources computationnelles sont moins contraignantes.</li> </ul> <p>Voici un aper\u00e7u d\u00e9taill\u00e9 de chaque mod\u00e8le, comprenant des liens vers leurs poids pr\u00e9-entra\u00een\u00e9s, les t\u00e2ches qu'ils prennent en charge et leur compatibilit\u00e9 avec diff\u00e9rents modes op\u00e9rationnels.</p> Type de mod\u00e8le Poids pr\u00e9-entra\u00een\u00e9s T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Export YOLO-NAS-s yolo_nas_s.pt D\u00e9tection d'objets \u2705 \u2705 \u274c \u2705 YOLO-NAS-m yolo_nas_m.pt D\u00e9tection d'objets \u2705 \u2705 \u274c \u2705 YOLO-NAS-l yolo_nas_l.pt D\u00e9tection d'objets \u2705 \u2705 \u274c \u2705"},{"location":"models/yolo-nas/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Si vous utilisez YOLO-NAS dans vos travaux de recherche ou de d\u00e9veloppement, veuillez citer SuperGradients :</p> BibTeX <pre><code>@misc{supergradients,\n      doi = {10.5281/ZENODO.7789328},\n      url = {https://zenodo.org/record/7789328},\n      author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\n      title = {Super-Gradients},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      year = {2021},\n}\n</code></pre> <p>Nous exprimons notre gratitude \u00e0 l'\u00e9quipe Super-Gradients de Deci AI pour ses efforts dans la cr\u00e9ation et la maintenance de cette pr\u00e9cieuse ressource pour la communaut\u00e9 de la vision par ordinateur. Nous sommes convaincus que YOLO-NAS, avec son architecture innovante et ses capacit\u00e9s de d\u00e9tection d'objets sup\u00e9rieures, deviendra un outil essentiel pour les d\u00e9veloppeurs et les chercheurs.</p> <p>keywords: YOLO-NAS, Deci AI, d\u00e9tection d'objets, apprentissage profond, recherche architecturale neuronale, API Python d'Ultralytics, mod\u00e8le YOLO, SuperGradients, mod\u00e8les pr\u00e9-entra\u00een\u00e9s, bloc de base compatible avec la quantification, sch\u00e9mas d'entra\u00eenement avanc\u00e9s, quantification apr\u00e8s l'entra\u00eenement, optimisation AutoNAC, COCO, Objects365, Roboflow 100</p>"},{"location":"models/yolov3/","title":"YOLOv3, YOLOv3-Ultralytics et YOLOv3u","text":""},{"location":"models/yolov3/#apercu","title":"Aper\u00e7u","text":"<p>Ce document pr\u00e9sente un aper\u00e7u de trois mod\u00e8les de d\u00e9tection d'objets \u00e9troitement li\u00e9s, \u00e0 savoir YOLOv3, YOLOv3-Ultralytics et YOLOv3u.</p> <ol> <li> <p>YOLOv3: Il s'agit de la troisi\u00e8me version de l'algorithme de d\u00e9tection d'objets You Only Look Once (YOLO). Initi\u00e9e par Joseph Redmon, YOLOv3 a am\u00e9lior\u00e9 ses pr\u00e9d\u00e9cesseurs en introduisant des fonctionnalit\u00e9s telles que des pr\u00e9dictions \u00e0 plusieurs \u00e9chelles et trois tailles diff\u00e9rentes de noyaux de d\u00e9tection.</p> </li> <li> <p>YOLOv3-Ultralytics: Il s'agit de l'impl\u00e9mentation par Ultralytics du mod\u00e8le YOLOv3. Il reproduit l'architecture d'origine de YOLOv3 et offre des fonctionnalit\u00e9s suppl\u00e9mentaires, telles que la prise en charge de plusieurs mod\u00e8les pr\u00e9-entra\u00een\u00e9s et des options de personnalisation plus faciles.</p> </li> <li> <p>YOLOv3u: Il s'agit d'une version mise \u00e0 jour de YOLOv3-Ultralytics qui int\u00e8gre la nouvelle t\u00eate de d\u00e9tection sans ancrage et sans objectivit\u00e9 utilis\u00e9e dans les mod\u00e8les YOLOv8. YOLOv3u conserve la m\u00eame architecture de base et de cou de YOLOv3, mais avec la nouvelle t\u00eate de d\u00e9tection de YOLOv8.</p> </li> </ol> <p></p>"},{"location":"models/yolov3/#caracteristiques-cles","title":"Caract\u00e9ristiques cl\u00e9s","text":"<ul> <li> <p>YOLOv3: A introduit l'utilisation de trois \u00e9chelles diff\u00e9rentes pour la d\u00e9tection, en tirant parti de trois tailles diff\u00e9rentes de noyaux de d\u00e9tection : 13x13, 26x26 et 52x52. Cela a consid\u00e9rablement am\u00e9lior\u00e9 la pr\u00e9cision de la d\u00e9tection pour les objets de diff\u00e9rentes tailles. De plus, YOLOv3 a ajout\u00e9 des fonctionnalit\u00e9s telles que des pr\u00e9dictions multi-\u00e9tiquettes pour chaque bo\u00eete englobante et un meilleur r\u00e9seau d'extraction de caract\u00e9ristiques.</p> </li> <li> <p>YOLOv3-Ultralytics: L'impl\u00e9mentation d'Ultralytics de YOLOv3 offre les m\u00eames performances que le mod\u00e8le d'origine, mais propose \u00e9galement un support suppl\u00e9mentaire pour plus de mod\u00e8les pr\u00e9-entra\u00een\u00e9s, des m\u00e9thodes d'entra\u00eenement suppl\u00e9mentaires et des options de personnalisation plus faciles. Cela le rend plus polyvalent et convivial pour les applications pratiques.</p> </li> <li> <p>YOLOv3u: Ce mod\u00e8le mis \u00e0 jour int\u00e8gre la nouvelle t\u00eate de d\u00e9tection sans ancrage et sans objectivit\u00e9 de YOLOv8. En \u00e9liminant le besoin de bo\u00eetes d'ancrage pr\u00e9d\u00e9finies et de scores d'objectivit\u00e9, cette conception de t\u00eate de d\u00e9tection peut am\u00e9liorer la capacit\u00e9 du mod\u00e8le \u00e0 d\u00e9tecter des objets de diff\u00e9rentes tailles et formes. Cela rend YOLOv3u plus robuste et pr\u00e9cis pour les t\u00e2ches de d\u00e9tection d'objets.</p> </li> </ul>"},{"location":"models/yolov3/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>Les mod\u00e8les de la s\u00e9rie YOLOv3, notamment YOLOv3, YOLOv3-Ultralytics et YOLOv3u, sont sp\u00e9cialement con\u00e7us pour les t\u00e2ches de d\u00e9tection d'objets. Ces mod\u00e8les sont r\u00e9put\u00e9s pour leur efficacit\u00e9 dans divers sc\u00e9narios r\u00e9els, alliant pr\u00e9cision et rapidit\u00e9. Chaque variante propose des fonctionnalit\u00e9s et des optimisations uniques, les rendant adapt\u00e9s \u00e0 une gamme d'applications.</p> <p>Les trois mod\u00e8les prennent en charge un ensemble complet de modes, garantissant ainsi leur polyvalence \u00e0 diff\u00e9rentes \u00e9tapes du d\u00e9ploiement et du d\u00e9veloppement du mod\u00e8le. Ces modes comprennent Inf\u00e9rence, Validation, Entra\u00eenement et Export, offrant aux utilisateurs un ensemble complet d'outils pour une d\u00e9tection d'objets efficace.</p> Type de mod\u00e8le T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Export YOLOv3 D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv3-Ultralytics D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv3u D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 <p>Ce tableau offre un aper\u00e7u rapide des capacit\u00e9s de chaque variante de YOLOv3, mettant en \u00e9vidence leur polyvalence et leur pertinence pour diverses t\u00e2ches et modes op\u00e9rationnels dans les flux de travail de d\u00e9tection d'objets.</p>"},{"location":"models/yolov3/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Cet exemple pr\u00e9sente des exemples simples d'entra\u00eenement et d'inf\u00e9rence de YOLOv3. Pour une documentation compl\u00e8te sur ces exemples et d'autres modes, consultez les pages de documentation sur Predict, Train, Val et Export.</p> <p>Exemple</p> PythonCLI <p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s PyTorch <code>*.pt</code>, ainsi que les fichiers de configuration <code>*.yaml</code>, peuvent \u00eatre transmis \u00e0 la classe <code>YOLO()</code> pour cr\u00e9er une instance de mod\u00e8le en Python :</p> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv3n pr\u00e9-entra\u00een\u00e9 avec COCO\nmodel = YOLO('yolov3n.pt')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ex\u00e9cuter l'inf\u00e9rence avec le mod\u00e8le YOLOv3n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLOv3n pr\u00e9-entra\u00een\u00e9 avec COCO et l'entra\u00eener sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nyolo train model=yolov3n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le YOLOv3n pr\u00e9-entra\u00een\u00e9 avec COCO et ex\u00e9cuter l'inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov3n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov3/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Si vous utilisez YOLOv3 dans le cadre de vos recherches, veuillez citer les articles originaux sur YOLO et le r\u00e9f\u00e9rentiel YOLOv3 d'Ultralytics :</p> BibTeX <pre><code>@article{redmon2018yolov3,\n  title={YOLOv3: An Incremental Improvement},\n  author={Redmon, Joseph and Farhadi, Ali},\n  journal={arXiv preprint arXiv:1804.02767},\n  year={2018}\n}\n</code></pre> <p>Merci \u00e0 Joseph Redmon et Ali Farhadi pour le d\u00e9veloppement du YOLOv3 original.</p>"},{"location":"models/yolov4/","title":"YOLOv4: D\u00e9tection d'Objets Rapide et Pr\u00e9cise","text":"<p>Bienvenue sur la page de documentation d'Ultralytics pour YOLOv4, un d\u00e9tecteur d'objets en temps r\u00e9el de pointe lanc\u00e9 en 2020 par Alexey Bochkovskiy sur https://github.com/AlexeyAB/darknet. YOLOv4 est con\u00e7u pour offrir un \u00e9quilibre optimal entre vitesse et pr\u00e9cision, en en faisant un excellent choix pour de nombreuses applications.</p> <p> Sch\u00e9ma d'architecture de YOLOv4. Pr\u00e9sentant la conception d\u00e9taill\u00e9e du r\u00e9seau de YOLOv4, comprenant les composants backbone, neck et head, ainsi que leurs couches interconnect\u00e9es pour une d\u00e9tection d'objets en temps r\u00e9el optimale.</p>"},{"location":"models/yolov4/#introduction","title":"Introduction","text":"<p>YOLOv4 signifie You Only Look Once version 4. Il s'agit d'un mod\u00e8le de d\u00e9tection d'objets en temps r\u00e9el d\u00e9velopp\u00e9 pour rem\u00e9dier aux limitations des versions pr\u00e9c\u00e9dentes de YOLO comme YOLOv3 et d'autres mod\u00e8les de d\u00e9tection d'objets. Contrairement \u00e0 d'autres d\u00e9tecteurs d'objets bas\u00e9s sur des r\u00e9seaux neuronaux convolutifs (CNN), YOLOv4 n'est pas seulement applicable aux syst\u00e8mes de recommandation, mais aussi \u00e0 la gestion de processus autonomes et \u00e0 la r\u00e9duction de l'entr\u00e9e humaine. Son utilisation sur des unit\u00e9s de traitement graphique (GPU) conventionnelles permet une utilisation massive \u00e0 un prix abordable, et il est con\u00e7u pour fonctionner en temps r\u00e9el sur un GPU conventionnel tout en ne n\u00e9cessitant qu'un seul de ces GPU pour l'entra\u00eenement.</p>"},{"location":"models/yolov4/#architecture","title":"Architecture","text":"<p>YOLOv4 utilise plusieurs fonctionnalit\u00e9s innovantes qui travaillent ensemble pour optimiser ses performances. Celles-ci incluent les connexions r\u00e9siduelles pond\u00e9r\u00e9es (WRC), les connexions partielles \u00e0 travers les \u00e9tapes (CSP), la normalisation mini-batch travers\u00e9e (CmBN), l'entra\u00eenement auto-antagoniste (SAT), l'activation Mish, l'augmentation des donn\u00e9es en mosa\u00efque, la r\u00e9gularisation DropBlock et la perte CIoU. Ces fonctionnalit\u00e9s sont combin\u00e9es pour obtenir des r\u00e9sultats de pointe.</p> <p>Un d\u00e9tecteur d'objets typique est compos\u00e9 de plusieurs parties, notamment l'entr\u00e9e, le backbone, le neck et le head. Le backbone de YOLOv4 est pr\u00e9-entra\u00een\u00e9 sur ImageNet et est utilis\u00e9 pour pr\u00e9dire les classes et les bo\u00eetes englobantes des objets. Le backbone peut provenir de plusieurs mod\u00e8les, notamment VGG, ResNet, ResNeXt ou DenseNet. La partie \"neck\" du d\u00e9tecteur est utilis\u00e9e pour collecter des cartes de caract\u00e9ristiques \u00e0 partir de diff\u00e9rentes \u00e9tapes et comprend g\u00e9n\u00e9ralement plusieurs chemins \"bottom-up\" et plusieurs chemins \"top-down\". La partie \"head\" est ce qui est utilis\u00e9 pour faire les d\u00e9tections et classifications finales des objets.</p>"},{"location":"models/yolov4/#ensemble-de-bonus","title":"Ensemble de Bonus","text":"<p>YOLOv4 utilise \u00e9galement des m\u00e9thodes appel\u00e9es \"ensemble de bonus\", qui sont des techniques permettant d'am\u00e9liorer la pr\u00e9cision du mod\u00e8le lors de l'entra\u00eenement sans augmenter le co\u00fbt de l'inf\u00e9rence. L'augmentation de donn\u00e9es est une technique commune de l'ensemble de bonus utilis\u00e9e dans la d\u00e9tection d'objets, qui augmente la variabilit\u00e9 des images d'entr\u00e9e pour am\u00e9liorer la robustesse du mod\u00e8le. Quelques exemples d'augmentation de donn\u00e9es incluent les distorsions photom\u00e9triques (ajustement de la luminosit\u00e9, du contraste, de la teinte, de la saturation et du bruit d'une image) et les distorsions g\u00e9om\u00e9triques (ajout d'\u00e9chelle al\u00e9atoire, de recadrage, de retournement et de rotation). Ces techniques aident le mod\u00e8le \u00e0 mieux g\u00e9n\u00e9raliser \u00e0 diff\u00e9rents types d'images.</p>"},{"location":"models/yolov4/#fonctionnalites-et-performances","title":"Fonctionnalit\u00e9s et Performances","text":"<p>YOLOv4 est con\u00e7u pour une vitesse et une pr\u00e9cision optimales dans la d\u00e9tection d'objets. L'architecture de YOLOv4 comprend CSPDarknet53 en tant que backbone, PANet en tant que neck et YOLOv3 en tant que detection head. Ce design permet \u00e0 YOLOv4 de r\u00e9aliser une d\u00e9tection d'objets \u00e0 une vitesse impressionnante, ce qui le rend adapt\u00e9 aux applications en temps r\u00e9el. YOLOv4 excelle \u00e9galement en pr\u00e9cision, atteignant des r\u00e9sultats de pointe dans les benchmarks de d\u00e9tection d'objets.</p>"},{"location":"models/yolov4/#exemples-dutilisation","title":"Exemples d'Utilisation","text":"<p>Au moment de la r\u00e9daction de ce document, Ultralytics ne prend pas en charge les mod\u00e8les YOLOv4. Par cons\u00e9quent, les utilisateurs int\u00e9ress\u00e9s par l'utilisation de YOLOv4 devront consulter directement le r\u00e9f\u00e9rentiel GitHub de YOLOv4 pour les instructions d'installation et d'utilisation.</p> <p>Voici un bref aper\u00e7u des \u00e9tapes typiques que vous pourriez suivre pour utiliser YOLOv4 :</p> <ol> <li> <p>Rendez-vous sur le r\u00e9f\u00e9rentiel GitHub de YOLOv4 : https://github.com/AlexeyAB/darknet.</p> </li> <li> <p>Suivez les instructions fournies dans le fichier README pour l'installation. Cela implique g\u00e9n\u00e9ralement de cloner le r\u00e9f\u00e9rentiel, d'installer les d\u00e9pendances n\u00e9cessaires et de configurer les variables d'environnement n\u00e9cessaires.</p> </li> <li> <p>Une fois l'installation termin\u00e9e, vous pouvez entra\u00eener et utiliser le mod\u00e8le selon les instructions d'utilisation fournies dans le r\u00e9f\u00e9rentiel. Cela implique g\u00e9n\u00e9ralement la pr\u00e9paration de votre ensemble de donn\u00e9es, la configuration des param\u00e8tres du mod\u00e8le, l'entra\u00eenement du mod\u00e8le, puis l'utilisation du mod\u00e8le entra\u00een\u00e9 pour effectuer la d\u00e9tection d'objets.</p> </li> </ol> <p>Veuillez noter que les \u00e9tapes sp\u00e9cifiques peuvent varier en fonction de votre cas d'utilisation sp\u00e9cifique et de l'\u00e9tat actuel du r\u00e9f\u00e9rentiel YOLOv4. Il est donc fortement recommand\u00e9 de se r\u00e9f\u00e9rer directement aux instructions fournies dans le r\u00e9f\u00e9rentiel GitHub de YOLOv4.</p> <p>Nous regrettons tout inconv\u00e9nient que cela pourrait causer et nous nous efforcerons de mettre \u00e0 jour ce document avec des exemples d'utilisation pour Ultralytics une fois que le support de YOLOv4 sera impl\u00e9ment\u00e9.</p>"},{"location":"models/yolov4/#conclusion","title":"Conclusion","text":"<p>YOLOv4 est un mod\u00e8le de d\u00e9tection d'objets puissant et efficace qui concilie vitesse et pr\u00e9cision. Son utilisation de fonctionnalit\u00e9s uniques et de techniques \"ensemble de bonus\" lors de l'entra\u00eenement lui permet de r\u00e9aliser d'excellentes performances dans les t\u00e2ches de d\u00e9tection d'objets en temps r\u00e9el. YOLOv4 peut \u00eatre entra\u00een\u00e9 et utilis\u00e9 par n'importe qui disposant d'un GPU conventionnel, le rendant accessible et pratique pour un large \u00e9ventail d'applications.</p>"},{"location":"models/yolov4/#citations-et-remerciements","title":"Citations et Remerciements","text":"<p>Nous tenons \u00e0 remercier les auteurs de YOLOv4 pour leurs contributions importantes dans le domaine de la d\u00e9tection d'objets en temps r\u00e9el :</p> BibTeX <pre><code>@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection},\n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>L'article original de YOLOv4 peut \u00eatre consult\u00e9 sur arXiv. Les auteurs ont rendu leur travail accessible au public, et le code source peut \u00eatre consult\u00e9 sur GitHub. Nous appr\u00e9cions leurs efforts pour faire progresser le domaine et rendre leur travail accessible \u00e0 la communaut\u00e9 \u00e9largie.</p>"},{"location":"models/yolov5/","title":"YOLOv5","text":""},{"location":"models/yolov5/#presentation","title":"Pr\u00e9sentation","text":"<p>YOLOv5u repr\u00e9sente une avanc\u00e9e dans les m\u00e9thodologies de d\u00e9tection d'objets. Originaire de l'architecture fondamentale du mod\u00e8le YOLOv5 d\u00e9velopp\u00e9 par Ultralytics, YOLOv5u int\u00e8gre la division sans ancre et sans objectivit\u00e9, une fonctionnalit\u00e9 pr\u00e9c\u00e9demment introduite dans les mod\u00e8les YOLOv8. Cette adaptation affine l'architecture du mod\u00e8le, ce qui conduit \u00e0 un meilleur compromis entre pr\u00e9cision et vitesse dans les t\u00e2ches de d\u00e9tection d'objets. Compte tenu des r\u00e9sultats empiriques et des fonctionnalit\u00e9s d\u00e9riv\u00e9es, YOLOv5u offre une alternative efficace pour ceux qui recherchent des solutions robustes \u00e0 la fois pour la recherche et les applications pratiques.</p> <p></p>"},{"location":"models/yolov5/#principales-fonctionnalites","title":"Principales fonctionnalit\u00e9s","text":"<ul> <li> <p>Division sans ancre Ultralytics : Les mod\u00e8les de d\u00e9tection d'objets traditionnels reposent sur des bo\u00eetes d'ancrage pr\u00e9d\u00e9finies pour pr\u00e9dire les emplacements des objets. Cependant, YOLOv5u modernise cette approche. En adoptant une division sans ancre Ultralytics, il garantit un m\u00e9canisme de d\u00e9tection plus flexible et adaptatif, ce qui am\u00e9liore les performances dans divers sc\u00e9narios.</p> </li> <li> <p>Bon compromis entre pr\u00e9cision et vitesse optimis\u00e9e : La vitesse et la pr\u00e9cision sont souvent oppos\u00e9es. Mais YOLOv5u remet en question ce compromis. Il offre un \u00e9quilibre calibr\u00e9, garantissant des d\u00e9tections en temps r\u00e9el sans compromettre la pr\u00e9cision. Cette fonctionnalit\u00e9 est particuli\u00e8rement pr\u00e9cieuse pour les applications qui demandent des r\u00e9ponses rapides, comme les v\u00e9hicules autonomes, la robotique et l'analyse vid\u00e9o en temps r\u00e9el.</p> </li> <li> <p>Vari\u00e9t\u00e9 de mod\u00e8les pr\u00e9-entra\u00een\u00e9s : Comprendre que diff\u00e9rentes t\u00e2ches n\u00e9cessitent diff\u00e9rents ensembles d'outils, YOLOv5u propose une pl\u00e9thore de mod\u00e8les pr\u00e9-entra\u00een\u00e9s. Que vous vous concentriez sur l'inf\u00e9rence, la validation ou l'entra\u00eenement, un mod\u00e8le sur mesure vous attend. Cette vari\u00e9t\u00e9 garantit que vous n'utilisez pas une solution universelle, mais un mod\u00e8le sp\u00e9cifiquement ajust\u00e9 \u00e0 votre d\u00e9fi unique.</p> </li> </ul>"},{"location":"models/yolov5/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>Les mod\u00e8les YOLOv5u, avec divers poids pr\u00e9-entra\u00een\u00e9s, excellent dans les t\u00e2ches de d\u00e9tection d'objets. Ils prennent en charge une gamme compl\u00e8te de modes, ce qui les rend adapt\u00e9s \u00e0 diverses applications, du d\u00e9veloppement au d\u00e9ploiement.</p> Type de mod\u00e8le Poids pr\u00e9-entra\u00een\u00e9s T\u00e2che Inf\u00e9rence Validation Entra\u00eenement Export YOLOv5u <code>yolov5nu</code>, <code>yolov5su</code>, <code>yolov5mu</code>, <code>yolov5lu</code>, <code>yolov5xu</code>, <code>yolov5n6u</code>, <code>yolov5s6u</code>, <code>yolov5m6u</code>, <code>yolov5l6u</code>, <code>yolov5x6u</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 <p>Ce tableau fournit un aper\u00e7u d\u00e9taill\u00e9 des variantes de mod\u00e8les YOLOv5u, mettant en \u00e9vidence leur applicabilit\u00e9 dans les t\u00e2ches de d\u00e9tection d'objets et leur prise en charge de divers modes op\u00e9rationnels tels que Inf\u00e9rence, Validation, Entra\u00eenement et Exportation. Cette prise en charge compl\u00e8te garantit que les utilisateurs peuvent exploiter pleinement les capacit\u00e9s des mod\u00e8les YOLOv5u dans un large \u00e9ventail de sc\u00e9narios de d\u00e9tection d'objets.</p>"},{"location":"models/yolov5/#metriques-de-performance","title":"M\u00e9triques de performance","text":"<p>Performance</p> D\u00e9tection <p>Consultez la documentation sur la d\u00e9tection pour des exemples d'utilisation avec ces mod\u00e8les form\u00e9s sur COCO, qui comprennent 80 classes pr\u00e9-entra\u00een\u00e9es.</p> Mod\u00e8le YAML taille<sup>(pixels) mAP<sup>val50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) yolov5nu.pt yolov5n.yaml 640 34,3 73,6 1,06 2,6 7,7 yolov5su.pt yolov5s.yaml 640 43,0 120,7 1,27 9,1 24,0 yolov5mu.pt yolov5m.yaml 640 49,0 233,9 1,86 25,1 64,2 yolov5lu.pt yolov5l.yaml 640 52,2 408,4 2,50 53,2 135,0 yolov5xu.pt yolov5x.yaml 640 53,2 763,2 3,81 97,2 246,4 yolov5n6u.pt yolov5n6.yaml 1280 42,1 211,0 1,83 4,3 7,8 yolov5s6u.pt yolov5s6.yaml 1280 48,6 422,6 2,34 15,3 24,6 yolov5m6u.pt yolov5m6.yaml 1280 53,6 810,9 4,36 41,2 65,7 yolov5l6u.pt yolov5l6.yaml 1280 55,7 1470,9 5,47 86,1 137,4 yolov5x6u.pt yolov5x6.yaml 1280 56,8 2436,5 8,98 155,4 250,7"},{"location":"models/yolov5/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Cet exemple pr\u00e9sente des exemples simples d'entra\u00eenement et d'inf\u00e9rence YOLOv5. Pour une documentation compl\u00e8te sur ces exemples et d'autres modes, consultez les pages de documentation Predict, Train, Val et Export.</p> <p>Exemple</p> PythonCLI <p>Les mod\u00e8les PyTorch pr\u00e9-entra\u00een\u00e9s <code>*.pt</code> ainsi que les fichiers de configuration <code>*.yaml</code> peuvent \u00eatre pass\u00e9s \u00e0 la classe <code>YOLO()</code> pour cr\u00e9er une instance de mod\u00e8le en python :</p> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv5n pr\u00e9-entra\u00een\u00e9 sur COCO\nmodel = YOLO('yolov5n.pt')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Former le mod\u00e8le sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ex\u00e9cuter l'inf\u00e9rence avec le mod\u00e8le YOLOv5n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLOv5n pr\u00e9-entra\u00een\u00e9 sur COCO et l'entra\u00eener sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 \u00e9poques\nyolo train model=yolov5n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le YOLOv5n pr\u00e9-entra\u00een\u00e9 sur COCO et ex\u00e9cuter l'inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov5n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov5/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Si vous utilisez YOLOv5 ou YOLOv5u dans vos recherches, veuillez citer le r\u00e9f\u00e9rentiel Ultralytics YOLOv5 comme suit :</p> BibTeX <pre><code>@software{yolov5,\n  title = {Ultralytics YOLOv5},\n  author = {Glenn Jocher},\n  year = {2020},\n  version = {7.0},\n  license = {AGPL-3.0},\n  url = {https://github.com/ultralytics/yolov5},\n  doi = {10.5281/zenodo.3908559},\n  orcid = {0000-0001-5950-6979}\n}\n</code></pre> <p>Veuillez noter que les mod\u00e8les YOLOv5 sont fournis sous les licences AGPL-3.0 et Enterprise.</p>"},{"location":"models/yolov6/","title":"Meituan YOLOv6","text":""},{"location":"models/yolov6/#vue-densemble","title":"Vue d'ensemble","text":"<p>Meituan YOLOv6 est un d\u00e9tecteur d'objets de pointe qui offre un \u00e9quilibre remarquable entre vitesse et pr\u00e9cision, ce qui en fait un choix populaire pour les applications en temps r\u00e9el. Ce mod\u00e8le introduit plusieurs am\u00e9liorations remarquables sur son architecture et son sch\u00e9ma d'entra\u00eenement, notamment la mise en \u0153uvre d'un module de concat\u00e9nation bidirectionnelle (BiC), d'une strat\u00e9gie d'entra\u00eenement assist\u00e9e par ancrage (AAT) et d'une conception am\u00e9lior\u00e9e de l'\u00e9pine dorsale et du cou pour une pr\u00e9cision de pointe sur l'ensemble de donn\u00e9es COCO.</p> <p> Aper\u00e7u de YOLOv6. Diagramme de l'architecture du mod\u00e8le montrant les composants du r\u00e9seau redessin\u00e9s et les strat\u00e9gies d'entra\u00eenement qui ont conduit \u00e0 d'importantes am\u00e9liorations des performances. (a) L'\u00e9pine dorsale de YOLOv6 (N et S sont indiqu\u00e9s). Notez que pour M/L, RepBlocks est remplac\u00e9 par CSPStackRep. (b) La structure d'un module BiC. (c) Un bloc SimCSPSPPF. (source).</p>"},{"location":"models/yolov6/#caracteristiques-principales","title":"Caract\u00e9ristiques principales","text":"<ul> <li>Module de concat\u00e9nation bidirectionnelle (BiC) : YOLOv6 introduit un module BiC dans le cou du d\u00e9tecteur, am\u00e9liorant les signaux de localisation et offrant des gains de performance avec une d\u00e9gradation de vitesse n\u00e9gligeable.</li> <li>Strat\u00e9gie d'entra\u00eenement assist\u00e9e par ancrage (AAT) : Ce mod\u00e8le propose AAT pour profiter des avantages des paradigmes bas\u00e9s sur ancrage et sans ancrage sans compromettre l'efficacit\u00e9 de l'inf\u00e9rence.</li> <li>Conception am\u00e9lior\u00e9e de l'\u00e9pine dorsale et du cou : En approfondissant YOLOv6 pour inclure une autre \u00e9tape dans l'\u00e9pine dorsale et le cou, ce mod\u00e8le atteint des performances de pointe sur l'ensemble de donn\u00e9es COCO avec une entr\u00e9e haute r\u00e9solution.</li> <li>Strat\u00e9gie d'autodistillation : Une nouvelle strat\u00e9gie d'autodistillation est mise en \u0153uvre pour am\u00e9liorer les performances des mod\u00e8les plus petits de YOLOv6, en am\u00e9liorant la branche de r\u00e9gression auxiliaire pendant l'entra\u00eenement et en la supprimant lors de l'inf\u00e9rence afin d'\u00e9viter une baisse notable de la vitesse.</li> </ul>"},{"location":"models/yolov6/#metriques-de-performance","title":"M\u00e9triques de performance","text":"<p>YOLOv6 propose diff\u00e9rents mod\u00e8les pr\u00e9-entra\u00een\u00e9s avec diff\u00e9rentes \u00e9chelles :</p> <ul> <li>YOLOv6-N : 37,5 % de pr\u00e9cision sur COCO val2017 \u00e0 1187 FPS avec le GPU NVIDIA Tesla T4.</li> <li>YOLOv6-S : 45,0 % de pr\u00e9cision \u00e0 484 FPS.</li> <li>YOLOv6-M : 50,0 % de pr\u00e9cision \u00e0 226 FPS.</li> <li>YOLOv6-L : 52,8 % de pr\u00e9cision \u00e0 116 FPS.</li> <li>YOLOv6-L6 : Pr\u00e9cision de pointe en temps r\u00e9el.</li> </ul> <p>YOLOv6 propose \u00e9galement des mod\u00e8les quantifi\u00e9s pour diff\u00e9rentes pr\u00e9cisions et des mod\u00e8les optimis\u00e9s pour les plates-formes mobiles.</p>"},{"location":"models/yolov6/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Cet exemple fournit des exemples simples d'entra\u00eenement et d'inf\u00e9rence de YOLOv6. Pour une documentation compl\u00e8te sur ces exemples et d'autres modes, consultez les pages de documentation Predict, Train, Val et Export.</p> <p>Exemple</p> PythonCLI <p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s PyTorch <code>*.pt</code>, ainsi que les fichiers de configuration <code>*.yaml</code>, peuvent \u00eatre utilis\u00e9s pour cr\u00e9er une instance de mod\u00e8le en python en utilisant la classe <code>YOLO()</code> :</p> <pre><code>from ultralytics import YOLO\n\n# Cr\u00e9er un mod\u00e8le YOLOv6n \u00e0 partir de z\u00e9ro\nmodel = YOLO('yolov6n.yaml')\n\n# Afficher les informations sur le mod\u00e8le (facultatif)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 epochs\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Effectuer une inf\u00e9rence avec le mod\u00e8le YOLOv6n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Cr\u00e9er un mod\u00e8le YOLOv6n \u00e0 partir de z\u00e9ro et l'entra\u00eener sur l'ensemble de donn\u00e9es d'exemple COCO8 pendant 100 epochs\nyolo train model=yolov6n.yaml data=coco8.yaml epochs=100 imgsz=640\n\n# Cr\u00e9er un mod\u00e8le YOLOv6n \u00e0 partir de z\u00e9ro et effectuer une inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov6n.yaml source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov6/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>La s\u00e9rie YOLOv6 propose une gamme de mod\u00e8les, chacun optimis\u00e9 pour la d\u00e9tection d'objets haute performance. Ces mod\u00e8les r\u00e9pondent \u00e0 des besoins computationnels et des exigences de pr\u00e9cision variables, ce qui les rend polyvalents pour une large gamme d'applications.</p> Type de mod\u00e8le Mod\u00e8les pr\u00e9-entra\u00een\u00e9s T\u00e2ches prises en charge Inf\u00e9rence Validation Entra\u00eenement Export YOLOv6-N <code>yolov6-n.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv6-S <code>yolov6-s.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv6-M <code>yolov6-m.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv6-L <code>yolov6-l.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 YOLOv6-L6 <code>yolov6-l6.pt</code> D\u00e9tection d'objets \u2705 \u2705 \u2705 \u2705 <p>Ce tableau fournit un aper\u00e7u d\u00e9taill\u00e9 des variantes du mod\u00e8le YOLOv6, mettant en \u00e9vidence leurs capacit\u00e9s dans les t\u00e2ches de d\u00e9tection d'objets et leur compatibilit\u00e9 avec diff\u00e9rents modes op\u00e9rationnels tels que l'Inf\u00e9rence, la Validation, l'Entra\u00eenement et l'Export. Cette prise en charge compl\u00e8te permet aux utilisateurs de tirer pleinement parti des capacit\u00e9s des mod\u00e8les YOLOv6 dans un large \u00e9ventail de sc\u00e9narios de d\u00e9tection d'objets.</p>"},{"location":"models/yolov6/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Nous tenons \u00e0 remercier les auteurs pour leur contribution importante dans le domaine de la d\u00e9tection d'objets en temps r\u00e9el :</p> BibTeX <pre><code>@misc{li2023yolov6,\n      title={YOLOv6 v3.0: A Full-Scale Reloading},\n      author={Chuyi Li and Lulu Li and Yifei Geng and Hongliang Jiang and Meng Cheng and Bo Zhang and Zaidan Ke and Xiaoming Xu and Xiangxiang Chu},\n      year={2023},\n      eprint={2301.05586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>Le document original de YOLOv6 peut \u00eatre consult\u00e9 sur arXiv. Les auteurs ont rendu leur travail accessible au public, et le code source peut \u00eatre consult\u00e9 sur GitHub. Nous appr\u00e9cions leurs efforts pour faire avancer le domaine et rendre leur travail accessible \u00e0 la communaut\u00e9 plus large.</p>"},{"location":"models/yolov7/","title":"YOLOv7 : Bag-of-Freebies Entra\u00eenable","text":"<p>YOLOv7 est un d\u00e9tecteur d'objets en temps r\u00e9el \u00e0 la pointe de la technologie qui surpasse tous les d\u00e9tecteurs d'objets connus en termes de vitesse et de pr\u00e9cision, dans une plage de 5 FPS \u00e0 160 FPS. Il pr\u00e9sente la pr\u00e9cision la plus \u00e9lev\u00e9e (56,8% AP) parmi tous les d\u00e9tecteurs d'objets en temps r\u00e9el connus avec un FPS de 30 ou plus sur GPU V100. De plus, YOLOv7 surpasse les autres d\u00e9tecteurs d'objets tels que YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5 et bien d'autres en termes de vitesse et de pr\u00e9cision. Le mod\u00e8le est entra\u00een\u00e9 \u00e0 partir de z\u00e9ro sur le jeu de donn\u00e9es MS COCO, sans utiliser d'autres jeux de donn\u00e9es ou de poids pr\u00e9-entra\u00een\u00e9s. Le code source de YOLOv7 est disponible sur GitHub.</p> <p> **Comparaison des d\u00e9tecteurs d'objets de pointe. ** \u00c0 partir des r\u00e9sultats du Tableau 2, nous savons que la m\u00e9thode propos\u00e9e pr\u00e9sente le meilleur compromis vitesse-pr\u00e9cision dans l'ensemble. Si nous comparons YOLOv7-tiny-SiLU avec YOLOv5-N (r6.1), notre m\u00e9thode est 127 FPS plus rapide et plus pr\u00e9cise de 10,7% en AP. De plus, YOLOv7 atteint 51,4% d'AP \u00e0 une fr\u00e9quence d'images de 161 FPS, tandis que PPYOLOE-L avec la m\u00eame AP atteint seulement 78 FPS. En termes d'utilisation des param\u00e8tres, YOLOv7 consomme 41% de moins que PPYOLOE-L. Si nous comparons YOLOv7-X avec une vitesse d'inf\u00e9rence de 114 FPS \u00e0 YOLOv5-L (r6.1) avec une vitesse d'inf\u00e9rence de 99 FPS, YOLOv7-X peut am\u00e9liorer l'AP de 3,9%. Si YOLOv7-X est compar\u00e9 \u00e0 YOLOv5-X (r6.1) de taille similaire, la vitesse d'inf\u00e9rence de YOLOv7-X est de 31 FPS plus rapide. De plus, en termes de nombre de param\u00e8tres et de calculs, YOLOv7-X r\u00e9duit de 22% les param\u00e8tres et de 8% les calculs par rapport \u00e0 YOLOv5-X (r6.1), mais am\u00e9liore l'AP de 2,2% (Source).</p>"},{"location":"models/yolov7/#apercu","title":"Aper\u00e7u","text":"<p>La d\u00e9tection d'objets en temps r\u00e9el est un composant important de nombreux syst\u00e8mes de vision par ordinateur, notamment le suivi multi-objets, la conduite autonome, la robotique et l'analyse d'images m\u00e9dicales. Ces derni\u00e8res ann\u00e9es, le d\u00e9veloppement de la d\u00e9tection d'objets en temps r\u00e9el s'est concentr\u00e9 sur la conception d'architectures efficaces et l'am\u00e9lioration de la vitesse d'inf\u00e9rence des CPU, des GPU et des unit\u00e9s de traitement neuronal (NPU) dans diff\u00e9rentes configurations. YOLOv7 prend en charge les GPU mobiles et les appareils GPU, de l'edge au cloud.</p> <p>Contrairement aux d\u00e9tecteurs d'objets en temps r\u00e9el traditionnels qui se concentrent sur l'optimisation de l'architecture, YOLOv7 introduit une approche ax\u00e9e sur l'optimisation du processus d'entra\u00eenement. Cela comprend des modules et des m\u00e9thodes d'optimisation con\u00e7us pour am\u00e9liorer la pr\u00e9cision de la d\u00e9tection d'objets sans augmenter le co\u00fbt de l'inf\u00e9rence, un concept connu sous le nom de \"bag-of-freebies entra\u00eenable\".</p>"},{"location":"models/yolov7/#fonctionnalites-principales","title":"Fonctionnalit\u00e9s Principales","text":"<p>YOLOv7 propose plusieurs fonctionnalit\u00e9s principales :</p> <ol> <li> <p>R\u00e9-param\u00e9trisation du Mod\u00e8le : YOLOv7 propose un mod\u00e8le re-param\u00e9tr\u00e9 planifi\u00e9, qui est une strat\u00e9gie applicable aux couches de diff\u00e9rents r\u00e9seaux avec le concept de propagation des gradients.</p> </li> <li> <p>Affectation Dynamique des \u00c9tiquettes : La formation du mod\u00e8le avec des couches de sortie multiples pr\u00e9sente un nouveau probl\u00e8me : \"Comment attribuer des cibles dynamiques aux sorties des diff\u00e9rentes branches ?\" Pour r\u00e9soudre ce probl\u00e8me, YOLOv7 introduit une nouvelle m\u00e9thode d'affectation des \u00e9tiquettes appel\u00e9e affectation des \u00e9tiquettes guid\u00e9e en cascade de grossi\u00e8res \u00e0 fines.</p> </li> <li> <p>Mise \u00e0 l'\u00c9chelle \u00c9tendue et Compos\u00e9e : YOLOv7 propose des m\u00e9thodes de \"mise \u00e0 l'\u00e9chelle \u00e9tendue\" et de \"mise \u00e0 l'\u00e9chelle compos\u00e9e\" pour le d\u00e9tecteur d'objets en temps r\u00e9el, qui permettent d'utiliser efficacement les param\u00e8tres et les calculs.</p> </li> <li> <p>Efficacit\u00e9 : La m\u00e9thode propos\u00e9e par YOLOv7 permet de r\u00e9duire efficacement environ 40% des param\u00e8tres et 50% des calculs du d\u00e9tecteur d'objets en temps r\u00e9el de pointe, tout en offrant une vitesse d'inf\u00e9rence plus rapide et une plus grande pr\u00e9cision de d\u00e9tection.</p> </li> </ol>"},{"location":"models/yolov7/#exemples-dutilisation","title":"Exemples d'Utilisation","text":"<p>Au moment de la r\u00e9daction de cet article, Ultralytics ne prend pas en charge les mod\u00e8les YOLOv7. Par cons\u00e9quent, tout utilisateur int\u00e9ress\u00e9 par l'utilisation de YOLOv7 devra se r\u00e9f\u00e9rer directement au d\u00e9p\u00f4t GitHub de YOLOv7 pour obtenir les instructions d'installation et d'utilisation.</p> <p>Voici un bref aper\u00e7u des \u00e9tapes typiques que vous pourriez suivre pour utiliser YOLOv7 :</p> <ol> <li> <p>Rendez-vous sur le d\u00e9p\u00f4t GitHub de YOLOv7 : https://github.com/WongKinYiu/yolov7.</p> </li> <li> <p>Suivez les instructions fournies dans le fichier README pour l'installation. Cela implique g\u00e9n\u00e9ralement de cloner le d\u00e9p\u00f4t, d'installer les d\u00e9pendances n\u00e9cessaires et de configurer les variables d'environnement n\u00e9cessaires.</p> </li> <li> <p>Une fois l'installation termin\u00e9e, vous pouvez entra\u00eener et utiliser le mod\u00e8le selon les instructions d'utilisation fournies dans le d\u00e9p\u00f4t. Cela implique g\u00e9n\u00e9ralement la pr\u00e9paration de votre ensemble de donn\u00e9es, la configuration des param\u00e8tres du mod\u00e8le, l'entra\u00eenement du mod\u00e8le, puis l'utilisation du mod\u00e8le entra\u00een\u00e9 pour effectuer la d\u00e9tection d'objets.</p> </li> </ol> <p>Veuillez noter que les \u00e9tapes sp\u00e9cifiques peuvent varier en fonction de votre cas d'utilisation sp\u00e9cifique et de l'\u00e9tat actuel du d\u00e9p\u00f4t YOLOv7. Par cons\u00e9quent, il est fortement recommand\u00e9 de vous reporter directement aux instructions fournies dans le d\u00e9p\u00f4t GitHub de YOLOv7.</p> <p>Nous nous excusons pour tout inconv\u00e9nient que cela pourrait causer et nous nous efforcerons de mettre \u00e0 jour ce document avec des exemples d'utilisation pour Ultralytics une fois la prise en charge de YOLOv7 mise en place.</p>"},{"location":"models/yolov7/#citations-et-remerciements","title":"Citations et Remerciements","text":"<p>Nous tenons \u00e0 remercier les auteurs de YOLOv7 pour leurs contributions significatives dans le domaine de la d\u00e9tection d'objets en temps r\u00e9el :</p> BibTeX <pre><code>@article{wang2022yolov7,\n  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  journal={arXiv preprint arXiv:2207.02696},\n  year={2022}\n}\n</code></pre> <p>Le document original de YOLOv7 peut \u00eatre consult\u00e9 sur arXiv. Les auteurs ont rendu leur travail accessible au public, et le code source peut \u00eatre consult\u00e9 sur GitHub. Nous appr\u00e9cions leurs efforts pour faire avancer le domaine et rendre leur travail accessible \u00e0 la communaut\u00e9 \u00e9largie.</p>"},{"location":"models/yolov8/","title":"YOLOv8","text":""},{"location":"models/yolov8/#apercu","title":"Aper\u00e7u","text":"<p>YOLOv8 est la derni\u00e8re it\u00e9ration de la s\u00e9rie YOLO de d\u00e9tecteurs d'objets en temps r\u00e9el, offrant des performances de pointe en termes de pr\u00e9cision et de vitesse. S'appuyant sur les avanc\u00e9es des versions pr\u00e9c\u00e9dentes de YOLO, YOLOv8 introduit de nouvelles fonctionnalit\u00e9s et optimisations qui en font un choix id\u00e9al pour diverses t\u00e2ches de d\u00e9tection d'objets dans une large gamme d'applications.</p> <p></p>"},{"location":"models/yolov8/#principales-fonctionnalites","title":"Principales fonctionnalit\u00e9s","text":"<ul> <li>Architectures avanc\u00e9es pour le tronc et le cou: YOLOv8 utilise des architectures de tronc et de cou de pointe, ce qui permet une meilleure extraction des caract\u00e9ristiques et des performances de d\u00e9tection d'objets am\u00e9lior\u00e9es.</li> <li>T\u00eate Ultralytics sans ancre: YOLOv8 adopte une t\u00eate Ultralytics sans ancre, ce qui contribue \u00e0 une meilleure pr\u00e9cision et \u00e0 un processus de d\u00e9tection plus efficace par rapport aux approches bas\u00e9es sur les ancres.</li> <li>\u00c9quilibre optimal entre pr\u00e9cision et vitesse optimis\u00e9: En mettant l'accent sur le maintien d'un \u00e9quilibre optimal entre pr\u00e9cision et vitesse, YOLOv8 convient aux t\u00e2ches de d\u00e9tection d'objets en temps r\u00e9el dans divers domaines d'application.</li> <li>Vari\u00e9t\u00e9 de mod\u00e8les pr\u00e9-entra\u00een\u00e9s: YOLOv8 propose une gamme de mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour r\u00e9pondre \u00e0 diff\u00e9rentes t\u00e2ches et exigences de performance, ce qui facilite la recherche du mod\u00e8le adapt\u00e9 \u00e0 votre cas d'utilisation sp\u00e9cifique.</li> </ul>"},{"location":"models/yolov8/#taches-et-modes-pris-en-charge","title":"T\u00e2ches et modes pris en charge","text":"<p>La s\u00e9rie YOLOv8 propose une gamme diversifi\u00e9e de mod\u00e8les, chacun sp\u00e9cialis\u00e9 dans des t\u00e2ches sp\u00e9cifiques en vision par ordinateur. Ces mod\u00e8les sont con\u00e7us pour r\u00e9pondre \u00e0 diverses exigences, de la d\u00e9tection d'objets \u00e0 des t\u00e2ches plus complexes telles que la segmentation d'instance, la d\u00e9tection de pose/points cl\u00e9s et la classification.</p> <p>Chaque variante de la s\u00e9rie YOLOv8 est optimis\u00e9e pour sa t\u00e2che respective, garantissant des performances et une pr\u00e9cision \u00e9lev\u00e9es. De plus, ces mod\u00e8les sont compatibles avec divers modes op\u00e9rationnels, notamment l'Inf\u00e9rence, la Validation, l'Entra\u00eenement et l'Exportation, ce qui facilite leur utilisation \u00e0 diff\u00e9rentes \u00e9tapes du d\u00e9ploiement et du d\u00e9veloppement.</p> Mod\u00e8le Noms de fichiers T\u00e2che Inf\u00e9rence Validation Entra\u00eenement Exportation YOLOv8 <code>yolov8n.pt</code> <code>yolov8s.pt</code> <code>yolov8m.pt</code> <code>yolov8l.pt</code> <code>yolov8x.pt</code> D\u00e9tection \u2705 \u2705 \u2705 \u2705 YOLOv8-seg <code>yolov8n-seg.pt</code> <code>yolov8s-seg.pt</code> <code>yolov8m-seg.pt</code> <code>yolov8l-seg.pt</code> <code>yolov8x-seg.pt</code> Segmentation d'instance \u2705 \u2705 \u2705 \u2705 YOLOv8-pose <code>yolov8n-pose.pt</code> <code>yolov8s-pose.pt</code> <code>yolov8m-pose.pt</code> <code>yolov8l-pose.pt</code> <code>yolov8x-pose.pt</code> <code>yolov8x-pose-p6.pt</code> Pose/Points cl\u00e9s \u2705 \u2705 \u2705 \u2705 YOLOv8-cls <code>yolov8n-cls.pt</code> <code>yolov8s-cls.pt</code> <code>yolov8m-cls.pt</code> <code>yolov8l-cls.pt</code> <code>yolov8x-cls.pt</code> Classification \u2705 \u2705 \u2705 \u2705 <p>Ce tableau donne un aper\u00e7u des variantes des mod\u00e8les YOLOv8, mettant en \u00e9vidence leur applicabilit\u00e9 dans des t\u00e2ches sp\u00e9cifiques et leur compatibilit\u00e9 avec diff\u00e9rents modes op\u00e9rationnels tels que l'inf\u00e9rence, la validation, l'entra\u00eenement et l'exportation. Il met en avant la polyvalence et la robustesse de la s\u00e9rie YOLOv8, ce qui les rend adapt\u00e9s \u00e0 une vari\u00e9t\u00e9 d'applications en vision par ordinateur.</p>"},{"location":"models/yolov8/#metriques-de-performance","title":"M\u00e9triques de performance","text":"<p>Performance</p> D\u00e9tection (COCO)D\u00e9tection (Open Images V7)Segmentation (COCO)Classification (ImageNet)Pose (COCO) <p>Consultez la doc de d\u00e9tection pour des exemples d'utilisation avec ces mod\u00e8les entra\u00een\u00e9s sur COCO, qui comprennent 80 classes pr\u00e9-entrain\u00e9es.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>val50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37,3 80,4 0,99 3,2 8,7 YOLOv8s 640 44,9 128,4 1,20 11,2 28,6 YOLOv8m 640 50,2 234,7 1,83 25,9 78,9 YOLOv8l 640 52,9 375,2 2,39 43,7 165,2 YOLOv8x 640 53,9 479,1 3,53 68,2 257,8 <p>Consultez la doc de d\u00e9tection pour des exemples d'utilisation avec ces mod\u00e8les entra\u00een\u00e9s sur Open Image V7, qui comprennent 600 classes pr\u00e9-entrain\u00e9es.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>val50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n 640 18,4 142,4 1,21 3,5 10,5 YOLOv8s 640 27,7 183,1 1,40 11,4 29,7 YOLOv8m 640 33,6 408,5 2,26 26,2 80,6 YOLOv8l 640 34,9 596,9 2,43 44,1 167,4 YOLOv8x 640 36,3 860,6 3,56 68,7 260,6 <p>Consultez la doc de segmentation pour des exemples d'utilisation avec ces mod\u00e8les entra\u00een\u00e9s sur COCO, qui comprennent 80 classes pr\u00e9-entrain\u00e9es.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>box50-95 mAP<sup>mask50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36,7 30,5 96,1 1,21 3,4 12,6 YOLOv8s-seg 640 44,6 36,8 155,7 1,47 11,8 42,6 YOLOv8m-seg 640 49,9 40,8 317,0 2,18 27,3 110,2 YOLOv8l-seg 640 52,3 42,6 572,4 2,79 46,0 220,5 YOLOv8x-seg 640 53,4 43,4 712,1 4,02 71,8 344,1 <p>Consultez la doc de classification pour des exemples d'utilisation avec ces mod\u00e8les entra\u00een\u00e9s sur ImageNet, qui comprennent 1000 classes pr\u00e9-entrain\u00e9es.</p> Mod\u00e8le taille<sup>(pixels) acc<sup>top1 acc<sup>top5 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) param\u00e8tres<sup>(M) FLOPs<sup>(B) at 640 YOLOv8n-cls 224 66,6 87,0 12,9 0,31 2,7 4,3 YOLOv8s-cls 224 72,3 91,1 23,4 0,35 6,4 13,5 YOLOv8m-cls 224 76,4 93,2 85,4 0,62 17,0 42,7 YOLOv8l-cls 224 78,0 94,1 163,0 0,87 37,5 99,7 YOLOv8x-cls 224 78,4 94,3 232,0 1,01 57,4 154,8 <p>Consultez la doc d'estimation de pose pour des exemples d'utilisation avec ces mod\u00e8les entra\u00een\u00e9s sur COCO, qui comprennent 1 classe pr\u00e9-entrain\u00e9e, 'person'.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50,4 80,1 131,8 1,18 3,3 9,2 YOLOv8s-pose 640 60,0 86,2 233,2 1,42 11,6 30,2 YOLOv8m-pose 640 65,0 88,8 456,3 2,00 26,4 81,0 YOLOv8l-pose 640 67,6 90,0 784,5 2,59 44,4 168,6 YOLOv8x-pose 640 69,2 90,2 1607,1 3,73 69,4 263,2 YOLOv8x-pose-p6 1280 71,6 91,2 4088,7 10,04 99,1 1066,4"},{"location":"models/yolov8/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Cet exemple fournit des exemples simples d'entra\u00eenement et d'inf\u00e9rence avec YOLOv8. Pour une documentation compl\u00e8te sur ces exemples et d'autres modes, consultez les pages de documentation Predict,  Train, Val et Export.</p> <p>Veuillez noter que l'exemple ci-dessous concerne les mod\u00e8les de d\u00e9tection YOLOv8. Pour d'autres t\u00e2ches prises en charge, consultez la documentation de Segmentation, Classification et Pose/Points cl\u00e9s.</p> <p>Exemple</p> PythonCLI <p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s PyTorch <code>*.pt</code> ainsi que les fichiers de configuration <code>*.yaml</code> peuvent \u00eatre utilis\u00e9s pour cr\u00e9er une instance de mod\u00e8le en python en passant aux classes <code>YOLO()</code> :</p> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO\nmodel = YOLO('yolov8n.pt')\n\n# Afficher les informations du mod\u00e8le (facultatif)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur l'exemple de jeu de donn\u00e9es COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Effectuer une inf\u00e9rence avec le mod\u00e8le YOLOv8n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO et l'entra\u00eener sur l'exemple de jeu de donn\u00e9es COCO8 pendant 100 \u00e9poques\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9 sur COCO et effectuer une inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/yolov8/#citations-et-remerciements","title":"Citations et remerciements","text":"<p>Si vous utilisez le mod\u00e8le YOLOv8 ou tout autre logiciel de ce r\u00e9f\u00e9rentiel dans votre travail, veuillez le citer selon le format suivant :</p> BibTeX <pre><code>@software{yolov8_ultralytics,\n  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n  title = {Ultralytics YOLOv8},\n  version = {8.0.0},\n  year = {2023},\n  url = {https://github.com/ultralytics/ultralytics},\n  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n  license = {AGPL-3.0}\n}\n</code></pre> <p>Veuillez noter que le DOI est en attente et sera ajout\u00e9 \u00e0 la citation d\u00e8s qu'il sera disponible. Les mod\u00e8les YOLOv8 sont fournis sous licence AGPL-3.0 et Enterprise.</p>"},{"location":"modes/","title":"Modes Ultralytics YOLOv8","text":""},{"location":"modes/#introduction","title":"Introduction","text":"<p>Ultralytics YOLOv8 n'est pas simplement un autre mod\u00e8le de d\u00e9tection d'objets ; c'est un cadre polyvalent con\u00e7u pour couvrir l'int\u00e9gralit\u00e9 du cycle de vie des mod\u00e8les d'apprentissage automatique \u2014 de l'ingestion de donn\u00e9es et l'entra\u00eenement des mod\u00e8les \u00e0 la validation, le d\u00e9ploiement et le suivi en conditions r\u00e9elles. Chaque mode remplit un objectif sp\u00e9cifique et est con\u00e7u pour vous offrir la flexibilit\u00e9 et l'efficacit\u00e9 n\u00e9cessaires pour diff\u00e9rentes t\u00e2ches et cas d'utilisation.</p> <p> Regardez : Tutoriel sur les modes Ultralytics : Entra\u00eenement, Validation, Pr\u00e9diction, Exportation &amp; Benchmark. </p>"},{"location":"modes/#apercu-des-modes","title":"Aper\u00e7u des Modes","text":"<p>Comprendre les diff\u00e9rents modes pris en charge par Ultralytics YOLOv8 est crucial pour tirer le maximum de vos mod\u00e8les :</p> <ul> <li>Mode d'entra\u00eenement (Train) : Affinez votre mod\u00e8le sur des jeux de donn\u00e9es personnalis\u00e9s ou pr\u00e9charg\u00e9s.</li> <li>Mode de validation (Val) : Un contr\u00f4le post-entra\u00eenement pour \u00e9valuer la performance du mod\u00e8le.</li> <li>Mode de pr\u00e9diction (Predict) : D\u00e9ployez la puissance pr\u00e9dictive de votre mod\u00e8le sur des donn\u00e9es du monde r\u00e9el.</li> <li>Mode d'exportation (Export) : Pr\u00e9parez votre mod\u00e8le au d\u00e9ploiement dans diff\u00e9rents formats.</li> <li>Mode de suivi (Track) : \u00c9tendez votre mod\u00e8le de d\u00e9tection d'objets \u00e0 des applications de suivi en temps r\u00e9el.</li> <li>Mode benchmark (Benchmark) : Analysez la vitesse et la pr\u00e9cision de votre mod\u00e8le dans divers environnements de d\u00e9ploiement.</li> </ul> <p>Ce guide complet vise \u00e0 vous donner un aper\u00e7u et des informations pratiques sur chaque mode, en vous aidant \u00e0 exploiter tout le potentiel de YOLOv8.</p>"},{"location":"modes/#entrainement-train","title":"Entra\u00eenement (Train)","text":"<p>Le mode d'entra\u00eenement est utilis\u00e9 pour entra\u00eener un mod\u00e8le YOLOv8 sur un jeu de donn\u00e9es personnalis\u00e9. Dans ce mode, le mod\u00e8le est entra\u00een\u00e9 en utilisant le jeu de donn\u00e9es et les hyperparam\u00e8tres sp\u00e9cifi\u00e9s. Le processus d'entra\u00eenement implique l'optimisation des param\u00e8tres du mod\u00e8le afin qu'il puisse pr\u00e9dire avec pr\u00e9cision les classes et les emplacements des objets dans une image.</p> <p>Exemples d'entra\u00eenement</p>"},{"location":"modes/#validation-val","title":"Validation (Val)","text":"<p>Le mode de validation est utilis\u00e9 pour valider un mod\u00e8le YOLOv8 apr\u00e8s qu'il ait \u00e9t\u00e9 entra\u00een\u00e9. Dans ce mode, le mod\u00e8le est \u00e9valu\u00e9 sur un ensemble de validation pour mesurer sa pr\u00e9cision et sa capacit\u00e9 de g\u00e9n\u00e9ralisation. Ce mode peut \u00eatre utilis\u00e9 pour ajuster les hyperparam\u00e8tres du mod\u00e8le afin d'am\u00e9liorer ses performances.</p> <p>Exemples de validation</p>"},{"location":"modes/#prediction-predict","title":"Pr\u00e9diction (Predict)","text":"<p>Le mode de pr\u00e9diction est utilis\u00e9 pour faire des pr\u00e9dictions \u00e0 l'aide d'un mod\u00e8le YOLOv8 entra\u00een\u00e9 sur de nouvelles images ou vid\u00e9os. Dans ce mode, le mod\u00e8le est charg\u00e9 \u00e0 partir d'un fichier de checkpoint, et l'utilisateur peut fournir des images ou vid\u00e9os pour effectuer l'inf\u00e9rence. Le mod\u00e8le pr\u00e9dit les classes et les emplacements des objets dans les images ou vid\u00e9os fournies.</p> <p>Exemples de pr\u00e9diction</p>"},{"location":"modes/#exportation-export","title":"Exportation (Export)","text":"<p>Le mode d'exportation est utilis\u00e9 pour exporter un mod\u00e8le YOLOv8 dans un format pouvant \u00eatre utilis\u00e9 pour le d\u00e9ploiement. Dans ce mode, le mod\u00e8le est converti dans un format pouvant \u00eatre utilis\u00e9 par d'autres applications logicielles ou dispositifs mat\u00e9riels. Ce mode est pratique pour d\u00e9ployer le mod\u00e8le dans des environnements de production.</p> <p>Exemples d'exportation</p>"},{"location":"modes/#suivi-track","title":"Suivi (Track)","text":"<p>Le mode de suivi est utilis\u00e9 pour suivre des objets en temps r\u00e9el \u00e0 l'aide d'un mod\u00e8le YOLOv8. Dans ce mode, le mod\u00e8le est charg\u00e9 \u00e0 partir d'un fichier de checkpoint, et l'utilisateur peut fournir un flux vid\u00e9o en direct pour effectuer le suivi d'objets en temps r\u00e9el. Ce mode est utile pour des applications telles que les syst\u00e8mes de surveillance ou les voitures autonomes.</p> <p>Exemples de suivi</p>"},{"location":"modes/#benchmark-benchmark","title":"Benchmark (Benchmark)","text":"<p>Le mode benchmark est utilis\u00e9 pour profiler la vitesse et la pr\u00e9cision de divers formats d'exportation pour YOLOv8. Les benchmarks fournissent des informations sur la taille du format export\u00e9, ses m\u00e9triques <code>mAP50-95</code> (pour la d\u00e9tection d'objets, la segmentation et la pose) ou <code>accuracy_top5</code> (pour la classification), et le temps d'inf\u00e9rence en millisecondes par image pour diff\u00e9rents formats d'exportation comme ONNX, OpenVINO, TensorRT et autres. Ces informations peuvent aider les utilisateurs \u00e0 choisir le format d'export optimal pour leur cas d'utilisation sp\u00e9cifique en fonction de leurs exigences de vitesse et de pr\u00e9cision.</p> <p>Exemples de benchmark</p>"},{"location":"modes/benchmark/","title":"Benchmarking de Mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introduction","title":"Introduction","text":"<p>Une fois votre mod\u00e8le entra\u00een\u00e9 et valid\u00e9, l'\u00e9tape logique suivante est d'\u00e9valuer ses performances dans divers sc\u00e9narios du monde r\u00e9el. Le mode benchmark dans Ultralytics YOLOv8 r\u00e9pond \u00e0 cet objectif en fournissant un cadre robuste pour \u00e9valuer la vitesse et l'exactitude de votre mod\u00e8le sur une gamme de formats d'exportation.</p>"},{"location":"modes/benchmark/#pourquoi-le-benchmarking-est-il-crucial","title":"Pourquoi le Benchmarking est-il Crucial ?","text":"<ul> <li>D\u00e9cisions \u00c9clair\u00e9es : Obtenez des insights sur les arbitrages entre la vitesse et l'exactitude.</li> <li>Allocation des Ressources : Comprenez comment les diff\u00e9rents formats d'exportation se comportent sur diff\u00e9rents mat\u00e9riels.</li> <li>Optimisation : D\u00e9couvrez quel format d'exportation offre la meilleure performance pour votre cas d'utilisation sp\u00e9cifique.</li> <li>Efficacit\u00e9 des Co\u00fbts : Utilisez les ressources mat\u00e9rielles plus efficacement en vous basant sur les r\u00e9sultats des benchmarks.</li> </ul>"},{"location":"modes/benchmark/#mesures-cles-en-mode-benchmark","title":"Mesures Cl\u00e9s en Mode Benchmark","text":"<ul> <li>mAP50-95 : Pour la d\u00e9tection d'objets, la segmentation et l'estimation de pose.</li> <li>accuracy_top5 : Pour la classification d'images.</li> <li>Temps d'Inf\u00e9rence : Temps pris pour chaque image en millisecondes.</li> </ul>"},{"location":"modes/benchmark/#formats-dexportation-supportes","title":"Formats d'Exportation Support\u00e9s","text":"<ul> <li>ONNX : Pour une performance optimale sur CPU.</li> <li>TensorRT : Pour une efficacit\u00e9 maximale sur GPU.</li> <li>OpenVINO : Pour l'optimisation du mat\u00e9riel Intel.</li> <li>CoreML, TensorFlow SavedModel, et Plus : Pour des besoins vari\u00e9s de d\u00e9ploiement.</li> </ul> <p>Conseil</p> <ul> <li>Exportez vers ONNX ou OpenVINO pour un gain de vitesse CPU jusqu'\u00e0 3x.</li> <li>Exportez vers TensorRT pour un gain de vitesse GPU jusqu'\u00e0 5x.</li> </ul>"},{"location":"modes/benchmark/#exemples-dutilisation","title":"Exemples d'Utilisation","text":"<p>Ex\u00e9cutez les benchmarks YOLOv8n sur tous les formats d'exportation support\u00e9s, y compris ONNX, TensorRT, etc. Consultez la section Arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark sur GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#arguments","title":"Arguments","text":"<p>Des arguments tels que <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> et <code>verbose</code> offrent aux utilisateurs la flexibilit\u00e9 d'ajuster pr\u00e9cis\u00e9ment les benchmarks \u00e0 leurs besoins sp\u00e9cifiques et de comparer facilement les performances de diff\u00e9rents formats d'exportation.</p> Cl\u00e9 Valeur Description <code>model</code> <code>None</code> chemin vers le fichier mod\u00e8le, par ex. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> chemin vers le YAML r\u00e9f\u00e9ren\u00e7ant le dataset de benchmarking (sous l'\u00e9tiquette <code>val</code>) <code>imgsz</code> <code>640</code> taille de l'image comme scalaire ou liste (h, w), par ex. (640, 480) <code>half</code> <code>False</code> quantification FP16 <code>int8</code> <code>False</code> quantification INT8 <code>device</code> <code>None</code> appareil sur lequel ex\u00e9cuter, par ex. appareil cuda=0 ou device=0,1,2,3 ou device=cpu <code>verbose</code> <code>False</code> ne pas continuer en cas d'erreur (bool), ou seuil de plancher val (float)"},{"location":"modes/benchmark/#formats-dexportation","title":"Formats d'Exportation","text":"<p>Les benchmarks tenteront de s'ex\u00e9cuter automatiquement sur tous les formats d'exportation possibles ci-dessous.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez les d\u00e9tails complets sur <code>export</code> dans la page Export.</p>"},{"location":"modes/export/","title":"Exportation de mod\u00e8le avec Ultralytics YOLO","text":""},{"location":"modes/export/#introduction","title":"Introduction","text":"<p>L'objectif ultime de l'entra\u00eenement d'un mod\u00e8le est de le d\u00e9ployer pour des applications dans le monde r\u00e9el. Le mode d'exportation de Ultralytics YOLOv8 offre une large gamme d'options pour exporter votre mod\u00e8le entra\u00een\u00e9 dans diff\u00e9rents formats, le rendant d\u00e9ployable sur diverses plateformes et appareils. Ce guide complet vise \u00e0 vous guider \u00e0 travers les nuances de l'exportation de mod\u00e8les, en montrant comment atteindre une compatibilit\u00e9 et des performances maximales.</p> <p> Regardez : Comment exporter un mod\u00e8le Ultralytics YOLOv8 entra\u00een\u00e9 personnalis\u00e9 et effectuer une inf\u00e9rence en direct sur webcam. </p>"},{"location":"modes/export/#pourquoi-choisir-le-mode-dexportation-yolov8","title":"Pourquoi choisir le mode d'exportation YOLOv8 ?","text":"<ul> <li>Polyvalence : Exportation vers plusieurs formats, y compris ONNX, TensorRT, CoreML et plus encore.</li> <li>Performance : Gagnez jusqu'\u00e0 5 fois la vitesse d'une GPU avec TensorRT et 3 fois la vitesse d'une CPU avec ONNX ou OpenVINO.</li> <li>Compatibilit\u00e9 : Rendez votre mod\u00e8le universellement d\u00e9ployable sur de nombreux environnements mat\u00e9riels et logiciels.</li> <li>Facilit\u00e9 d'utilisation : Interface en ligne de commande (CLI) et API Python simples pour une exportation rapide et directe du mod\u00e8le.</li> </ul>"},{"location":"modes/export/#caracteristiques-cles-du-mode-dexportation","title":"Caract\u00e9ristiques cl\u00e9s du mode d'exportation","text":"<p>Voici quelques-unes des fonctionnalit\u00e9s remarquables :</p> <ul> <li>Exportation en un clic : Commandes simples pour exporter vers diff\u00e9rents formats.</li> <li>Exportation group\u00e9e : Exportez des mod\u00e8les capables d'inf\u00e9rence par lot.</li> <li>Inf\u00e9rence optimis\u00e9e : Les mod\u00e8les export\u00e9s sont optimis\u00e9s pour des temps d'inf\u00e9rence plus rapides.</li> <li>Vid\u00e9os tutorielles : Guides d\u00e9taill\u00e9s et tutoriels pour une exp\u00e9rience d'exportation fluide.</li> </ul> <p>Conseil</p> <ul> <li>Exportez vers ONNX ou OpenVINO pour une acc\u00e9l\u00e9ration de la CPU jusqu'\u00e0 3 fois.</li> <li>Exportez vers TensorRT pour une acc\u00e9l\u00e9ration de la GPU jusqu'\u00e0 5 fois.</li> </ul>"},{"location":"modes/export/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Exportez un mod\u00e8le YOLOv8n vers un format diff\u00e9rent tel que ONNX ou TensorRT. Voir la section Arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # chargez un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # chargez un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporter mod\u00e8le officiel\nyolo export model=path/to/best.pt format=onnx  # exporter mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre>"},{"location":"modes/export/#arguments","title":"Arguments","text":"<p>Les param\u00e8tres d'exportation pour les mod\u00e8les YOLO se r\u00e9f\u00e8rent aux diverses configurations et options utilis\u00e9es pour sauvegarder ou exporter le mod\u00e8le pour utilisation dans d'autres environnements ou plateformes. Ces param\u00e8tres peuvent affecter la performance, la taille et la compatibilit\u00e9 du mod\u00e8le avec diff\u00e9rents syst\u00e8mes. Certains param\u00e8tres d'exportation YOLO courants incluent le format du fichier mod\u00e8le export\u00e9 (par exemple, ONNX, TensorFlow SavedModel), le dispositif sur lequel le mod\u00e8le sera ex\u00e9cut\u00e9 (par exemple, CPU, GPU), et la pr\u00e9sence de fonctionnalit\u00e9s suppl\u00e9mentaires telles que des masques ou des \u00e9tiquettes multiples par bo\u00eete. D'autres facteurs qui peuvent affecter le processus d'exportation incluent la t\u00e2che sp\u00e9cifique pour laquelle le mod\u00e8le est utilis\u00e9 et les exigences ou contraintes de l'environnement ou de la plateforme cible. Il est important de consid\u00e9rer et de configurer ces param\u00e8tres avec soin pour s'assurer que le mod\u00e8le export\u00e9 est optimis\u00e9 pour le cas d'utilisation vis\u00e9 et peut \u00eatre utilis\u00e9 efficacement dans l'environnement cible.</p> Cl\u00e9 Valeur Description <code>format</code> <code>'torchscript'</code> format vers lequel exporter <code>imgsz</code> <code>640</code> taille d'image sous forme scalaire ou liste (h, w), par ex. (640, 480) <code>keras</code> <code>False</code> utilisez Keras pour l'exportation TensorFlow SavedModel <code>optimize</code> <code>False</code> TorchScript : optimisation pour mobile <code>half</code> <code>False</code> quantification FP16 <code>int8</code> <code>False</code> quantification INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT : axes dynamiques <code>simplify</code> <code>False</code> ONNX/TensorRT : simplifier le mod\u00e8le <code>opset</code> <code>None</code> ONNX : version de l'ensemble d'op\u00e9rations (facultatif, par d\u00e9faut \u00e0 la derni\u00e8re) <code>workspace</code> <code>4</code> TensorRT : taille de l'espace de travail (GB) <code>nms</code> <code>False</code> CoreML : ajout de la NMS"},{"location":"modes/export/#formats-dexportation","title":"Formats d'exportation","text":"<p>Les formats d'exportation disponibles pour YOLOv8 sont dans le tableau ci-dessous. Vous pouvez exporter vers n'importe quel format en utilisant l'argument <code>format</code>, par ex. <code>format='onnx'</code> ou <code>format='engine'</code>.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Pr\u00e9diction de Mod\u00e8le avec Ultralytics YOLO","text":""},{"location":"modes/predict/#introduction","title":"Introduction","text":"<p>Dans l'univers de l'apprentissage automatique et de la vision par ordinateur, le processus de donner du sens aux donn\u00e9es visuelles est appel\u00e9 'inf\u00e9rence' ou 'pr\u00e9diction'. Ultralytics YOLOv8 propose une fonctionnalit\u00e9 puissante connue sous le nom de mode de pr\u00e9diction adapt\u00e9 pour l'inf\u00e9rence en temps r\u00e9el et haute performance sur une large gamme de sources de donn\u00e9es.</p> <p> Regardez : Comment Extraire les Sorties du Mod\u00e8le Ultralytics YOLOv8 pour des Projets Personnalis\u00e9s. </p>"},{"location":"modes/predict/#applications-reelles","title":"Applications R\u00e9elles","text":"Fabrication Sports S\u00e9curit\u00e9 D\u00e9tection des Pi\u00e8ces de V\u00e9hicules D\u00e9tection des Joueurs de Football D\u00e9tection de Chutes de Personnes"},{"location":"modes/predict/#pourquoi-utiliser-ultralytics-yolo-pour-linference","title":"Pourquoi Utiliser Ultralytics YOLO pour l'Inf\u00e9rence ?","text":"<p>Voici pourquoi vous devriez consid\u00e9rer le mode de pr\u00e9diction YOLOv8 pour vos besoins vari\u00e9s en inf\u00e9rence :</p> <ul> <li>Polyvalence : Capable de faire des inf\u00e9rences sur des images, des vid\u00e9os et m\u00eame des flux en direct.</li> <li>Performance : Con\u00e7u pour le traitement en temps r\u00e9el \u00e0 grande vitesse sans sacrifier la pr\u00e9cision.</li> <li>Facilit\u00e9 d'Utilisation : Interfaces Python et CLI intuitives pour un d\u00e9ploiement et des tests rapides.</li> <li>Tr\u00e8s Personnalisable : Divers param\u00e8tres et r\u00e9glages pour ajuster le comportement d'inf\u00e9rence du mod\u00e8le selon vos besoins sp\u00e9cifiques.</li> </ul>"},{"location":"modes/predict/#caracteristiques-cles-du-mode-de-prediction","title":"Caract\u00e9ristiques Cl\u00e9s du Mode de Pr\u00e9diction","text":"<p>Le mode de pr\u00e9diction YOLOv8 est con\u00e7u pour \u00eatre robuste et polyvalent, avec des fonctionnalit\u00e9s telles que :</p> <ul> <li>Compatibilit\u00e9 avec Plusieurs Sources de Donn\u00e9es : Que vos donn\u00e9es soient sous forme d'images individuelles, d'une collection d'images, de fichiers vid\u00e9o ou de flux vid\u00e9o en temps r\u00e9el, le mode de pr\u00e9diction r\u00e9pond \u00e0 vos besoins.</li> <li>Mode Streaming : Utilisez la fonctionnalit\u00e9 de streaming pour g\u00e9n\u00e9rer un g\u00e9n\u00e9rateur efficace en termes de m\u00e9moire d'objets <code>Results</code>. Activez-le en r\u00e9glant <code>stream=True</code> dans la m\u00e9thode d'appel du pr\u00e9dicteur.</li> <li>Traitement par Lots : La capacit\u00e9 de traiter plusieurs images ou trames vid\u00e9o dans un seul lot, acc\u00e9l\u00e9rant ainsi le temps d'inf\u00e9rence.</li> <li>Facile \u00e0 Int\u00e9grer : S'int\u00e8gre facilement dans les pipelines de donn\u00e9es existants et autres composants logiciels, gr\u00e2ce \u00e0 son API souple.</li> </ul> <p>Les mod\u00e8les YOLO d'Ultralytics renvoient soit une liste d'objets <code>Results</code> Python, soit un g\u00e9n\u00e9rateur Python efficace en termes de m\u00e9moire d'objets <code>Results</code> lorsque <code>stream=True</code> est pass\u00e9 au mod\u00e8le pendant l'inf\u00e9rence :</p> <p>Pr\u00e9dire</p> Renvoie une liste avec <code>stream=False</code>Renvoie un g\u00e9n\u00e9rateur avec <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\n\n# Ex\u00e9cuter une inf\u00e9rence par lots sur une liste d'images\nresults = model(['im1.jpg', 'im2.jpg'])  # renvoie une liste d'objets Results\n\n# Traiter la liste des r\u00e9sultats\nfor result in results:\n    boxes = result.boxes  # Objet Boxes pour les sorties bbox\n    masks = result.masks  # Objet Masks pour les masques de segmentation\n    keypoints = result.keypoints  # Objet Keypoints pour les sorties de pose\n    probs = result.probs  # Objet Probs pour les sorties de classification\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\n\n# Ex\u00e9cuter une inf\u00e9rence par lots sur une liste d'images\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # renvoie un g\u00e9n\u00e9rateur d'objets Results\n\n# Traiter le g\u00e9n\u00e9rateur de r\u00e9sultats\nfor result in results:\n    boxes = result.boxes  # Objet Boxes pour les sorties bbox\n    masks = result.masks  # Objet Masks pour les masques de segmentation\n    keypoints = result.keypoints  # Objet Keypoints pour les sorties de pose\n    probs = result.probs  # Objet Probs pour les sorties de classification\n</code></pre>"},{"location":"modes/predict/#sources-dinference","title":"Sources d'Inf\u00e9rence","text":"<p>YOLOv8 peut traiter diff\u00e9rents types de sources d'entr\u00e9e pour l'inf\u00e9rence, comme illustr\u00e9 dans le tableau ci-dessous. Les sources incluent des images statiques, des flux vid\u00e9os et divers formats de donn\u00e9es. Le tableau indique \u00e9galement si chaque source peut \u00eatre utilis\u00e9e en mode streaming avec l'argument <code>stream=True</code> \u2705. Le mode streaming est b\u00e9n\u00e9fique pour traiter des vid\u00e9os ou des flux en direct car il cr\u00e9e un g\u00e9n\u00e9rateur de r\u00e9sultats au lieu de charger tous les cadres en m\u00e9moire.</p> <p>Astuce</p> <p>Utilisez <code>stream=True</code> pour traiter des vid\u00e9os longues ou des jeux de donn\u00e9es volumineux afin de g\u00e9rer efficacement la m\u00e9moire. Quand <code>stream=False</code>, les r\u00e9sultats pour tous les cadres ou points de donn\u00e9es sont stock\u00e9s en m\u00e9moire, ce qui peut rapidement s'accumuler et provoquer des erreurs de m\u00e9moire insuffisante pour de grandes entr\u00e9es. En revanche, <code>stream=True</code> utilise un g\u00e9n\u00e9rateur, qui ne garde que les r\u00e9sultats du cadre ou point de donn\u00e9es actuel en m\u00e9moire, r\u00e9duisant consid\u00e9rablement la consommation de m\u00e9moire et pr\u00e9venant les probl\u00e8mes de m\u00e9moire insuffisante.</p> Source Argument Type Notes image <code>'image.jpg'</code> <code>str</code> ou <code>Path</code> Fichier image unique. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL vers une image. capture d'\u00e9cran <code>'screen'</code> <code>str</code> Prendre une capture d'\u00e9cran. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Format HWC avec canaux RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Format HWC avec canaux BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Format HWC avec canaux BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Format BCHW avec canaux RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> ou <code>Path</code> Fichier CSV contenant des chemins vers des images, vid\u00e9os ou r\u00e9pertoires. vid\u00e9o \u2705 <code>'video.mp4'</code> <code>str</code> ou <code>Path</code> Fichier vid\u00e9o dans des formats comme MP4, AVI, etc. r\u00e9pertoire \u2705 <code>'chemin/'</code> <code>str</code> ou <code>Path</code> Chemin vers un r\u00e9pertoire contenant des images ou des vid\u00e9os. motif global \u2705 <code>'chemin/*.jpg'</code> <code>str</code> Motif glob pour faire correspondre plusieurs fichiers. Utilisez le caract\u00e8re <code>*</code> comme joker. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL vers une vid\u00e9o YouTube. flux \u2705 <code>'rtsp://exemple.com/media.mp4'</code> <code>str</code> URL pour des protocoles de streaming comme RTSP, RTMP, TCP, ou une adresse IP. multi-flux \u2705 <code>'liste.streams'</code> <code>str</code> ou <code>Path</code> Fichier texte <code>*.streams</code> avec une URL de flux par ligne, c'est-\u00e0-dire que 8 flux s'ex\u00e9cuteront avec une taille de lot de 8. <p>Ci-dessous des exemples de code pour utiliser chaque type de source :</p> <p>Sources de pr\u00e9diction</p> imagecapture d'\u00e9cranURLPILOpenCVnumpytorch <p>Ex\u00e9cutez une inf\u00e9rence sur un fichier image. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir le chemin vers le fichier image\nsource = 'chemin/vers/image.jpg'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur le contenu actuel de l'\u00e9cran sous forme de capture d'\u00e9cran. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir la capture d'\u00e9cran actuelle comme source\nsource = 'screen'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image ou vid\u00e9o h\u00e9berg\u00e9e \u00e0 distance via URL. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir l'URL d'une image ou vid\u00e9o distante\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image ouverte avec la biblioth\u00e8que Python Imaging Library (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Ouvrir une image avec PIL\nsource = Image.open('chemin/vers/image.jpg')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image lue avec OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Lire une image avec OpenCV\nsource = cv2.imread('chemin/vers/image.jpg')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image repr\u00e9sent\u00e9e sous forme de tableau numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Cr\u00e9er un tableau numpy al\u00e9atoire de forme HWC (640, 640, 3) avec des valeurs dans l'intervalle [0, 255] et de type uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image repr\u00e9sent\u00e9e sous forme de tenseur PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#charger-un-modele-yolov8n-pre-entraine","title":"Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#creer-un-tenseur-aleatoire-torch-de-forme-bchw-1-3-640-640-avec-des-valeurs-dans-lintervalle-0-1-et-de-type-float32","title":"Cr\u00e9er un tenseur al\u00e9atoire torch de forme BCHW (1, 3, 640, 640) avec des valeurs dans l'intervalle [0, 1] et de type float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#executer-une-inference-sur-la-source","title":"Ex\u00e9cuter une inf\u00e9rence sur la source","text":"<p>results = model(source)  # liste d'objets Results</p>"},{"location":"modes/track/","title":"Suivi Multi-Objets avec Ultralytics YOLO","text":"<p>Le suivi d'objets dans le domaine de l'analyse vid\u00e9o est une t\u00e2che essentielle qui non seulement identifie l'emplacement et la classe des objets \u00e0 l'int\u00e9rieur de l'image, mais maintient \u00e9galement un identifiant unique pour chaque objet d\u00e9tect\u00e9 au fur et \u00e0 mesure que la vid\u00e9o progresse. Les applications sont illimit\u00e9es, allant de la surveillance et de la s\u00e9curit\u00e9 \u00e0 l'analytique sportive en temps r\u00e9el.</p>"},{"location":"modes/track/#pourquoi-choisir-ultralytics-yolo-pour-le-suivi-dobjet","title":"Pourquoi Choisir Ultralytics YOLO pour le Suivi d'Objet ?","text":"<p>La sortie des traceurs Ultralytics est coh\u00e9rente avec la d\u00e9tection standard d'objets mais apporte la valeur ajout\u00e9e des identifiants d'objets. Cela facilite le suivi des objets dans les flux vid\u00e9o et effectue des analyses subs\u00e9quentes. Voici pourquoi vous devriez envisager d'utiliser Ultralytics YOLO pour vos besoins de suivi d'objet :</p> <ul> <li>Efficacit\u00e9 : Traitez les flux vid\u00e9o en temps r\u00e9el sans compromettre la pr\u00e9cision.</li> <li>Flexibilit\u00e9 : Prend en charge de multiples algorithmes de suivi et configurations.</li> <li>Facilit\u00e9 d'Utilisation : API Python simple et options CLI pour une int\u00e9gration et un d\u00e9ploiement rapides.</li> <li>Personnalisabilit\u00e9 : Facile \u00e0 utiliser avec des mod\u00e8les YOLO entra\u00een\u00e9s sur mesure, permettant une int\u00e9gration dans des applications sp\u00e9cifiques au domaine.</li> </ul> <p> Regardez : D\u00e9tection et suivi d'objets avec Ultralytics YOLOv8. </p>"},{"location":"modes/track/#applications-dans-le-monde-reel","title":"Applications dans le Monde R\u00e9el","text":"Transport Distribution Aquaculture Suivi de V\u00e9hicules Suivi de Personnes Suivi de Poissons"},{"location":"modes/track/#caracteristiques-en-bref","title":"Caract\u00e9ristiques en Bref","text":"<p>Ultralytics YOLO \u00e9tend ses fonctionnalit\u00e9s de d\u00e9tection d'objets pour fournir un suivi d'objets robuste et polyvalent :</p> <ul> <li>Suivi en Temps R\u00e9el : Suivi fluide d'objets dans des vid\u00e9os \u00e0 fr\u00e9quence d'images \u00e9lev\u00e9e.</li> <li>Prise en Charge de Multiples Traceurs : Choisissez parmi une vari\u00e9t\u00e9 d'algorithmes de suivi \u00e9prouv\u00e9s.</li> <li>Configurations de Traceurs Personnalisables : Adaptez l'algorithme de suivi pour r\u00e9pondre \u00e0 des exigences sp\u00e9cifiques en r\u00e9glant divers param\u00e8tres.</li> </ul>"},{"location":"modes/track/#traceurs-disponibles","title":"Traceurs Disponibles","text":"<p>Ultralytics YOLO prend en charge les algorithmes de suivi suivants. Ils peuvent \u00eatre activ\u00e9s en passant le fichier de configuration YAML correspondant tel que <code>tracker=tracker_type.yaml</code> :</p> <ul> <li>BoT-SORT - Utilisez <code>botsort.yaml</code> pour activer ce traceur.</li> <li>ByteTrack - Utilisez <code>bytetrack.yaml</code> pour activer ce traceur.</li> </ul> <p>Le traceur par d\u00e9faut est BoT-SORT.</p>"},{"location":"modes/track/#suivi","title":"Suivi","text":"<p>Pour ex\u00e9cuter le traceur sur des flux vid\u00e9o, utilisez un mod\u00e8le Detect, Segment ou Pose form\u00e9 tel que YOLOv8n, YOLOv8n-seg et YOLOv8n-pose.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le officiel ou personnalis\u00e9\nmodel = YOLO('yolov8n.pt')  # Charger un mod\u00e8le Detect officiel\nmodel = YOLO('yolov8n-seg.pt')  # Charger un mod\u00e8le Segment officiel\nmodel = YOLO('yolov8n-pose.pt')  # Charger un mod\u00e8le Pose officiel\nmodel = YOLO('chemin/vers/best.pt')  # Charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Effectuer le suivi avec le mod\u00e8le\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Suivi avec le traceur par d\u00e9faut\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Suivi avec le traceur ByteTrack\n</code></pre> <pre><code># Effectuer le suivi avec divers mod\u00e8les en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Detect officiel\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Segment officiel\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Pose officiel\nyolo track model=chemin/vers/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Suivi en utilisant le traceur ByteTrack\nyolo track model=chemin/vers/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Comme on peut le voir dans l'utilisation ci-dessus, le suivi est disponible pour tous les mod\u00e8les Detect, Segment et Pose ex\u00e9cut\u00e9s sur des vid\u00e9os ou des sources de diffusion.</p>"},{"location":"modes/track/#configuration","title":"Configuration","text":""},{"location":"modes/track/#arguments-de-suivi","title":"Arguments de Suivi","text":"<p>La configuration du suivi partage des propri\u00e9t\u00e9s avec le mode Pr\u00e9diction, telles que <code>conf</code>, <code>iou</code>, et <code>show</code>. Pour des configurations suppl\u00e9mentaires, r\u00e9f\u00e9rez-vous \u00e0 la page Predict du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurer les param\u00e8tres de suivi et ex\u00e9cuter le traceur\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurer les param\u00e8tres de suivi et ex\u00e9cuter le traceur en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#selection-du-traceur","title":"S\u00e9lection du Traceur","text":"<p>Ultralytics vous permet \u00e9galement d'utiliser un fichier de configuration de traceur modifi\u00e9. Pour cela, faites simplement une copie d'un fichier de configuration de traceur (par exemple, <code>custom_tracker.yaml</code>) \u00e0 partir de ultralytics/cfg/trackers et modifiez toute configuration (\u00e0 l'exception du <code>tracker_type</code>) selon vos besoins.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger le mod\u00e8le et ex\u00e9cuter le traceur avec un fichier de configuration personnalis\u00e9\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Charger le mod\u00e8le et ex\u00e9cuter le traceur avec un fichier de configuration personnalis\u00e9 en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Pour une liste compl\u00e8te des arguments de suivi, r\u00e9f\u00e9rez-vous \u00e0 la page ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#exemples-python","title":"Exemples Python","text":""},{"location":"modes/track/#boucle-de-persistance-des-pistes","title":"Boucle de Persistance des Pistes","text":"<p>Voici un script Python utilisant OpenCV (<code>cv2</code>) et YOLOv8 pour ex\u00e9cuter le suivi d'objet sur des images vid\u00e9o. Ce script suppose toujours que vous avez d\u00e9j\u00e0 install\u00e9 les packages n\u00e9cessaires (<code>opencv-python</code> et <code>ultralytics</code>). L'argument <code>persist=True</code> indique au traceur que l'image ou la trame actuelle est la suivante dans une s\u00e9quence et s'attend \u00e0 ce que les pistes de l'image pr\u00e9c\u00e9dente soient pr\u00e9sentes dans l'image actuelle.</p> <p>Boucle for streaming avec suivi</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Charger le mod\u00e8le YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Ouvrir le fichier vid\u00e9o\nvideo_path = \"chemin/vers/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Parcourir les images vid\u00e9o\nwhile cap.isOpened():\n    # Lire une image de la vid\u00e9o\n    success, frame = cap.read()\n\n    if success:\n        # Ex\u00e9cuter le suivi YOLOv8 sur l'image, en persistant les pistes entre les images\n        results = model.track(frame, persist=True)\n\n        # Visualiser les r\u00e9sultats sur l'image\n        annotated_frame = results[0].plot()\n\n        # Afficher l'image annot\u00e9e\n        cv2.imshow(\"Suivi YOLOv8\", annotated_frame)\n\n        # Interrompre la boucle si 'q' est press\u00e9e\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Interrompre la boucle si la fin de la vid\u00e9o est atteinte\n        break\n\n# Rel\u00e2cher l'objet de capture vid\u00e9o et fermer la fen\u00eatre d'affichage\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Veuillez noter le changement de <code>model(frame)</code> \u00e0 <code>model.track(frame)</code>, qui active le suivi d'objet \u00e0 la place de la simple d\u00e9tection. Ce script modifi\u00e9 ex\u00e9cutera le traceur sur chaque image de la vid\u00e9o, visualisera les r\u00e9sultats et les affichera dans une fen\u00eatre. La boucle peut \u00eatre quitt\u00e9e en appuyant sur 'q'.</p>"},{"location":"modes/track/#contribuer-de-nouveaux-traceurs","title":"Contribuer de Nouveaux Traceurs","text":"<p>\u00cates-vous comp\u00e9tent en suivi multi-objets et avez-vous r\u00e9ussi \u00e0 impl\u00e9menter ou adapter un algorithme de suivi avec Ultralytics YOLO ? Nous vous invitons \u00e0 contribuer \u00e0 notre section Traceurs sur ultralytics/cfg/trackers ! Vos applications et solutions dans le monde r\u00e9el pourraient \u00eatre inestimables pour les utilisateurs travaillant sur des t\u00e2ches de suivi.</p> <p>En contribuant \u00e0 cette section, vous aidez \u00e0 \u00e9largir l'\u00e9ventail des solutions de suivi disponibles au sein du cadre Ultralytics YOLO, ajoutant une autre couche de fonctionnalit\u00e9 et d'utilit\u00e9 pour la communaut\u00e9.</p> <p>Pour initier votre contribution, veuillez vous r\u00e9f\u00e9rer \u00e0 notre Guide de Contribution pour des instructions compl\u00e8tes sur la soumission d'une Pull Request (PR) \ud83d\udee0\ufe0f. Nous sommes impatients de voir ce que vous apportez \u00e0 la table !</p> <p>Ensemble, am\u00e9liorons les capacit\u00e9s de suivi de l'\u00e9cosyst\u00e8me Ultralytics YOLO \ud83d\ude4f !</p>"},{"location":"modes/train/","title":"Entra\u00eenement de mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/train/#introduction","title":"Introduction","text":"<p>L'entra\u00eenement d'un mod\u00e8le d'apprentissage profond implique de lui fournir des donn\u00e9es et d'ajuster ses param\u00e8tres afin qu'il puisse faire des pr\u00e9dictions pr\u00e9cises. Le mode Entra\u00eenement de Ultralytics YOLOv8 est con\u00e7u pour un entra\u00eenement efficace et performant de mod\u00e8les de d\u00e9tection d'objets, en utilisant pleinement les capacit\u00e9s du mat\u00e9riel moderne. Ce guide vise \u00e0 couvrir tous les d\u00e9tails n\u00e9cessaires pour commencer \u00e0 entra\u00eener vos propres mod\u00e8les en utilisant l'ensemble robuste de fonctionnalit\u00e9s de YOLOv8.</p> <p> Regardez : Comment entra\u00eener un mod\u00e8le YOLOv8 sur votre jeu de donn\u00e9es personnalis\u00e9 dans Google Colab. </p>"},{"location":"modes/train/#pourquoi-choisir-ultralytics-yolo-pour-lentrainement","title":"Pourquoi choisir Ultralytics YOLO pour l'entra\u00eenement ?","text":"<p>Voici quelques raisons convaincantes de choisir le mode Entra\u00eenement de YOLOv8 :</p> <ul> <li>Efficacit\u00e9 : Optimisez l'utilisation de votre mat\u00e9riel, que vous soyez sur une configuration mono-GPU ou que vous \u00e9chelonnier sur plusieurs GPUs.</li> <li>Polyvalence : Entra\u00eenez sur des jeux de donn\u00e9es personnalis\u00e9s en plus de ceux d\u00e9j\u00e0 disponibles comme COCO, VOC et ImageNet.</li> <li>Convivialit\u00e9 : Interfaces CLI et Python simples mais puissantes pour une exp\u00e9rience d'entra\u00eenement directe.</li> <li>Flexibilit\u00e9 des hyperparam\u00e8tres : Un large \u00e9ventail d'hyperparam\u00e8tres personnalisables pour peaufiner les performances du mod\u00e8le.</li> </ul>"},{"location":"modes/train/#principales-caracteristiques-du-mode-entrainement","title":"Principales caract\u00e9ristiques du mode Entra\u00eenement","text":"<p>Voici quelques caract\u00e9ristiques remarquables du mode Entra\u00eenement de YOLOv8 :</p> <ul> <li>T\u00e9l\u00e9chargement automatique de jeux de donn\u00e9es : Les jeux de donn\u00e9es standards comme COCO, VOC et ImageNet sont t\u00e9l\u00e9charg\u00e9s automatiquement lors de la premi\u00e8re utilisation.</li> <li>Support multi-GPU : \u00c9chelonnez vos efforts de formation de mani\u00e8re fluide sur plusieurs GPUs pour acc\u00e9l\u00e9rer le processus.</li> <li>Configuration des hyperparam\u00e8tres : La possibilit\u00e9 de modifier les hyperparam\u00e8tres via des fichiers de configuration YAML ou des arguments CLI.</li> <li>Visualisation et suivi : Suivi en temps r\u00e9el des m\u00e9triques d'entra\u00eenement et visualisation du processus d'apprentissage pour de meilleures perspectives.</li> </ul> <p>Astuce</p> <ul> <li>Les jeux de donn\u00e9es YOLOv8 comme COCO, VOC, ImageNet et bien d'autres se t\u00e9l\u00e9chargent automatiquement lors de la premi\u00e8re utilisation, par exemple <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Entra\u00eenez YOLOv8n sur le jeu de donn\u00e9es COCO128 pendant 100 \u00e9poques avec une taille d'image de 640. Le dispositif d'entra\u00eenement peut \u00eatre sp\u00e9cifi\u00e9 \u00e0 l'aide de l'argument <code>device</code>. Si aucun argument n'est pass\u00e9, le GPU <code>device=0</code> sera utilis\u00e9 s'il est disponible, sinon <code>device=cpu</code> sera utilis\u00e9. Consultez la section Arguments ci-dessous pour obtenir une liste compl\u00e8te des arguments d'entra\u00eenement.</p> <p>Exemple d'entra\u00eenement mono-GPU et CPU</p> <p>Le dispositif est d\u00e9termin\u00e9 automatiquement. Si un GPU est disponible, il sera utilis\u00e9, sinon l'entra\u00eenement commencera sur CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir de YAML\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construire \u00e0 partir de YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir de YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir de YAML, transf\u00e9rer les poids pr\u00e9entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#entrainement-multi-gpu","title":"Entra\u00eenement multi-GPU","text":"<p>L'entra\u00eenement multi-GPU permet une utilisation plus efficace des ressources mat\u00e9rielles disponibles en r\u00e9partissant la charge d'entra\u00eenement sur plusieurs GPUs. Cette fonctionnalit\u00e9 est disponible via l'API Python et l'interface de ligne de commande. Pour activer l'entra\u00eenement multi-GPU, sp\u00e9cifiez les ID des dispositifs GPU que vous souhaitez utiliser.</p> <p>Exemple d'entra\u00eenement multi-GPU</p> <p>Pour s'entra\u00eener avec 2 GPUs, les dispositifs CUDA 0 et 1, utilisez les commandes suivantes. D\u00e9veloppez \u00e0 des GPUs suppl\u00e9mentaires selon le besoin.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\n\n# Entra\u00eener le mod\u00e8le avec 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt en utilisant les GPUs 0 et 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#entrainement-mps-avec-apple-m1-et-m2","title":"Entra\u00eenement MPS avec Apple M1 et M2","text":"<p>Avec le support pour les puces Apple M1 et M2 int\u00e9gr\u00e9 dans les mod\u00e8les Ultralytics YOLO, il est maintenant possible d'entra\u00eener vos mod\u00e8les sur des dispositifs utilisant le puissant framework Metal Performance Shaders (MPS). Le MPS offre un moyen performant d'ex\u00e9cuter des t\u00e2ches de calcul et de traitement d'image sur le silicium personnalis\u00e9 d'Apple.</p> <p>Pour activer l'entra\u00eenement sur les puces Apple M1 et M2, vous devez sp\u00e9cifier 'mps' comme votre dispositif lors du lancement du processus d'entra\u00eenement. Voici un exemple de la mani\u00e8re dont vous pourriez le faire en Python et via la ligne de commande :</p> <p>Exemple d'entra\u00eenement MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\n\n# Entra\u00eener le mod\u00e8le avec MPS\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt avec MPS\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Tout en exploitant la puissance de calcul des puces M1/M2, cela permet un traitement plus efficace des t\u00e2ches d'entra\u00eenement. Pour des conseils plus d\u00e9taill\u00e9s et des options de configuration avanc\u00e9e, veuillez consulter la documentation MPS de PyTorch.</p>"},{"location":"modes/train/#journalisation","title":"Journalisation","text":"<p>Lors de l'entra\u00eenement d'un mod\u00e8le YOLOv8, il peut \u00eatre pr\u00e9cieux de suivre la performance du mod\u00e8le au fil du temps. C'est l\u00e0 que la journalisation entre en jeu. YOLO d'Ultralytics prend en charge trois types de journaux - Comet, ClearML et TensorBoard.</p> <p>Pour utiliser un journal, s\u00e9lectionnez-le dans le menu d\u00e9roulant ci-dessus et ex\u00e9cutez-le. Le journal choisi sera install\u00e9 et initialis\u00e9.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet est une plateforme qui permet aux scientifiques de donn\u00e9es et aux d\u00e9veloppeurs de suivre, comparer, expliquer et optimiser les exp\u00e9riences et les mod\u00e8les. Elle offre des fonctionnalit\u00e9s telles que le suivi en temps r\u00e9el des mesures, les diff\u00e9rences de code et le suivi des hyperparam\u00e8tres.</p> <p>Pour utiliser Comet :</p> <p>Exemple</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>N'oubliez pas de vous connecter \u00e0 votre compte Comet sur leur site web et d'obtenir votre cl\u00e9 API. Vous devrez ajouter cela \u00e0 vos variables d'environnement ou \u00e0 votre script pour enregistrer vos exp\u00e9riences.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML est une plateforme open source qui automatise le suivi des exp\u00e9riences et aide \u00e0 partager efficacement les ressources. Elle est con\u00e7ue pour aider les \u00e9quipes \u00e0 g\u00e9rer, ex\u00e9cuter et reproduire leur travail en ML plus efficacement.</p> <p>Pour utiliser ClearML :</p> <p>Exemple</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Apr\u00e8s avoir ex\u00e9cut\u00e9 ce script, vous devrez vous connecter \u00e0 votre compte ClearML sur le navigateur et authentifier votre session.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard est un ensemble d'outils de visualisation pour TensorFlow. Il vous permet de visualiser votre graphique TensorFlow, de tracer des mesures quantitatives sur l'ex\u00e9cution de votre graphique et de montrer des donn\u00e9es suppl\u00e9mentaires comme des images qui le traversent.</p> <p>Pour utiliser TensorBoard dans Google Colab :</p> <p>Exemple</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # remplacer par le r\u00e9pertoire 'runs'\n</code></pre> <p>Pour utiliser TensorBoard localement, ex\u00e9cutez la commande ci-dessous et consultez les r\u00e9sultats \u00e0 l'adresse http://localhost:6006/.</p> <p>Exemple</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # remplacer par le r\u00e9pertoire 'runs'\n</code></pre> <p>Cela chargera TensorBoard et le dirigera vers le r\u00e9pertoire o\u00f9 vos journaux d'entra\u00eenement sont sauvegard\u00e9s.</p> <p>Apr\u00e8s avoir configur\u00e9 votre journal, vous pouvez ensuite poursuivre l'entra\u00eenement de votre mod\u00e8le. Toutes les m\u00e9triques d'entra\u00eenement seront automatiquement enregistr\u00e9es sur votre plateforme choisie, et vous pourrez acc\u00e9der \u00e0 ces journaux pour surveiller les performances de votre mod\u00e8le au fil du temps, comparer diff\u00e9rents mod\u00e8les et identifier les domaines d'am\u00e9lioration.</p>"},{"location":"modes/val/","title":"Validation des mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/val/#introduction","title":"Introduction","text":"<p>La validation est une \u00e9tape cruciale dans le pipeline d'apprentissage automatique, vous permettant d'\u00e9valuer la qualit\u00e9 de vos mod\u00e8les entra\u00een\u00e9s. Le mode Val dans Ultralytics YOLOv8 offre une gamme robuste d'outils et de m\u00e9triques pour \u00e9valuer la performance de vos mod\u00e8les de d\u00e9tection d'objets. Ce guide sert de ressource compl\u00e8te pour comprendre comment utiliser efficacement le mode Val pour assurer que vos mod\u00e8les sont \u00e0 la fois pr\u00e9cis et fiables.</p>"},{"location":"modes/val/#pourquoi-valider-avec-ultralytics-yolo","title":"Pourquoi valider avec Ultralytics YOLO ?","text":"<p>Voici pourquoi l'utilisation du mode Val de YOLOv8 est avantageuse :</p> <ul> <li>Pr\u00e9cision : Obtenez des m\u00e9triques pr\u00e9cises telles que mAP50, mAP75 et mAP50-95 pour \u00e9valuer de mani\u00e8re exhaustive votre mod\u00e8le.</li> <li>Convenance : Utilisez des fonctionnalit\u00e9s int\u00e9gr\u00e9es qui se souviennent des param\u00e8tres d'entra\u00eenement, simplifiant ainsi le processus de validation.</li> <li>Flexibilit\u00e9 : Validez votre mod\u00e8le avec les m\u00eames jeux de donn\u00e9es ou des jeux diff\u00e9rents et des tailles d'image vari\u00e9es.</li> <li>R\u00e9glage des hyperparam\u00e8tres : Utilisez les m\u00e9triques de validation pour peaufiner votre mod\u00e8le pour de meilleures performances.</li> </ul>"},{"location":"modes/val/#caracteristiques-cles-du-mode-val","title":"Caract\u00e9ristiques cl\u00e9s du mode Val","text":"<p>Voici les fonctionnalit\u00e9s notables offertes par le mode Val de YOLOv8 :</p> <ul> <li>Param\u00e8tres Automatis\u00e9s : Les mod\u00e8les se souviennent de leurs configurations d'entra\u00eenement pour une validation simple.</li> <li>Support Multi-m\u00e9trique : \u00c9valuez votre mod\u00e8le en fonction d'une gamme de m\u00e9triques de pr\u00e9cision.</li> <li>CLI et API Python : Choisissez entre l'interface en ligne de commande ou l'API Python en fonction de vos pr\u00e9f\u00e9rences pour la validation.</li> <li>Compatibilit\u00e9 des Donn\u00e9es : Fonctionne de mani\u00e8re transparente avec les jeux de donn\u00e9es utilis\u00e9s pendant la phase d'entra\u00eenement ainsi qu'avec les jeux personnalis\u00e9s.</li> </ul> <p>Conseil</p> <ul> <li>Les mod\u00e8les YOLOv8 se souviennent automatiquement de leurs param\u00e8tres d'entra\u00eenement, vous pouvez donc facilement valider un mod\u00e8le \u00e0 la m\u00eame taille d'image et sur le jeu de donn\u00e9es original avec juste <code>yolo val model=yolov8n.pt</code> ou <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128. Aucun argument n'a besoin d'\u00eatre pass\u00e9 car le <code>mod\u00e8le</code> conserve ses <code>donn\u00e9es</code> d'entra\u00eenement et arguments comme attributs du mod\u00e8le. Consultez la section des arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # pas besoin d'arguments, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val mod\u00e8le officiel\nyolo detect val model=chemin/vers/meilleur.pt  # val mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"modes/val/#arguments","title":"Arguments","text":"<p>Les param\u00e8tres de validation pour les mod\u00e8les YOLO font r\u00e9f\u00e9rence aux divers hyperparam\u00e8tres et configurations utilis\u00e9s pour \u00e9valuer la performance du mod\u00e8le sur un jeu de donn\u00e9es de validation. Ces param\u00e8tres peuvent affecter la performance, la vitesse et la pr\u00e9cision du mod\u00e8le. Certains param\u00e8tres de validation YOLO courants incluent la taille du lot, la fr\u00e9quence \u00e0 laquelle la validation est effectu\u00e9e pendant l'entra\u00eenement et les m\u00e9triques utilis\u00e9es pour \u00e9valuer la performance du mod\u00e8le. D'autres facteurs pouvant affecter le processus de validation incluent la taille et la composition du jeu de donn\u00e9es de validation et la t\u00e2che sp\u00e9cifique pour laquelle le mod\u00e8le est utilis\u00e9. Il est important de r\u00e9gler et d'exp\u00e9rimenter soigneusement ces param\u00e8tres pour s'assurer que le mod\u00e8le fonctionne bien sur le jeu de donn\u00e9es de validation et pour d\u00e9tecter et pr\u00e9venir le surajustement.</p> Cl\u00e9 Valeur Description <code>data</code> <code>None</code> chemin vers le fichier de donn\u00e9es, par exemple coco128.yaml <code>imgsz</code> <code>640</code> taille des images d'entr\u00e9e en tant qu'entier <code>batch</code> <code>16</code> nombre d'images par lot (-1 pour AutoBatch) <code>save_json</code> <code>False</code> sauvegarder les r\u00e9sultats dans un fichier JSON <code>save_hybrid</code> <code>False</code> sauvegarder la version hybride des \u00e9tiquettes (\u00e9tiquettes + pr\u00e9dictions suppl\u00e9mentaires) <code>conf</code> <code>0.001</code> seuil de confiance de l'objet pour la d\u00e9tection <code>iou</code> <code>0.6</code> seuil d'intersection sur union (IoU) pour la NMS <code>max_det</code> <code>300</code> nombre maximum de d\u00e9tections par image <code>half</code> <code>True</code> utiliser la pr\u00e9cision moiti\u00e9 (FP16) <code>device</code> <code>None</code> appareil sur lequel ex\u00e9cuter, par exemple cuda device=0/1/2/3 ou device=cpu <code>dnn</code> <code>False</code> utiliser OpenCV DNN pour l'inf\u00e9rence ONNX <code>plots</code> <code>False</code> afficher les graphiques lors de la formation <code>rect</code> <code>False</code> val rectangulaire avec chaque lot regroup\u00e9 pour un minimum de rembourrage <code>split</code> <code>val</code> fraction du jeu de donn\u00e9es \u00e0 utiliser pour la validation, par exemple 'val', 'test' ou 'train'"},{"location":"tasks/","title":"T\u00e2ches d'Ultralytics YOLOv8","text":"<p>YOLOv8 est un cadre d'intelligence artificielle qui prend en charge de multiples t\u00e2ches de vision par ordinateur. Le cadre peut \u00eatre utilis\u00e9 pour effectuer de la d\u00e9tection, de la segmentation, de la classification et de l'estimation de la pose. Chacune de ces t\u00e2ches a un objectif et un cas d'utilisation diff\u00e9rents.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p> <p> Regardez : Explorez les T\u00e2ches YOLO Ultralytics : D\u00e9tection d'Objets, Segmentation, Suivi et Estimation de la Pose. </p>"},{"location":"tasks/#detection","title":"D\u00e9tection","text":"<p>La d\u00e9tection est la t\u00e2che principale prise en charge par YOLOv8. Elle implique de d\u00e9tecter des objets dans une image ou une trame vid\u00e9o et de dessiner des bo\u00eetes englobantes autour d'eux. Les objets d\u00e9tect\u00e9s sont class\u00e9s dans diff\u00e9rentes cat\u00e9gories en fonction de leurs caract\u00e9ristiques. YOLOv8 peut d\u00e9tecter plusieurs objets dans une seule image ou trame vid\u00e9o avec une grande pr\u00e9cision et rapidit\u00e9.</p> <p>Exemples de D\u00e9tection</p>"},{"location":"tasks/#segmentation","title":"Segmentation","text":"<p>La segmentation est une t\u00e2che qui implique de segmenter une image en diff\u00e9rentes r\u00e9gions en fonction du contenu de l'image. Chaque r\u00e9gion se voit attribuer une \u00e9tiquette en fonction de son contenu. Cette t\u00e2che est utile dans des applications telles que la segmentation d'image et l'imagerie m\u00e9dicale. YOLOv8 utilise une variante de l'architecture U-Net pour effectuer la segmentation.</p> <p>Exemples de Segmentation</p>"},{"location":"tasks/#classification","title":"Classification","text":"<p>La classification est une t\u00e2che qui implique de classer une image dans diff\u00e9rentes cat\u00e9gories. YOLOv8 peut \u00eatre utilis\u00e9 pour classifier des images en fonction de leur contenu. Il utilise une variante de l'architecture EfficientNet pour effectuer la classification.</p> <p>Exemples de Classification</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>La d\u00e9tection de pose/points cl\u00e9s est une t\u00e2che qui implique de d\u00e9tecter des points sp\u00e9cifiques dans une image ou une trame vid\u00e9o. Ces points sont appel\u00e9s points cl\u00e9s et sont utilis\u00e9s pour suivre le mouvement ou pour l'estimation de la pose. YOLOv8 peut d\u00e9tecter des points cl\u00e9s dans une image ou une trame vid\u00e9o avec une grande pr\u00e9cision et rapidit\u00e9.</p> <p>Exemples de Pose</p>"},{"location":"tasks/#conclusion","title":"Conclusion","text":"<p>YOLOv8 prend en charge de multiples t\u00e2ches, y compris la d\u00e9tection, la segmentation, la classification et la d\u00e9tection de points cl\u00e9s. Chacune de ces t\u00e2ches a des objectifs et des cas d'utilisation diff\u00e9rents. En comprenant les diff\u00e9rences entre ces t\u00e2ches, vous pouvez choisir la t\u00e2che appropri\u00e9e pour votre application de vision par ordinateur.</p>"},{"location":"tasks/classify/","title":"Classification d'images","text":"<p>La classification d'images est la t\u00e2che la plus simple des trois et consiste \u00e0 classer une image enti\u00e8re dans l'une d'un ensemble de classes pr\u00e9d\u00e9finies.</p> <p>Le r\u00e9sultat d'un classificateur d'images est une \u00e9tiquette de classe unique et un score de confiance. La classification d'images est utile lorsque vous avez besoin de savoir seulement \u00e0 quelle classe appartient une image et que vous n'avez pas besoin de conna\u00eetre l'emplacement des objets de cette classe ou leur forme exacte.</p> <p>Astuce</p> <p>Les mod\u00e8les YOLOv8 Classify utilisent le suffixe <code>-cls</code>, par exemple <code>yolov8n-cls.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur ImageNet.</p>"},{"location":"tasks/classify/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Classify pr\u00e9-entra\u00een\u00e9s YOLOv8 sont pr\u00e9sent\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le dataset COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le dataset ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement depuis la derni\u00e8re version Ultralytics release lors de la premi\u00e8re utilisation.</p> Mod\u00e8le taille<sup>(pixels) acc<sup>top1 acc<sup>top5 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) \u00e0 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Les valeurs acc sont les pr\u00e9cisions des mod\u00e8les sur le jeu de donn\u00e9es de validation d'ImageNet.   Pour reproduire : <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Les vitesses sont calcul\u00e9es sur les images de validation d'ImageNet \u00e0 l'aide d'une instance Amazon EC2 P4d.   Pour reproduire : <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez le mod\u00e8le YOLOv8n-cls sur le dataset MNIST160 pendant 100 \u00e9poques avec une taille d'image de 64. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#format-du-dataset","title":"Format du dataset","text":"<p>Le format du dataset de classification YOLO peut \u00eatre trouv\u00e9 en d\u00e9tails dans le Guide des Datasets.</p>"},{"location":"tasks/classify/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-cls entra\u00een\u00e9 sur le dataset MNIST160. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # aucun argument n\u00e9cessaire, les donn\u00e9es et les param\u00e8tres sont m\u00e9moris\u00e9s\nmetrics.top1   # pr\u00e9cision top 1\nmetrics.top5   # pr\u00e9cision top 5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # valider le mod\u00e8le officiel\nyolo classify val model=path/to/best.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/classify/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-cls entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nresults = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9diction avec le mod\u00e8le officiel\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9diction avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Voir les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/classify/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n-cls dans un format diff\u00e9rent comme ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=path/to/best.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation disponibles pour YOLOv8-cls sont pr\u00e9sent\u00e9s dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur les mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-cls.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le une fois l'exportation termin\u00e9e.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Voir les d\u00e9tails complets de l'<code>exportation</code> sur la page Export.</p>"},{"location":"tasks/detect/","title":"D\u00e9tection d'Objets","text":"<p>La d\u00e9tection d'objets est une t\u00e2che qui implique l'identification de l'emplacement et de la classe des objets dans une image ou un flux vid\u00e9o.</p> <p>La sortie d'un d\u00e9tecteur d'objets est un ensemble de bo\u00eetes englobantes qui entourent les objets de l'image, accompagn\u00e9es de libell\u00e9s de classe et de scores de confiance pour chaque bo\u00eete. La d\u00e9tection d'objets est un bon choix lorsque vous avez besoin d'identifier des objets d'int\u00e9r\u00eat dans une sc\u00e8ne, mais que vous n'avez pas besoin de conna\u00eetre exactement o\u00f9 se trouve l'objet ou sa forme exacte.</p> <p> Regardez : D\u00e9tection d'Objets avec le Mod\u00e8le Pr\u00e9-entra\u00een\u00e9 Ultralytics YOLOv8. </p> <p>Conseil</p> <p>Les mod\u00e8les Detect YOLOv8 sont les mod\u00e8les YOLOv8 par d\u00e9faut, c.-\u00e0-d. <code>yolov8n.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO.</p>"},{"location":"tasks/detect/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s Detect YOLOv8 sont pr\u00e9sent\u00e9s ici. Les mod\u00e8les Detect, Segment, et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement \u00e0 partir de la derni\u00e8re version d'Ultralytics lors de la premi\u00e8re utilisation.</p> Mod\u00e8le Taille<sup>(pixels) mAP<sup>val50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) Param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Les valeurs de mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO val2017.   Reproductible avec <code>yolo val detect data=coco.yaml device=0</code></li> <li>La Vitesse est moyenn\u00e9e sur les images COCO val en utilisant une instance Amazon EC2 P4d.   Reproductible avec <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez le mod\u00e8le YOLOv8n sur le jeu de donn\u00e9es COCO128 pendant 100 \u00e9poques \u00e0 la taille d'image de 640. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir de YAML\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construire \u00e0 partir de YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir de YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir de YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#format-des-donnees","title":"Format des donn\u00e9es","text":"<p>Le format des jeux de donn\u00e9es de d\u00e9tection YOLO est d\u00e9taill\u00e9 dans le Guide des Jeux de Donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant depuis d'autres formats (comme COCO, etc.) vers le format YOLO, veuillez utiliser l'outil JSON2YOLO par Ultralytics.</p>"},{"location":"tasks/detect/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128. Aucun argument n'est n\u00e9cessaire puisque le <code>mod\u00e8le</code> conserve ses <code>donn\u00e9es</code> d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # pas d'arguments n\u00e9cessaires, jeu de donn\u00e9es et param\u00e8tres enregistr\u00e9s\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # valider le mod\u00e8le officiel\nyolo detect val model=chemin/vers/best.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/detect/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nresults = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le officiel\nyolo detect predict model=chemin/vers/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Consultez les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/detect/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n dans un format diff\u00e9rent tel que ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=chemin/vers/best.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8 disponibles sont pr\u00e9sent\u00e9s dans le tableau ci-dessous. Vous pouvez directement pr\u00e9dire ou valider sur des mod\u00e8les export\u00e9s, c'est-\u00e0-dire <code>yolo predict model=yolov8n.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le apr\u00e8s l'exportation compl\u00e8te.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> Mod\u00e8le TF Enregistr\u00e9 <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> GraphDef TF <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TPU Edge TF <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez tous les d\u00e9tails <code>export</code> sur la page Exporter.</p>"},{"location":"tasks/pose/","title":"Estimation de Pose","text":"<p>L'estimation de pose est une t\u00e2che qui consiste \u00e0 identifier l'emplacement de points sp\u00e9cifiques dans une image, souvent appel\u00e9s points cl\u00e9s. Ces points cl\u00e9s peuvent repr\u00e9senter diff\u00e9rentes parties de l'objet telles que les articulations, les rep\u00e8res ou d'autres caract\u00e9ristiques distinctives. L'emplacement des points cl\u00e9s est g\u00e9n\u00e9ralement repr\u00e9sent\u00e9 par un ensemble de coordonn\u00e9es 2D <code>[x, y]</code> ou 3D <code>[x, y, visible]</code>.</p> <p>La sortie d'un mod\u00e8le d'estimation de pose est un ensemble de points repr\u00e9sentant les points cl\u00e9s sur un objet dans l'image, g\u00e9n\u00e9ralement accompagn\u00e9s des scores de confiance pour chaque point. L'estimation de pose est un bon choix lorsque vous avez besoin d'identifier des parties sp\u00e9cifiques d'un objet dans une sc\u00e8ne, et leur emplacement les uns par rapport aux autres.</p> <p></p> <p>Conseil</p> <p>Les mod\u00e8les YOLOv8 pose utilisent le suffixe <code>-pose</code>, c'est-\u00e0-dire <code>yolov8n-pose.pt</code>. Ces mod\u00e8les sont entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO keypoints et conviennent \u00e0 une vari\u00e9t\u00e9 de t\u00e2ches d'estimation de pose.</p>"},{"location":"tasks/pose/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Pose pr\u00e9-entra\u00een\u00e9s YOLOv8 sont montr\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les Mod\u00e8les se t\u00e9l\u00e9chargent automatiquement \u00e0 partir de la derni\u00e8re version d'Ultralytics release lors de la premi\u00e8re utilisation.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>Les valeurs de mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO Keypoints val2017.   Reproduire avec <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>La vitesse moyenne sur les images de validation COCO en utilisant une instance Amazon EC2 P4d.   Reproduire avec <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez un mod\u00e8le YOLOv8-pose sur le jeu de donn\u00e9es COCO128-pose.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-pose.pt')    # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nr\u00e9sultats = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#format-du-jeu-de-donnees","title":"Format du jeu de donn\u00e9es","text":"<p>Le format du jeu de donn\u00e9es YOLO pose peut \u00eatre trouv\u00e9 en d\u00e9tail dans le Guide des jeux de donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant \u00e0 partir d'autres formats (comme COCO, etc.) vers le format YOLO, veuillez utiliser l'outil JSON2YOLO d'Ultralytics.</p>"},{"location":"tasks/pose/#val","title":"Val","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-pose entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128-pose. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')     # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nm\u00e9triques = model.val()  # aucun argument n\u00e9cessaire, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nm\u00e9triques.box.map    # map50-95\nm\u00e9triques.box.map50  # map50\nm\u00e9triques.box.map75  # map75\nm\u00e9triques.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # val mod\u00e8le officiel\nyolo pose val model=chemin/vers/best.pt  # val mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/pose/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-pose entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')     # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec mod\u00e8le officiel\nyolo pose predict model=chemin/vers/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec mod\u00e8le personnalis\u00e9\n</code></pre> <p>Consultez les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/pose/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n Pose dans un autre format tel que ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')      # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')   # charger un mod\u00e8le personnalis\u00e9 entra\u00een\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exporter mod\u00e8le officiel\nyolo export model=chemin/vers/best.pt format=onnx  # exporter mod\u00e8le personnalis\u00e9 entra\u00een\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8-pose disponibles sont dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur des mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-pose.onnx</code>. Des exemples d'utilisation sont montr\u00e9s pour votre mod\u00e8le apr\u00e8s la fin de l'exportation.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez les d\u00e9tails complets de <code>export</code> sur la page Exporter.</p>"},{"location":"tasks/segment/","title":"Segmentation d'Instance","text":"<p>La segmentation d'instance va plus loin que la d\u00e9tection d'objet et implique d'identifier des objets individuels dans une image et de les segmenter du reste de l'image.</p> <p>Le r\u00e9sultat d'un mod\u00e8le de segmentation d'instance est un ensemble de masques ou de contours qui d\u00e9limitent chaque objet dans l'image, accompagn\u00e9s d'\u00e9tiquettes de classe et de scores de confiance pour chaque objet. La segmentation d'instance est utile lorsque vous avez besoin de savoir non seulement o\u00f9 se trouvent les objets dans une image, mais aussi quelle est leur forme exacte.</p> <p> Regarder : Ex\u00e9cutez la Segmentation avec le Mod\u00e8le Ultralytics YOLOv8 Pr\u00e9-Entra\u00een\u00e9 en Python. </p> <p>Astuce</p> <p>Les mod\u00e8les YOLOv8 Segment utilisent le suffixe <code>-seg</code>, par exemple <code>yolov8n-seg.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur COCO.</p>"},{"location":"tasks/segment/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Segment pr\u00e9-entra\u00een\u00e9s YOLOv8 sont indiqu\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement depuis la derni\u00e8re version Ultralytics lors de la premi\u00e8re utilisation.</p> Mod\u00e8le Taille<sup>(pixels) mAP<sup>bo\u00eete50-95 mAP<sup>masque50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) Param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Les valeurs mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO val2017.   Pour reproduire, utilisez <code>yolo val segment data=coco.yaml device=0</code></li> <li>Vitesse moyenn\u00e9e sur les images COCO val en utilisant une instance Amazon EC2 P4d.   Pour reproduire, utilisez <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#formation","title":"Formation","text":"<p>Entra\u00eenez YOLOv8n-seg sur le jeu de donn\u00e9es COCO128-seg pendant 100 \u00e9poques \u00e0 la taille d'image 640. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour la formation)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nr\u00e9sultats = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer la formation \u00e0 partir de z\u00e9ro\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Commencer la formation \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer la formation\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#format-des-donnees","title":"Format des donn\u00e9es","text":"<p>Le format des donn\u00e9es de segmentation YOLO peut \u00eatre trouv\u00e9 en d\u00e9tail dans le Guide du Jeu de Donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant \u00e0 partir d'autres formats (comme COCO, etc.) au format YOLO, veuillez utiliser l'outil JSON2YOLO par Ultralytics.</p>"},{"location":"tasks/segment/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-seg entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128-seg. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es de formation et ses arguments comme attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nm\u00e9triques = model.val()  # aucun argument n\u00e9cessaire, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nm\u00e9triques.box.map    # map50-95(B)\nm\u00e9triques.box.map50  # map50(B)\nm\u00e9triques.box.map75  # map75(B)\nm\u00e9triques.box.maps   # une liste contient map50-95(B) de chaque cat\u00e9gorie\nm\u00e9triques.seg.map    # map50-95(M)\nm\u00e9triques.seg.map50  # map50(M)\nm\u00e9triques.seg.map75  # map75(M)\nm\u00e9triques.seg.maps   # une liste contient map50-95(M) de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # valider le mod\u00e8le officiel\nyolo segment val model=chemin/vers/le/meilleur.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/segment/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-seg entra\u00een\u00e9 pour effectuer des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le officiel\nyolo segment predict model=chemin/vers/le/meilleur.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Voir les d\u00e9tails complets du mode <code>predict</code> sur la page Predict.</p>"},{"location":"tasks/segment/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n-seg vers un format diff\u00e9rent comme ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=chemin/vers/le/meilleur.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8-seg disponibles sont dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur les mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-seg.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le apr\u00e8s l'exportation.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Voir les d\u00e9tails complets d'<code>export</code> sur la page Export.</p>"}]}