{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Accueil","text":"<p>Pr\u00e9sentation d'Ultralytics YOLOv8, la derni\u00e8re version du mod\u00e8le r\u00e9put\u00e9 de d\u00e9tection d'objets en temps r\u00e9el et de segmentation d'images. YOLOv8 est construit sur des avanc\u00e9es de pointe en apprentissage profond et vision par ordinateur, offrant des performances in\u00e9gal\u00e9es en termes de vitesse et de pr\u00e9cision. Sa conception \u00e9pur\u00e9e le rend adapt\u00e9 \u00e0 diverses applications et facilement adaptable \u00e0 diff\u00e9rentes plateformes mat\u00e9rielles, des appareils de bord aux API cloud.</p> <p>Explorez les Docs YOLOv8, une ressource compl\u00e8te con\u00e7ue pour vous aider \u00e0 comprendre et \u00e0 utiliser ses fonctionnalit\u00e9s et capacit\u00e9s. Que vous soyez un praticien chevronn\u00e9 de l'apprentissage automatique ou nouveau dans le domaine, ce hub vise \u00e0 maximiser le potentiel de YOLOv8 dans vos projets.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"#par-ou-commencer","title":"Par o\u00f9 commencer","text":"<ul> <li>Installer <code>ultralytics</code> avec pip et d\u00e9marrer en quelques minutes \u00a0  Commencer</li> <li>Pr\u00e9dire de nouvelles images et vid\u00e9os avec YOLOv8 \u00a0  Pr\u00e9dire sur Images</li> <li>Entra\u00eener un nouveau mod\u00e8le YOLOv8 sur votre propre ensemble de donn\u00e9es customis\u00e9 \u00a0  Entra\u00eener un mod\u00e8le</li> <li>Explorer les t\u00e2ches YOLOv8 comme la segmentation, la classification, l'estimation de pose et le suivi \u00a0  Explorer les t\u00e2ches</li> </ul> <p> Regarder : Comment entra\u00eener un mod\u00e8le YOLOv8 sur votre ensemble de donn\u00e9es customis\u00e9 dans Google Colab. </p>"},{"location":"#yolo-un-bref-historique","title":"YOLO : Un bref historique","text":"<p>YOLO (You Only Look Once), un mod\u00e8le populaire de d\u00e9tection d'objets et de segmentation d'images, a \u00e9t\u00e9 d\u00e9velopp\u00e9 par Joseph Redmon et Ali Farhadi \u00e0 l'Universit\u00e9 de Washington. Lanc\u00e9 en 2015, YOLO a rapidement gagn\u00e9 en popularit\u00e9 pour sa vitesse et sa pr\u00e9cision \u00e9lev\u00e9es.</p> <ul> <li>YOLOv2, publi\u00e9 en 2016, a am\u00e9lior\u00e9 le mod\u00e8le original en int\u00e9grant la normalisation par lots, les bo\u00eetes d'ancrage et les clusters de dimensions.</li> <li>YOLOv3, lanc\u00e9 en 2018, a davantage am\u00e9lior\u00e9 la performance du mod\u00e8le en utilisant un r\u00e9seau dorsal plus efficace, des ancres multiples et un pool pyramidal spatial.</li> <li>YOLOv4 a \u00e9t\u00e9 publi\u00e9 en 2020, introduisant des innovations telles que l'augmentation de donn\u00e9es Mosaic, une nouvelle t\u00eate de d\u00e9tection sans ancre et une nouvelle fonction de perte.</li> <li>YOLOv5 a encore am\u00e9lior\u00e9 la performance du mod\u00e8le et a ajout\u00e9 des fonctionnalit\u00e9s nouvelles telles que l'optimisation des hyperparam\u00e8tres, le suivi int\u00e9gr\u00e9 des exp\u00e9riences et l'export automatique vers des formats d'exportation populaires.</li> <li>YOLOv6 a \u00e9t\u00e9 rendu open-source par Meituan en 2022 et est utilis\u00e9 dans de nombreux robots de livraison autonomes de l'entreprise.</li> <li>YOLOv7 a ajout\u00e9 des t\u00e2ches suppl\u00e9mentaires telles que l'estimation de pose sur le jeu de donn\u00e9es de points cl\u00e9s COCO.</li> <li>YOLOv8 est la derni\u00e8re version de YOLO par Ultralytics. En tant que mod\u00e8le de pointe et dernier cri (state-of-the-art, SOTA), YOLOv8 s'appuie sur le succ\u00e8s des versions pr\u00e9c\u00e9dentes, introduisant de nouvelles fonctionnalit\u00e9s et am\u00e9liorations pour des performances, une flexibilit\u00e9 et une efficacit\u00e9 renforc\u00e9es. YOLOv8 prend en charge une gamme compl\u00e8te de t\u00e2ches d'intelligence artificielle visuelle, y compris la d\u00e9tection, la segmentation, l'estimation de pose, le suivi et la classification. Cette polyvalence permet aux utilisateurs de tirer parti des capacit\u00e9s de YOLOv8 dans diverses applications et domaines.</li> </ul>"},{"location":"#licences-yolo-comment-est-licencie-ultralytics-yolo","title":"Licences YOLO : Comment est licenci\u00e9 Ultralytics YOLO ?","text":"<p>Ultralytics offre deux options de licence pour r\u00e9pondre aux diff\u00e9rents cas d'utilisation :</p> <ul> <li>Licence AGPL-3.0 : Cette licence open source approuv\u00e9e par OSI est id\u00e9ale pour les \u00e9tudiants et les passionn\u00e9s, favorisant la collaboration ouverte et le partage des connaissances. Voir le fichier LICENSE pour plus de d\u00e9tails.</li> <li>Licence Enterprise : Con\u00e7ue pour un usage commercial, cette licence permet l'int\u00e9gration transparente des logiciels et mod\u00e8les d'IA Ultralytics dans des biens et services commerciaux, en contournant les exigences open source de l'AGPL-3.0. Si votre sc\u00e9nario implique l'incorporation de nos solutions dans une offre commerciale, n'h\u00e9sitez pas \u00e0 contacter Ultralytics Licensing.</li> </ul> <p>Notre strat\u00e9gie de licence est con\u00e7ue pour garantir que toute am\u00e9lioration de nos projets open source soit restitu\u00e9e \u00e0 la communaut\u00e9. Nous tenons les principes de l'open source \u00e0 c\u0153ur \u2764\ufe0f, et notre mission est de garantir que nos contributions puissent \u00eatre utilis\u00e9es et d\u00e9velopp\u00e9es de mani\u00e8re b\u00e9n\u00e9fique pour tous.</p>"},{"location":"quickstart/","title":"D\u00e9marrage rapide","text":""},{"location":"quickstart/#installer-ultralytics","title":"Installer Ultralytics","text":"<p>Ultralytics propose diverses m\u00e9thodes d'installation, y compris pip, conda et Docker. Installez YOLOv8 via le package <code>ultralytics</code> avec pip pour obtenir la derni\u00e8re version stable ou en clonant le r\u00e9pertoire GitHub d'Ultralytics pour la version la plus r\u00e9cente. Docker peut \u00eatre utilis\u00e9 pour ex\u00e9cuter le package dans un conteneur isol\u00e9, \u00e9vitant l'installation locale.</p> <p>Installer</p> Installation avec Pip (recommand\u00e9)Installation avec CondaClone Git <p>Installez le package <code>ultralytics</code> en utilisant pip, ou mettez \u00e0 jour une installation existante en ex\u00e9cutant <code>pip install -U ultralytics</code>. Visitez l'Index des Packages Python (PyPI) pour plus de d\u00e9tails sur le package <code>ultralytics</code> : https://pypi.org/project/ultralytics/.</p> <p> </p> <pre><code># Installer le package ultralytics depuis PyPI\npip install ultralytics\n</code></pre> <p>Vous pouvez \u00e9galement installer le package <code>ultralytics</code> directement depuis le r\u00e9pertoire GitHub. Cela peut \u00eatre utile si vous voulez la version de d\u00e9veloppement la plus r\u00e9cente. Assurez-vous d'avoir l'outil en ligne de commande Git install\u00e9 sur votre syst\u00e8me. La commande <code>@main</code> installe la branche <code>main</code> et peut \u00eatre modifi\u00e9e pour une autre branche, p. ex. <code>@my-branch</code>, ou supprim\u00e9e enti\u00e8rement pour revenir par d\u00e9faut \u00e0 la branche <code>main</code>.</p> <pre><code># Installer le package ultralytics depuis GitHub\npip install git+https://github.com/ultralytics/ultralytics.git@main\n</code></pre> <p>Conda est un gestionnaire de packages alternatif \u00e0 pip qui peut \u00e9galement \u00eatre utilis\u00e9 pour l'installation. Visitez Anaconda pour plus de d\u00e9tails \u00e0 https://anaconda.org/conda-forge/ultralytics. Le r\u00e9pertoire feedstock d'Ultralytics pour la mise \u00e0 jour du package conda est sur https://github.com/conda-forge/ultralytics-feedstock/.</p> <p> </p> <pre><code># Installer le package ultralytics en utilisant conda\nconda install -c conda-forge ultralytics\n</code></pre> <p>Note</p> <p>Si vous installez dans un environnement CUDA, la meilleure pratique est d'installer <code>ultralytics</code>, <code>pytorch</code> et <code>pytorch-cuda</code> dans la m\u00eame commande pour permettre au gestionnaire de package conda de r\u00e9soudre les conflits, ou bien d'installer <code>pytorch-cuda</code> en dernier pour lui permettre de remplacer le package <code>pytorch</code> sp\u00e9cifique aux CPU si n\u00e9cessaire. <pre><code># Installer tous les packages ensemble en utilisant conda\nconda install -c pytorch -c nvidia -c conda-forge pytorch torchvision pytorch-cuda=11.8 ultralytics\n</code></pre></p> <p>Clonez le r\u00e9pertoire <code>ultralytics</code> si vous \u00eates int\u00e9ress\u00e9 par la contribution au d\u00e9veloppement ou si vous souhaitez exp\u00e9rimenter avec le dernier code source. Apr\u00e8s le clonage, naviguez dans le r\u00e9pertoire et installez le package en mode \u00e9ditable <code>-e</code> en utilisant pip. <pre><code># Cloner le r\u00e9pertoire ultralytics\ngit clone https://github.com/ultralytics/ultralytics\n\n# Naviguer vers le r\u00e9pertoire clon\u00e9\ncd ultralytics\n\n# Installer le package en mode \u00e9ditable pour le d\u00e9veloppement\npip install -e .\n</code></pre></p> <p>Voir le fichier requirements.txt d'<code>ultralytics</code> pour une liste des d\u00e9pendances. Notez que tous les exemples ci-dessus installent toutes les d\u00e9pendances requises.</p> <p> Watch: Ultralytics YOLO Quick Start Guide </p> <p>Conseil</p> <p>Les pr\u00e9requis de PyTorch varient selon le syst\u00e8me d'exploitation et les exigences CUDA, donc il est recommand\u00e9 d'installer PyTorch en premier en suivant les instructions sur https://pytorch.org/get-started/locally.</p> <p> </p>"},{"location":"quickstart/#image-docker-conda","title":"Image Docker Conda","text":"<p>Les images Docker Conda d'Ultralytics sont \u00e9galement disponibles sur DockerHub. Ces images sont bas\u00e9es sur Miniconda3 et constituent un moyen simple de commencer \u00e0 utiliser <code>ultralytics</code> dans un environnement Conda.</p> <pre><code># D\u00e9finir le nom de l'image comme variable\nt=ultralytics/ultralytics:latest-conda\n\n# T\u00e9l\u00e9charger la derni\u00e8re image ultralytics de Docker Hub\nsudo docker pull $t\n\n# Ex\u00e9cuter l'image ultralytics dans un conteneur avec support GPU\nsudo docker run -it --ipc=host --gpus all $t  # tous les GPUs\nsudo docker run -it --ipc=host --gpus '\"device=2,3\"' $t  # sp\u00e9cifier les GPUs\n</code></pre>"},{"location":"quickstart/#utiliser-ultralytics-avec-cli","title":"Utiliser Ultralytics avec CLI","text":"<p>L'interface en ligne de commande (CLI) d'Ultralytics permet l'utilisation de commandes simples en une seule ligne sans n\u00e9cessiter d'environnement Python. La CLI ne requiert pas de personnalisation ou de code Python. Vous pouvez simplement ex\u00e9cuter toutes les t\u00e2ches depuis le terminal avec la commande <code>yolo</code>. Consultez le Guide CLI pour en savoir plus sur l'utilisation de YOLOv8 depuis la ligne de commande.</p> <p>Exemple</p> SyntaxeEntra\u00eenementPr\u00e9dictionValidationExportationSp\u00e9cial <p>Les commandes <code>yolo</code> d'Ultralytics utilisent la syntaxe suivante : <pre><code>yolo T\u00c2CHE MODE ARGS\n\nO\u00f9   T\u00c2CHE (facultatif) est l'une de [detect, segment, classify]\n     MODE (obligatoire) est l'un de [train, val, predict, export, track]\n     ARGS (facultatif) sont n'importe quel nombre de paires personnalis\u00e9es 'arg=valeur' comme 'imgsz=320' qui remplacent les valeurs par d\u00e9faut.\n</code></pre> Voyez tous les ARGS dans le Guide de Configuration complet ou avec <code>yolo cfg</code></p> <p>Entra\u00eenez un mod\u00e8le de d\u00e9tection pour 10 epochs avec un learning_rate initial de 0.01 <pre><code>yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n</code></pre></p> <p>Pr\u00e9disez une vid\u00e9o YouTube en utilisant un mod\u00e8le de segmentation pr\u00e9-entra\u00een\u00e9 \u00e0 une taille d'image de 320 : <pre><code>yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n</code></pre></p> <p>Validez un mod\u00e8le de d\u00e9tection pr\u00e9-entra\u00een\u00e9 avec un batch-size de 1 et une taille d'image de 640 : <pre><code>yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n</code></pre></p> <p>Exportez un mod\u00e8le de classification YOLOv8n au format ONNX \u00e0 une taille d'image de 224 par 128 (pas de T\u00c2CHE requise) <pre><code>yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n</code></pre></p> <p>Ex\u00e9cutez des commandes sp\u00e9ciales pour voir la version, afficher les param\u00e8tres, effectuer des v\u00e9rifications et plus encore : <pre><code>yolo help\nyolo checks\nyolo version\nyolo settings\nyolo copy-cfg\nyolo cfg\n</code></pre></p> <p>Avertissement</p> <p>Les arguments doivent \u00eatre pass\u00e9s sous forme de paires <code>arg=val</code>, s\u00e9par\u00e9s par un signe \u00e9gal <code>=</code> et d\u00e9limit\u00e9s par des espaces <code></code> entre les paires. N'utilisez pas de pr\u00e9fixes d'arguments <code>--</code> ou de virgules <code>,</code> entre les arguments.</p> <ul> <li><code>yolo predict model=yolov8n.pt imgsz=640 conf=0.25</code> \u00a0 \u2705</li> <li><code>yolo predict model yolov8n.pt imgsz 640 conf 0.25</code> \u00a0 \u274c</li> <li><code>yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25</code> \u00a0 \u274c</li> </ul> <p>Guide CLI</p>"},{"location":"quickstart/#utiliser-ultralytics-avec-python","title":"Utiliser Ultralytics avec Python","text":"<p>L'interface Python de YOLOv8 permet une int\u00e9gration transparente dans vos projets Python, facilitant le chargement, l'ex\u00e9cution et le traitement de la sortie du mod\u00e8le. Con\u00e7ue avec simplicit\u00e9 et facilit\u00e9 d'utilisation \u00e0 l'esprit, l'interface Python permet aux utilisateurs de mettre en \u0153uvre rapidement la d\u00e9tection d'objets, la segmentation et la classification dans leurs projets. Cela fait de l'interface Python de YOLOv8 un outil inestimable pour quiconque cherche \u00e0 int\u00e9grer ces fonctionnalit\u00e9s dans ses projets Python.</p> <p>Par exemple, les utilisateurs peuvent charger un mod\u00e8le, l'entra\u00eener, \u00e9valuer ses performances sur un set de validation, et m\u00eame l'exporter au format ONNX avec seulement quelques lignes de code. Consultez le Guide Python pour en savoir plus sur l'utilisation de YOLOv8 au sein de vos projets Python.</p> <p>Exemple</p> <pre><code>from ultralytics import YOLO\n\n# Cr\u00e9er un nouveau mod\u00e8le YOLO \u00e0 partir de z\u00e9ro\nmodel = YOLO('yolov8n.yaml')\n\n# Charger un mod\u00e8le YOLO pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.pt')\n\n# Entra\u00eener le mod\u00e8le en utilisant le jeu de donn\u00e9es 'coco128.yaml' pour 3 epochs\nr\u00e9sultats = model.train(data='coco128.yaml', epochs=3)\n\n# \u00c9valuer la performance du mod\u00e8le sur le set de validation\nr\u00e9sultats = model.val()\n\n# Effectuer la d\u00e9tection d'objets sur une image en utilisant le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')\n\n# Exporter le mod\u00e8le au format ONNX\nsucc\u00e8s = model.export(format='onnx')\n</code></pre> <p>Guide Python</p>"},{"location":"datasets/","title":"Aper\u00e7u des ensembles de donn\u00e9es","text":"<p>Ultralytics fournit un soutien pour divers ensembles de donn\u00e9es pour faciliter les t\u00e2ches de vision par ordinateur telles que la d\u00e9tection, la segmentation d'instance, l'estimation de la pose, la classification et le suivi multi-objets. Ci-dessous se trouve une liste des principaux ensembles de donn\u00e9es Ultralytics, suivie d'un r\u00e9sum\u00e9 de chaque t\u00e2che de vision par ordinateur et des ensembles de donn\u00e9es respectifs.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en cours de construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"datasets/#ensembles-de-donnees-de-detection","title":"Ensembles de donn\u00e9es de d\u00e9tection","text":"<p>La d\u00e9tection d'objets par bo\u00eete englobante est une technique de vision par ordinateur qui consiste \u00e0 d\u00e9tecter et localiser des objets dans une image en dessinant une bo\u00eete englobante autour de chaque objet.</p> <ul> <li>Argoverse : Un ensemble de donn\u00e9es contenant des donn\u00e9es de suivi 3D et de pr\u00e9vision de mouvement dans des environnements urbains avec des annotations d\u00e9taill\u00e9es.</li> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle con\u00e7u pour la d\u00e9tection d'objets, la segmentation et l'annotation avec plus de 200K images \u00e9tiquet\u00e9es.</li> <li>COCO8 : Contient les 4 premi\u00e8res images de COCO train et COCO val, adapt\u00e9es pour des tests rapides.</li> <li>Global Wheat 2020 : Un ensemble de donn\u00e9es d'images de t\u00eates de bl\u00e9 recueillies dans le monde entier pour les t\u00e2ches de d\u00e9tection et de localisation d'objets.</li> <li>Objects365 : Un ensemble de donn\u00e9es de grande qualit\u00e9 et \u00e0 grande \u00e9chelle pour la d\u00e9tection d'objets avec 365 cat\u00e9gories d'objets et plus de 600K images annot\u00e9es.</li> <li>OpenImagesV7 : Un ensemble de donn\u00e9es complet de Google avec 1.7M d'images d'entra\u00eenement et 42k images de validation.</li> <li>SKU-110K : Un ensemble de donn\u00e9es mettant en vedette la d\u00e9tection d'objets denses dans les environnements de vente au d\u00e9tail avec plus de 11K images et 1.7 million de bo\u00eetes englobantes.</li> <li>VisDrone : Un ensemble de donn\u00e9es contenant des donn\u00e9es de d\u00e9tection d'objets et de suivi multi-objets \u00e0 partir d'images captur\u00e9es par drone avec plus de 10K images et s\u00e9quences vid\u00e9o.</li> <li>VOC : L'ensemble de donn\u00e9es de classes d'objets visuels Pascal (VOC) pour la d\u00e9tection d'objets et la segmentation avec 20 classes d'objets et plus de 11K images.</li> <li>xView : Un ensemble de donn\u00e9es pour la d\u00e9tection d'objets dans l'imagerie a\u00e9rienne avec 60 cat\u00e9gories d'objets et plus d'un million d'objets annot\u00e9s.</li> </ul>"},{"location":"datasets/#ensembles-de-donnees-de-segmentation-dinstance","title":"Ensembles de donn\u00e9es de segmentation d'instance","text":"<p>La segmentation d'instance est une technique de vision par ordinateur qui consiste \u00e0 identifier et localiser des objets dans une image au niveau des pixels.</p> <ul> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle con\u00e7u pour la d\u00e9tection d'objets, la segmentation et les t\u00e2ches d'annotation avec plus de 200K images \u00e9tiquet\u00e9es.</li> <li>COCO8-seg : Un ensemble de donn\u00e9es plus petit pour les t\u00e2ches de segmentation d'instance, contenant un sous-ensemble de 8 images COCO avec des annotations de segmentation.</li> </ul>"},{"location":"datasets/#estimation-de-pose","title":"Estimation de pose","text":"<p>L'estimation de la pose est une technique utilis\u00e9e pour d\u00e9terminer la pose de l'objet par rapport \u00e0 la cam\u00e9ra ou au syst\u00e8me de coordonn\u00e9es mondial.</p> <ul> <li>COCO : Un ensemble de donn\u00e9es de grande \u00e9chelle avec des annotations de poses humaines con\u00e7u pour les t\u00e2ches d'estimation de la pose.</li> <li>COCO8-pose : Un ensemble de donn\u00e9es plus petit pour les t\u00e2ches d'estimation de la pose, contenant un sous-ensemble de 8 images COCO avec des annotations de pose humaine.</li> <li>Tiger-pose : Un ensemble de donn\u00e9es compact compos\u00e9 de 263 images centr\u00e9es sur les tigres, annot\u00e9es avec 12 points par tigre pour les t\u00e2ches d'estimation de la pose.</li> </ul>"},{"location":"datasets/#classification","title":"Classification","text":"<p>La classification d'images est une t\u00e2che de vision par ordinateur qui implique de cat\u00e9goriser une image dans une ou plusieurs classes ou cat\u00e9gories pr\u00e9d\u00e9finies en fonction de son contenu visuel.</p> <ul> <li>Caltech 101 : Un ensemble de donn\u00e9es contenant des images de 101 cat\u00e9gories d'objets pour les t\u00e2ches de classification d'images.</li> <li>Caltech 256 : Une version \u00e9tendue de Caltech 101 avec 256 cat\u00e9gories d'objets et des images plus complexes.</li> <li>CIFAR-10 : Un ensemble de donn\u00e9es de 60K images couleur 32x32 r\u00e9parties en 10 classes, avec 6K images par classe.</li> <li>CIFAR-100 : Une version \u00e9tendue de CIFAR-10 avec 100 cat\u00e9gories d'objets et 600 images par classe.</li> <li>Fashion-MNIST : Un ensemble de donn\u00e9es compos\u00e9 de 70 000 images en niveaux de gris de 10 cat\u00e9gories de mode pour les t\u00e2ches de classification d'images.</li> <li>ImageNet : Un ensemble de donn\u00e9es \u00e0 grande \u00e9chelle pour la d\u00e9tection d'objets et la classification d'images avec plus de 14 millions d'images et 20 000 cat\u00e9gories.</li> <li>ImageNet-10 : Un sous-ensemble plus petit d'ImageNet avec 10 cat\u00e9gories pour des exp\u00e9riences et des tests plus rapides.</li> <li>Imagenette : Un sous-ensemble plus petit d'ImageNet qui contient 10 classes facilement distinctes pour un entra\u00eenement et des tests plus rapides.</li> <li>Imagewoof : Un sous-ensemble d'ImageNet plus difficile contenant 10 cat\u00e9gories de races de chiens pour les t\u00e2ches de classification d'images.</li> <li>MNIST : Un ensemble de donn\u00e9es de 70 000 images en niveaux de gris de chiffres manuscrits pour les t\u00e2ches de classification d'images.</li> </ul>"},{"location":"datasets/#boites-englobantes-orientees-obb","title":"Bo\u00eetes Englobantes Orient\u00e9es (OBB)","text":"<p>Les Bo\u00eetes Englobantes Orient\u00e9es (OBB) sont une m\u00e9thode en vision par ordinateur pour d\u00e9tecter des objets inclin\u00e9s dans les images en utilisant des bo\u00eetes englobantes rotatives, souvent appliqu\u00e9e \u00e0 l'imagerie a\u00e9rienne et satellite.</p> <ul> <li>DOTAv2 : Un ensemble de donn\u00e9es d'imagerie a\u00e9rienne populaire avec 1.7 million d'instances et 11 268 images.</li> </ul>"},{"location":"datasets/#suivi-multi-objets","title":"Suivi Multi-Objets","text":"<p>Le suivi multi-objets est une technique de vision par ordinateur qui consiste \u00e0 d\u00e9tecter et suivre plusieurs objets dans le temps dans une s\u00e9quence vid\u00e9o.</p> <ul> <li>Argoverse : Un ensemble de donn\u00e9es contenant des donn\u00e9es de suivi 3D et de pr\u00e9vision de mouvement dans des environnements urbains avec des annotations d\u00e9taill\u00e9es pour les t\u00e2ches de suivi multi-objets.</li> <li>VisDrone : Un ensemble de donn\u00e9es contenant des donn\u00e9es de d\u00e9tection d'objets et de suivi multi-objets \u00e0 partir d'images captur\u00e9es par drone avec plus de 10K images et s\u00e9quences vid\u00e9o.</li> </ul>"},{"location":"datasets/#contribuer-de-nouveaux-ensembles-de-donnees","title":"Contribuer de Nouveaux Ensembles de Donn\u00e9es","text":"<p>Contribuer un nouvel ensemble de donn\u00e9es implique plusieurs \u00e9tapes pour s'assurer qu'il s'aligne bien avec l'infrastructure existante. Voici les \u00e9tapes n\u00e9cessaires :</p>"},{"location":"datasets/#etapes-pour-contribuer-un-nouvel-ensemble-de-donnees","title":"\u00c9tapes pour Contribuer un Nouvel Ensemble de Donn\u00e9es","text":"<ol> <li> <p>Collecter des Images : Rassemblez les images qui appartiennent \u00e0 l'ensemble de donn\u00e9es. Celles-ci pourraient \u00eatre collect\u00e9es \u00e0 partir de diff\u00e9rentes sources, telles que des bases de donn\u00e9es publiques ou votre propre collection.</p> </li> <li> <p>Annoter des Images : Annotez ces images avec des bo\u00eetes englobantes, des segments ou des points cl\u00e9s, en fonction de la t\u00e2che.</p> </li> <li> <p>Exporter des Annotations : Convertissez ces annotations au format de fichier YOLO *.txt pris en charge par Ultralytics.</p> </li> <li> <p>Organiser l'Ensemble de Donn\u00e9es : Rangez votre ensemble de donn\u00e9es dans la bonne structure de dossiers. Vous devriez avoir des r\u00e9pertoires de niveau sup\u00e9rieur <code>train/</code> et <code>val/</code>, et \u00e0 l'int\u00e9rieur de chacun, un sous-r\u00e9pertoire <code>images/</code> et <code>labels/</code>.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 labels/\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2514\u2500\u2500 labels/\n</code></pre> </li> <li> <p>Cr\u00e9er un Fichier <code>data.yaml</code> : Dans le r\u00e9pertoire racine de votre ensemble de donn\u00e9es, cr\u00e9ez un fichier <code>data.yaml</code> qui d\u00e9crit l'ensemble de donn\u00e9es, les classes et les autres informations n\u00e9cessaires.</p> </li> <li> <p>Optimiser les Images (Optionnel) : Si vous souhaitez r\u00e9duire la taille de l'ensemble de donn\u00e9es pour un traitement plus efficace, vous pouvez optimiser les images en utilisant le code ci-dessous. Ceci n'est pas requis, mais recommand\u00e9 pour des tailles d'ensemble de donn\u00e9es plus petites et des vitesses de t\u00e9l\u00e9chargement plus rapides.</p> </li> <li> <p>Zipper l'Ensemble de Donn\u00e9es : Compressez le dossier complet de l'ensemble de donn\u00e9es dans un fichier zip.</p> </li> <li> <p>Documenter et PR : Cr\u00e9ez une page de documentation d\u00e9crivant votre ensemble de donn\u00e9es et comment il s'int\u00e8gre dans le cadre existant. Apr\u00e8s cela, soumettez une Pull Request (PR). R\u00e9f\u00e9rez-vous aux lignes directrices de contribution Ultralytics pour plus de d\u00e9tails sur la mani\u00e8re de soumettre une PR.</p> </li> </ol>"},{"location":"datasets/#exemple-de-code-pour-optimiser-et-zipper-un-ensemble-de-donnees","title":"Exemple de Code pour Optimiser et Zipper un Ensemble de Donn\u00e9es","text":"<p>Optimiser et Zipper un Ensemble de Donn\u00e9es</p> Python <pre><code>from pathlib import Path\nfrom ultralytics.data.utils import compress_one_image\nfrom ultralytics.utils.downloads import zip_directory\n\n# D\u00e9finir le r\u00e9pertoire de l'ensemble de donn\u00e9es\npath = Path('chemin/vers/ensemble-de-donn\u00e9es')\n\n# Optimiser les images dans l'ensemble de donn\u00e9es (optionnel)\nfor f in path.rglob('*.jpg'):\n    compress_one_image(f)\n\n# Zipper l'ensemble de donn\u00e9es dans 'chemin/vers/ensemble-de-donn\u00e9es.zip'\nzip_directory(path)\n</code></pre> <p>En suivant ces \u00e9tapes, vous pouvez contribuer un nouvel ensemble de donn\u00e9es qui s'int\u00e8gre bien avec la structure existante d'Ultralytics.</p>"},{"location":"models/","title":"Mod\u00e8les pris en charge par Ultralytics","text":"<p>Bienvenue dans la documentation des mod\u00e8les d'Ultralytics ! Nous proposons une prise en charge d'une large gamme de mod\u00e8les, chacun adapt\u00e9 \u00e0 des t\u00e2ches sp\u00e9cifiques comme la d\u00e9tection d'objets, la segmentation d'instances, la classification d'images, l'estimation de posture et le suivi multi-objets. Si vous souhaitez contribuer avec votre architecture de mod\u00e8le \u00e0 Ultralytics, consultez notre Guide de Contribution.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en construction et nous travaillons activement \u00e0 l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p>"},{"location":"models/#modeles-en-vedette","title":"Mod\u00e8les en vedette","text":"<p>Voici quelques-uns des mod\u00e8les cl\u00e9s pris en charge :</p> <ol> <li>YOLOv3 : La troisi\u00e8me it\u00e9ration de la famille de mod\u00e8les YOLO, originellement par Joseph Redmon, reconnue pour ses capacit\u00e9s de d\u00e9tection d'objets en temps r\u00e9el efficaces.</li> <li>YOLOv4 : Une mise \u00e0 jour de YOLOv3 native de darknet, publi\u00e9e par Alexey Bochkovskiy en 2020.</li> <li>YOLOv5 : Une version am\u00e9lior\u00e9e de l'architecture YOLO par Ultralytics, offrant de meilleurs compromis de performance et de vitesse par rapport aux versions pr\u00e9c\u00e9dentes.</li> <li>YOLOv6 : Publi\u00e9 par Meituan en 2022, et utilis\u00e9 dans de nombreux robots de livraison autonomes de l'entreprise.</li> <li>YOLOv7 : Mod\u00e8les YOLO mis \u00e0 jour et sortis en 2022 par les auteurs de YOLOv4.</li> <li>YOLOv8 : La derni\u00e8re version de la famille YOLO, avec des capacit\u00e9s am\u00e9lior\u00e9es telles que la segmentation d\u2019instances, l'estimation de pose/points cl\u00e9s, et la classification.</li> <li>Segment Anything Model (SAM) : Le mod\u00e8le Segment Anything Model (SAM) de Meta.</li> <li>Mobile Segment Anything Model (MobileSAM) : MobileSAM pour les applications mobiles, par l'Universit\u00e9 de Kyung Hee.</li> <li>Fast Segment Anything Model (FastSAM) : FastSAM par le groupe d\u2019Analyse Image et Vid\u00e9o, Institut d'Automatisation, Acad\u00e9mie Chinoise des Sciences.</li> <li>YOLO-NAS : Mod\u00e8les YOLO Neural Architecture Search (NAS).</li> <li>Realtime Detection Transformers (RT-DETR) : Mod\u00e8les de Realtime Detection Transformer (RT-DETR) de Baidu's PaddlePaddle.</li> </ol> <p> Regardez : Ex\u00e9cutez les mod\u00e8les YOLO d'Ultralytics en seulement quelques lignes de code. </p>"},{"location":"models/#pour-commencer-exemples-dutilisation","title":"Pour commencer : Exemples d'utilisation","text":"<p>Exemple</p> PythonCLI <p>Les mod\u00e8les pr\u00e9entrain\u00e9s <code>*.pt</code> ainsi que les fichiers de configuration <code>*.yaml</code> peuvent \u00eatre pass\u00e9s aux classes <code>YOLO()</code>, <code>SAM()</code>, <code>NAS()</code> et <code>RTDETR()</code> pour cr\u00e9er une instance du mod\u00e8le en Python :</p> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9entrain\u00e9 sur COCO\nmodel = YOLO('yolov8n.pt')\n\n# Afficher les informations du mod\u00e8le (optionnel)\nmodel.info()\n\n# Entra\u00eener le mod\u00e8le sur l'exemple de jeu de donn\u00e9es COCO8 pendant 100 \u00e9poques\nresults = model.train(data='coco8.yaml', epochs=100, imgsz=640)\n\n# Ex\u00e9cuter l'inf\u00e9rence avec le mod\u00e8le YOLOv8n sur l'image 'bus.jpg'\nresults = model('path/to/bus.jpg')\n</code></pre> <p>Des commandes CLI sont disponibles pour ex\u00e9cuter directement les mod\u00e8les :</p> <pre><code># Charger un mod\u00e8le YOLOv8n pr\u00e9entrain\u00e9 sur COCO et l'entra\u00eener sur l'exemple de jeu de donn\u00e9es COCO8 pendant 100 \u00e9poques\nyolo train model=yolov8n.pt data=coco8.yaml epochs=100 imgsz=640\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9entrain\u00e9 sur COCO et ex\u00e9cuter l'inf\u00e9rence sur l'image 'bus.jpg'\nyolo predict model=yolov8n.pt source=path/to/bus.jpg\n</code></pre>"},{"location":"models/#contribuer-de-nouveaux-modeles","title":"Contribuer de nouveaux mod\u00e8les","text":"<p>Int\u00e9ress\u00e9 \u00e0 contribuer votre mod\u00e8le \u00e0 Ultralytics ? Super ! Nous sommes toujours ouverts \u00e0 l'expansion de notre portefeuille de mod\u00e8les.</p> <ol> <li> <p>Forker le R\u00e9pertoire : Commencez par forker le r\u00e9pertoire GitHub d'Ultralytics.</p> </li> <li> <p>Cloner Votre Fork : Clonez votre fork sur votre machine locale et cr\u00e9ez une nouvelle branche pour travailler dessus.</p> </li> <li> <p>Impl\u00e9menter Votre Mod\u00e8le : Ajoutez votre mod\u00e8le en suivant les standards et directives de codage fournis dans notre Guide de Contribution.</p> </li> <li> <p>Tester Rigoureusement : Assurez-vous de tester votre mod\u00e8le de mani\u00e8re rigoureuse, \u00e0 la fois isol\u00e9ment et en tant que partie du pipeline.</p> </li> <li> <p>Cr\u00e9er une Pull Request : Une fois que vous \u00eates satisfait de votre mod\u00e8le, cr\u00e9ez une demand\u0435 de tirage (pull request) vers le r\u00e9pertoire principal pour examen.</p> </li> <li> <p>Revue de Code &amp; Fusion : Apr\u00e8s la revue, si votre mod\u00e8le r\u00e9pond \u00e0 nos crit\u00e8res, il sera fusionn\u00e9 dans le r\u00e9pertoire principal.</p> </li> </ol> <p>Pour des \u00e9tapes d\u00e9taill\u00e9es, consultez notre Guide de Contribution.</p>"},{"location":"modes/","title":"Modes Ultralytics YOLOv8","text":""},{"location":"modes/#introduction","title":"Introduction","text":"<p>Ultralytics YOLOv8 n'est pas simplement un autre mod\u00e8le de d\u00e9tection d'objets ; c'est un cadre polyvalent con\u00e7u pour couvrir l'int\u00e9gralit\u00e9 du cycle de vie des mod\u00e8les d'apprentissage automatique \u2014 de l'ingestion de donn\u00e9es et l'entra\u00eenement des mod\u00e8les \u00e0 la validation, le d\u00e9ploiement et le suivi en conditions r\u00e9elles. Chaque mode remplit un objectif sp\u00e9cifique et est con\u00e7u pour vous offrir la flexibilit\u00e9 et l'efficacit\u00e9 n\u00e9cessaires pour diff\u00e9rentes t\u00e2ches et cas d'utilisation.</p> <p> Regardez : Tutoriel sur les modes Ultralytics : Entra\u00eenement, Validation, Pr\u00e9diction, Exportation &amp; Benchmark. </p>"},{"location":"modes/#apercu-des-modes","title":"Aper\u00e7u des Modes","text":"<p>Comprendre les diff\u00e9rents modes pris en charge par Ultralytics YOLOv8 est crucial pour tirer le maximum de vos mod\u00e8les :</p> <ul> <li>Mode d'entra\u00eenement (Train) : Affinez votre mod\u00e8le sur des jeux de donn\u00e9es personnalis\u00e9s ou pr\u00e9charg\u00e9s.</li> <li>Mode de validation (Val) : Un contr\u00f4le post-entra\u00eenement pour \u00e9valuer la performance du mod\u00e8le.</li> <li>Mode de pr\u00e9diction (Predict) : D\u00e9ployez la puissance pr\u00e9dictive de votre mod\u00e8le sur des donn\u00e9es du monde r\u00e9el.</li> <li>Mode d'exportation (Export) : Pr\u00e9parez votre mod\u00e8le au d\u00e9ploiement dans diff\u00e9rents formats.</li> <li>Mode de suivi (Track) : \u00c9tendez votre mod\u00e8le de d\u00e9tection d'objets \u00e0 des applications de suivi en temps r\u00e9el.</li> <li>Mode benchmark (Benchmark) : Analysez la vitesse et la pr\u00e9cision de votre mod\u00e8le dans divers environnements de d\u00e9ploiement.</li> </ul> <p>Ce guide complet vise \u00e0 vous donner un aper\u00e7u et des informations pratiques sur chaque mode, en vous aidant \u00e0 exploiter tout le potentiel de YOLOv8.</p>"},{"location":"modes/#entrainement-train","title":"Entra\u00eenement (Train)","text":"<p>Le mode d'entra\u00eenement est utilis\u00e9 pour entra\u00eener un mod\u00e8le YOLOv8 sur un jeu de donn\u00e9es personnalis\u00e9. Dans ce mode, le mod\u00e8le est entra\u00een\u00e9 en utilisant le jeu de donn\u00e9es et les hyperparam\u00e8tres sp\u00e9cifi\u00e9s. Le processus d'entra\u00eenement implique l'optimisation des param\u00e8tres du mod\u00e8le afin qu'il puisse pr\u00e9dire avec pr\u00e9cision les classes et les emplacements des objets dans une image.</p> <p>Exemples d'entra\u00eenement</p>"},{"location":"modes/#validation-val","title":"Validation (Val)","text":"<p>Le mode de validation est utilis\u00e9 pour valider un mod\u00e8le YOLOv8 apr\u00e8s qu'il ait \u00e9t\u00e9 entra\u00een\u00e9. Dans ce mode, le mod\u00e8le est \u00e9valu\u00e9 sur un ensemble de validation pour mesurer sa pr\u00e9cision et sa capacit\u00e9 de g\u00e9n\u00e9ralisation. Ce mode peut \u00eatre utilis\u00e9 pour ajuster les hyperparam\u00e8tres du mod\u00e8le afin d'am\u00e9liorer ses performances.</p> <p>Exemples de validation</p>"},{"location":"modes/#prediction-predict","title":"Pr\u00e9diction (Predict)","text":"<p>Le mode de pr\u00e9diction est utilis\u00e9 pour faire des pr\u00e9dictions \u00e0 l'aide d'un mod\u00e8le YOLOv8 entra\u00een\u00e9 sur de nouvelles images ou vid\u00e9os. Dans ce mode, le mod\u00e8le est charg\u00e9 \u00e0 partir d'un fichier de checkpoint, et l'utilisateur peut fournir des images ou vid\u00e9os pour effectuer l'inf\u00e9rence. Le mod\u00e8le pr\u00e9dit les classes et les emplacements des objets dans les images ou vid\u00e9os fournies.</p> <p>Exemples de pr\u00e9diction</p>"},{"location":"modes/#exportation-export","title":"Exportation (Export)","text":"<p>Le mode d'exportation est utilis\u00e9 pour exporter un mod\u00e8le YOLOv8 dans un format pouvant \u00eatre utilis\u00e9 pour le d\u00e9ploiement. Dans ce mode, le mod\u00e8le est converti dans un format pouvant \u00eatre utilis\u00e9 par d'autres applications logicielles ou dispositifs mat\u00e9riels. Ce mode est pratique pour d\u00e9ployer le mod\u00e8le dans des environnements de production.</p> <p>Exemples d'exportation</p>"},{"location":"modes/#suivi-track","title":"Suivi (Track)","text":"<p>Le mode de suivi est utilis\u00e9 pour suivre des objets en temps r\u00e9el \u00e0 l'aide d'un mod\u00e8le YOLOv8. Dans ce mode, le mod\u00e8le est charg\u00e9 \u00e0 partir d'un fichier de checkpoint, et l'utilisateur peut fournir un flux vid\u00e9o en direct pour effectuer le suivi d'objets en temps r\u00e9el. Ce mode est utile pour des applications telles que les syst\u00e8mes de surveillance ou les voitures autonomes.</p> <p>Exemples de suivi</p>"},{"location":"modes/#benchmark-benchmark","title":"Benchmark (Benchmark)","text":"<p>Le mode benchmark est utilis\u00e9 pour profiler la vitesse et la pr\u00e9cision de divers formats d'exportation pour YOLOv8. Les benchmarks fournissent des informations sur la taille du format export\u00e9, ses m\u00e9triques <code>mAP50-95</code> (pour la d\u00e9tection d'objets, la segmentation et la pose) ou <code>accuracy_top5</code> (pour la classification), et le temps d'inf\u00e9rence en millisecondes par image pour diff\u00e9rents formats d'exportation comme ONNX, OpenVINO, TensorRT et autres. Ces informations peuvent aider les utilisateurs \u00e0 choisir le format d'export optimal pour leur cas d'utilisation sp\u00e9cifique en fonction de leurs exigences de vitesse et de pr\u00e9cision.</p> <p>Exemples de benchmark</p>"},{"location":"modes/benchmark/","title":"Benchmarking de Mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/benchmark/#introduction","title":"Introduction","text":"<p>Une fois votre mod\u00e8le entra\u00een\u00e9 et valid\u00e9, l'\u00e9tape logique suivante est d'\u00e9valuer ses performances dans divers sc\u00e9narios du monde r\u00e9el. Le mode benchmark dans Ultralytics YOLOv8 r\u00e9pond \u00e0 cet objectif en fournissant un cadre robuste pour \u00e9valuer la vitesse et l'exactitude de votre mod\u00e8le sur une gamme de formats d'exportation.</p>"},{"location":"modes/benchmark/#pourquoi-le-benchmarking-est-il-crucial","title":"Pourquoi le Benchmarking est-il Crucial ?","text":"<ul> <li>D\u00e9cisions \u00c9clair\u00e9es : Obtenez des insights sur les arbitrages entre la vitesse et l'exactitude.</li> <li>Allocation des Ressources : Comprenez comment les diff\u00e9rents formats d'exportation se comportent sur diff\u00e9rents mat\u00e9riels.</li> <li>Optimisation : D\u00e9couvrez quel format d'exportation offre la meilleure performance pour votre cas d'utilisation sp\u00e9cifique.</li> <li>Efficacit\u00e9 des Co\u00fbts : Utilisez les ressources mat\u00e9rielles plus efficacement en vous basant sur les r\u00e9sultats des benchmarks.</li> </ul>"},{"location":"modes/benchmark/#mesures-cles-en-mode-benchmark","title":"Mesures Cl\u00e9s en Mode Benchmark","text":"<ul> <li>mAP50-95 : Pour la d\u00e9tection d'objets, la segmentation et l'estimation de pose.</li> <li>accuracy_top5 : Pour la classification d'images.</li> <li>Temps d'Inf\u00e9rence : Temps pris pour chaque image en millisecondes.</li> </ul>"},{"location":"modes/benchmark/#formats-dexportation-supportes","title":"Formats d'Exportation Support\u00e9s","text":"<ul> <li>ONNX : Pour une performance optimale sur CPU.</li> <li>TensorRT : Pour une efficacit\u00e9 maximale sur GPU.</li> <li>OpenVINO : Pour l'optimisation du mat\u00e9riel Intel.</li> <li>CoreML, TensorFlow SavedModel, et Plus : Pour des besoins vari\u00e9s de d\u00e9ploiement.</li> </ul> <p>Conseil</p> <ul> <li>Exportez vers ONNX ou OpenVINO pour un gain de vitesse CPU jusqu'\u00e0 3x.</li> <li>Exportez vers TensorRT pour un gain de vitesse GPU jusqu'\u00e0 5x.</li> </ul>"},{"location":"modes/benchmark/#exemples-dutilisation","title":"Exemples d'Utilisation","text":"<p>Ex\u00e9cutez les benchmarks YOLOv8n sur tous les formats d'exportation support\u00e9s, y compris ONNX, TensorRT, etc. Consultez la section Arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics.utils.benchmarks import benchmark\n\n# Benchmark sur GPU\nbenchmark(model='yolov8n.pt', data='coco8.yaml', imgsz=640, half=False, device=0)\n</code></pre> <pre><code>yolo benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n</code></pre>"},{"location":"modes/benchmark/#arguments","title":"Arguments","text":"<p>Des arguments tels que <code>model</code>, <code>data</code>, <code>imgsz</code>, <code>half</code>, <code>device</code> et <code>verbose</code> offrent aux utilisateurs la flexibilit\u00e9 d'ajuster pr\u00e9cis\u00e9ment les benchmarks \u00e0 leurs besoins sp\u00e9cifiques et de comparer facilement les performances de diff\u00e9rents formats d'exportation.</p> Cl\u00e9 Valeur Description <code>model</code> <code>None</code> chemin vers le fichier mod\u00e8le, par ex. yolov8n.pt, yolov8n.yaml <code>data</code> <code>None</code> chemin vers le YAML r\u00e9f\u00e9ren\u00e7ant le dataset de benchmarking (sous l'\u00e9tiquette <code>val</code>) <code>imgsz</code> <code>640</code> taille de l'image comme scalaire ou liste (h, w), par ex. (640, 480) <code>half</code> <code>False</code> quantification FP16 <code>int8</code> <code>False</code> quantification INT8 <code>device</code> <code>None</code> appareil sur lequel ex\u00e9cuter, par ex. appareil cuda=0 ou device=0,1,2,3 ou device=cpu <code>verbose</code> <code>False</code> ne pas continuer en cas d'erreur (bool), ou seuil de plancher val (float)"},{"location":"modes/benchmark/#formats-dexportation","title":"Formats d'Exportation","text":"<p>Les benchmarks tenteront de s'ex\u00e9cuter automatiquement sur tous les formats d'exportation possibles ci-dessous.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez les d\u00e9tails complets sur <code>export</code> dans la page Export.</p>"},{"location":"modes/export/","title":"Exportation de mod\u00e8le avec Ultralytics YOLO","text":""},{"location":"modes/export/#introduction","title":"Introduction","text":"<p>L'objectif ultime de l'entra\u00eenement d'un mod\u00e8le est de le d\u00e9ployer pour des applications dans le monde r\u00e9el. Le mode d'exportation de Ultralytics YOLOv8 offre une large gamme d'options pour exporter votre mod\u00e8le entra\u00een\u00e9 dans diff\u00e9rents formats, le rendant d\u00e9ployable sur diverses plateformes et appareils. Ce guide complet vise \u00e0 vous guider \u00e0 travers les nuances de l'exportation de mod\u00e8les, en montrant comment atteindre une compatibilit\u00e9 et des performances maximales.</p> <p> Regardez : Comment exporter un mod\u00e8le Ultralytics YOLOv8 entra\u00een\u00e9 personnalis\u00e9 et effectuer une inf\u00e9rence en direct sur webcam. </p>"},{"location":"modes/export/#pourquoi-choisir-le-mode-dexportation-yolov8","title":"Pourquoi choisir le mode d'exportation YOLOv8 ?","text":"<ul> <li>Polyvalence : Exportation vers plusieurs formats, y compris ONNX, TensorRT, CoreML et plus encore.</li> <li>Performance : Gagnez jusqu'\u00e0 5 fois la vitesse d'une GPU avec TensorRT et 3 fois la vitesse d'une CPU avec ONNX ou OpenVINO.</li> <li>Compatibilit\u00e9 : Rendez votre mod\u00e8le universellement d\u00e9ployable sur de nombreux environnements mat\u00e9riels et logiciels.</li> <li>Facilit\u00e9 d'utilisation : Interface en ligne de commande (CLI) et API Python simples pour une exportation rapide et directe du mod\u00e8le.</li> </ul>"},{"location":"modes/export/#caracteristiques-cles-du-mode-dexportation","title":"Caract\u00e9ristiques cl\u00e9s du mode d'exportation","text":"<p>Voici quelques-unes des fonctionnalit\u00e9s remarquables :</p> <ul> <li>Exportation en un clic : Commandes simples pour exporter vers diff\u00e9rents formats.</li> <li>Exportation group\u00e9e : Exportez des mod\u00e8les capables d'inf\u00e9rence par lot.</li> <li>Inf\u00e9rence optimis\u00e9e : Les mod\u00e8les export\u00e9s sont optimis\u00e9s pour des temps d'inf\u00e9rence plus rapides.</li> <li>Vid\u00e9os tutorielles : Guides d\u00e9taill\u00e9s et tutoriels pour une exp\u00e9rience d'exportation fluide.</li> </ul> <p>Conseil</p> <ul> <li>Exportez vers ONNX ou OpenVINO pour une acc\u00e9l\u00e9ration de la CPU jusqu'\u00e0 3 fois.</li> <li>Exportez vers TensorRT pour une acc\u00e9l\u00e9ration de la GPU jusqu'\u00e0 5 fois.</li> </ul>"},{"location":"modes/export/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Exportez un mod\u00e8le YOLOv8n vers un format diff\u00e9rent tel que ONNX ou TensorRT. Voir la section Arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # chargez un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # chargez un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporter mod\u00e8le officiel\nyolo export model=path/to/best.pt format=onnx  # exporter mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre>"},{"location":"modes/export/#arguments","title":"Arguments","text":"<p>Les param\u00e8tres d'exportation pour les mod\u00e8les YOLO se r\u00e9f\u00e8rent aux diverses configurations et options utilis\u00e9es pour sauvegarder ou exporter le mod\u00e8le pour utilisation dans d'autres environnements ou plateformes. Ces param\u00e8tres peuvent affecter la performance, la taille et la compatibilit\u00e9 du mod\u00e8le avec diff\u00e9rents syst\u00e8mes. Certains param\u00e8tres d'exportation YOLO courants incluent le format du fichier mod\u00e8le export\u00e9 (par exemple, ONNX, TensorFlow SavedModel), le dispositif sur lequel le mod\u00e8le sera ex\u00e9cut\u00e9 (par exemple, CPU, GPU), et la pr\u00e9sence de fonctionnalit\u00e9s suppl\u00e9mentaires telles que des masques ou des \u00e9tiquettes multiples par bo\u00eete. D'autres facteurs qui peuvent affecter le processus d'exportation incluent la t\u00e2che sp\u00e9cifique pour laquelle le mod\u00e8le est utilis\u00e9 et les exigences ou contraintes de l'environnement ou de la plateforme cible. Il est important de consid\u00e9rer et de configurer ces param\u00e8tres avec soin pour s'assurer que le mod\u00e8le export\u00e9 est optimis\u00e9 pour le cas d'utilisation vis\u00e9 et peut \u00eatre utilis\u00e9 efficacement dans l'environnement cible.</p> Cl\u00e9 Valeur Description <code>format</code> <code>'torchscript'</code> format vers lequel exporter <code>imgsz</code> <code>640</code> taille d'image sous forme scalaire ou liste (h, w), par ex. (640, 480) <code>keras</code> <code>False</code> utilisez Keras pour l'exportation TensorFlow SavedModel <code>optimize</code> <code>False</code> TorchScript : optimisation pour mobile <code>half</code> <code>False</code> quantification FP16 <code>int8</code> <code>False</code> quantification INT8 <code>dynamic</code> <code>False</code> ONNX/TensorRT : axes dynamiques <code>simplify</code> <code>False</code> ONNX/TensorRT : simplifier le mod\u00e8le <code>opset</code> <code>None</code> ONNX : version de l'ensemble d'op\u00e9rations (facultatif, par d\u00e9faut \u00e0 la derni\u00e8re) <code>workspace</code> <code>4</code> TensorRT : taille de l'espace de travail (GB) <code>nms</code> <code>False</code> CoreML : ajout de la NMS"},{"location":"modes/export/#formats-dexportation","title":"Formats d'exportation","text":"<p>Les formats d'exportation disponibles pour YOLOv8 sont dans le tableau ci-dessous. Vous pouvez exporter vers n'importe quel format en utilisant l'argument <code>format</code>, par ex. <code>format='onnx'</code> ou <code>format='engine'</code>.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code>"},{"location":"modes/predict/","title":"Pr\u00e9diction de Mod\u00e8le avec Ultralytics YOLO","text":""},{"location":"modes/predict/#introduction","title":"Introduction","text":"<p>Dans l'univers de l'apprentissage automatique et de la vision par ordinateur, le processus de donner du sens aux donn\u00e9es visuelles est appel\u00e9 'inf\u00e9rence' ou 'pr\u00e9diction'. Ultralytics YOLOv8 propose une fonctionnalit\u00e9 puissante connue sous le nom de mode de pr\u00e9diction adapt\u00e9 pour l'inf\u00e9rence en temps r\u00e9el et haute performance sur une large gamme de sources de donn\u00e9es.</p> <p> Regardez : Comment Extraire les Sorties du Mod\u00e8le Ultralytics YOLOv8 pour des Projets Personnalis\u00e9s. </p>"},{"location":"modes/predict/#applications-reelles","title":"Applications R\u00e9elles","text":"Fabrication Sports S\u00e9curit\u00e9 D\u00e9tection des Pi\u00e8ces de V\u00e9hicules D\u00e9tection des Joueurs de Football D\u00e9tection de Chutes de Personnes"},{"location":"modes/predict/#pourquoi-utiliser-ultralytics-yolo-pour-linference","title":"Pourquoi Utiliser Ultralytics YOLO pour l'Inf\u00e9rence ?","text":"<p>Voici pourquoi vous devriez consid\u00e9rer le mode de pr\u00e9diction YOLOv8 pour vos besoins vari\u00e9s en inf\u00e9rence :</p> <ul> <li>Polyvalence : Capable de faire des inf\u00e9rences sur des images, des vid\u00e9os et m\u00eame des flux en direct.</li> <li>Performance : Con\u00e7u pour le traitement en temps r\u00e9el \u00e0 grande vitesse sans sacrifier la pr\u00e9cision.</li> <li>Facilit\u00e9 d'Utilisation : Interfaces Python et CLI intuitives pour un d\u00e9ploiement et des tests rapides.</li> <li>Tr\u00e8s Personnalisable : Divers param\u00e8tres et r\u00e9glages pour ajuster le comportement d'inf\u00e9rence du mod\u00e8le selon vos besoins sp\u00e9cifiques.</li> </ul>"},{"location":"modes/predict/#caracteristiques-cles-du-mode-de-prediction","title":"Caract\u00e9ristiques Cl\u00e9s du Mode de Pr\u00e9diction","text":"<p>Le mode de pr\u00e9diction YOLOv8 est con\u00e7u pour \u00eatre robuste et polyvalent, avec des fonctionnalit\u00e9s telles que :</p> <ul> <li>Compatibilit\u00e9 avec Plusieurs Sources de Donn\u00e9es : Que vos donn\u00e9es soient sous forme d'images individuelles, d'une collection d'images, de fichiers vid\u00e9o ou de flux vid\u00e9o en temps r\u00e9el, le mode de pr\u00e9diction r\u00e9pond \u00e0 vos besoins.</li> <li>Mode Streaming : Utilisez la fonctionnalit\u00e9 de streaming pour g\u00e9n\u00e9rer un g\u00e9n\u00e9rateur efficace en termes de m\u00e9moire d'objets <code>Results</code>. Activez-le en r\u00e9glant <code>stream=True</code> dans la m\u00e9thode d'appel du pr\u00e9dicteur.</li> <li>Traitement par Lots : La capacit\u00e9 de traiter plusieurs images ou trames vid\u00e9o dans un seul lot, acc\u00e9l\u00e9rant ainsi le temps d'inf\u00e9rence.</li> <li>Facile \u00e0 Int\u00e9grer : S'int\u00e8gre facilement dans les pipelines de donn\u00e9es existants et autres composants logiciels, gr\u00e2ce \u00e0 son API souple.</li> </ul> <p>Les mod\u00e8les YOLO d'Ultralytics renvoient soit une liste d'objets <code>Results</code> Python, soit un g\u00e9n\u00e9rateur Python efficace en termes de m\u00e9moire d'objets <code>Results</code> lorsque <code>stream=True</code> est pass\u00e9 au mod\u00e8le pendant l'inf\u00e9rence :</p> <p>Pr\u00e9dire</p> Renvoie une liste avec <code>stream=False</code>Renvoie un g\u00e9n\u00e9rateur avec <code>stream=True</code> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\n\n# Ex\u00e9cuter une inf\u00e9rence par lots sur une liste d'images\nresults = model(['im1.jpg', 'im2.jpg'])  # renvoie une liste d'objets Results\n\n# Traiter la liste des r\u00e9sultats\nfor result in results:\n    boxes = result.boxes  # Objet Boxes pour les sorties bbox\n    masks = result.masks  # Objet Masks pour les masques de segmentation\n    keypoints = result.keypoints  # Objet Keypoints pour les sorties de pose\n    probs = result.probs  # Objet Probs pour les sorties de classification\n</code></pre> <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\n\n# Ex\u00e9cuter une inf\u00e9rence par lots sur une liste d'images\nresults = model(['im1.jpg', 'im2.jpg'], stream=True)  # renvoie un g\u00e9n\u00e9rateur d'objets Results\n\n# Traiter le g\u00e9n\u00e9rateur de r\u00e9sultats\nfor result in results:\n    boxes = result.boxes  # Objet Boxes pour les sorties bbox\n    masks = result.masks  # Objet Masks pour les masques de segmentation\n    keypoints = result.keypoints  # Objet Keypoints pour les sorties de pose\n    probs = result.probs  # Objet Probs pour les sorties de classification\n</code></pre>"},{"location":"modes/predict/#sources-dinference","title":"Sources d'Inf\u00e9rence","text":"<p>YOLOv8 peut traiter diff\u00e9rents types de sources d'entr\u00e9e pour l'inf\u00e9rence, comme illustr\u00e9 dans le tableau ci-dessous. Les sources incluent des images statiques, des flux vid\u00e9os et divers formats de donn\u00e9es. Le tableau indique \u00e9galement si chaque source peut \u00eatre utilis\u00e9e en mode streaming avec l'argument <code>stream=True</code> \u2705. Le mode streaming est b\u00e9n\u00e9fique pour traiter des vid\u00e9os ou des flux en direct car il cr\u00e9e un g\u00e9n\u00e9rateur de r\u00e9sultats au lieu de charger tous les cadres en m\u00e9moire.</p> <p>Astuce</p> <p>Utilisez <code>stream=True</code> pour traiter des vid\u00e9os longues ou des jeux de donn\u00e9es volumineux afin de g\u00e9rer efficacement la m\u00e9moire. Quand <code>stream=False</code>, les r\u00e9sultats pour tous les cadres ou points de donn\u00e9es sont stock\u00e9s en m\u00e9moire, ce qui peut rapidement s'accumuler et provoquer des erreurs de m\u00e9moire insuffisante pour de grandes entr\u00e9es. En revanche, <code>stream=True</code> utilise un g\u00e9n\u00e9rateur, qui ne garde que les r\u00e9sultats du cadre ou point de donn\u00e9es actuel en m\u00e9moire, r\u00e9duisant consid\u00e9rablement la consommation de m\u00e9moire et pr\u00e9venant les probl\u00e8mes de m\u00e9moire insuffisante.</p> Source Argument Type Notes image <code>'image.jpg'</code> <code>str</code> ou <code>Path</code> Fichier image unique. URL <code>'https://ultralytics.com/images/bus.jpg'</code> <code>str</code> URL vers une image. capture d'\u00e9cran <code>'screen'</code> <code>str</code> Prendre une capture d'\u00e9cran. PIL <code>Image.open('im.jpg')</code> <code>PIL.Image</code> Format HWC avec canaux RGB. OpenCV <code>cv2.imread('im.jpg')</code> <code>np.ndarray</code> Format HWC avec canaux BGR <code>uint8 (0-255)</code>. numpy <code>np.zeros((640,1280,3))</code> <code>np.ndarray</code> Format HWC avec canaux BGR <code>uint8 (0-255)</code>. torch <code>torch.zeros(16,3,320,640)</code> <code>torch.Tensor</code> Format BCHW avec canaux RGB <code>float32 (0.0-1.0)</code>. CSV <code>'sources.csv'</code> <code>str</code> ou <code>Path</code> Fichier CSV contenant des chemins vers des images, vid\u00e9os ou r\u00e9pertoires. vid\u00e9o \u2705 <code>'video.mp4'</code> <code>str</code> ou <code>Path</code> Fichier vid\u00e9o dans des formats comme MP4, AVI, etc. r\u00e9pertoire \u2705 <code>'chemin/'</code> <code>str</code> ou <code>Path</code> Chemin vers un r\u00e9pertoire contenant des images ou des vid\u00e9os. motif global \u2705 <code>'chemin/*.jpg'</code> <code>str</code> Motif glob pour faire correspondre plusieurs fichiers. Utilisez le caract\u00e8re <code>*</code> comme joker. YouTube \u2705 <code>'https://youtu.be/LNwODJXcvt4'</code> <code>str</code> URL vers une vid\u00e9o YouTube. flux \u2705 <code>'rtsp://exemple.com/media.mp4'</code> <code>str</code> URL pour des protocoles de streaming comme RTSP, RTMP, TCP, ou une adresse IP. multi-flux \u2705 <code>'liste.streams'</code> <code>str</code> ou <code>Path</code> Fichier texte <code>*.streams</code> avec une URL de flux par ligne, c'est-\u00e0-dire que 8 flux s'ex\u00e9cuteront avec une taille de lot de 8. <p>Ci-dessous des exemples de code pour utiliser chaque type de source :</p> <p>Sources de pr\u00e9diction</p> imagecapture d'\u00e9cranURLPILOpenCVnumpytorch <p>Ex\u00e9cutez une inf\u00e9rence sur un fichier image. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir le chemin vers le fichier image\nsource = 'chemin/vers/image.jpg'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur le contenu actuel de l'\u00e9cran sous forme de capture d'\u00e9cran. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir la capture d'\u00e9cran actuelle comme source\nsource = 'screen'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image ou vid\u00e9o h\u00e9berg\u00e9e \u00e0 distance via URL. <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# D\u00e9finir l'URL d'une image ou vid\u00e9o distante\nsource = 'https://ultralytics.com/images/bus.jpg'\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image ouverte avec la biblioth\u00e8que Python Imaging Library (PIL). <pre><code>from PIL import Image\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Ouvrir une image avec PIL\nsource = Image.open('chemin/vers/image.jpg')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image lue avec OpenCV. <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Lire une image avec OpenCV\nsource = cv2.imread('chemin/vers/image.jpg')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image repr\u00e9sent\u00e9e sous forme de tableau numpy. <pre><code>import numpy as np\nfrom ultralytics import YOLO\n\n# Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9\nmodel = YOLO('yolov8n.pt')\n\n# Cr\u00e9er un tableau numpy al\u00e9atoire de forme HWC (640, 640, 3) avec des valeurs dans l'intervalle [0, 255] et de type uint8\nsource = np.random.randint(low=0, high=255, size=(640, 640, 3), dtype='uint8')\n\n# Ex\u00e9cuter une inf\u00e9rence sur la source\nresults = model(source)  # liste d'objets Results\n</code></pre></p> <p>Ex\u00e9cutez une inf\u00e9rence sur une image repr\u00e9sent\u00e9e sous forme de tenseur PyTorch. ```python import torch from ultralytics import YOLO</p>"},{"location":"modes/predict/#charger-un-modele-yolov8n-pre-entraine","title":"Charger un mod\u00e8le YOLOv8n pr\u00e9-entra\u00een\u00e9","text":"<p>model = YOLO('yolov8n.pt')</p>"},{"location":"modes/predict/#creer-un-tenseur-aleatoire-torch-de-forme-bchw-1-3-640-640-avec-des-valeurs-dans-lintervalle-0-1-et-de-type-float32","title":"Cr\u00e9er un tenseur al\u00e9atoire torch de forme BCHW (1, 3, 640, 640) avec des valeurs dans l'intervalle [0, 1] et de type float32","text":"<p>source = torch.rand(1, 3, 640, 640, dtype=torch.float32)</p>"},{"location":"modes/predict/#executer-une-inference-sur-la-source","title":"Ex\u00e9cuter une inf\u00e9rence sur la source","text":"<p>results = model(source)  # liste d'objets Results</p>"},{"location":"modes/track/","title":"Suivi Multi-Objets avec Ultralytics YOLO","text":"<p>Le suivi d'objets dans le domaine de l'analyse vid\u00e9o est une t\u00e2che essentielle qui non seulement identifie l'emplacement et la classe des objets \u00e0 l'int\u00e9rieur de l'image, mais maintient \u00e9galement un identifiant unique pour chaque objet d\u00e9tect\u00e9 au fur et \u00e0 mesure que la vid\u00e9o progresse. Les applications sont illimit\u00e9es, allant de la surveillance et de la s\u00e9curit\u00e9 \u00e0 l'analytique sportive en temps r\u00e9el.</p>"},{"location":"modes/track/#pourquoi-choisir-ultralytics-yolo-pour-le-suivi-dobjet","title":"Pourquoi Choisir Ultralytics YOLO pour le Suivi d'Objet ?","text":"<p>La sortie des traceurs Ultralytics est coh\u00e9rente avec la d\u00e9tection standard d'objets mais apporte la valeur ajout\u00e9e des identifiants d'objets. Cela facilite le suivi des objets dans les flux vid\u00e9o et effectue des analyses subs\u00e9quentes. Voici pourquoi vous devriez envisager d'utiliser Ultralytics YOLO pour vos besoins de suivi d'objet :</p> <ul> <li>Efficacit\u00e9 : Traitez les flux vid\u00e9o en temps r\u00e9el sans compromettre la pr\u00e9cision.</li> <li>Flexibilit\u00e9 : Prend en charge de multiples algorithmes de suivi et configurations.</li> <li>Facilit\u00e9 d'Utilisation : API Python simple et options CLI pour une int\u00e9gration et un d\u00e9ploiement rapides.</li> <li>Personnalisabilit\u00e9 : Facile \u00e0 utiliser avec des mod\u00e8les YOLO entra\u00een\u00e9s sur mesure, permettant une int\u00e9gration dans des applications sp\u00e9cifiques au domaine.</li> </ul> <p> Regardez : D\u00e9tection et suivi d'objets avec Ultralytics YOLOv8. </p>"},{"location":"modes/track/#applications-dans-le-monde-reel","title":"Applications dans le Monde R\u00e9el","text":"Transport Distribution Aquaculture Suivi de V\u00e9hicules Suivi de Personnes Suivi de Poissons"},{"location":"modes/track/#caracteristiques-en-bref","title":"Caract\u00e9ristiques en Bref","text":"<p>Ultralytics YOLO \u00e9tend ses fonctionnalit\u00e9s de d\u00e9tection d'objets pour fournir un suivi d'objets robuste et polyvalent :</p> <ul> <li>Suivi en Temps R\u00e9el : Suivi fluide d'objets dans des vid\u00e9os \u00e0 fr\u00e9quence d'images \u00e9lev\u00e9e.</li> <li>Prise en Charge de Multiples Traceurs : Choisissez parmi une vari\u00e9t\u00e9 d'algorithmes de suivi \u00e9prouv\u00e9s.</li> <li>Configurations de Traceurs Personnalisables : Adaptez l'algorithme de suivi pour r\u00e9pondre \u00e0 des exigences sp\u00e9cifiques en r\u00e9glant divers param\u00e8tres.</li> </ul>"},{"location":"modes/track/#traceurs-disponibles","title":"Traceurs Disponibles","text":"<p>Ultralytics YOLO prend en charge les algorithmes de suivi suivants. Ils peuvent \u00eatre activ\u00e9s en passant le fichier de configuration YAML correspondant tel que <code>tracker=tracker_type.yaml</code> :</p> <ul> <li>BoT-SORT - Utilisez <code>botsort.yaml</code> pour activer ce traceur.</li> <li>ByteTrack - Utilisez <code>bytetrack.yaml</code> pour activer ce traceur.</li> </ul> <p>Le traceur par d\u00e9faut est BoT-SORT.</p>"},{"location":"modes/track/#suivi","title":"Suivi","text":"<p>Pour ex\u00e9cuter le traceur sur des flux vid\u00e9o, utilisez un mod\u00e8le Detect, Segment ou Pose form\u00e9 tel que YOLOv8n, YOLOv8n-seg et YOLOv8n-pose.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le officiel ou personnalis\u00e9\nmodel = YOLO('yolov8n.pt')  # Charger un mod\u00e8le Detect officiel\nmodel = YOLO('yolov8n-seg.pt')  # Charger un mod\u00e8le Segment officiel\nmodel = YOLO('yolov8n-pose.pt')  # Charger un mod\u00e8le Pose officiel\nmodel = YOLO('chemin/vers/best.pt')  # Charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Effectuer le suivi avec le mod\u00e8le\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True)  # Suivi avec le traceur par d\u00e9faut\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", show=True, tracker=\"bytetrack.yaml\")  # Suivi avec le traceur ByteTrack\n</code></pre> <pre><code># Effectuer le suivi avec divers mod\u00e8les en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Detect officiel\nyolo track model=yolov8n-seg.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Segment officiel\nyolo track model=yolov8n-pose.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le Pose officiel\nyolo track model=chemin/vers/best.pt source=\"https://youtu.be/LNwODJXcvt4\"  # Mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Suivi en utilisant le traceur ByteTrack\nyolo track model=chemin/vers/best.pt tracker=\"bytetrack.yaml\"\n</code></pre> <p>Comme on peut le voir dans l'utilisation ci-dessus, le suivi est disponible pour tous les mod\u00e8les Detect, Segment et Pose ex\u00e9cut\u00e9s sur des vid\u00e9os ou des sources de diffusion.</p>"},{"location":"modes/track/#configuration","title":"Configuration","text":""},{"location":"modes/track/#arguments-de-suivi","title":"Arguments de Suivi","text":"<p>La configuration du suivi partage des propri\u00e9t\u00e9s avec le mode Pr\u00e9diction, telles que <code>conf</code>, <code>iou</code>, et <code>show</code>. Pour des configurations suppl\u00e9mentaires, r\u00e9f\u00e9rez-vous \u00e0 la page Predict du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Configurer les param\u00e8tres de suivi et ex\u00e9cuter le traceur\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", conf=0.3, iou=0.5, show=True)\n</code></pre> <pre><code># Configurer les param\u00e8tres de suivi et ex\u00e9cuter le traceur en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" conf=0.3, iou=0.5 show\n</code></pre>"},{"location":"modes/track/#selection-du-traceur","title":"S\u00e9lection du Traceur","text":"<p>Ultralytics vous permet \u00e9galement d'utiliser un fichier de configuration de traceur modifi\u00e9. Pour cela, faites simplement une copie d'un fichier de configuration de traceur (par exemple, <code>custom_tracker.yaml</code>) \u00e0 partir de ultralytics/cfg/trackers et modifiez toute configuration (\u00e0 l'exception du <code>tracker_type</code>) selon vos besoins.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger le mod\u00e8le et ex\u00e9cuter le traceur avec un fichier de configuration personnalis\u00e9\nmodel = YOLO('yolov8n.pt')\nresults = model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker='custom_tracker.yaml')\n</code></pre> <pre><code># Charger le mod\u00e8le et ex\u00e9cuter le traceur avec un fichier de configuration personnalis\u00e9 en utilisant l'interface en ligne de commande\nyolo track model=yolov8n.pt source=\"https://youtu.be/LNwODJXcvt4\" tracker='custom_tracker.yaml'\n</code></pre> <p>Pour une liste compl\u00e8te des arguments de suivi, r\u00e9f\u00e9rez-vous \u00e0 la page ultralytics/cfg/trackers.</p>"},{"location":"modes/track/#exemples-python","title":"Exemples Python","text":""},{"location":"modes/track/#boucle-de-persistance-des-pistes","title":"Boucle de Persistance des Pistes","text":"<p>Voici un script Python utilisant OpenCV (<code>cv2</code>) et YOLOv8 pour ex\u00e9cuter le suivi d'objet sur des images vid\u00e9o. Ce script suppose toujours que vous avez d\u00e9j\u00e0 install\u00e9 les packages n\u00e9cessaires (<code>opencv-python</code> et <code>ultralytics</code>). L'argument <code>persist=True</code> indique au traceur que l'image ou la trame actuelle est la suivante dans une s\u00e9quence et s'attend \u00e0 ce que les pistes de l'image pr\u00e9c\u00e9dente soient pr\u00e9sentes dans l'image actuelle.</p> <p>Boucle for streaming avec suivi</p> <pre><code>import cv2\nfrom ultralytics import YOLO\n\n# Charger le mod\u00e8le YOLOv8\nmodel = YOLO('yolov8n.pt')\n\n# Ouvrir le fichier vid\u00e9o\nvideo_path = \"chemin/vers/video.mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Parcourir les images vid\u00e9o\nwhile cap.isOpened():\n    # Lire une image de la vid\u00e9o\n    success, frame = cap.read()\n\n    if success:\n        # Ex\u00e9cuter le suivi YOLOv8 sur l'image, en persistant les pistes entre les images\n        results = model.track(frame, persist=True)\n\n        # Visualiser les r\u00e9sultats sur l'image\n        annotated_frame = results[0].plot()\n\n        # Afficher l'image annot\u00e9e\n        cv2.imshow(\"Suivi YOLOv8\", annotated_frame)\n\n        # Interrompre la boucle si 'q' est press\u00e9e\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Interrompre la boucle si la fin de la vid\u00e9o est atteinte\n        break\n\n# Rel\u00e2cher l'objet de capture vid\u00e9o et fermer la fen\u00eatre d'affichage\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Veuillez noter le changement de <code>model(frame)</code> \u00e0 <code>model.track(frame)</code>, qui active le suivi d'objet \u00e0 la place de la simple d\u00e9tection. Ce script modifi\u00e9 ex\u00e9cutera le traceur sur chaque image de la vid\u00e9o, visualisera les r\u00e9sultats et les affichera dans une fen\u00eatre. La boucle peut \u00eatre quitt\u00e9e en appuyant sur 'q'.</p>"},{"location":"modes/track/#contribuer-de-nouveaux-traceurs","title":"Contribuer de Nouveaux Traceurs","text":"<p>\u00cates-vous comp\u00e9tent en suivi multi-objets et avez-vous r\u00e9ussi \u00e0 impl\u00e9menter ou adapter un algorithme de suivi avec Ultralytics YOLO ? Nous vous invitons \u00e0 contribuer \u00e0 notre section Traceurs sur ultralytics/cfg/trackers ! Vos applications et solutions dans le monde r\u00e9el pourraient \u00eatre inestimables pour les utilisateurs travaillant sur des t\u00e2ches de suivi.</p> <p>En contribuant \u00e0 cette section, vous aidez \u00e0 \u00e9largir l'\u00e9ventail des solutions de suivi disponibles au sein du cadre Ultralytics YOLO, ajoutant une autre couche de fonctionnalit\u00e9 et d'utilit\u00e9 pour la communaut\u00e9.</p> <p>Pour initier votre contribution, veuillez vous r\u00e9f\u00e9rer \u00e0 notre Guide de Contribution pour des instructions compl\u00e8tes sur la soumission d'une Pull Request (PR) \ud83d\udee0\ufe0f. Nous sommes impatients de voir ce que vous apportez \u00e0 la table !</p> <p>Ensemble, am\u00e9liorons les capacit\u00e9s de suivi de l'\u00e9cosyst\u00e8me Ultralytics YOLO \ud83d\ude4f !</p>"},{"location":"modes/train/","title":"Entra\u00eenement de mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/train/#introduction","title":"Introduction","text":"<p>L'entra\u00eenement d'un mod\u00e8le d'apprentissage profond implique de lui fournir des donn\u00e9es et d'ajuster ses param\u00e8tres afin qu'il puisse faire des pr\u00e9dictions pr\u00e9cises. Le mode Entra\u00eenement de Ultralytics YOLOv8 est con\u00e7u pour un entra\u00eenement efficace et performant de mod\u00e8les de d\u00e9tection d'objets, en utilisant pleinement les capacit\u00e9s du mat\u00e9riel moderne. Ce guide vise \u00e0 couvrir tous les d\u00e9tails n\u00e9cessaires pour commencer \u00e0 entra\u00eener vos propres mod\u00e8les en utilisant l'ensemble robuste de fonctionnalit\u00e9s de YOLOv8.</p> <p> Regardez : Comment entra\u00eener un mod\u00e8le YOLOv8 sur votre jeu de donn\u00e9es personnalis\u00e9 dans Google Colab. </p>"},{"location":"modes/train/#pourquoi-choisir-ultralytics-yolo-pour-lentrainement","title":"Pourquoi choisir Ultralytics YOLO pour l'entra\u00eenement ?","text":"<p>Voici quelques raisons convaincantes de choisir le mode Entra\u00eenement de YOLOv8 :</p> <ul> <li>Efficacit\u00e9 : Optimisez l'utilisation de votre mat\u00e9riel, que vous soyez sur une configuration mono-GPU ou que vous \u00e9chelonnier sur plusieurs GPUs.</li> <li>Polyvalence : Entra\u00eenez sur des jeux de donn\u00e9es personnalis\u00e9s en plus de ceux d\u00e9j\u00e0 disponibles comme COCO, VOC et ImageNet.</li> <li>Convivialit\u00e9 : Interfaces CLI et Python simples mais puissantes pour une exp\u00e9rience d'entra\u00eenement directe.</li> <li>Flexibilit\u00e9 des hyperparam\u00e8tres : Un large \u00e9ventail d'hyperparam\u00e8tres personnalisables pour peaufiner les performances du mod\u00e8le.</li> </ul>"},{"location":"modes/train/#principales-caracteristiques-du-mode-entrainement","title":"Principales caract\u00e9ristiques du mode Entra\u00eenement","text":"<p>Voici quelques caract\u00e9ristiques remarquables du mode Entra\u00eenement de YOLOv8 :</p> <ul> <li>T\u00e9l\u00e9chargement automatique de jeux de donn\u00e9es : Les jeux de donn\u00e9es standards comme COCO, VOC et ImageNet sont t\u00e9l\u00e9charg\u00e9s automatiquement lors de la premi\u00e8re utilisation.</li> <li>Support multi-GPU : \u00c9chelonnez vos efforts de formation de mani\u00e8re fluide sur plusieurs GPUs pour acc\u00e9l\u00e9rer le processus.</li> <li>Configuration des hyperparam\u00e8tres : La possibilit\u00e9 de modifier les hyperparam\u00e8tres via des fichiers de configuration YAML ou des arguments CLI.</li> <li>Visualisation et suivi : Suivi en temps r\u00e9el des m\u00e9triques d'entra\u00eenement et visualisation du processus d'apprentissage pour de meilleures perspectives.</li> </ul> <p>Astuce</p> <ul> <li>Les jeux de donn\u00e9es YOLOv8 comme COCO, VOC, ImageNet et bien d'autres se t\u00e9l\u00e9chargent automatiquement lors de la premi\u00e8re utilisation, par exemple <code>yolo train data=coco.yaml</code></li> </ul>"},{"location":"modes/train/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Entra\u00eenez YOLOv8n sur le jeu de donn\u00e9es COCO128 pendant 100 \u00e9poques avec une taille d'image de 640. Le dispositif d'entra\u00eenement peut \u00eatre sp\u00e9cifi\u00e9 \u00e0 l'aide de l'argument <code>device</code>. Si aucun argument n'est pass\u00e9, le GPU <code>device=0</code> sera utilis\u00e9 s'il est disponible, sinon <code>device=cpu</code> sera utilis\u00e9. Consultez la section Arguments ci-dessous pour obtenir une liste compl\u00e8te des arguments d'entra\u00eenement.</p> <p>Exemple d'entra\u00eenement mono-GPU et CPU</p> <p>Le dispositif est d\u00e9termin\u00e9 automatiquement. Si un GPU est disponible, il sera utilis\u00e9, sinon l'entra\u00eenement commencera sur CPU.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir de YAML\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construire \u00e0 partir de YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir de YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir de YAML, transf\u00e9rer les poids pr\u00e9entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"modes/train/#entrainement-multi-gpu","title":"Entra\u00eenement multi-GPU","text":"<p>L'entra\u00eenement multi-GPU permet une utilisation plus efficace des ressources mat\u00e9rielles disponibles en r\u00e9partissant la charge d'entra\u00eenement sur plusieurs GPUs. Cette fonctionnalit\u00e9 est disponible via l'API Python et l'interface de ligne de commande. Pour activer l'entra\u00eenement multi-GPU, sp\u00e9cifiez les ID des dispositifs GPU que vous souhaitez utiliser.</p> <p>Exemple d'entra\u00eenement multi-GPU</p> <p>Pour s'entra\u00eener avec 2 GPUs, les dispositifs CUDA 0 et 1, utilisez les commandes suivantes. D\u00e9veloppez \u00e0 des GPUs suppl\u00e9mentaires selon le besoin.</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\n\n# Entra\u00eener le mod\u00e8le avec 2 GPUs\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])\n</code></pre> <pre><code># Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt en utilisant les GPUs 0 et 1\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1\n</code></pre>"},{"location":"modes/train/#entrainement-mps-avec-apple-m1-et-m2","title":"Entra\u00eenement MPS avec Apple M1 et M2","text":"<p>Avec le support pour les puces Apple M1 et M2 int\u00e9gr\u00e9 dans les mod\u00e8les Ultralytics YOLO, il est maintenant possible d'entra\u00eener vos mod\u00e8les sur des dispositifs utilisant le puissant framework Metal Performance Shaders (MPS). Le MPS offre un moyen performant d'ex\u00e9cuter des t\u00e2ches de calcul et de traitement d'image sur le silicium personnalis\u00e9 d'Apple.</p> <p>Pour activer l'entra\u00eenement sur les puces Apple M1 et M2, vous devez sp\u00e9cifier 'mps' comme votre dispositif lors du lancement du processus d'entra\u00eenement. Voici un exemple de la mani\u00e8re dont vous pourriez le faire en Python et via la ligne de commande :</p> <p>Exemple d'entra\u00eenement MPS</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\n\n# Entra\u00eener le mod\u00e8le avec MPS\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640, device='mps')\n</code></pre> <pre><code># Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le pr\u00e9entra\u00een\u00e9 *.pt avec MPS\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=mps\n</code></pre> <p>Tout en exploitant la puissance de calcul des puces M1/M2, cela permet un traitement plus efficace des t\u00e2ches d'entra\u00eenement. Pour des conseils plus d\u00e9taill\u00e9s et des options de configuration avanc\u00e9e, veuillez consulter la documentation MPS de PyTorch.</p>"},{"location":"modes/train/#journalisation","title":"Journalisation","text":"<p>Lors de l'entra\u00eenement d'un mod\u00e8le YOLOv8, il peut \u00eatre pr\u00e9cieux de suivre la performance du mod\u00e8le au fil du temps. C'est l\u00e0 que la journalisation entre en jeu. YOLO d'Ultralytics prend en charge trois types de journaux - Comet, ClearML et TensorBoard.</p> <p>Pour utiliser un journal, s\u00e9lectionnez-le dans le menu d\u00e9roulant ci-dessus et ex\u00e9cutez-le. Le journal choisi sera install\u00e9 et initialis\u00e9.</p>"},{"location":"modes/train/#comet","title":"Comet","text":"<p>Comet est une plateforme qui permet aux scientifiques de donn\u00e9es et aux d\u00e9veloppeurs de suivre, comparer, expliquer et optimiser les exp\u00e9riences et les mod\u00e8les. Elle offre des fonctionnalit\u00e9s telles que le suivi en temps r\u00e9el des mesures, les diff\u00e9rences de code et le suivi des hyperparam\u00e8tres.</p> <p>Pour utiliser Comet :</p> <p>Exemple</p> Python <pre><code># pip install comet_ml\nimport comet_ml\n\ncomet_ml.init()\n</code></pre> <p>N'oubliez pas de vous connecter \u00e0 votre compte Comet sur leur site web et d'obtenir votre cl\u00e9 API. Vous devrez ajouter cela \u00e0 vos variables d'environnement ou \u00e0 votre script pour enregistrer vos exp\u00e9riences.</p>"},{"location":"modes/train/#clearml","title":"ClearML","text":"<p>ClearML est une plateforme open source qui automatise le suivi des exp\u00e9riences et aide \u00e0 partager efficacement les ressources. Elle est con\u00e7ue pour aider les \u00e9quipes \u00e0 g\u00e9rer, ex\u00e9cuter et reproduire leur travail en ML plus efficacement.</p> <p>Pour utiliser ClearML :</p> <p>Exemple</p> Python <pre><code># pip install clearml\nimport clearml\n\nclearml.browser_login()\n</code></pre> <p>Apr\u00e8s avoir ex\u00e9cut\u00e9 ce script, vous devrez vous connecter \u00e0 votre compte ClearML sur le navigateur et authentifier votre session.</p>"},{"location":"modes/train/#tensorboard","title":"TensorBoard","text":"<p>TensorBoard est un ensemble d'outils de visualisation pour TensorFlow. Il vous permet de visualiser votre graphique TensorFlow, de tracer des mesures quantitatives sur l'ex\u00e9cution de votre graphique et de montrer des donn\u00e9es suppl\u00e9mentaires comme des images qui le traversent.</p> <p>Pour utiliser TensorBoard dans Google Colab :</p> <p>Exemple</p> CLI <pre><code>load_ext tensorboard\ntensorboard --logdir ultralytics/runs  # remplacer par le r\u00e9pertoire 'runs'\n</code></pre> <p>Pour utiliser TensorBoard localement, ex\u00e9cutez la commande ci-dessous et consultez les r\u00e9sultats \u00e0 l'adresse http://localhost:6006/.</p> <p>Exemple</p> CLI <pre><code>tensorboard --logdir ultralytics/runs  # remplacer par le r\u00e9pertoire 'runs'\n</code></pre> <p>Cela chargera TensorBoard et le dirigera vers le r\u00e9pertoire o\u00f9 vos journaux d'entra\u00eenement sont sauvegard\u00e9s.</p> <p>Apr\u00e8s avoir configur\u00e9 votre journal, vous pouvez ensuite poursuivre l'entra\u00eenement de votre mod\u00e8le. Toutes les m\u00e9triques d'entra\u00eenement seront automatiquement enregistr\u00e9es sur votre plateforme choisie, et vous pourrez acc\u00e9der \u00e0 ces journaux pour surveiller les performances de votre mod\u00e8le au fil du temps, comparer diff\u00e9rents mod\u00e8les et identifier les domaines d'am\u00e9lioration.</p>"},{"location":"modes/val/","title":"Validation des mod\u00e8les avec Ultralytics YOLO","text":""},{"location":"modes/val/#introduction","title":"Introduction","text":"<p>La validation est une \u00e9tape cruciale dans le pipeline d'apprentissage automatique, vous permettant d'\u00e9valuer la qualit\u00e9 de vos mod\u00e8les entra\u00een\u00e9s. Le mode Val dans Ultralytics YOLOv8 offre une gamme robuste d'outils et de m\u00e9triques pour \u00e9valuer la performance de vos mod\u00e8les de d\u00e9tection d'objets. Ce guide sert de ressource compl\u00e8te pour comprendre comment utiliser efficacement le mode Val pour assurer que vos mod\u00e8les sont \u00e0 la fois pr\u00e9cis et fiables.</p>"},{"location":"modes/val/#pourquoi-valider-avec-ultralytics-yolo","title":"Pourquoi valider avec Ultralytics YOLO ?","text":"<p>Voici pourquoi l'utilisation du mode Val de YOLOv8 est avantageuse :</p> <ul> <li>Pr\u00e9cision : Obtenez des m\u00e9triques pr\u00e9cises telles que mAP50, mAP75 et mAP50-95 pour \u00e9valuer de mani\u00e8re exhaustive votre mod\u00e8le.</li> <li>Convenance : Utilisez des fonctionnalit\u00e9s int\u00e9gr\u00e9es qui se souviennent des param\u00e8tres d'entra\u00eenement, simplifiant ainsi le processus de validation.</li> <li>Flexibilit\u00e9 : Validez votre mod\u00e8le avec les m\u00eames jeux de donn\u00e9es ou des jeux diff\u00e9rents et des tailles d'image vari\u00e9es.</li> <li>R\u00e9glage des hyperparam\u00e8tres : Utilisez les m\u00e9triques de validation pour peaufiner votre mod\u00e8le pour de meilleures performances.</li> </ul>"},{"location":"modes/val/#caracteristiques-cles-du-mode-val","title":"Caract\u00e9ristiques cl\u00e9s du mode Val","text":"<p>Voici les fonctionnalit\u00e9s notables offertes par le mode Val de YOLOv8 :</p> <ul> <li>Param\u00e8tres Automatis\u00e9s : Les mod\u00e8les se souviennent de leurs configurations d'entra\u00eenement pour une validation simple.</li> <li>Support Multi-m\u00e9trique : \u00c9valuez votre mod\u00e8le en fonction d'une gamme de m\u00e9triques de pr\u00e9cision.</li> <li>CLI et API Python : Choisissez entre l'interface en ligne de commande ou l'API Python en fonction de vos pr\u00e9f\u00e9rences pour la validation.</li> <li>Compatibilit\u00e9 des Donn\u00e9es : Fonctionne de mani\u00e8re transparente avec les jeux de donn\u00e9es utilis\u00e9s pendant la phase d'entra\u00eenement ainsi qu'avec les jeux personnalis\u00e9s.</li> </ul> <p>Conseil</p> <ul> <li>Les mod\u00e8les YOLOv8 se souviennent automatiquement de leurs param\u00e8tres d'entra\u00eenement, vous pouvez donc facilement valider un mod\u00e8le \u00e0 la m\u00eame taille d'image et sur le jeu de donn\u00e9es original avec juste <code>yolo val model=yolov8n.pt</code> ou <code>model('yolov8n.pt').val()</code></li> </ul>"},{"location":"modes/val/#exemples-dutilisation","title":"Exemples d'utilisation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128. Aucun argument n'a besoin d'\u00eatre pass\u00e9 car le <code>mod\u00e8le</code> conserve ses <code>donn\u00e9es</code> d'entra\u00eenement et arguments comme attributs du mod\u00e8le. Consultez la section des arguments ci-dessous pour une liste compl\u00e8te des arguments d'exportation.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # pas besoin d'arguments, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # val mod\u00e8le officiel\nyolo detect val model=chemin/vers/meilleur.pt  # val mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"modes/val/#arguments","title":"Arguments","text":"<p>Les param\u00e8tres de validation pour les mod\u00e8les YOLO font r\u00e9f\u00e9rence aux divers hyperparam\u00e8tres et configurations utilis\u00e9s pour \u00e9valuer la performance du mod\u00e8le sur un jeu de donn\u00e9es de validation. Ces param\u00e8tres peuvent affecter la performance, la vitesse et la pr\u00e9cision du mod\u00e8le. Certains param\u00e8tres de validation YOLO courants incluent la taille du lot, la fr\u00e9quence \u00e0 laquelle la validation est effectu\u00e9e pendant l'entra\u00eenement et les m\u00e9triques utilis\u00e9es pour \u00e9valuer la performance du mod\u00e8le. D'autres facteurs pouvant affecter le processus de validation incluent la taille et la composition du jeu de donn\u00e9es de validation et la t\u00e2che sp\u00e9cifique pour laquelle le mod\u00e8le est utilis\u00e9. Il est important de r\u00e9gler et d'exp\u00e9rimenter soigneusement ces param\u00e8tres pour s'assurer que le mod\u00e8le fonctionne bien sur le jeu de donn\u00e9es de validation et pour d\u00e9tecter et pr\u00e9venir le surajustement.</p> Cl\u00e9 Valeur Description <code>data</code> <code>None</code> chemin vers le fichier de donn\u00e9es, par exemple coco128.yaml <code>imgsz</code> <code>640</code> taille des images d'entr\u00e9e en tant qu'entier <code>batch</code> <code>16</code> nombre d'images par lot (-1 pour AutoBatch) <code>save_json</code> <code>False</code> sauvegarder les r\u00e9sultats dans un fichier JSON <code>save_hybrid</code> <code>False</code> sauvegarder la version hybride des \u00e9tiquettes (\u00e9tiquettes + pr\u00e9dictions suppl\u00e9mentaires) <code>conf</code> <code>0.001</code> seuil de confiance de l'objet pour la d\u00e9tection <code>iou</code> <code>0.6</code> seuil d'intersection sur union (IoU) pour la NMS <code>max_det</code> <code>300</code> nombre maximum de d\u00e9tections par image <code>half</code> <code>True</code> utiliser la pr\u00e9cision moiti\u00e9 (FP16) <code>device</code> <code>None</code> appareil sur lequel ex\u00e9cuter, par exemple cuda device=0/1/2/3 ou device=cpu <code>dnn</code> <code>False</code> utiliser OpenCV DNN pour l'inf\u00e9rence ONNX <code>plots</code> <code>False</code> afficher les graphiques lors de la formation <code>rect</code> <code>False</code> val rectangulaire avec chaque lot regroup\u00e9 pour un minimum de rembourrage <code>split</code> <code>val</code> fraction du jeu de donn\u00e9es \u00e0 utiliser pour la validation, par exemple 'val', 'test' ou 'train'"},{"location":"tasks/","title":"T\u00e2ches d'Ultralytics YOLOv8","text":"<p>YOLOv8 est un cadre d'intelligence artificielle qui prend en charge de multiples t\u00e2ches de vision par ordinateur. Le cadre peut \u00eatre utilis\u00e9 pour effectuer de la d\u00e9tection, de la segmentation, de la classification et de l'estimation de la pose. Chacune de ces t\u00e2ches a un objectif et un cas d'utilisation diff\u00e9rents.</p> <p>Note</p> <p>\ud83d\udea7 Notre documentation multilingue est actuellement en construction et nous travaillons dur pour l'am\u00e9liorer. Merci de votre patience ! \ud83d\ude4f</p> <p> Regardez : Explorez les T\u00e2ches YOLO Ultralytics : D\u00e9tection d'Objets, Segmentation, Suivi et Estimation de la Pose. </p>"},{"location":"tasks/#detection","title":"D\u00e9tection","text":"<p>La d\u00e9tection est la t\u00e2che principale prise en charge par YOLOv8. Elle implique de d\u00e9tecter des objets dans une image ou une trame vid\u00e9o et de dessiner des bo\u00eetes englobantes autour d'eux. Les objets d\u00e9tect\u00e9s sont class\u00e9s dans diff\u00e9rentes cat\u00e9gories en fonction de leurs caract\u00e9ristiques. YOLOv8 peut d\u00e9tecter plusieurs objets dans une seule image ou trame vid\u00e9o avec une grande pr\u00e9cision et rapidit\u00e9.</p> <p>Exemples de D\u00e9tection</p>"},{"location":"tasks/#segmentation","title":"Segmentation","text":"<p>La segmentation est une t\u00e2che qui implique de segmenter une image en diff\u00e9rentes r\u00e9gions en fonction du contenu de l'image. Chaque r\u00e9gion se voit attribuer une \u00e9tiquette en fonction de son contenu. Cette t\u00e2che est utile dans des applications telles que la segmentation d'image et l'imagerie m\u00e9dicale. YOLOv8 utilise une variante de l'architecture U-Net pour effectuer la segmentation.</p> <p>Exemples de Segmentation</p>"},{"location":"tasks/#classification","title":"Classification","text":"<p>La classification est une t\u00e2che qui implique de classer une image dans diff\u00e9rentes cat\u00e9gories. YOLOv8 peut \u00eatre utilis\u00e9 pour classifier des images en fonction de leur contenu. Il utilise une variante de l'architecture EfficientNet pour effectuer la classification.</p> <p>Exemples de Classification</p>"},{"location":"tasks/#pose","title":"Pose","text":"<p>La d\u00e9tection de pose/points cl\u00e9s est une t\u00e2che qui implique de d\u00e9tecter des points sp\u00e9cifiques dans une image ou une trame vid\u00e9o. Ces points sont appel\u00e9s points cl\u00e9s et sont utilis\u00e9s pour suivre le mouvement ou pour l'estimation de la pose. YOLOv8 peut d\u00e9tecter des points cl\u00e9s dans une image ou une trame vid\u00e9o avec une grande pr\u00e9cision et rapidit\u00e9.</p> <p>Exemples de Pose</p>"},{"location":"tasks/#conclusion","title":"Conclusion","text":"<p>YOLOv8 prend en charge de multiples t\u00e2ches, y compris la d\u00e9tection, la segmentation, la classification et la d\u00e9tection de points cl\u00e9s. Chacune de ces t\u00e2ches a des objectifs et des cas d'utilisation diff\u00e9rents. En comprenant les diff\u00e9rences entre ces t\u00e2ches, vous pouvez choisir la t\u00e2che appropri\u00e9e pour votre application de vision par ordinateur.</p>"},{"location":"tasks/classify/","title":"Classification d'images","text":"<p>La classification d'images est la t\u00e2che la plus simple des trois et consiste \u00e0 classer une image enti\u00e8re dans l'une d'un ensemble de classes pr\u00e9d\u00e9finies.</p> <p>Le r\u00e9sultat d'un classificateur d'images est une \u00e9tiquette de classe unique et un score de confiance. La classification d'images est utile lorsque vous avez besoin de savoir seulement \u00e0 quelle classe appartient une image et que vous n'avez pas besoin de conna\u00eetre l'emplacement des objets de cette classe ou leur forme exacte.</p> <p>Astuce</p> <p>Les mod\u00e8les YOLOv8 Classify utilisent le suffixe <code>-cls</code>, par exemple <code>yolov8n-cls.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur ImageNet.</p>"},{"location":"tasks/classify/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Classify pr\u00e9-entra\u00een\u00e9s YOLOv8 sont pr\u00e9sent\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le dataset COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le dataset ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement depuis la derni\u00e8re version Ultralytics release lors de la premi\u00e8re utilisation.</p> Mod\u00e8le taille<sup>(pixels) acc<sup>top1 acc<sup>top5 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) \u00e0 640 YOLOv8n-cls 224 66.6 87.0 12.9 0.31 2.7 4.3 YOLOv8s-cls 224 72.3 91.1 23.4 0.35 6.4 13.5 YOLOv8m-cls 224 76.4 93.2 85.4 0.62 17.0 42.7 YOLOv8l-cls 224 78.0 94.1 163.0 0.87 37.5 99.7 YOLOv8x-cls 224 78.4 94.3 232.0 1.01 57.4 154.8 <ul> <li>Les valeurs acc sont les pr\u00e9cisions des mod\u00e8les sur le jeu de donn\u00e9es de validation d'ImageNet.   Pour reproduire : <code>yolo val classify data=path/to/ImageNet device=0</code></li> <li>Les vitesses sont calcul\u00e9es sur les images de validation d'ImageNet \u00e0 l'aide d'une instance Amazon EC2 P4d.   Pour reproduire : <code>yolo val classify data=path/to/ImageNet batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/classify/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez le mod\u00e8le YOLOv8n-cls sur le dataset MNIST160 pendant 100 \u00e9poques avec une taille d'image de 64. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='mnist160', epochs=100, imgsz=64)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\n</code></pre>"},{"location":"tasks/classify/#format-du-dataset","title":"Format du dataset","text":"<p>Le format du dataset de classification YOLO peut \u00eatre trouv\u00e9 en d\u00e9tails dans le Guide des Datasets.</p>"},{"location":"tasks/classify/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-cls entra\u00een\u00e9 sur le dataset MNIST160. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # aucun argument n\u00e9cessaire, les donn\u00e9es et les param\u00e8tres sont m\u00e9moris\u00e9s\nmetrics.top1   # pr\u00e9cision top 1\nmetrics.top5   # pr\u00e9cision top 5\n</code></pre> <pre><code>yolo classify val model=yolov8n-cls.pt  # valider le mod\u00e8le officiel\nyolo classify val model=path/to/best.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/classify/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-cls entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nresults = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9diction avec le mod\u00e8le officiel\nyolo classify predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9diction avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Voir les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/classify/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n-cls dans un format diff\u00e9rent comme ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-cls.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('path/to/best.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-cls.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=path/to/best.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation disponibles pour YOLOv8-cls sont pr\u00e9sent\u00e9s dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur les mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-cls.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le une fois l'exportation termin\u00e9e.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-cls.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-cls.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-cls.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-cls_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-cls.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-cls.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-cls_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-cls.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-cls.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-cls_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-cls_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-cls_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-cls_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Voir les d\u00e9tails complets de l'<code>exportation</code> sur la page Export.</p>"},{"location":"tasks/detect/","title":"D\u00e9tection d'Objets","text":"<p>La d\u00e9tection d'objets est une t\u00e2che qui implique l'identification de l'emplacement et de la classe des objets dans une image ou un flux vid\u00e9o.</p> <p>La sortie d'un d\u00e9tecteur d'objets est un ensemble de bo\u00eetes englobantes qui entourent les objets de l'image, accompagn\u00e9es de libell\u00e9s de classe et de scores de confiance pour chaque bo\u00eete. La d\u00e9tection d'objets est un bon choix lorsque vous avez besoin d'identifier des objets d'int\u00e9r\u00eat dans une sc\u00e8ne, mais que vous n'avez pas besoin de conna\u00eetre exactement o\u00f9 se trouve l'objet ou sa forme exacte.</p> <p> Regardez : D\u00e9tection d'Objets avec le Mod\u00e8le Pr\u00e9-entra\u00een\u00e9 Ultralytics YOLOv8. </p> <p>Conseil</p> <p>Les mod\u00e8les Detect YOLOv8 sont les mod\u00e8les YOLOv8 par d\u00e9faut, c.-\u00e0-d. <code>yolov8n.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO.</p>"},{"location":"tasks/detect/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s Detect YOLOv8 sont pr\u00e9sent\u00e9s ici. Les mod\u00e8les Detect, Segment, et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement \u00e0 partir de la derni\u00e8re version d'Ultralytics lors de la premi\u00e8re utilisation.</p> Mod\u00e8le Taille<sup>(pixels) mAP<sup>val50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) Param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 <ul> <li>Les valeurs de mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO val2017.   Reproductible avec <code>yolo val detect data=coco.yaml device=0</code></li> <li>La Vitesse est moyenn\u00e9e sur les images COCO val en utilisant une instance Amazon EC2 P4d.   Reproductible avec <code>yolo val detect data=coco128.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/detect/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez le mod\u00e8le YOLOv8n sur le jeu de donn\u00e9es COCO128 pendant 100 \u00e9poques \u00e0 la taille d'image de 640. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir de YAML\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n.yaml').load('yolov8n.pt')  # construire \u00e0 partir de YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nresults = model.train(data='coco128.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir de YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir de YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/detect/#format-des-donnees","title":"Format des donn\u00e9es","text":"<p>Le format des jeux de donn\u00e9es de d\u00e9tection YOLO est d\u00e9taill\u00e9 dans le Guide des Jeux de Donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant depuis d'autres formats (comme COCO, etc.) vers le format YOLO, veuillez utiliser l'outil JSON2YOLO par Ultralytics.</p>"},{"location":"tasks/detect/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128. Aucun argument n'est n\u00e9cessaire puisque le <code>mod\u00e8le</code> conserve ses <code>donn\u00e9es</code> d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nmetrics = model.val()  # pas d'arguments n\u00e9cessaires, jeu de donn\u00e9es et param\u00e8tres enregistr\u00e9s\nmetrics.box.map    # map50-95\nmetrics.box.map50  # map50\nmetrics.box.map75  # map75\nmetrics.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo detect val model=yolov8n.pt  # valider le mod\u00e8le officiel\nyolo detect val model=chemin/vers/best.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/detect/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nresults = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le officiel\nyolo detect predict model=chemin/vers/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Consultez les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/detect/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n dans un format diff\u00e9rent tel que ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=chemin/vers/best.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8 disponibles sont pr\u00e9sent\u00e9s dans le tableau ci-dessous. Vous pouvez directement pr\u00e9dire ou valider sur des mod\u00e8les export\u00e9s, c'est-\u00e0-dire <code>yolo predict model=yolov8n.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le apr\u00e8s l'exportation compl\u00e8te.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> Mod\u00e8le TF Enregistr\u00e9 <code>saved_model</code> <code>yolov8n_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> GraphDef TF <code>pb</code> <code>yolov8n.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TPU Edge TF <code>edgetpu</code> <code>yolov8n_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez tous les d\u00e9tails <code>export</code> sur la page Exporter.</p>"},{"location":"tasks/pose/","title":"Estimation de Pose","text":"<p>L'estimation de pose est une t\u00e2che qui consiste \u00e0 identifier l'emplacement de points sp\u00e9cifiques dans une image, souvent appel\u00e9s points cl\u00e9s. Ces points cl\u00e9s peuvent repr\u00e9senter diff\u00e9rentes parties de l'objet telles que les articulations, les rep\u00e8res ou d'autres caract\u00e9ristiques distinctives. L'emplacement des points cl\u00e9s est g\u00e9n\u00e9ralement repr\u00e9sent\u00e9 par un ensemble de coordonn\u00e9es 2D <code>[x, y]</code> ou 3D <code>[x, y, visible]</code>.</p> <p>La sortie d'un mod\u00e8le d'estimation de pose est un ensemble de points repr\u00e9sentant les points cl\u00e9s sur un objet dans l'image, g\u00e9n\u00e9ralement accompagn\u00e9s des scores de confiance pour chaque point. L'estimation de pose est un bon choix lorsque vous avez besoin d'identifier des parties sp\u00e9cifiques d'un objet dans une sc\u00e8ne, et leur emplacement les uns par rapport aux autres.</p> <p></p> <p>Conseil</p> <p>Les mod\u00e8les YOLOv8 pose utilisent le suffixe <code>-pose</code>, c'est-\u00e0-dire <code>yolov8n-pose.pt</code>. Ces mod\u00e8les sont entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO keypoints et conviennent \u00e0 une vari\u00e9t\u00e9 de t\u00e2ches d'estimation de pose.</p>"},{"location":"tasks/pose/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Pose pr\u00e9-entra\u00een\u00e9s YOLOv8 sont montr\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les Mod\u00e8les se t\u00e9l\u00e9chargent automatiquement \u00e0 partir de la derni\u00e8re version d'Ultralytics release lors de la premi\u00e8re utilisation.</p> Mod\u00e8le taille<sup>(pixels) mAP<sup>pose50-95 mAP<sup>pose50 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) params<sup>(M) FLOPs<sup>(B) YOLOv8n-pose 640 50.4 80.1 131.8 1.18 3.3 9.2 YOLOv8s-pose 640 60.0 86.2 233.2 1.42 11.6 30.2 YOLOv8m-pose 640 65.0 88.8 456.3 2.00 26.4 81.0 YOLOv8l-pose 640 67.6 90.0 784.5 2.59 44.4 168.6 YOLOv8x-pose 640 69.2 90.2 1607.1 3.73 69.4 263.2 YOLOv8x-pose-p6 1280 71.6 91.2 4088.7 10.04 99.1 1066.4 <ul> <li>Les valeurs de mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO Keypoints val2017.   Reproduire avec <code>yolo val pose data=coco-pose.yaml device=0</code></li> <li>La vitesse moyenne sur les images de validation COCO en utilisant une instance Amazon EC2 P4d.   Reproduire avec <code>yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/pose/#entrainement","title":"Entra\u00eenement","text":"<p>Entra\u00eenez un mod\u00e8le YOLOv8-pose sur le jeu de donn\u00e9es COCO128-pose.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-pose.pt')    # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour l'entra\u00eenement)\nmodel = YOLO('yolov8n-pose.yaml').load('yolov8n-pose.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nr\u00e9sultats = model.train(data='coco8-pose.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer l'entra\u00eenement \u00e0 partir de z\u00e9ro\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\n\n# Commencer l'entra\u00eenement \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer l'entra\u00eenement\nyolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/pose/#format-du-jeu-de-donnees","title":"Format du jeu de donn\u00e9es","text":"<p>Le format du jeu de donn\u00e9es YOLO pose peut \u00eatre trouv\u00e9 en d\u00e9tail dans le Guide des jeux de donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant \u00e0 partir d'autres formats (comme COCO, etc.) vers le format YOLO, veuillez utiliser l'outil JSON2YOLO d'Ultralytics.</p>"},{"location":"tasks/pose/#val","title":"Val","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-pose entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128-pose. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es d'entra\u00eenement et arguments en tant qu'attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')     # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nm\u00e9triques = model.val()  # aucun argument n\u00e9cessaire, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nm\u00e9triques.box.map    # map50-95\nm\u00e9triques.box.map50  # map50\nm\u00e9triques.box.map75  # map75\nm\u00e9triques.box.maps   # une liste contenant map50-95 de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo pose val model=yolov8n-pose.pt  # val mod\u00e8le officiel\nyolo pose val model=chemin/vers/best.pt  # val mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/pose/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-pose entra\u00een\u00e9 pour ex\u00e9cuter des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')     # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec mod\u00e8le officiel\nyolo pose predict model=chemin/vers/best.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec mod\u00e8le personnalis\u00e9\n</code></pre> <p>Consultez les d\u00e9tails complets du mode <code>predict</code> sur la page Pr\u00e9dire.</p>"},{"location":"tasks/pose/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n Pose dans un autre format tel que ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-pose.pt')      # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/best.pt')   # charger un mod\u00e8le personnalis\u00e9 entra\u00een\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-pose.pt format=onnx  # exporter mod\u00e8le officiel\nyolo export model=chemin/vers/best.pt format=onnx  # exporter mod\u00e8le personnalis\u00e9 entra\u00een\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8-pose disponibles sont dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur des mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-pose.onnx</code>. Des exemples d'utilisation sont montr\u00e9s pour votre mod\u00e8le apr\u00e8s la fin de l'exportation.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-pose.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-pose.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-pose.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-pose_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-pose.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-pose.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-pose_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-pose.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-pose.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-pose_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-pose_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-pose_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-pose_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Consultez les d\u00e9tails complets de <code>export</code> sur la page Exporter.</p>"},{"location":"tasks/segment/","title":"Segmentation d'Instance","text":"<p>La segmentation d'instance va plus loin que la d\u00e9tection d'objet et implique d'identifier des objets individuels dans une image et de les segmenter du reste de l'image.</p> <p>Le r\u00e9sultat d'un mod\u00e8le de segmentation d'instance est un ensemble de masques ou de contours qui d\u00e9limitent chaque objet dans l'image, accompagn\u00e9s d'\u00e9tiquettes de classe et de scores de confiance pour chaque objet. La segmentation d'instance est utile lorsque vous avez besoin de savoir non seulement o\u00f9 se trouvent les objets dans une image, mais aussi quelle est leur forme exacte.</p> <p> Regarder : Ex\u00e9cutez la Segmentation avec le Mod\u00e8le Ultralytics YOLOv8 Pr\u00e9-Entra\u00een\u00e9 en Python. </p> <p>Astuce</p> <p>Les mod\u00e8les YOLOv8 Segment utilisent le suffixe <code>-seg</code>, par exemple <code>yolov8n-seg.pt</code> et sont pr\u00e9-entra\u00een\u00e9s sur COCO.</p>"},{"location":"tasks/segment/#modeles","title":"Mod\u00e8les","text":"<p>Les mod\u00e8les Segment pr\u00e9-entra\u00een\u00e9s YOLOv8 sont indiqu\u00e9s ici. Les mod\u00e8les Detect, Segment et Pose sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es COCO, tandis que les mod\u00e8les Classify sont pr\u00e9-entra\u00een\u00e9s sur le jeu de donn\u00e9es ImageNet.</p> <p>Les mod\u00e8les se t\u00e9l\u00e9chargent automatiquement depuis la derni\u00e8re version Ultralytics lors de la premi\u00e8re utilisation.</p> Mod\u00e8le Taille<sup>(pixels) mAP<sup>bo\u00eete50-95 mAP<sup>masque50-95 Vitesse<sup>CPU ONNX(ms) Vitesse<sup>A100 TensorRT(ms) Param\u00e8tres<sup>(M) FLOPs<sup>(B) YOLOv8n-seg 640 36.7 30.5 96.1 1.21 3.4 12.6 YOLOv8s-seg 640 44.6 36.8 155.7 1.47 11.8 42.6 YOLOv8m-seg 640 49.9 40.8 317.0 2.18 27.3 110.2 YOLOv8l-seg 640 52.3 42.6 572.4 2.79 46.0 220.5 YOLOv8x-seg 640 53.4 43.4 712.1 4.02 71.8 344.1 <ul> <li>Les valeurs mAP<sup>val</sup> sont pour un seul mod\u00e8le \u00e0 une seule \u00e9chelle sur le jeu de donn\u00e9es COCO val2017.   Pour reproduire, utilisez <code>yolo val segment data=coco.yaml device=0</code></li> <li>Vitesse moyenn\u00e9e sur les images COCO val en utilisant une instance Amazon EC2 P4d.   Pour reproduire, utilisez <code>yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu</code></li> </ul>"},{"location":"tasks/segment/#formation","title":"Formation","text":"<p>Entra\u00eenez YOLOv8n-seg sur le jeu de donn\u00e9es COCO128-seg pendant 100 \u00e9poques \u00e0 la taille d'image 640. Pour une liste compl\u00e8te des arguments disponibles, consultez la page Configuration.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.yaml')  # construire un nouveau mod\u00e8le \u00e0 partir du YAML\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (recommand\u00e9 pour la formation)\nmodel = YOLO('yolov8n-seg.yaml').load('yolov8n.pt')  # construire \u00e0 partir du YAML et transf\u00e9rer les poids\n\n# Entra\u00eener le mod\u00e8le\nr\u00e9sultats = model.train(data='coco128-seg.yaml', epochs=100, imgsz=640)\n</code></pre> <pre><code># Construire un nouveau mod\u00e8le \u00e0 partir du YAML et commencer la formation \u00e0 partir de z\u00e9ro\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\n\n# Commencer la formation \u00e0 partir d'un mod\u00e8le *.pt pr\u00e9-entra\u00een\u00e9\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\n\n# Construire un nouveau mod\u00e8le \u00e0 partir du YAML, transf\u00e9rer les poids pr\u00e9-entra\u00een\u00e9s et commencer la formation\nyolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tasks/segment/#format-des-donnees","title":"Format des donn\u00e9es","text":"<p>Le format des donn\u00e9es de segmentation YOLO peut \u00eatre trouv\u00e9 en d\u00e9tail dans le Guide du Jeu de Donn\u00e9es. Pour convertir votre jeu de donn\u00e9es existant \u00e0 partir d'autres formats (comme COCO, etc.) au format YOLO, veuillez utiliser l'outil JSON2YOLO par Ultralytics.</p>"},{"location":"tasks/segment/#validation","title":"Validation","text":"<p>Validez la pr\u00e9cision du mod\u00e8le YOLOv8n-seg entra\u00een\u00e9 sur le jeu de donn\u00e9es COCO128-seg. Aucun argument n'est n\u00e9cessaire car le <code>mod\u00e8le</code> conserve ses donn\u00e9es de formation et ses arguments comme attributs du mod\u00e8le.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Valider le mod\u00e8le\nm\u00e9triques = model.val()  # aucun argument n\u00e9cessaire, jeu de donn\u00e9es et param\u00e8tres m\u00e9moris\u00e9s\nm\u00e9triques.box.map    # map50-95(B)\nm\u00e9triques.box.map50  # map50(B)\nm\u00e9triques.box.map75  # map75(B)\nm\u00e9triques.box.maps   # une liste contient map50-95(B) de chaque cat\u00e9gorie\nm\u00e9triques.seg.map    # map50-95(M)\nm\u00e9triques.seg.map50  # map50(M)\nm\u00e9triques.seg.map75  # map75(M)\nm\u00e9triques.seg.maps   # une liste contient map50-95(M) de chaque cat\u00e9gorie\n</code></pre> <pre><code>yolo segment val model=yolov8n-seg.pt  # valider le mod\u00e8le officiel\nyolo segment val model=chemin/vers/le/meilleur.pt  # valider le mod\u00e8le personnalis\u00e9\n</code></pre>"},{"location":"tasks/segment/#prediction","title":"Pr\u00e9diction","text":"<p>Utilisez un mod\u00e8le YOLOv8n-seg entra\u00een\u00e9 pour effectuer des pr\u00e9dictions sur des images.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le personnalis\u00e9\n\n# Pr\u00e9dire avec le mod\u00e8le\nr\u00e9sultats = model('https://ultralytics.com/images/bus.jpg')  # pr\u00e9dire sur une image\n</code></pre> <pre><code>yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le officiel\nyolo segment predict model=chemin/vers/le/meilleur.pt source='https://ultralytics.com/images/bus.jpg'  # pr\u00e9dire avec le mod\u00e8le personnalis\u00e9\n</code></pre> <p>Voir les d\u00e9tails complets du mode <code>predict</code> sur la page Predict.</p>"},{"location":"tasks/segment/#exportation","title":"Exportation","text":"<p>Exportez un mod\u00e8le YOLOv8n-seg vers un format diff\u00e9rent comme ONNX, CoreML, etc.</p> <p>Exemple</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Charger un mod\u00e8le\nmodel = YOLO('yolov8n-seg.pt')  # charger un mod\u00e8le officiel\nmodel = YOLO('chemin/vers/le/meilleur.pt')  # charger un mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n\n# Exporter le mod\u00e8le\nmodel.export(format='onnx')\n</code></pre> <pre><code>yolo export model=yolov8n-seg.pt format=onnx  # exporter le mod\u00e8le officiel\nyolo export model=chemin/vers/le/meilleur.pt format=onnx  # exporter le mod\u00e8le entra\u00een\u00e9 personnalis\u00e9\n</code></pre> <p>Les formats d'exportation YOLOv8-seg disponibles sont dans le tableau ci-dessous. Vous pouvez pr\u00e9dire ou valider directement sur les mod\u00e8les export\u00e9s, par exemple <code>yolo predict model=yolov8n-seg.onnx</code>. Des exemples d'utilisation sont pr\u00e9sent\u00e9s pour votre mod\u00e8le apr\u00e8s l'exportation.</p> Format Argument <code>format</code> Mod\u00e8le M\u00e9tadonn\u00e9es Arguments PyTorch - <code>yolov8n-seg.pt</code> \u2705 - TorchScript <code>torchscript</code> <code>yolov8n-seg.torchscript</code> \u2705 <code>imgsz</code>, <code>optimize</code> ONNX <code>onnx</code> <code>yolov8n-seg.onnx</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>opset</code> OpenVINO <code>openvino</code> <code>yolov8n-seg_openvino_model/</code> \u2705 <code>imgsz</code>, <code>half</code> TensorRT <code>engine</code> <code>yolov8n-seg.engine</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>dynamic</code>, <code>simplify</code>, <code>workspace</code> CoreML <code>coreml</code> <code>yolov8n-seg.mlpackage</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code>, <code>nms</code> TF SavedModel <code>saved_model</code> <code>yolov8n-seg_saved_model/</code> \u2705 <code>imgsz</code>, <code>keras</code> TF GraphDef <code>pb</code> <code>yolov8n-seg.pb</code> \u274c <code>imgsz</code> TF Lite <code>tflite</code> <code>yolov8n-seg.tflite</code> \u2705 <code>imgsz</code>, <code>half</code>, <code>int8</code> TF Edge TPU <code>edgetpu</code> <code>yolov8n-seg_edgetpu.tflite</code> \u2705 <code>imgsz</code> TF.js <code>tfjs</code> <code>yolov8n-seg_web_model/</code> \u2705 <code>imgsz</code> PaddlePaddle <code>paddle</code> <code>yolov8n-seg_paddle_model/</code> \u2705 <code>imgsz</code> ncnn <code>ncnn</code> <code>yolov8n-seg_ncnn_model/</code> \u2705 <code>imgsz</code>, <code>half</code> <p>Voir les d\u00e9tails complets d'<code>export</code> sur la page Export.</p>"}]}